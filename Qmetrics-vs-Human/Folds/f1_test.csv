paper_id,reviewer,title,abstract,review_text,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,politeness_score,venue,hedging,Human_Actionability,Human_Clarity_and_Readability,Human_Comprehensiveness,Human_Constructiveness,Human_Factuality,Human_Fairness,Human_Objectivity,Human_Overall_Quality,Human_Politeness,Human_Relevance_Alignment,Human_Sentiment_Polarity,Human_Usage_of_Technical_Terms,Human_Vagueness,Qwen_Actionability,Qwen_Clarity_and_Readability,Qwen_Comprehensiveness,Qwen_Constructiveness,Qwen_Factuality,Qwen_Fairness,Qwen_Objectivity,Qwen_Overall_Quality,Qwen_Politeness,Qwen_Relevance_Alignment,Qwen_Sentiment_Polarity,Qwen_Usage_of_Technical_Terms,Qwen_Vagueness,Llama_Actionability,Llama_Clarity_and_Readability,Llama_Comprehensiveness,Llama_Constructiveness,Llama_Factuality,Llama_Fairness,Llama_Objectivity,Llama_Overall_Quality,Llama_Politeness,Llama_Relevance_Alignment,Llama_Sentiment_Polarity,Llama_Usage_of_Technical_Terms,Llama_Vagueness,GPT_Actionability,GPT_Clarity_and_Readability,GPT_Comprehensiveness,GPT_Constructiveness,GPT_Factuality,GPT_Fairness,GPT_Objectivity,GPT_Overall_Quality,GPT_Politeness,GPT_Relevance_Alignment,GPT_Sentiment_Polarity,GPT_Usage_of_Technical_Terms,GPT_Vagueness,Phi_Actionability,Phi_Clarity_and_Readability,Phi_Comprehensiveness,Phi_Constructiveness,Phi_Factuality,Phi_Fairness,Phi_Objectivity,Phi_Overall_Quality,Phi_Politeness,Phi_Relevance_Alignment,Phi_Sentiment_Polarity,Phi_Usage_of_Technical_Terms,Phi_Vagueness
166,Reviewer-7mFW,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","This work focuses on the Q-function value overestimation issue. Observing that the overestimation issue will latter becomes underestimation during the learning process. Thus motivated, this work proposes the Blended Exploitation and Exploration (BEE) operator to take advantage of the historical best-perforation actions. The proposed operator is then used in both model-free and model-based settings and show better performance than previous methods. 1. The proposed BEE operator utilizes the Bellman exploitation operator and exploration operator to address the under-exploitation issue. The proposed operator can be easily incorporated into the RL algorithms.
2. The experiments show that the proposed operator can effectively reduce the estimation error and achieve better performance comparing with other RL algorithms. 1. The terminology can be misleading. The overestimation issue in the Q-value approximation generally is due to the changing order of expectation and $\max$. It is incorrect to say that  the $Q$-function will have ""underestimation when encountering successes"" in Fig 1 (a). The authors need to clarify the context and difference of the statement in order to avoid confusion.
2. In order to investigate on the under-exploitation, the metric $\Delta(\cdot,\cdot)$ is defined on the current Q-function approximation. Intuitively,   $\Delta(\cdot,\cdot)$ shows that the current Q-function approximation can be either overestimate or underestimate given different policy, i.e., $\mu_k$ and $\pi_k$. It is unclear what is the meaning of this metric. Considering most of the algorithm will update the policy and Q-function approximation at the same time, e.g., Actor-Critic, the Q-function should be evaluated under the current policy instead of the policy obtained earlier. The authors need to clarify why the definition here makes sense for the under-exploitation investigation. See the weakness above.",273,0,5,0.7173,0.1245098039,0.8775630593,49,22.7412,0.0999,iclr,0.0,4,4,2,3,factual,4,4,67,polite,4,neutral,4,high,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,3,3,3,partially factual,3,3,60,neutral,4,neutral,4,moderate,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
100,Enrico-Daga,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","The authors have performed significant changes to the content, which significantly improve the article with respect to the previous submission. Following the four SWJ criteria for survey articles, the current version is indeed a good (1) introductory text, targeted at researchers, PhD students, or practitioners, to get started on the covered topic.  The addition of a methodology section gives reasonable justification for (2) how comprehensive and how balanced are the presentation and coverage. However, a few more details about the sources of the survey would be useful, especially if mentioning keywords and phrases used in the search (Scopus? Google Scholar? Microsoft Academia? …). In addition, it is still a bit opaque what is intended with ""refining and balancing the structure of the covered areas"" - end of Section 2. However, I consider these minor issues that can be fixed during the preparation of the camera-ready. Finally, the article is readable and clear (3) and the content is relevant to the community (4).",162,0,1,0.8103,0.1645833333,0.6678649187,60,31.31,0.1149,semanticweb,0.0196078431372549,4,4,3,4,factual,5,4,80,polite,4,positive,2,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
100,Thierry-Declerck,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","Not really a lot to add. I see that the revised version of the submission was taking good care of former comments and suggestions. Just a minor point: 1) Ensure that footnotes are always placed after the punctuation signs (for consistency across the paper, se fn 2 and 3 which are not placed consistently). So, very few corrections to do.",60,0,0,0.81,0.058,0.6966200471,105,64.71,0.0548,semanticweb,0.0,4,4,3,4,partially factual,4,4,60,polite,3,positive,1,high,5,5,2,5,factual,5,5,60,polite,5,neutral,2,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,neutral,4.0,none,2,4,2,2,factual,4,3,3,polite,3,positive,1,low,1,4,1,1,factual,3,2,35,polite,2,positive,0,high
194,Reviewer-j1mL,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","In this paper, we propose a new method called WTA-CRS (Winner-Take-All Column Row Sampling) to address the main memory bottleneck issue during training, which arises from storing feature maps. To reduce memory usage during training, we sample the most likely column indices during backpropagation.

Furthermore, they proposed method demonstrates the ability to significantly reduce peak memory usage, by approximately up to 2.7 times, when fine-tuning downstream tasks. It also showcases the potential for higher throughput, enabling more efficient training. 1. The work clearly states its motivation and its solution and is easy to follow.
2. The authors show that their method reaches comparable performance with backpropagation using the full activation when combined with LoRA.
3. They also empirically measure throughput gains obtained by increasing batch size, which demonstrates the practical applicability of their method. 1. The paper needs a comparative analysis of other researchs, such as gradient checkpoint/recalculation and CRS, aimed at reducing activation memory during the training phase, as shown in Fig. 6 and Fig. 9.
2. The paper should include an analysis of the overhead associated with the proposed WTS-CRS method, which involves sampling rows and columns. It is crucial to consider factors such as the computational cost of Equation 3 and any potential effects of lowering on the overall performance. Providing this analysis would enhance the clarity and completeness of the research.
3. There is a need of analysis on the effectiveness of the proposed approach, WTS-CRS, in distributed training environments such as tensor parallelism or pipeline parallelism.
4. It seems necessary to conduct performance evaluations on various LLMs of the GPT family, such as LLaMA and OPT. * In Figure 9, it can be observed that the throughput of WTS-CRS is lower than that of full when the batch size is small. Is this due to the overhead caused by lowering?
* When comparing the training throughput, how does CRS differ from full in terms of throughput?
* Could the authors include statistics for GPU utilization in their experiments? It would be helpful to analyze the causes of improved performance more thoroughly.
* Considering that most large models are trained using multiple levels of parallelism, would it be possible to verify results for pipeline parallel, tensor parallel, etc.? Also, it is unclear from the paper whether the data parallelism used was distributed data parallelism or naïve data parallelism. * As previously mentioned, it would be valuable to include additional experimental results for models that are more challenging to quantify, such as GPT-series (OPT, LLaMA). This would enhance the validity and applicability of the proposed method across a broader range of models.
* Considering that most large-scale models are trained using multiple levels of parallelism, it is important to assess how much the proposed methods, such as pipeline parallelism and tensor parallelism, can increase throughput while taking into account overhead (such as GPU-to-GPU or node-to-node communication), memory reduction, and computational cost. Furthermore, it is not clear from the paper whether the data parallel processing used is distributed data parallelism or naive data parallelism.",506,0,8,0.8129000000000001,0.1168538059,0.8539184332,215,31.6518,0.1355,neurips,0.011111111111111,4,3,4,4,partially factual,3,4,70,polite,4,neutral,4,moderate,4,5,4,4,factual,4,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
38,Reviewer-CnQu,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper concerns the estimation of precision matrix under $l_p$ norm sparcity penal. The solution is a variational inference through normalizing flow, which is a function of shrinkage parameter $\lambda$ and non-negative norm parameter $p$. It allows for straightforward computation of solution paths for the intervals of $\lambda$ and $p$, and was empirically evaluated on two relatively small data sets. Framework for GGM estimation based on conditional normalizing flows, indeed appears novel. Supporting math seems solid. 

Using simulated annealing algorithm to recover a path of solutions for varying $\lambda$ and $p$ is useful, in particular for the case of $p$, as in case of $\lambda$ it was fairly straightforward to perform it with other methods too. I am just wondering how costly and scalable it is under the new framework, an empirical/theoretical analysis would be appreciated. Empirical evaluation appears limited. It does not contain comparison with other (e.g. frequentist) approaches to derive the solution paths. Both in terms of estimation accuracy and in terms of computational cost. In synthetic data example, why did you choose to have more samples than dimensions ( $n>d$ )? Since in that case GGM can be obtained with matrix inverse, and no need for penalized objective. Limitations were not discussed.",205,0,2,0.787,0.1207251082,0.8797656298000001,215,36.2535,0.2573,neurips,0.0120481927710843,1,4,3,1,unfactual,2,2,50,neutral,3,negative,3,moderate,4,4,4,4,factual,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
181,Reviewer-sna7,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","This paper presents ULTRA, a single model that can be directly used/finetuned for link prediction over different knowledge graphs. The key is to model the transferrable relationships between different relations across knowledge graphs. Specifically, a NBFNet is used to learn relative relation representations and generate relation embeddings, which is then fed into another NBFNet to perform link predictions. Extensive experiments are performed over many knowledge graph to demonstrate the performance of this model. - This paper is well written and easy to follow
- The core method around the relative relationships between relations is clever and interesting.
- The experiments demonstrate the gains of the method. It is especially impressive to see the competitive zero-shot performance of ULTRA over different knowledge graphs. - The proposed method relies entirely on knowledge graph structure and does not consider using node embedding such as textual features of the knowledge graphs. In reality, text embedding of nodes and edges could be a better transferrable embedding. Such transferability has already been demonstrated by PRODIGY (https://arxiv.org/abs/2305.12600) and should be addressed.
- The model does not scale well as the authors already pointed out.
- The zero shot and fine-tuning performances are worse or on-par with the per dataset model performance, rendering pretraining not effective performance-wise. 
- Some notations are a bit hard to understand. See questions. - What are u and v in h_{u|v} in section 4.2?
- Why are supervised SOTA baselines only reported for some datasets in Figure 4?",245,1,1,0.7559,0.0971320346,0.9083012938,49,38.7951,0.072,iclr,0.0,4,4,2,3,factual,4,3,70,neutral,4,positive,2,moderate,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,3,85,polite,5,neutral,4,low
9,Reviewer-KmBd,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This paper introduces a novel approach called Utility-based Perturbed Gradient Descent (UPGD) to address catastrophic forgetting and loss of plasticity in neural networks. UPGD combines gradient updates with a mask to protect useful weights from being forgotten and reuse less useful weights. The paper also proposes metrics to evaluate loss of plasticity and catastrophic forgetting. Empirically, the method outperforms existing methods in streaming learning problems in terms of retaining plasticity and avoiding catastrophic forgetting. **Originality**
Conceptually, the problem addressed by the authors of avoiding forgetting while retaining plasticity in streaming learning settings remains underexplored. The specific method proposed by the authors is relatively straightforward and conceptually similar to prior approaches; however, it empirically outperforms prior methods as the experimental results demonstrate.

**Quality**
The convergence guarantee results are valuable. The experiments are generally comprehensive and well-conducted. Assessing the quality of the approximated utilities in section 4.1 is 
of critical importance, and the results are convincing. Conducting miniImagenet scale experiments is a solid addition to the experimental section. The ablation study in Figure 8 is also insightful.

**Clarity**
The writing is generally clear and the figures are well-illustrated.

**Significance**
Overall, the paper addresses a major issue in the field of streaming learning. Given that the paper doesn't investigate the theoretical properties of UPGD, the significance of the paper hinges on the strength of the empirical results. Since the proposed method lacks theoretical performance guarantees, its empirical performance is critical. The authors have generally done a good job demonstrating that UPGD avoids forgetting and maintains plasticity; however, a few concerns remain:

- It appears that S-EWC does not have too much of a gap with UPGD judging from figure 7: it entirely avoids catastrophic forgetting, and the only setting where it loses plasticity where UPGD does not is on MNIST
- S-MAS outperforms UPGD on miniImagenet at the end of training, and does not have a large gap overall
- The ablation of figure 8 checks the contribution of each component of UPGD sequentially as they are added to regular SGD. Ideally, the ablation would study how each component affects UPGD when they are *individually* removed (e.g. UPGD without WP).

**Minor comments**
Figure 7 is referred to before Figure 6; ideally, their order would be swapped.
I see in Section 4 that the results are averaged over 20 trials, but the meaning of the error margins in some of the figures is not made clear (e.g. figure 2). I would also suggest increasing the number of trials to smooth out the curves if possible. Is it possible to show theoretical performance guarantees for UPGD? For instance, can the approximation error of equation 2 be bounded? Alternatively, if the true utilities are used in equation 3, is it possible to derive some guarantees against forgetting or loss of plasticity?

How much more significantly does UPGD improve upon baselines S-EWC and S-MAS?

How does UPGD-W perform with WP and WD removed individually?",487,0,1,0.732,0.1304191468,0.9185432792,47,29.0538,0.1695,iclr,0.0,4,3,4,4,factual,3,4,90,neutral,3,neutral,4,none,5,5,4,5,factual,5,5,92,polite,5,neutral,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
9,Reviewer-XNx6,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This paper proposes Utility-based Perturbed Gradient Descent(UPGD). A modification to the vanilla gradient descent update rule that helps the model to operate in a more challenging scenario of streaming learning. The authors introduced their utility function as an importance weight for each parameter of a neural network. The authors show the effectiveness of their contribution compared to common importance assignment methods in the continual learning literature. **Clear Structure and Writing:** The paper benefits from a clear structure and concise writing style.

**Addressing a Complex Issue:** The authors tackle an underexplored yet challenging problem, and I appreciate their efforts to address online continual learning.

**Mathematical Foundation:** The definition of utility introduced in the paper is based on simple and sound mathematical derivations. 

**New metric:** The introduction of a new plasticity metric is a nice contribution to the relatively uncharted territory of streaming learning. **Unscaled perturbations:** My main point of issue is the reasoning behind the perturbations in the update rule. The authors claim that by adding the perturbation we are making the unimportant weights more plastic however I am not really convinced by this explanation I believe it requires elaboration both in the rebuttal and in the paper. 

Another related issue with the proposed perturbation is the fact that all of them are getting drawn from the same standard normal distribution. This design choice is strange to me since the parameters of a neural network usually differ in magnitude from layer to layer. By adding an unscaled random perturbation to all of the weights we are ignoring this scale difference which I believe is sub-optimal. I know that in the unprotected version, they are getting weighted by different values but this particular scaling is more correlating with changes in the loss value rather than the parameter magnitudes.

Highly relevant to the above issue, I believe it is also necessary to have an additional ablation study, investigating the role of having and not having the perturbations in the update rule. I also want to disentangle the effect of weight decay. The only time that UG is added in the ablation is in the presence of WD. More specifically I am curious about the following scenarios in Figure 8: 

* Added ablations:
    + SGD + UG + WP + WD (present in the paper)
    + SGD + UG + WP 
    + SGD + UG + WD
    + SGD + UG

    
**Including more diverse experiments:** Moreover, in the experiments section I believe the authors need to include more diverse experiments. All of the streaming tasks are permutations of the same task. Whether in the label or in the input space. It is not as obvious as the authors' claim that after the permutation of the input space the previously learned representations are not relevant anymore (end of page 6). In the input-permuted scenario, only the first layer needs to have significant change. This is especially true for the label-permuted tasks as the network does a good job of clustering the data up to the final FC layer. I encourage the authors to use the Cifar100 superclass dataset (or any similar sequence of tasks that does not simply rely on the permutation).

**Visualization:** Finally, I believe the visualization needs several improvements: the legends on the plot are very hard to read (Fig 2, 3, 4, 5). Some colors are similar to each other and the width of the lines in the legends is too thin. (Especially in figure 4). In Figure 7, some numbers in dark blue cells are almost impossible to read. **Q1:** Have the authors tried to use an scaled version of the perturbation that takes the magnitude of the parameters into account? (Other than the unprotected version). Also I would appreciate the if you could elaborate on the effect of perturbations.

**Q2:** Could you also explain about the average online accuracy? it is stated that ""The average online accuracy is the percentage of correct predictions within each task."" I cannot see the average part here. Is it calculating the accuracy on each task separately then averaging over the number of tasks?",679,0,0,0.7295,0.0576258913,0.915908277,59,42.2021,0.929,iclr,0.0,4,4,5,4,factual,4,4,88,neutral,4,neutral,4,low,5,4,5,5,5,5,5,90,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
67,Reviewer-um1j,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","The paper introduces a new method for molecular modeling, QuinNet, which incorporates five-body interactions using only dihedral angles. The authors first introduce relevant concepts related to machine learning force fields and related work in the field related to a variety of equivariant models. Next, the paper describes pertinent definitions of force fields, group equivariance, and methods for calculating empirical force fields. In the methods section, the authors describe their approach for integrating five-body terms into the architecture of QuiNet using only dihedral angles and incorporating model designs from prior work (PaiNN for 3-body interactions, ViSNet for 4-body interactions) and new definitions for different topologies of 5-body interactions. In addition to the architectural description, the authors provide relevant mathematical formulations and a complexity analysis. In their results, the authors showcase QuiNets performance on a low (MD17) and high complexity (MD-22) dataset in terms of energy and force modeling, including an ablation for different body terms in Figure 5. The paper has the following strengths:
* Originality: The proposed architecture incorporates relevant terms for molecular modeling that are physically relevant, but have not been incorporated before.
* Quality: The method and experimental design showcase relevant cases for applying GNN models for molecular modeling with the idea behind the architecture being well-motivated.
* Clarity: The paper presents a cohesive formulation of their method, both in figures and mathematics, and experiment descriptions with relevant takeaways.
* Significance: The proposed architecture shows improved modeling performance, especially in forces, and provides a potential framework for incorporating physical interactions into GNNs. The paper could be improved by the following:
* Providing a clear and concise discussion of limitations. \[Quality, Significance\]
* Adding more context for the results in Figure 4. The MD simulations are only briefly described in Section 5.1, which is on a different page then the figure and easy to miss. \[Clarity\]
* A description of the case in which a greater set of many-body interactions is beneficial. This is briefly mentioned in the discussion between MD17 and MD22, but it would be good to put in greater context in terms of the experimental results and could serve as part of the conclusion. \[Clarity\] * Could you provide additional details on the limitations of QuinNet? E.g. Is it limited to modeling mainly molecular systems? What sizes of molecules do you think QuinNet can be effective in and why?
* Do you have data that supports your compute complexity analysis compared to other methods? If so, what kind of speedup do you generally find, if any? The authors do not provide a discussion on limitations, which I raised as a weakness. I would like to see a discussion of limitations in future versions and/or during the discussion period.",452,0,1,0.7681,0.1465895563,0.867398262,215,29.1615,0.464,neurips,0.0,5,4,3,3,factual,4,5,70,polite,4,neutral,4,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
67,Reviewer-YmDt,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","In this work, the authors propose to incorporate features from five-body interaction into machine-learning force field models and develop QuinNet. To efficiently incorporate such high-order information, the authors are motivated by the topology of many-body interactions and design sophisticated components. Experiments on several benchmarks are conducted to demonstrate the performance of QuinNet. 1. The target problem of this paper, the development of machine learning force field models, is of great significance. 1. **The motivation for the designed components of many-body interaction is puzzling**. As introduced in Section 4, the development of four-body interaction (improper torsions) and five-body interactions are based on the topology. First, such analysis is purely qualitative. The authors did not provide further completeness proof or quantitative evidence about these interaction schemes in real-world data. Second, the reasons for deriving Eq (4)-(9) are not well explained. It is suggested to clarify how these components are motivated according to the topology analysis.


2. **On the experimental evaluation**. Additionally, there are several aspects of the experiments that are concerned:
    - The empirical performance is not consistently better than other baselines. Among the evaluated benchmarks, the proposed QuinNet cannot outperform the baselines significantly. For example, in MD17, the newly developed five-body interaction modules do not significantly improve performance. In rMD17, the best performance is diversely distributed among the compared models. Overall, the experimental evaluation does not well demonstrate the power of newly developed modules.
    - The computation efficiency evaluation is missing. Although the authors provide complexity analysis, it is better to further show the time/memory cost comparison between the proposed QuinNet and baselines. Besides, the model parameters should also be provided for all compared models.
    - The scale of the chosen benchmarks is rather small. Both the dataset size and sample size (number of atoms) are limited. It is suggested to further evaluate the proposed QuinNet on large-scale benchmarks, e.g., Open Catalyst Project \[1\].
    - The ablation study. First, as shown in Figure 5, the inclusion of Five-body@I even induces further errors, which would make readers curious about whether such a phenomenon generally exists. Second, as introduced in VisNet, the improper angle was also considered. The authors should add further discussions and empirical comparisons between it and the newly proposed four-body interaction (improper torsion).


3. **The writing does not meet the requirement of an acceptable paper in this conference**. First, Section 3.2 can be thoroughly extended (e.g., in the appendix) to introduce the background of force fields and highlight the importance of torsion potential, improper torsions, and higher-order many-body interactions. Second, there lack of formal descriptions of QuinNet. Figure 3 can hardly be understood by readers that are not familiar with the related works in this area. 

\[1\] Chanussot L, Das A, Goyal S, et al. Open catalyst 2020 (OC20) dataset and community challenges\[J\]. Acs Catalysis, 2021, 11(10): 6059-6072.
    -  Please refer to the Weakness section to address the concerns. The authors did not discuss the limitations of this work.",489,2,6,0.7931,0.0741489571,0.8583066463000001,215,34.6703,0.1939,neurips,0.0,5,5,4,4,factual,3,3,80,polite,5,negative,4,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,1.0,3.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,moderate,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
67,Reviewer-YYfR,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","This paper introduces a machine learning force field that is a neural network with explicit interactions for up to 5-body terms.  The authors evaluate the model on a couple of public datasets and show demonstrate the competence or superiority of this new model compared to the state of the art in this field. The paper provides an important addition to a series of ever-improving machine learning potentials.  The contribution is clear and simple to understand at the high level, though the details are often unclear.  The benchmarks were compared against a set of reasonably strong published methods in this area.  In my opinion, if this work was presented in an unambiguously clear fashion and accompanied by code, it could be a strong contribution to this conference.  

\[The paper improved significantly following the first round of feedback from reviewers, so I'm raising my rating to a 7.\] The complexity analysis is very limited.  How many total interactions did the typical molecule have as a function of their atoms, and how did the practical experimental complexity scale for the evaluation of these molecules.  One of the main reasons that 5-body terms were not used in traditional MD simulations was the poor scaling of the number of interactions one would need to calculate.

The MD simulation mentioned in section 5.1 and Fig 4 are not described anywhere.  The following sentences suggest that there would be some explanations in the supplement, but I couldn't find them: ""Additionally, we perform MD simulations using trained QuinNets as force fields and plot the distribution of interatomic distances h(r) for these 7 molecules in Fig. 4. Further details regarding additional settings can be found in the Supplementary Materials."" 

These sentences in the supplement, page2, are confusing or wrong: ""Similarly, five-body@III interaction (Fig. S1 (c)) is a special case of six-body interaction when nodes i and k4 in Fig. S1 (d) superpose each other. Thus, the QuinNet model captures all five-body interactions and a portion of six-body interactions, making it a versatile and comprehensive tool for modeling complex molecular systems.""  There is no six-body interaction if two of the bodies are the same, and there is no physically acceptable case where two different atoms could superpose each other.

The code is not provided, so it is not possible for me to assess the reproducibility of this method.  The diagram in Figure 3 seems reasonable at the very high level, but it lacks the definitions of most of the terms annotated in the figure, thus rendering it confusing.  (What is $Y_l$? is it the set of all spherical harmonics $Y_{lm}$ for a given angular momentum $l$? What is $n_j$? What is $s_j$?  $W$?...) Could the authors add the presentation of the QM9 quantities estimated in the recent publication for Allegro?  (https://www.nature.com/articles/s41467-023-36329-y Table 3)

How long and how stable were the actual MD simulations?  What were the exact codes/protocols used?

What is the practical performance of the model during evaluation? No potential negative societal impacts from this work.",497,1,2,0.764,0.0333794423,0.8763324022000001,215,43.7997,0.1199,neurips,0.018018018018018,3,4,3,3,factual,4,4,70,polite,4,positive,4,low,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,3.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,positive,3.0,low,3,3,4,3,partially factual,4,3,70,polite,5,positive,4,moderate,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
103,Reviewer-NRqK,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","This paper proposes an algorithm for learning MDP state abstractions that preserve information needed for planning (namely, the values of states). A major differentiator from symbolic approaches is the idea that these state abstractions should be continuous rather than discrete. The key assumption is that you are given a set of options and a dataset obtained by rolling them out. Experiments are conducted in a few simple domains: pinball and antmaze, and demonstrate that the learned abstractions are sensible. The paper addresses an important topic (abstraction learning) and I appreciate the theoretically motivated algorithms. This line of work is of great interest to many attendees of ICLR. I also appreciate that the authors were clear about wanting continuous representations right off-the-bat. The math is also correct as far as I was able to tell, though I didn't check the proofs in the appendix in careful detail. Unfortunately, I recommend rejection for this paper due to 4 major reasons: 1) unconvincing experiments, 2) missing key citations to related work, 3) issues in technical details, and 4) unclear motivation.

1) unconvincing experiments

The experiments in this paper are very basic and only serve as a simple proof-of-concept that the learned abstractions are somewhat useful. To really scale up the experiments to the level expected for a conference paper, I would expect to see evidence that the learned abstractions are useful in more hierarchical domains (e.g., classic domains from the options literature like keys and doors). In such domains, we could test whether the value-preserving property holds empirically, by comparing the values from planning under the abstract model to the (ground truth) values from planning under the true model.

Additionally, I would like to see comparisons to many more RL algorithms, especially hierarchical ones like HIRO (https://arxiv.org/abs/1805.08296), HVF (https://arxiv.org/abs/1909.05829), and Director (https://arxiv.org/abs/2206.04114). This is because at the end of the day, the authors are proposing to learn a state encoder $\phi$, and despite all the theory that has gone into their algorithm, the question that must be answered is whether this $\phi$ outperforms the encoders learned by all these other SOTA hierarchical RL algorithms.

2) missing key citations to related work

The authors are missing several key citations, the most important of which is the line of work by David Abel, such as ""Near optimal behavior via approximate state abstraction"" (https://proceedings.mlr.press/v48/abel16.html) and ""Value preserving state-action abstractions"" (https://proceedings.mlr.press/v108/abel20a/abel20a.pdf). Those papers have very similar theory to what appears in this one, and so the novelty of the proposed approach is unclear. There are also less-famous but still important-to-cite papers from other authors, like ""Abstract value iteration for hierarchical reinforcement learning"" (https://proceedings.mlr.press/v130/jothimurugan21a/jothimurugan21a.pdf) and ""Deciding what to model: Value-equivalent sampling for reinforcement learning"" (https://proceedings.neurips.cc/paper_files/paper/2022/hash/3b18d368150474ac6fc9bb665d3eb3da-Abstract-Conference.html). It is important for the authors to contextualize the contributions of this paper against all these related works.

3) issues in technical details

The authors say in Section 3.2 that when B = \bar{B}, ""then simulating a trajectory in the abstract model is the same as in the ground model"". But I don't think this is true, because we need the rewards to match between the two trajectories too, and $B_t$ says nothing about rewards, only dynamics. The authors go on to say: ""Therefore, planning in the abstract model is accurate, in the sense, that the value of an abstract state z computed in the abstract model is the same as the one would get from trajectories from the ground MDP for the abstraction operator G."" Again, I think this is wrong because it ignores the abstract reward function, which could be arbitrarily different from the ground one. In fact, in the proof of corollary 3.8, the authors assume $E_{s \sim G(\cdot \mid z)}\[R(s, o)\] = \bar{R}(z, o)$, and it's only _under this assumption_ that the claims hold. But combining this assumption on reward function with Definition 3.6 ends us back up at the bisimulation conditions, and then it's not clear what the contributions of this paper are.
 
As a separate point, the second term in the mutual information expression of Section 4.2, $MI(S'; Z, A)$, seems very extreme! It is saying that you have to be able to predict the entire ground next state from the current abstract state and action. Doesn't this means the abstraction can't lose any information? This seems like an important technical limitation of the approach.

4) unclear motivation

The authors often state that a discrete abstract state space is bad, when pointing to work on symbolic abstraction learning (e.g., PDDL). But it's not clear why this is really bad. The authors say discrete abstract states are ""not applicable when planning with the available high-level actions requires a continuous state representation"", but this doesn't make sense to me, as the options have to act in the ground environment states, not in the abstract state space, and so the options could be defined with respect to either a discrete or a continuous abstract state space. Furthermore, it can be much easier to plan in a discrete abstraction (e.g., using powerful symbolic planners).

I believe a fruitful research direction would be to compare the abstractions learned by a symbolic approach against the abstractions learned by a continuous approach (like the authors'). Questions:
* Not much is said about the dataset $\mathcal{D}$, but intuitively, it has to be ""good"" in order for the learned state abstraction to be reasonable. In particular, the agent must see all the options being executed in a variety of settings, and obtain good coverage over the state-action space. Are there any concrete statements we can make about what properties we need this dataset to have?
* ""we must build a model of its effect"" Do you mean to say ""of the effect of each option""?
* ""with mean value equal to that by planning with the original MDP"" What is the mean over?
* Why did we switch from using O (denoting the option set) everywhere to using A throughout Section 4? Shouldn't we continue to use O, unless I am misunderstanding something?
* Section 4.3: Why should there be any cost/reward associated with executing skills? Shouldn't a sparse reward for reaching the goal be enough?
* Eq 2: What are the ""I"" random variables inside the mutual information expression referring to?

Minor edits:
* ""make the same decision"" To clarify, we just need that the policy maps all states in z to the same action distribution. A stochastic policy isn't really committing to a ""decision"" about what action to take.
* ""Abstractions alleviate this tension: action abstractions enable agents to plan at larger temporal scales and state abstractions reduce the complexity of learning and planning"" I would say that both of them do both of these. Action abstractions certainly reduce the complexity of planning, which is typically exponential in the branching factor.
* ""learns a further abstraction"" --> ""learn a further abstraction""
* ""otherwise it is referred as learning"" I would say ""policy learning"" to distinguish from other things you might learn
* ""when it is the given position"" --> ""when it is in the given position""
* ""referred as learning"" --> ""referred to as learning""
* ""results a bounded value loss"" --> ""results in a bounded value loss""
* In definition 3.5, the authors use $s_o$ in a few places where they mean $s_0$.",1210,7,0,0.7753,0.0567315252,0.9024221301,49,44.7468,0.6075,iclr,0.0196078431372549,5,5,5,5,factual,5,4,94,polite,4,negative,5,none,5,5,5,5,factual,5,5,90,polite,5,negative,5,none,2.0,4.0,4.0,3.0,factual,4.0,3.0,60.0,neutral,5.0,negative,5.0,low,5,5,5,5,factual,5,5,95,polite,5,negative,5,none,3,4,4,3,partially factual,4,3,70,neutral,5,negative,5,low
103,Reviewer-gKE9,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","This paper proposes a grounded abstract model formulation with a dynamic preserving abstraction. This abstract state representation (and model) guarantees not only accurate future predictions but also the bounded values in the abstracted rollouts. This paper then provides its implementation using contrastive learning to maximize mutual information between the future state, and the current abstract state and option. The results show that training DDQN in imagination using the abstract model improves the sample efficiency. * The paper proposes a solid foundation of the abstract model that preserves dynamics and values.

* The paper is well written.

* The visualization in Figure 3 clearly shows that the abstract state representations focus on important features in the original observation space. * The main focus of the paper is to show the efficiency of planning and learning when using the proposed abstract MDP. The experiments in the paper are a bit simple to showcase the benefits of the abstract model for planning. It would be stronger if the experiment was done in more complex environments with much longer-horizon tasks, such as AntMaze experiments (Hafner 2022) or robotic manipulation tasks \[a\].

* Similarly, the comparisons in Figure 5 are essentially between model-free RL (ground) and model-based RL (abstract), which does not seem fair. It might be fair to compare the proposed method with other model-based RL approaches, such as Dreamer and TD-MPC.

* Exhaustive comparisons to the alternatives to the dynamics preserving abstraction would be interesting, such as bisimulation.

* Some highly relevant works on temporally-extended models \[a,b\] are missing in the paper. Proper comparisons to these approaches are necessary.

\[a\] Shi et al. Skill-based Model-based Reinforcement Learning. CoRL 2022

\[b\] Zhang et al. Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains. 2023 Please address the weaknesses mentioned above.


### Minor questions and suggestions

* Figure 1 may want to explain why abstract state representations and options are helpful for planning and learning. However, Figure 1 does not seem to help understand the paper. To understand this figure, we first need to know about options and abstract state representations, and how they simplify planning.

* In Section 4.2, it is unclear whether $\mathcal{L}^T_{\theta, \phi}$ is used to update $f_\phi$ or not.

* For multi-goal experiments in the paper, using the same amount of environment steps for the abstract planning and the ground baseline would make it easier to understand how better or worse a method is.

* The appendix could be included in the main paper for easier navigation.

* What is the difference between Figure 7 and 8?

* Training the abstract planning method longer in Figure 7 and 8 would be helpful to see how it learns. Using different x-scales for two methods is okay but it would be better to have the same scale.

* Many minor typos in the paper.


---

Thank you for author responses. I would love to see comparisons to Dreamer-like baselines, but couldn't find the results by the end of the rebuttal period. Thus, I keep my rating, borderline reject.",507,0,3,0.7684000000000001,0.1385185185,0.9183520675,71,48.6501,0.8246,iclr,0.0,4,5,5,4,factual,4,5,89,polite,4,positive,4,none,5,5,4,5,factual,5,5,85,polite,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,3.0,4.0,60.0,polite,4.0,neutral,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
141,Reviewer-H6rR,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","This paper tackles the problem of code patch representation learning -- how to represent edits on code to support downstream tasks like commit message generation, patch correctness assessment, etc. 

This paper proposes a pretraining framework, with triplet losses on text-code contrastive loss, text-code matching loss and text generation loss based on code patch. The pretraining data includes 90K pairs of code change and synthesized commit messages. 

On downstream task of commit message generation upon FIRA\[1\] dataset, PatchSynth showed performance gains over public & self-ablation baselines.

\[1\] Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, Wenjie Zhang, and Dan Hao. Fira: Fine-grained graph-based code change representation for automated commit message generation. 2022. 1. Unlike from previous approaches(CCRep\[1\], Cache\[2\]) where code change is encoded with two streams (code-before-change, code-after-change), this work encodes code change(patch) with a standard transformer on a single patch file (like git commit diff). This is inline with the general trend in LLMs community that ultimately LLMs should be able to understand and capture internal structure without explicitly modelling it.
2. This paper applies representation pretraining with triplet losses -- which is quite known in multimodal pretraining domain (BLIP\[3\], BLIP-2\[4\], etc) -- to code patch representation. It empirically showed that such pretraining is helpful for downstream task of commit message generation.


\[1\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[2\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022.

\[3\] Junnan Li, Dongxu Li, Caiming Xiong, & Steven C. H. Hoi (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In ICML (pp. 12888–12900). PMLR.

\[4\] Junnan Li, Dongxu Li, Silvio Savarese, & Steven C. H. Hoi (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML (pp. 19730–19742). PMLR. I have multiple major concerns on the paper based on its current form. The most concerning issues are:

1. The paper claims ""The core part of PATCHSYNTH lies a state-of-the-art synthetic description generator"" in multiple places (3rd paragraph of Introduction, 2nd contribution in last part of Introduction, Section 2.3). However, there is no details on this synthetic description generator. The only mention is Section 4.4 with just one line ""Capitalizing on benchmarks from seminal works\[1,2\], our dataset, primarily focused on Java samples includes 90,661 patches with their **attendant** descriptions""

    i. Are these **attendant** descriptions generated by the author but simply taken from \[1,2\]? If the latter, then claiming such synthetic description generation as a key feature in this paper is highly problematic.

2. The task of representation learning of code patch, defaultly assigns 1 vector for a code patch (w/ 1 or more edits), which is the case for all previous works including CC2Vec\[3\], CCRep\[4\], Cache\[5\]. However, PATCHSYNTH seems to encode the code patch to a sequence of vectors (Figure 1). 

    i. Such change needs explicit explanation and justification which authors have failed to deliver.

3. Missing LLM baselines: with code patch being encoded with a sequence of vectors, the authors should compare with code-aware LLMs like Code-llama, or WizardCoder, as they also encode code patch to a sequence of vectors. 
    
    i. As the recent code-aware LLMs have shown great abilities in general instruction following in coding-related tasks, a very timely baseline would be applying code-aware LLMs to the downstream task of commit message generation, with few-shot prompting or finetuning. 

    ii. A comparison of PATCHSYNTH vs code-aware LLMs would very helpful for the community to understand the edge and relevance of the proposed method in LLM era, which the authors have failed to deliver.

4. Only 1 downstream task evaluated: The authors claimed that the method is designed both for generative and discriminative tasks. However, the empirical experiments were only conducted on commit message generation. As the encoding changed from one vector to a sequence of vectors, it's important to show how can such encoding can be adapted to tackle retrieval or classification tasks. Also, to claim it as a pretrain model, the authors need to evaluate on multiple downstream datasets.

5. Fairness in comparison: 

    i. Is PATCHSYNTH firstly pretrained on 90K and then finetuned on 75K data of FIRA? If so, it's not so fair to compare PATCHSYNTH with CCRep and FIRA methods, as they are not trained on 90K pretraining data. For example, Is it possible to also pretrain CCRep with 90K data?

    ii. As mentioned in point 2, CCRep has a more compact encoding of 1 vector while PATCHSYNTH encodes to a sequence of vectors. It is thus not fair to compare without explicitly mentioning such differences.

6. Concerns on Pretraining:

    i. details of creating negative pairs: one common technique in contrastive training is hard-negative-mining. However, the authors didn't disclose how they create negative pairs

    ii. For vision-language representation learning, a large batch size (>= 512) and a large pool to select negative examples have been shown to be necessary. This paper mentions the batch size of 32, which seems pretty small. I will need more verification on ablation of 1) batch size, 2) negative example selection and 3) pretraining metrics to be convinced that such setting is adequate for code-text representation pretraining. 

7. Writing & formatting issues

    i. On page 8, the chart of Figure 2 is partially blocked by its top legend

    ii. On page 8, the paragraph for \[Performance cross different patch attention\] is repetitive: it repeats twice in introducing the numerical performance. Besides, I don't think it's a good idea to verbosely list down all numbers when they are clearly seen in Figure 2.

    iii. In Section 2.1, there's no mention on recent code aware LLMs like Code-LLaMA. 

    iv. In Section 2.3, there's no citation to any work. Besides, CCRep\[4\] doesn't have the gap mentioned in Section 2.3 as it can both do discriminative and generative tasks and it doesn't reply on AST information. So an explicit comparison to CCRep in Related Work should be present.

\[1\] Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In 2013 35th International Conference on Software Engineering (ICSE), pp. 422–431. IEEE, 2013.

\[2\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[3\] Thong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. Cc2vec: Distributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering, pp. 518–529, 2020.

\[4\] Zhongxin Liu, Zhijie Tang, Xin Xia, and Xiaohu Yang. Ccrep: Learning code change representations via pre-trained code model and query back. In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 17–29. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00014. URL https://doi.org/10.1109/ICSE48619. 2023.00014.

\[5\] Bo Lin, Shangwen Wang, Ming Wen, and Xiaoguang Mao. Context-aware code change embedding for better patch correctness assessment. ACM Transactions on Software Engineering and Methodology (TOSEM), 31(3):1–29, 2022. 1. Why did you use CodeBERT to initiate both text encoder and decoder? For example, did you consider encoder-decoder model like Code-T5?

2. In Experiment Setup, you mentioned ""Model dimensions are meticulously calibrated"". May I know how are the hyper-parameters searched? Are you using the downstream task performance or some pretraining metrics?",1254,27,37,0.7999,0.069050849,0.8986387253,50,47.6556,0.1651,iclr,0.0,4,2,4,4,factual,3,4,80,polite,4,neutral,4,moderate,5,4,5,5,factual,5,5,92,polite,5,neutral,5,none,1.0,4.0,3.0,4.0,partially factual,2.0,3.0,60.0,polite,4.0,neutral,4.0,moderate,5,4,5,5,factual,4,5,90,neutral,5,negative,5,none,4,4,4,4,partially factual,4,3,85,neutral,5,negative,5,low
28,Reviewer-xyNq,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper considers the problem of certifying individual fairness (IF), which is of great importance to reliable machine learning algorithms. To this end, the authors propose a novel convex relation of IF constraints that greatly reduces the computational cost. In addition, the authors propose to certify distributional individual fairness, ensuring that the neural network has guaranteed individually fair predictions for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball. 1. This paper is technically sound.
2. The extensive experiments validate the effectiveness of the proposed methods. The paper studies individual fairness and distributional fairness. To my opinion, the two topics seem to be independent. However, it is possible that I misunderstand this paper. It would be better if the authors can present more relations between these topics. ## Miscellaneous
1.	Line 106: feed forward $\to$ feedforward
2.	Line 168: $d$ is indeed a vector; however, the denotation $\sqrt{d}$ should be defined more specifically.
 none",156,0,5,0.8035,0.28125,0.9500498772,215,29.3366,0.1213,neurips,0.0,3,5,3,3,factual,4,4,60,polite,4,neutral,4,moderate,4,5,3,4,factual,4,4,85,polite,5,positive,4,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,4,low,2,4,3,3,partially factual,3,3,75,polite,4,positive,4,low
47,Jens-Foell,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript in question describes different methods to visualize data acquired through MRI/fMRI scans in a three-dimensional manner. This is something that is sometimes done in current neuroimaging research, but that is rarely done in a standardized manner, which makes this guide timely and relevant. In many cases, researchers choose to use 2D images instead, which can sometimes distort or omit information, as fMRI depictions are derived from an inherently 3-dimensional signal. The current manuscript separately describes ways to visualize clusters of activation (i.e. activation as it would be found when running an fMRI experiment) and anatomical regions of interest. It also provides hyperlinks to download relevant visualization software. The author goes into sufficient detail to include, for example, information on price and OS compatibility of different software packages. Also, the text provides details about how to create the images within a particular software package, or functions that increase user efficiency. Information like this, in addition to several informative illustrations in the manuscript, will make this text particularly useful for many people working in neuroimaging, and I am convinced that the publication of this manuscript will lead to a fruitful online discussion about the best ways to visualize and report 3D brain data.The title, abstract, and structuring of the manuscript are well-written and appropriate for its purpose as a brief guide.Overall, this concise and informative guide is useful, interesting, and well-written. I recommend its indexing after some very minor comments (listed below) have been addressed to increase the readability of the manuscript. Minor suggestions:While the term ‘3D’ could be considered to be a household word, I would still recommend to spell it out as ‘three-dimensional (3D)’ or ‘3-dimensional (3D)’ the first time the term is used in the text. Likewise, the term ‘glass brain’ is intuitive, but not always used in the same way by all researchers. A quick description of the concept at the first mention of the term in the text would make the manuscript more accessible to the general reader.",400,0,0,0.8096,0.195990991,0.9470573664,2,29.18,0.1953,f1000,0.0105263157894737,3,4,3,3,partially factual,3,3,65,polite,3,positive,3,moderate,4,5,5,5,factual,5,5,95,polite,5,positive,5,none,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,5,factual,4,4,92,polite,5,positive,5,low
47,Anders-Eklund,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this is a useful paper. Here are some minor commentsYou do not mention anything about multiple comparisons for the thresholding. I understand that these visualizations are mainly for obtaining a better understanding of the brain activation, but it would still be nice to mention the problems of multiple testing. For cluster level inference, I prefer if a cluster p-value threshold is used, and not an arbitrary cluster size like 400 mm3 or 50 voxels. Cluster p-values can be obtained through parametric methods (Gaussian random field theory, available in SPM and FSL) or non-parametric methods (permutation testing, available in SnPM, FSL and BROCCOLI). I know that a very common approach is to use a cluster defining threshold of p = 0.001 or p = 0.005 (uncorrected for multiple comparisons), combined with an arbitrary cluster size threshold of 10 voxels. Such approaches should in my opinion be avoided, since the method is ad-hoc; it is impossible to know what the (corrected) p-value is for the combined procedure.The following paper may be of interest:Choong-Wan Woo, Anjali Krishnan, Tor D. Wager, Cluster-extent based thresholding in fMRI analyses: Pitfalls and recommendations, NeuroImage, Volume 91, 1 May 2014, Pages 412-419, ISSN 1053-8119, http://dx.doi.org/10.1016/j.neuroimage.2013.12.058-------You may mention two additional pieces of software, pysurfer and MevisLab.Pysurfer is a python tool for visualizing cortical surface representationshttps://pysurfer.github.io/MevisLab is a free software that can be used for image processing and visualization. MevisLab includes functions from the libraries VTK and ITK, and it is easy to setup more advanced volume rendering pipelines, where you for example have several volume renderers, clip planes and more advanced transfer functions.http://www.mevislab.de/-------You do not mention anything about visualization research regarding fMRI. A more advanced way to visualize brain activation is to treat the activation as a light source in the anatomical volume, making the activity ""glow"" from the inside. You could include some of the following papers.Nguyen, T. K., Eklund, A., Ohlsson, H., Hernell, F., Ljung, P., Forsell, C., Andersson, M., Knutsson, H., Ynnerman, A., Concurrent Volume Visualization of Real-time fMRI, Proceedings of the 8th IEEE/EG International Conference on Volume Graphics, 53-60, 2010, http://dx.doi.org/10.2312/VG/VG10/053-060Janoos, F., Nouanesengsy, B., Machiraju, R., Shen, H. W., Sammet, S., Knopp, M. and Mórocz, I. Á. (2009), Visual Analysis of Brain Activity from fMRI Data. Computer Graphics Forum, 28: 903–910. doi: 10.1111/j.1467-8659.2009.01458.xJainek, W. M., Born, S., Bartz, D., Straßer, W. and Fischer, J. (2008), Illustrative Hybrid Visualization and Exploration of Anatomical and Functional Brain Data. Computer Graphics Forum, 27: 855–862. doi: 10.1111/j.1467-8659.2008.01217.xRieder, C., Ritter, F., Raspe, M. and Peitgen, H.-O. (2008), Interactive Visualization of Multimodal Volume Data for Neurosurgical Tumor Treatment. Computer Graphics Forum, 27: 1055–1062. doi: 10.1111/j.1467-8659.2008.01242.x",504,9,6,0.8115,0.128375,0.8943301439,6,37.0,0.2561,f1000,0.0099009900990099,5,5,4,5,factual,4,4,90,polite,5,positive,5,low,5,5,4,5,factual,5,5,95,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,5,factual,5,4,85,polite,5,positive,5,low,5,4,4,5,factual,4,4,92,polite,5,positive,5,low
47,Matthew-Wall,Creating 3D visualizations of MRI data: A brief guide,"While magnetic resonance imaging (MRI) data is itself 3D, it is often difficult to adequately present the results papers and slides in 3D. As a result, findings of MRI studies are often presented in 2D instead. A solution is to create figures that include perspective and can convey 3D information; such figures can sometimes be produced by standard functional magnetic resonance imaging (fMRI) analysis packages and related specialty programs. However, many options cannot provide functionality such as visualizing activation clusters that are both cortical and subcortical (i.e., a 3D glass brain), the production of several statistical maps with an identical perspective in the 3D rendering, or animated renderings. Here I detail an approach for creating 3D visualizations of MRI data that satisfies all of these criteria. Though a 3D ‘glass brain’ rendering can sometimes be difficult to interpret, they are useful in showing a more overall representation of the results, whereas the traditional slices show a more local view. Combined, presenting both 2D and 3D representations of MR images can provide a more comprehensive view of the study’s findings.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a very useful guide to an important issue that is currently largely overlooked in the literature; producing high-quality presentations of brain imaging results that are informative, clear, and useful. The article is comprehensive and easy to follow, and the examples provided are appropriate, and produce very attractive images. This is an extremely useful paper that deserves wide readership in the field.While I agree with the author that ‘glass-brain’ visualisations are extremely useful for providing a comprehensive overview of patterns of brain activity in fMRI experiments, that doesn’t mean that conventional 2D slice views are not also useful. In fact, 2D views of particular activation clusters are really the only way to get a good idea of the precise position of a cluster, in relation to the sulcal/gyral anatomy, which is often important. An optimal strategy for comprehensive visualisation and localisation might then be to combine 2D and 3D views of results in the same figure. The author has done this more-or-less in Figure 3 (which includes coronal slices), but I wonder if perhaps an additional example figure which combines 2D and 3D views might be helpful? Perhaps as an example of the kinds of ‘real’ figures that could be produced for publications and presentations.Minor points of grammar, etc.:Abstract:""they are useful in showing a more overall representation of the results"" More overall? Somewhat clumsy; replace with ""more general"" or just ""overall"".Page 2 first paragraph: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that work in these scenarios, as well as readily generalize to most other instances."" Something wrong with the tenses here; would suggest: ""Here I briefly detail a straight- forward approach for creating 3D visualizations of MRI data that works in these scenarios, and also readily generalizes to most other instances.""Page 4. Section on obtaining and thresholding the images. Fine, but the procedure outlined here is pretty cumbersome, as the author admits! This procedure might be optimal for those who use SPM as their primary analysis tool, but the 'fslmaths' function included with FSL could achieve this in a single command-line entry. Maybe include a sentence saying something like ""Other options for thresholding are available, such as the basic functions included with FSL.""",439,0,1,0.805,0.1733333333,0.9432629347,34,29.79,0.0613,f1000,0.01,3,4,3,4,partially factual,3,3,75,polite,4,positive,3,low,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,4,4,4,5,factual,5,4,85,polite,5,positive,4,low,5,4,4,5,factual,4,4,92,polite,5,positive,5,low
144,Reviewer-PpGy,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","The paper proposes the integration of an energy-based model (EBM) to learn the distribution of the noise that is added to samples in a dataset to be modeled by a physics-informed neural network (PINN). A joint loss function is used to train the EBM and the PINN, whereas the EBM can be trained at the same time or with a delayed start with respect to the PINN. Numerical experiments use synthetic data governed by several well-known PDEs from physics, polluted with a variety of noise distributions, to test the performance of the proposed approach. The approach is principled, the description is clear, the results are convincing. The proposed approach integrates two well-known models from the literature; the approach is straightforward and the results are not surprising. EBMs have been used before in classification, generative modeling, and regression problems; the authors state that the novelty is in the leveraging of physical knowledge within PINNs. In addition, all the results are focused on synthetic data. Thus the impact of the proposed approach appears limited to the current combination of tools for the usual applications of PINNs.

Minor comments:

In Algorithm 1, within the training loop, i should be updated. To better evaluate the impact of the proposed approach, it would be good to discuss the following questions:

(1) How is the formulation of the proposed approach different from the integration of EBM to a regular neural network?

(2) Is there real-world data that would usually be modeled by a PINN where non-Gaussian additive noise is present and for which the proposed approach can be shown to provide better solutions than the baseline PINN?",271,0,0,0.7132000000000001,0.0896616541,0.921692729,51,43.8468,0.1262,iclr,0.0104166666666666,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,5,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,3,4,factual,4,4,85,polite,5,neutral,4,low
144,Reviewer-pToC,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","the paper propose a method to handle measurement noise that has non-zero bias (Eq.7) and algorithm 1. the paper is indeed very hard to read. I would suggest the authors rewrite the paper to allow readers to understand and therefore use this paper for the progress of science. then resubmit the paper in the next conference. I will explain why the paper is hard to read in the next section. A learning method to handle more sophisticated measurement noise. I try to help the authors by explaining why the paper is hard to read to me. I hope these feedback can help improve the writing for a future paper.

1. math symbols are not defined when they are first used. examples:

1a. page3, line 3, D_d = {d_d, y_d}. these symbols are not explained and define. y_d was explained only towards end of page 3.

1b. page3, line 3, what is ""d""? is this the index of the data point? furthermore D_d is just a set with two elements. how to learn from a set of two elements?

1c. what is the math object of y_d? is it \mathbb{R}^m or \mathbb{R}? t_d \in \mathbb{R}? what is \lambda and what dimension is it?

1d. Eq2. t_c, how to get the colocation points?

1e. algorithm 1, ""if i<i_ebm then"", what is i?

2. page3 second paragraph. I read this paragraph many times, I still cannot understand it. this paragraph needs to be expanded and writing needs to be clear.

overall the math formulation needs to be improved a lot.

assessment on the results and experiment section becomes invalid if the methods section of the paper is not clear and people cannot reproduce this work. see above 'weakness' section.",286,0,9,0.6966,0.05234375,0.8311564326,51,75.1547,0.0795,iclr,0.0093457943925233,4,4,4,4,factual,4,4,75,polite,4,negative,4,low,4,4,4,4,factual,4,4,65,polite,5,negative,5,moderate,2.0,1.0,2.0,3.0,partially factual,4.0,4.0,60.0,neutral,4.0,negative,3.0,moderate,4,2,3,4,factual,3,3,60,neutral,4,negative,3,low,4,2,3,4,partially factual,3,3,60,neutral,5,negative,4,low
147,Reviewer-ARok,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper proposes a method to propagate knowledge update to LMs via training a student model through context distillation, such that the LM can make inference on an entity even though the relative context/knowledge of the entity is not given. The framework involves two steps: 1) create a transfer set that contains the knowledge that the student model will be learning from; 2) compute the distribution of the transfer set tokens for both the teacher model (while given context, i.e. a definitional sentence) and the student model and update the student model's parameters by minimizing the KL divergence of the two distributions. The paper evaluates the student model with two sets, Entity Inferences and ECBD to show that the knowledge has successfully propagated. This paper is more efficient with multi-entity editing and achieves competitive performance on the two evaluation set in terms of propagation success (accuracy and decrease in perplexity) while causing little impact on specificity. It seems like the paper is more focusing on new knowledge ingestion, either in Entity Inference (synthetic entities) or ECBD (introducing new entities after 2022). While this is an important aspect, a harder task is to update existing knowledge in the old model. One dimension could be temporal shifts, e.g. after a new election, population/economic changes (potentially resulting changes in superlative statements), factual changing official announcement (e.g. solar system has 9 planets before 2006 and Pluto was downgraded to dwarf planet in 2006 - solar system has 8 planets now). It is unclear whether the model can adapt to the new facts while maintain low specificity.

Another baseline is to try prompting the LLMs with new knowledge and see how it propagates. If the existing LLMs can handle such knowledge updates well, it may be hard to justify why we need to train a separate student model. On line 126, it states the distillation is done through updating $M_s$ parameters to minimize the KL divergence. Does it update all the parameters in $M_s$, or is it possible to combine the distillation with other network editing techniques to only a local set of parameters? How much would it negatively impact the performance if only a local edit is allowed? Asking since if we want to extend this framework to larger LLMs (as current good-quality LLMs usually have 100B+ parameters and updating all parameters seem to be impossible). As mentioned by the authors, this work mainly uses relatively small size LMs for experiments and its generalizability to LLMs is unknown. While it may apply to LLM trainers/creators to adapt this method to update their models, it does not extend to end users/organizations of the LLMs who want to ingest or update knowledge, e.g. from specific domains or confidential sources, potentially through local edits.",457,0,0,0.7902,0.0358824734,0.9262851477,215,37.6838,0.0987,neurips,0.0,3,4,3,3,factual,4,3,65,neutral,4,neutral,3,low,4,4,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
147,Reviewer-LLss,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","The paper propose a context distillation-based approach that can both impart knowledge about entities and propagate that knowledge to enable broader inferences. This approach consists of two stages: transfer set generation and distillation on the transfer set. In the first stage, a transfer set is generated by prompting a language model to generate a continuation from the entity definition. In the second stage, the model parameters are updated so that the distribution of the LM (the student) matches the distribution of the LM conditioned on the definition (the teacher) on the transfer set.
The authors' experiments demonstrate that this approach is more effective in propagating knowledge updates compared to fine-tuning and other gradient-based knowledge-editing methods without compromising performance in other contexts, even when injecting the definitions of up to 150 entities at once. 1. A straightforward motivation that conditioning on information about the entity can lead to lower perplexities.

2. The authors' method of generating a transfer set by prompting an LM to generate a continuation from the entity definition is a unique contribution to the field.

3. The authors compare their method with other knowledge injection methods, including fine-tuning, and demonstrate the superiority of their approach. They also conduct an in-depth analysis of the types of continuations needed in the transfer set.

4. The authors' method provides a scalable and effective way to update the knowledge of LMs.
 1. As the authors concede in Section'Limitations', their proposed methodology has yet to be substantiated on models of a larger scale. For instance, LLaMA-65B may present a fitting candidate for such validation.

2. The experiments in the paper focus on a specific type of knowledge update: adding definitions for entities. It's unclear how well this method would work for other types of knowledge updates, such as knowledge revision.

3. The results of Finetuning on transfer set (full) are not shown in Table 2.

4. Writing content issues.
  (1) It would be clearer to add arrows in the table to show whether the larger or smaller values are better.
  (2) What are Finetuning on definition (full) and Finetuning on definition (last only)?
 What are Finetuning on definition (full) and Finetuning on definition (last only)? Yes.",363,0,10,0.7365,0.1574074074,0.9659975171,215,36.8878,0.072,neurips,0.0,5,5,5,4,factual,5,5,80,polite,5,neutral,5,none,4,4,4,4,partially factual,4,5,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
125,Elizaveta-Kon,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Literature concerning PRP use in rotator cuff pathology is mainly oriented towards intra-operative use of this biological strategy. However, recently, a double blinded randomized controlled trial on 39 patients has been published (Rha DW et al. (2012) Comparison of the therapeutic effects of ultrasound-guided platelet-rich plasma injection and dry needling in rotator cuff disease: a randomized controlled trial). This article is a case report on the same topic, with all the scientific limitations related to the nature of such kind of article. At the present moment, also considering the controversies arisen on PRP application in tendon pathology, we need well designed high quality trials to assess the efficacy of this treatment option. The article is written in a fair manner without big methodological bias. However method is not only how you did what you did but also what you could have done better. Of course case reports provide poor evidence and it is impossible to rely just on findings from this kind of study. The author of the present study should have used some clinical scores (there are many available for the shoulder) to document outcome over time, MRI pre- and post-treatment should be added to better assess tendon healing and the features of PRP used should be discussed as this is one of the crucial points of current debate on PRP application. These changes could improve the scientific value of this case report, and it is important to be exhaustive when you have a single patient examined.",315,1,0,0.8383,0.1165756303,0.846734941,13,25.53,0.2027,f1000,0.0104166666666666,2,3,2,2,factual,4,4,75,neutral,4,negative,4,moderate,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
124,Xihao-Li,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study provides a systematic evaluation of the effect of adapter trimming on the accuracy of germline short variant calling in the human genome, utilizing both whole-genome sequencing (WGS) and whole-exome sequencing (WES) datasets. The comparison of multiple variant calling tools, with and without adapter trimming, reveals minimal impact on WGS data but suggests modest improvements in certain WES samples, particularly in indel detection. The study concludes that while adapter trimming may not be essential for WGS, it shows some benefits for specific WES cases. The manuscript is well-written and clear, making it accessible for readers. Below are a few comments for the authors to consider: The authors mention that “adapter trimming had very limited effects on both precision and recall.” It would be helpful to clarify and quantify the threshold for ""limited."" Providing a statistical measure, such as a p-value or confidence interval, would strengthen the interpretation of the findings. Additional explanation is needed for the samples that showed positive effects in Figure 1. Clarifying why these samples differ from the others would help contextualize the observed improvements. The authors are encouraged to elaborate on the reasons why results differ between SNP and indel calling. Further discussion on potential underlying mechanisms would enhance understanding. The statement that “trimming may even decrease the accuracy of analysis” warrants further discussion. Exploring potential reasons behind this observation could provide valuable insights into the circumstances in which trimming could be detrimental.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",383,0,1,0.8028000000000001,0.1083887657,0.964974463,150,26.61,0.072,f1000,0.0103092783505154,4,4,3,5,factual,5,5,65,polite,5,neutral,4,low,5,5,4,5,factual,4,4,90,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low
177,Maarten-Franciscus-Schim-van-der-Loeff,The development of mouthwashes without anti-gonococcal activity for controlled clinical trials: an in vitro study,"Background: The oropharynx plays a major role in the development and spread of antimicrobial resistant Neisseria gonorrhoeae among men who have sex with men. Trials are currently assessing the efficacy of bactericidal mouthwashes as possible therapeutic or preventive options against these pharyngeal gonococcal infections. Controlled clinical trials require the use of a placebo mouthwash without anti-gonococcal activity. So far, no such mouthwash has been described. We describe the development of a mouthwash for this purpose. Methods: The in vitro anti-gonococcal activity of Corsodyl®, Listerine Cool Mint®, Biotene®, phosphate buffered saline and six in-house placebo mouthwashes was evaluated. Three gonococcal isolates from patients with pharyngeal infection were exposed to the mouthwashes for a duration ranging from 30 seconds to 60 minutes. Isolates were then plated onto blood agar (5% horse blood) and incubated for 24 hours (5-7% CO2, 35 ± 2°C). Growth of N. gonorrhoeae was scored on a five-point scale (0 to 4). All experiments were conducted in duplicate. Results: Corsodyl® and Listerine Cool Mint® were bactericidal to all isolates. For the other mouthwashes, the median growth score after 60 minutes of exposure was 4 (interquartile range 4-4) for phosphate buffered saline; 1 (interquartile range 1-3) for Biotene®; and ranged between 0 and 2 for the in-house composed mouthwashes. An in-house composed mouthwash (Placebo 6) performed best, with a growth score of 2 (interquartile range 2-3). Conclusions: All of the evaluated potential placebo mouthwashes were bacteriostatic after gonococcal exposure of 30 to 60 minutes. In-house composed Placebo 6 showed less inhibition on gonococcal growth than Biotene® and the other in-house placebos and demonstrates, in our opinion, a good trade-off between anti-gonococcal properties and taste.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This report by Van Dijck et al. seeks to evaluate the anti-gonococcal activity of several commercially available mouth washes, as well as of one commercial and several in-house produced 'placebo' mouth washes. The authors find that all tested commercially available mouth washes have some anti-gonococcal activity, and that some of the 'placebo' mouth washes did as well.  This study is important as mouth washes have been suggested as a potential tool for prevention of gonorrhoea on an individual and a population level. Trials examining the efficacy of oral mouth wash need a 'placebo' without anti-gonococcal activity. This report provides important data towards that. This brief report is clearly written. The conclusions are based on the data. The limitations of the small study are clearly described in the Discussion. I am a physician and epidemiologist and recommend that also a microbiologist should review the manuscript. I have a few minor comments: In the abstract it is not clear how the 5-point scale of N. gonorrhoeae growth is to be interpreted; make it explicit that 0 means no growth and 4 extensive growth.  The abstract mentions an IQR of 2-3 for placebo 6 at 60 minutes; Table 4 mentions an IQR of 1-3. Please check and correct.  Not all readers may be familiar with the term ""pharmaecological"" (perhaps better spelled as ""pharma-ecological""?) so a fuller explanation may be helpful.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",370,0,1,0.7574000000000001,0.1513157895,0.9179583192,61,38.21,0.1201,f1000,0.0210526315789473,2,4,2,1,partially factual,3,2,50,polite,3,positive,2,moderate,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,3,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
91,Reviewer-pVGE,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This paper presents a novel approach to Multi-Task Learning (MTL) by integrating Sharpness-aware Minimization, a technique that enhances single-task learning generalization. This new methodology aims to find flat minima for each task, improving overall generalization across multiple tasks. The paper showcases the effectiveness of this approach through extensive experiments, differentiating it from existing gradient-based MTL methods. The proposed method addresses the challenges of overfitting and negative transfer in MTL, contributing to more robust solutions in various applications. The integration of SAM and MTL is somewhat new in transfer learning community. Furthermore, the application of SAM into existing gradient-based MTL studies is compatible. It improves the generalizability over various model architectures and tasks.

It is reasonable to assume that by leveraging flat minima on the multi-task learning setting, we could prevent over-fitting issue to the specific task or gradient intervention between different tasks.

The application of SAM on MTL requires some indirect adaptations. e.g. separate update rules for non-shared parts and shared part. The author successfully designed rules for each part. The statement of Theorem 1 is too intuitive, which does not require rigorous proof on it. At the right side of Theorem 1, maximum of maximum is utilized for deriving the upper bound. It is intuitive based on my knowledge.

The analytical decomposition of SAM gradient into 1) loss and 2) flatness parts are not novel at all. It is well known analysis based on existing methods (SAM, GSAM, GAM). Rather, The new modeling parts of SAM-MTL is gradient decomposition on each task and gradient aggregation based on whole tasks. However, i do not get convinced why these gradient decomposition and re-organization are required in the context of multi-task learning. This is not empirically validated by additional ablation studies.

In Figure 4, the author claim that suggested algorithms significantly improves the task-wise flatness than ERM algorithm. What if we conduct simple SAM on MTL, not based on your gradient decomposition and re-organization? I conjecture that the flatness would be similar to SAM-MTL, your method. The extensive comparison with SAM variants (SAM,GSAM,GAM) is required.

Please empirically provide the computation cost increments by applying SAM-MTL. SAM is well known for increasing the computation cost about 2 times than ERM. is there any other increments during the adaptation of SAM-MTL? Please see Weaknesses section.",381,0,2,0.7938000000000001,0.0882617383,0.9497602582,52,33.5264,0.3688,iclr,0.0,3,3,3,2,factual,2,2,60,neutral,3,negative,3,low,5,4,4,5,partially factual,5,5,85,5,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,3,85,polite,5,neutral,4,low
170,Sangeeta-Saha,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors, in the manuscript, have proposed a compartmental epidemic model of dengue transmission where the mosquito biting rate and the transmission rates from host to vector as well as vector to host are assumed to be temperature dependent. The calculations are basic ones and seem to be ok, but there are few points which I need to mention. Firstly, whenever a model is proposed, it is very important to show the biological well-posedness of the system. So, proving the non-negativity and boundedness of the system variables make the base on which the rest of the analysis is performed. Secondly, I am unable to understand how the transmission from mosquito to human depends on the temperature with two types of conditions (noted in equations 15 and 16). It could have been analysed appropriately. Moreover, it is not demonstrated properly how the time variable is connected with the temperature. So, a proper analysis of the second subfigures of each of Figure 2- Figure 4 could improve the work. Also, as per the model assumption, the parameter denoting 'the increase in female mosquito population' should also depend on temperature, but it is chosen as a constant value only. The reason supporting it needs to be mentioned. Altogether I have found the concept interesting, but the mentioned points, if taken care of, will make the work more strong and presentable only.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",373,0,1,0.7553000000000001,0.143034188,0.8884497881000001,546,33.54,0.063,f1000,0.0,2,1,3,3,partially factual,4,4,60,neutral,4,negative,3,moderate,5,4,4,5,partially factual,4,5,75,polite,5,negative,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,3,70,neutral,5,negative,4,low
170,J-H-Arias-Castro,Simulation model for the dynamics of dengue with asymptomatic transmission and the effect of temperature,"Background: One of the fastest spreading vector-borne diseases in tropical and subtropical regions is dengue, which generates cost overruns for public health entities. Several factors can influence the dynamics of dengue virus transmission: environmental and climatic (abundance of vectors), interactions between hosts (infections by asymptomatic individuals), and population immunological factors. Given these conditions, it is necessary to carry out theoretical studies based on meteorological factors and asymptomatic transmission that are associated with both the existence of the vector and its incidence, in order to provide a scientific basis for health entities in decision-making. Methods: A mathematical model based on nonlinear ordinary differential equations is proposed to interpret the dynamics of dengue transmission in humans coupled to the dynamics of the Aedes aegypti species, considering the population of symptomatic and asymptomatic infected humans and the effect of temperature variability. The basic reproduction number was found and some simulation results based on the Runge-Kutta numerical method were obtained. Results: The simulations showed that the temperature had a directly proportional relationship with the basic reproduction number. The cases of infected people and carrier mosquitoes increased when the temperature peaks increased drastically; in low temperatures the infection persisted with low morbidity due to the survival of asymptomatic people. Conclusions: High temperatures tolerable by mosquitoes increase their life expectancy and their numbers in the environment which, together with a reservoir of asymptomatic infected people, leads to a higher incidence of the dengue virus in certain seasons or maintains its circulation in seasons of low temperatures, despite lower vector survival rates.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article aims to analyze the effects of temperature on dengue transmission considering the asymptomatic population. Initially, a model in which some temperature-dependent parameters are considered is presented. But then, the classical analysis of the model is performed without considering the dependence of the parameters on temperature, which simplifies the analysis of the model and puts it in the classical scheme, which practically makes the subject to be treated lose novelty. Additionally, some scenarios are presented in Figures 3 and 4, which turn out to be analogous because they model situations that have no differences, since the equations turn out to be equivalent, in the case of asymptomatic and infected humans.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",255,0,1,0.7286,0.1503623188,0.8830835223,546,22.55,0.0513,f1000,0.0103092783505154,2,1,3,3,partially factual,4,4,60,neutral,4,negative,3,moderate,2,4,3,2,partially factual,3,3,45,impolite,5,negative,5,low,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,4.0,low,2,4,3,2,factual,3,4,60,neutral,4,negative,3,low,2,4,3,3,partially factual,3,3,65,neutral,4,negative,4,low
168,Reviewer-XYg3,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","The paper presents a new framework for semi-supervised domain adaptation (SSDA) that establishes an upper bound on target error. This framework introduces a method called Joint Error-based Triplet Alignment (JTA), which performs alignments not only between the labeled source domain and the unlabeled target domain but also between the labeled source domain and the labeled target domain. As a result, their empirical studies demonstrate that JTA can reduce domain gaps and enhance feature learning by explicitly considering the alignment for the labeled target data. The paper also introduces a dissimilarity metric known as Maximum Cross Margin Discrepancy (MCMD) to bridge the gap between theory and algorithm, ensuring the consistency of the target error bound. The main problem of this paper is the lack of sufficient details to understand and follow their motivation and derivation. Given the promising empirical results presented in the paper, I strongly recommend that the authors consider a complete rewrite of the paper, focusing on delivering a clear and well-motivated presentation. This should involve providing comprehensive derivations with sufficient details or citations, ensuring that each step of each equation is transparently explained for the benefit of the reader's understanding. The performance of the proposed work is promising. 1. I find the paper's motivation unclear. To be specific, the upper bound of the hypothesis regarding the unlabeled target domain should be the most crucial starting point for readers to comprehend what the proposed method aims to address. However, the lack of an explanation for the proof of Equation (1) makes it extremely difficult for me to grasp and follow. Concerning D.1, I am unsure how the first equation of the unlabeled target error bound was derived. If it stems from Ben David's theorem (assuming my recollection is accurate, Ben David did not derive any error bound under semi-supervised settings) or the work of others, it would be beneficial to provide citations so that readers can fully contextualize and understand the subject matter.

2. What is the source of the intractability, particularly for f_{S} and f_{V}? Given that both S and V are fully labeled, it seems reasonable to assume that a straightforward optimization approach like empirical risk minimization (ERM) could yield a reasonable approximation for f_{S} and f_{V). The mention of intractability is often made within the framework of variational inference, where certain integrations cannot be feasibly solved. Providing a clear explanation of this intractability would significantly enhance the paper's motivation.

3. How is the reduction of the error term achieved between two fixed true labeling functions? I want to emphasize that ""true"" here means unchanging or fixed. The paper is proving a complex upper bound derivation, and its clarity is hindered by inconsistent definitions throughout, making it difficult to follow.

4. The t-SNE visualization, without any indications of the class labels for each data sample, fails to convey meaningful information. In fact, I find the t-SNE visualization rather perplexing. I recommend that the authors consider sharing the code for their implementation with the reviewers. This would serve not only to confirm the reproducibility of their work but also to enhance the reviewers' understanding of the proposed methodology.

5. The experimental setup lacks clarity, particularly in the context of semi-supervised domain adaptation, where the number of labeled target samples and the way to select the labeled target sample are crucial. It is important to provide sufficient details regarding the sample selection process. 

6. The authors assert that \[1\] violates the triangle inequality without providing a thorough explanation or derivation. This is a strong claim, as it implies \[1\] is a departure from well-established theoretical foundations, especially considering that \[1\] is published on a top tire. To support their claim, the authors should conduct in-depth elaboration and studies.

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019. 1. Could you please clarify what is meant by the conditional distribution referred to in Section 3.1? To be specific, which random variables are conditioned on which other random variables? Based on the authors’ preliminary at the beginning of the section that both f_{S} and f_{V} are true labeling functions (true means fixed and deterministic). Meanwhile, I am confused by the idea of describing a mapping function (mapping function is normally deterministic) as a distribution (sampling from a distribution is stochastic). How come a stochastic term can be used to describe a deterministic notation? Can you elaborate on this?

2. To me, the loss introduced in this work appears to be an extension of the one (MDD) presented in \[1\] to the semi-supervised setting. I would appreciate it if the authors could offer a comprehensive discussion outlining the primary distinctions between \[1\] and their proposed approach, excluding the consideration of the semi-supervised setting and the violation of the triangle inequality. 

### Reference

\[1\] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 7404–7413. PMLR, 2019.",849,7,12,0.7813,0.0869150691,0.9562900662,49,35.4208,0.9511,iclr,0.0,5,4,4,5,factual,4,4,85,neutral,4,negative,4,none,5,5,5,5,partially factual,5,5,88,polite,5,negative,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,5,5,5,5,factual,5,5,95,polite,5,negative,5,none,5,4,4,4,partially factual,4,3,85,neutral,5,negative,5,low
115,Reviewer-DYrH,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","This paper focused on large language models' causal and moral intuitions and investigated the alignment between LLMs and humans' causal and moral judgments. For this purpose, the authors collected story datasets from the field of cognitive science and manually annotated each story with human judgments and underlying latent factors. Based on this dataset, a diverse range of LLMs with different model scales and training methods are evaluated. The authors then statistically revealed that LLMs weigh factors differently than humans, indicating divergent implicit preferences and emphasizing the importance of curated datasets and cognitive science insights in understanding model preferences and alignment. * This paper is well-motivated by philosophy and cognitive science and focused on an exciting topic, LLMs' causal and moral intuitions. Such an interdisciplinary insight would benefit the better understanding of LLMs' behaviours.
* The authors summarized a systematic framework of the underly latent factors of casual and moral judgements based on cognitive science, which might help improve the interpretability of LLMs.
* The constructed judgment dataset is high-quality, with a well-designed annotation protocol and high inter-rater agreement (>0.8).
* The authors benchmarked the alignment level between humans and a wide range of LLMs. They also conducted comprehensive analyses and made inspiring conclusions like those in Sec. 4.2.2, e.g., differences in Benefits. * The constructed dataset is too small, and the coverage is limited. Two hundred six instances are highly insufficient to investigate LLMs' properties which might make the conclusion biased. This can be observed in Table 3 (a). The relatively high bootstrapped confidence interval indicates a high variance and unreliable results. This is my biggest concern of this work.

* Some essential results need more in-depth analysis and explanation. (1) The unnatural results in Table 3(a) need more analysis. Why did the aligned and larger Alpaca-7B get lower Agg than GPT3-curie-v1 on Causal Judgement? Why did davinci-002 outperform the well-aligned davinci-003 on moral judgement？ (2) The authors should provide some (even initial) analysis of the differences introduced in Sec. 4.2.2 though they are attractive. * How do you explain the unnatural results in Table 3(a): the aligned and larger Alpaca-7B got lower Agg than GPT3-curie-v1 on Causal Judgement; GPT-4 performed even worse than davinci-003 on Causal Judgement; davinci-002 outperformed the well-aligned davinci-003 on moral judgement.
* Would you release your Judgement dataset? The authors have discussed the ethical considerations in Appendix A. However, the authors should also include more discussions of limitations, like the small dataset and variance of the results, as stated above.",415,0,0,0.801,0.0800793651,0.9510885477,215,34.414,0.33,neurips,0.01010101010101,4,4,4,4,factual,3,4,84,neutral,4,negative,4,low,4,4,4,4,factual,4,4,85,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
154,Reviewer-5GRr,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","This paper studies a classic problem of recovering clusters in a random graph. Concretely, the authors consider the stochastic block model. Here there is an underlying graph on n nodes. The n nodes are partitioned into k unknown clusters. There is then an edge independently between any two nodes in the same cluster with probability p and between any two in different clusters with probability q < p.

This is an extensively studied problem and many algorithms have been designed that allow the recovery of all clusters of a reasonable size (somewhat larger than sqrt(n), which is anyway a requirement for computational efficiency under the planted clique conjecture). The previous state of the art allows recovering clusters under two assumptions (here simplified for clarity and brevity):

1. The clusters to recover have size at least max(sqrt(n), k)/(p-q).
2. There is a number alpha of about sqrt(n)/(p-q) such that no cluster has size in the interval \[alpha/C, alpha\] for a constant C.

The assumption that the cluster sizes are at least sqrt(n) for those to be recovered is natural as mentioned above. However, the dependency on k is unfortunate when there are many small clusters. These would prevent the recovery of medium sizes clusters when k >> sqrt(n). Secondly, the assumption about the empty interval is quite unnatural.

The main contribution of this work is to remove the dependency on k in 1. and to remove the assumption 2. all together.

The authors also present applications of their algorithm in the related problem of clustering with a faulty oracle. Here one can ask whether two nodes v, w are in the same cluster or not. One is then returned a noise answer. Here the paper also improves over the state of the art in terms of the cardinality of clusters that can be recovered. -The problem studied is fundamental in graph clustering.
-Removing the dependency on k and the requirement of an empty interval of cluster sizes is significant and the algorithm guarantees of the algorithm much more natural than previously
-The authors have implemented their algorithm and compared experimentally to previous work. The comparison is overall in favour of the new algorithm. -I know this is a theoretical contribution, and also the authors probably did not attempt to optimize constants that much, but a factor 2^13 in the guarantees is quite severe in practice. Hopefully and probably, this constant is smaller in practice. -Could you say a bit about the running time of your algorithm in practice compared to previous work?
-Can you comment on whether the 2^13 constant can be reduced to a more reasonable constant without too much effort? Yes",442,0,2,0.7448,0.0261784512,0.9240825176,221,47.9531,0.1585,neurips,0.0,3,4,4,4,factual,4,3,85,polite,4,neutral,4,low,4,5,4,4,factual,5,5,88,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,5,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
154,Reviewer-Ajy4,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","The paper deals with the problem of community detection for unbalanced community sizes. Specifically, the paper concentrates on the situation where both large (O(\sqrt{n})) and small communities exist in the network. The paper proposes a stepwise method of recovering the large clusters in the presence of small clusters for planted clique SBM and faulty oracle models. The main strengths of the paper are as follows - 

(1) The paper addresses a gap in the literature on the simultaneous recovery of large and small communities in networks.

(2) The paper deals with the problem of community recovery of large communities in the presence of small communities. The paper provides a stepwise method of recovering large communities in planted clique SBM and faulty oracle models.

(3) The paper provides theoretical results supporting the recovery of large communities by overcoming the ""small cluster barrier"" of the size of the remaining small clusters.

(4) The paper is well-written. The main weaknesses of the paper are as follows - 

(1) The paper misses some relevant literature. Such as - Li, Tianxi, et.al. ""Hierarchical community detection by recursive partitioning."" Journal of the American Statistical Association 117, no. 538 (2022): 951-968. It describes an algorithm that is very similar to the algorithm proposed in this work.

(2) Algorithms 2 and 3 assumes the knowledge of p and q, which are very strong assumptions. It is not immediately clear how the algorithm can be extended for general SBM.

(3) The stopping criterion of the proposed algorithm is not clear. 
 (1) Does the proposed algorithm assume the knowledge of p and q?

(2) Does the proposed algorithm assume the knowledge of the number of communities, or is there a stopping criteria of the proposed algorithm for recovery of the number of large communities? N/A",295,1,2,0.6443,0.0658666667,0.9451859593,221,42.8963,0.1041,neurips,0.0,3,3,4,4,factual,2,4,75,neutral,4,negative,4,low,2,3,3,3,partially factual,4,4,55,polite,4,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
27,Prajwal-Ghimire,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors have presented a rare case report of a well recognised drug induced neurologic emergency of Neuroleptic malignant syndrome due to Ziprasidone. Sedhai et al. have highlighted major challenges and salient points during management of these conditions including the current knowledge regarding its pathophysiology. The case report raises the awareness regarding this potentially life-threatening condition during use of an emerging drug which is now more commonly used for neuro-psychiatric conditions of schizophrenia and bipolar disorders. The case report is well written and highlights the current knowledge and brief literature review in the discussion section with relevant references. It certainly adds a vital information regarding the drug to the current available knowledge in the literature.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",249,0,2,0.7907000000000001,0.0816707718,0.8509092927,16,23.16,0.0999,f1000,0.0108695652173913,2,4,2,2,partially factual,3,2,50,polite,3,positive,3,moderate,2,5,4,3,factual,5,5,80,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,5,4,2,factual,4,4,4,polite,5,positive,4,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
151,Reviewer-U9Yx,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The authors introduce rare event sampling via normalizing flows. For this they parameterize the rare event set via a function $g$ such that the rare event set is the set of points where $g \leq 0$. Then they introduce a sequence of decreasing sets $\Omega_{a_i}$ such that this goes to $\Omega$ for $i = M$. Now a normalizing flow is trained for approximate each set $\Omega_{a_i}$, which corresponds to a temperature schedule for the rare event probability measure.  The normalizing flows are trained each on their own using the reverse KL and then the weights up to flow $i-1$ are frozen for training the flow $i$. The approach is benchmarked against other rare event sampling methods such as SUS, SIR, .. on toy examples of varying information. The paper does a good job at explaining its approach. The experimental results seem impressive and its design choices seem well-motivated via ablation studies. Furthermore, using a normalizing flows makes a lot of sense for this kind of task. 1) I am not convinced of the novelty of this approach. This paper mostly cites pre 2021 papers. Please clarify the relation to more modern approaches such as \[1,2\]. 

2) The flows are trained with the reverse KL. This comes with some caveats. First one assumes differentiability of the function $g$. Please comment on whether this is realistic. Furthermore, the reverse KL is known to be mode seeking. I think for most applications in the field of rare event sampling it is crucial to cover all the modes of a density. There has been some recent line of work for normalizing flows such as \[3\] to overcome this but this seems like a major limitation. 

3) Similarly, the evaluation should also include some measure of the distance to the true measure and not only the estimated probability. As far as I understand the paper, this should be possible. 

4) Please also cite relevant papers such as \[4\], who introduced a kind of log det schedule for covering multimodal distributions, which I think is related to way the different $\Omega_{a_i}$ are constructed. 

5) This paper does not come with any code. Do the authors intend to make their code public? Appendix C does not suffice for reproducibility in my opinion. 

6) The heuristic why MCMC wont cut it for this problem makes sense for vanilla MH. But if one takes gradient informed steps such as HMC or MALA, I am not sure why this rationale outlined in section 3.3 should hold true. What is the proposal for MCMC taken in the experiments? 

\[1\] A Flow-Based Generative Model for Rare-Event Simulation, Gibson et al 

\[2\] Conditioning Normalizing Flows for Rare Event Sampling, Falkner et al 

\[3\] Flow Annealed Importance Sampling Bootstrap, Midgley et al. 

\[4\] Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modal Solution Characterization for Computational Imaging , Sun et al. See weaknesses. I think the paper follows a nice idea, has several benchmarks, but does a poor job at literature review. Also I think uploading the code is very important for reproducibility, since this paper is mostly applied.",513,7,1,0.7976000000000001,0.1984201389,0.9326137304,55,57.4112,0.2119,iclr,0.0,5,4,4,5,factual,4,4,77,neutral,3,negative,5,low,5,5,4,5,factual,4,5,85,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
198,Reviewer-TxAH,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","The authors propose a new input mixup method for node classification problems. The proposed method, known as iGraphMix, generates virtual nodes by interpolating input features. The edges of these virtual nodes are generated by sampling neighboring nodes. The authors provide theoretical analysis to show that iGraphMix leads to better generalization performance compared to that without augmentation. S1. The proposed method is easy to understand. 

S2. The authors conduct extensive experiments to show that their proposed method outperforms multiple baselines. 

S3. The authors provide a theoretical analysis of the generalization gap. W1. The improvement of iGraphMix is marginal. Overall, the improvement beyond the second-best method is always less than 1%. I suggest the authors conduct experiments on more challenging datasets to make the result more convincing. 

W2. How do the authors compute the generalization gap in Sec. 6.2? Why the test loss of iGraphMix is much higher than the ""no augmentation""?

W3. In Appendix B, how can this $AX=AX'=A\tilde{X}$ holds? It would be much better if the authors could provide a rough proof idea before presenting all the details. 

W4. The baselines compared are all very simple methods. There are more advanced graph data augmentation methods to compare with, such as \[1\].

W5. There are many existing graph mixup methods for graph classification tasks. It would be nice to add a discussion to better place this work in the literature.

\[1\] Kong, Kezhi, et al. ""Robust optimization as data augmentation for large-scale graphs."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022. Q1. I don't understand why iGraphMix is versatile with other augmentation methods. Can authors provide more explanations? 

Q2. Why do authors only use the Micro-F1 score as the only metric? Accuracy is a more common choice. 

Q3. Does iGraphMix train GNNs using all virtual nodes, like how it is done in Mixup? In other words, no original nodes are used during training.",316,2,13,0.797,0.194235322,0.922558248,75,51.1893,0.1249,iclr,0.0,5,4,4,4,factual,4,4,85,polite,3,neutral,5,low,4,4,5,4,5,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
7,Reviewer-Yoim,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper presents a novel approach called Probability Tree State Abstraction (PTSA) to improve the efficiency of Monte Carlo Tree Search (MCTS) algorithms, which have shown remarkable performance in challenging tasks. The computational complexity of MCTS algorithms is influenced by the size of the search space, and the proposed PTSA algorithm aims to address this issue. The algorithm introduces a general tree state abstraction with path transitivity, which helps in reducing the number of mistakes during the aggregation step. The theoretical guarantees of transitivity and aggregation error bound are also provided. The PTSA algorithm is integrated with state-of-the-art MCTS-based algorithms, including Sampled MuZero and Gumbel MuZero, and experimental results on various tasks demonstrate its effectiveness. The PTSA algorithm accelerates the training process of these algorithms, achieving a search space reduction of 10% to 45%.
 1. The approach of aggregation considers the entire path, not only a state, is novel and unique.

2. The PTSA algorithm presented in this paper can be applied with any other state abstraction functions mentioned in previous studies, in a general way.

3. The paper provides extensive experimental data. It includes environments such as Atari games, as well as tasks with continuous action spaces like CartPole and LunarLander, and board games like Gomoku. The rich variety of experimental environments demonstrates the effectiveness of the proposed method across various tasks.

4. Integrate PTSA with state-of-the-art algorithms can achieve comparable performance with smaller branching factors. In other words, PTSA provides a more efficient method with less computational cost.
 1. The meaning of probability in PTSA (in Definition 4.3) is not well-defined and requires further clarification. This will be addressed in the Questions section below.

2. There are some errors in the proofs presented. This will be discussed in detail in the Questions section as well.
 1. Why does $v_0.pruning$ do in line 17 in Algorithm 1? Any difference from $S_L.delete(b_j)$. 

2. What role does the probability $\mathbb{P}$ in Definition 4.3 play in Algorithm 1?And, how to calculate $\phi$ in line 15 in Algorithm 1? In other words, when does $\phi(b_i)=\phi(b_s)$ hold true? Are both related? 

3. In your paper, you mentioned a previous work titled ""Monte Carlo Tree Search with Iteratively Refining State Abstractions."" That method directly calculates the distance between states and performs aggregation if the distance, denoted as $d(s_1, s_2)$, is below a threshold. This approach differs from the method proposed in your paper, but both aim to reduce the branching factor of MCTS. Have you conducted any experiments comparing your method with the approach mentioned above? I couldn't find any analysis of that method in Table 1 or the experimental section below. Some insight into the reason for this omission should be provided. 

4. This paper mentioned “reduce the computation time” with abstraction. My question (or curiosity) is how much overhead the checking operations (in Lines 14 and 15) incur. Note that in line 207 there is a time complexity which is required to be described in more detail, like $\log N_s$. 

5. Equation (19) in the appendix is written incorrectly. Need to be fixed. For example, the final $p_{bM}(b_2, b_3)$ should be $p_{bM}(b_1, b_3)$. Also fix some other wrong indices in (19). 
 N.A.",529,0,12,0.7856000000000001,0.077027027,0.9623354077,216,43.858,0.1879,neurips,0.0,4,4,3,4,factual,4,3,80,polite,4,positive,3,low,5,4,4,5,partially factual,5,5,85,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,5,4.0,4.0,80.0,4,5.0,3,5.0,2,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
102,Reviewer-n6Pj,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This article addresses two issues:

1. The low efficiency of the LonvConv-based model during inference.
2. Whether it is advantageous to perform independent long convolutions on each channel or reduce the total number of filters without loss in quality.

To tackle problem (1), the authors propose distilling the LonvConv into a Diagonal State space model and train it using the $\ell_2$ loss function.

For problem (2), the authors suggest sharing long convolution coefficients across multiple channels, resulting in the MultiHyena structure.

The effectiveness of the proposed methods is validated on multiple datasets. Distilling LonvConv into a Diagonal State space model is indeed a novel and meaningful approach. The conclusion of sharing long convolution coefficients across multiple channels is also innovative. I think the main issue with this article lies in the focus of the writing. From Equation 3.4, it is clear that the ultimate goal is to represent the LonvConv coefficients using a Diagonal State Space Model. However, a significant portion of the article is spent describing unrelated aspects. The most crucial part, determining the hidden dimension of the State Space Model, is merely brushed over, even though it is the key factor that affects the quality of the distillation and the efficiency of the final inference. On the other hand, the motivation behind the design of MultiHyena should be addressed in the main text since it is of utmost importance. 1. The method for determining the hidden dimension of the State Space Model needs to be explained in more detail. I referred to Appendix E.3.2, and I'm wondering if the core idea is to perform an SVD decomposition and then select the dimension for dimensionality reduction based on the eigenvalues?

2. The solution to Equation 3.4 requires a more detailed algorithm description or pseudocode to help readers follow along. I attempted to replicate it following Appendix B.1, but the results were not quite good. Could you provide the training configurations and an estimate of the training time?

3. The motivation behind the design of MultiHyena should be included in the main text, and ablation studies (sharing coefficients vs not sharing coefficients) should be conducted to validate the design's rationale. The experiments should compare the effects and speeds.

4. Does MultiHyena utilize the Local conv + Global conv structure of Hyena? If so, how many layers are used? This should be mentioned in the experiments.

5. Is the Algorithm 1 on page 28 is the implementation of Figure 4.1?

6. Regarding the implementation of MultiHyena, in Algorithm 1 on page 28, for $z^m_t \in \mathbb R^{L\times N\times N}$, what does the subscript $t$ represent? On the other hand, is the shape of $T_h$ $L$ (all features share one set of convolution coefficients) or $L\times N\times N$ (each feature has independent convolution coefficients)?

7. Continuing with the implementation of MultiHyena, in Algorithm 1 on page 28, is the shape of $T_hz^m_t$ ${L\times N\times N}$? If so, does $T_h(z^m_t)  q_t^m$ mean performing matrix multiplication between $\[T_h(z^m_t)\]_i \in \mathbb R^{N\times N}$ (for $i=0,\ldots,L-1$) and $q_t^m\in \mathbb R^{N}$, resulting in an output of shape $\mathbb R^{N}$? If not, what is the computation like?

8. The algorithm's output is $\bar{y} \leftarrow\left(\sum_m\right) y^m / M\in \mathbb R^{L\times N}$, while the input has a shape of $L\times D$. Is this inconsistency problematic, or did the algorithm omit something?

9. Algorithm 1 has several ambiguities. It is suggested to reorganize it for better clarity. Yes.",567,0,11,0.7288,0.0885162602,0.8189634085,215,44.2964,0.5533,neurips,0.025,4,4,4,4,factual,4,4,85,polite,4,neutral,4,low,5,4,4,5,5,5,5,85,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
102,Reviewer-4kyN,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This paper proposes an approach that enables constant-memory, recurrent inference for long convolution architectures. They introduce LaughingHyena, a distilation approach that consists of extracting compact linear SSMs from each convolution layer. Combined with weight-tying, it results in state-of-the-art performance and efficiency (i.e. throughput) without any drop in quality.  The paper is well structured and written.

The approach seems sound, reasonable and is performant The models used in most experiments are small.

The helm evaluation is not very convincing. Is it possible to benchmark against more recent open source models such as Llamma? Broader Impacts section is missing.",97,0,0,0.8150000000000001,0.0756410256,0.8799487948,215,35.4172,0.143,neurips,0.0238095238095238,2,4,2,2,factual,3,3,60,polite,4,positive,2,moderate,4,4,4,4,partially factual,4,4,75,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
180,Vivek-Gupta,To study the utility of tumor budding as a histopathological marker in comparison to various histopathological parameters and TNM staging in breast carcinoma,"Background Breast cancer is the leading cause of death in Indian females. Detection of breast cancer in later stages leads to poorer prognosis and therefore decreases patient survival. Various new modalities such as mammography and USG guided FNACs are developed and many new markers are available to diagnose breast cancer; however, tumour budding is a cost-effective method which can be helpful in early diagnosis. Tumour buds are found to have a positive correlation with various histopathological prognostic markers in breast cancer. The present study will be conducted to evaluate tumour buds as a prognostic marker in breast cancer. This study aims to compare tumour budding with histopathological prognostic markers, TNM staging and IHC phenotypes.  Methods The study will be observational, cross- sectional, and prospective, will include 60 cases and will be conducted at Jawaharlal Nehru Medical College (JNMC) Wardha in the Pathology Department.  Results Data will be collected and combined together over a period of two years and will be analysed statistically for tumour budding as a marker and its correlation with breast prognosis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The rationale for the study is well-defined and has clarity. It highlights the gap in the literature and the research question. The objectives are in sequence and lead to clarity in assessing the tumor bud in breast carcinoma. Objective 4 needs to be reframed to “Assessing the tumor bud status in carcinoma breast.” The histopathological examination may be removed as the same has already been mentioned in earlier objectives. The study design is apt for study. It mentions inclusion and exclusion. They have graded tumor budding as ≤ 4/10 HPF – low tumor budding and > 4/10 HPF – high tumor budding. However, it can be graded as ≤ 4/10 HPF, 4 – 9/10 HPF, and >10/10 HPF. An optimal cut-off for the number of tumor budding and lymph node metastasis can also be correlated. The protocol provides sufficient details for the evaluation of tumor budding. Microscopic pictures of high and low tumor buds can be more effective.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Not applicable",273,0,1,0.6864,0.146091954,0.8472419977000001,23,47.08,0.0999,f1000,0.0098039215686274,3,3,3,3,partially factual,3,3,70,polite,4,neutral,3,low,4,4,4,5,factual,5,5,85,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
82,Reviewer-NDsr,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper studies robust first order optimization in a challenging setting where noise may be unbounded, a setting that arises often for real world optimization problems. Because this problem is intractable in general, one needs to make plausible assumptions on the noise, that are realistic on one hand but allow for efficient analysis.

The noise model proposed here allows for noise to be unbounded, and introduces two simple constraints:

1. The unbounded noise when computing a gradient is *oblivious* in the following sense: there are two noise components, one that is well-behaved (zero mean and bounded variation), and one that is unbounded but oblivious/indpendent of both the location in which gradient is computed and the value of the well-behaved part of the noise.</li>

2. We assume the unbounded noise has probability bounded away from 0 to be equal to zero (i.e., to not exist at all).

It turns out that these two relatively weak conditions allow for efficient robust first order optimization. Specifically, these conditions allow for list-decodable robust optimization, where the goal is to output a list of candidate outputs where at least one should be a good approximation of the correct optimization outcome. The main technical result shows how to solve this problem by reducing it to list-decodable mean estimation, a problem that enjoyed substantial progress in recent years. The authors also show a reduction in the opposite direction. A substantial component in the technical analysis is a procedure that the authors develop for location estimation in an appropriate noisy setting. 1. Interesting and important goal, of better understanding the beyond worst case landscape for (first order) optimization.

2. Writing is very clear and relatively easy to follow for me (a non-expert outsider). 

3. The assumptions required for the analysis are weak and seemingly realistic. 1. The technical novelty is perhaps somewhat limited, the work relies heavily on reductions to existing results in robust mean estimation.

 Comment: my review is a low-confidence one (as a non expert in the field) and I may have missed central points in the paper, so may update the score after subsequent reviewers and authors discussions. N/A",354,0,5,0.7719,0.078497426,0.9102973938,215,36.2086,0.6247,neurips,0.01,1,2,1,1,unfactual,3,1,30,polite,1,neutral,1,high,2,4,4,3,partially factual,4,4,65,polite,4,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,2,factual,4,3,60,polite,4,positive,4,low,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
126,Reviewer-eWjQ,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","The paper explores reasons for this performance gap between neural and symbolic methods in NetHack:
Symbolic agents use hierarchical policies and parsers to extract high-level features
Symbolic agents have handcrafted heuristics and error correction
Neural agents lack inductive biases like hierarchy that may be needed for sparse rewards
Experiments show hierarchy, scale, and combining imitation and RL help improve neural agents:
Hierarchical behavior cloning improves over flat BC
Larger Transformer-based architectures improve over LSTMs
RL fine-tuning provides gains, especially for underfitting models
But significant gaps to symbolic agents remain The experimental design is very clever, the chart is very clear, and the experimental effect is obvious. The paper explores a novel problem domain of applying neural networks to master the game NetHack, where current methods struggle compared to symbolic AI. The authors introduce a new large-scale dataset of NetHack demonstrations called HiHack to facilitate this analysis. The idea of using demonstrations to help neural networks learn better policies in sparse, long-horizon environments like NetHack is creative.The methods are detailed appropriately to replicate experiments. Results are presented logically and incorporate useful visualizations. The conclusion summarizes takeaways concisely.Mastering complex environments like NetHack with sparse rewards and long time horizons remains an open challenge for deep RL. This paper provides significant evidence and analysis characterizing the limitations of current neural network methods in these settings, and points the way towards progress, whether via incorporating stronger inductive biases like hierarchy or combining neural and symbolic approaches. The insights will broadly impact research in sparse reward RL, imitation learning, and integrating neural and classical AI. This model is based on the nethack, and the results hold up on the above models, and whether the above results can still hold up on the other models。The authors recognize the limited generality so far of methods tested on NetHack to other complex environments.No obvious harmful biases or problematic data sources are introduced in this work. The NetHack environment itself seems relatively innocuous.
 Can you add some experiments, add some theoretical derivation, whether the contribution of this article is more. The model is not so representative, can switch a more popular model。Overall, the authors demonstrate good care and thoughtfulness regarding the limitations and potential negative impacts of this research direction. The discussion seems sufficient without being overreaching or distracting from the primary technical contributions. I do not have any major suggestions for improvement.",394,0,0,0.8136,0.0944551101,0.9210098386,232,17.9759,0.3146,neurips,0.0123456790123457,2,3,2,1,unfactual,3,2,60,neutral,2,positive,2,high,3,4,4,4,5,5,5,85,polite,5,positive,5,moderate,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
3,Katherine-Thornton,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n","The paper A Shape Expression approach for assessing the quality of Linked Open Data in Libraries is a strong candidate for publication in the Special Issue Cultural Heritage 2021. I recommend that it is ready to be accepted for publication. The manuscript is original in that it is the first discussion of validating bibliographic data in RDF using ShEx. Many interactive examples are presented and readers can try out ShEx validation for themselves to more fully understand the points the authors make in the paper.  The importance of this paper is that it addresses a practical application of semantic web technologies to a real-life workflow issue of validation of bibliographic data in RDF. The usefulness of this paper is high in that the online validation examples are practical for others to consult and see in action. Data from several organizations can be validated using the pre-composed manifests and schemas. This will help readers understand the utility of creating quality assessment pipelines in additional contexts.  The relevance of this paper is very high because many libraries are interested in converting some of their bibliographic data to RDF and are looking for useful tooling.  The stability of the validation workflow depends on an external tool, the ShEx2 Simple Online Validator. This tool has been available on the web for several years, if it remains available then the example manifests and schemas will continue to be working examples. In my opinion many readers interested in the Special Issue on Cultural Heritage and the Semantic Web will find this paper valuable.",257,0,0,0.7855000000000001,0.2496247619,0.9288473129,87,32.83,0.1858,semanticweb,0.0,3,4,4,3,factual,3,3,70,polite,4,positive,4,low,3,5,4,3,factual,3,4,85,polite,5,positive,5,none,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,1,4,3,1,factual,4,3,60,polite,5,positive,4,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
11,Palwinder-Singh,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This reviewer read the manuscript with interest and rather several times to see what new science has been explored. To my surprise the subject is the compound 1,3-bis(p-hydroxyphenyl)urea. Some of the questions that arise are: Why did the authors choose this compound for the study?  What is the rationale for the selection of 1,3-bis(p-hydroxyphenyl)urea?  Endless data has been recorded by the authors. What reference drug/compound was used? Diclofenac is a COX-1/2 non-selective NSAID. In the first paragraph of ‘Results’ section, it is not clear whether the urea derivative is more potent than diclofenac or not.  Is it not possible to calculate IC50 for this urea derivative against COX-1 and COX-2?  Since this compound has already been studied for its analgesic effect, are  the results of the present study comparable to those already reported?  What exactly is the mode of action of this urea derivative? Does it act through COX-2 inhibition or some other pathway?  What about the COX-1, COX-2 selectivity?  In the light of above mentioned issues, this reviewer is not in favour of indexing this manuscript until the objectives are clear.  Is the work clearly and accurately presented and does it cite the current literature? No  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? No  Are the conclusions drawn adequately supported by the results? No",333,0,1,0.7545000000000001,0.1541341991,0.7552205324,62,39.84,0.145,f1000,0.0,5,4,1,5,partially factual,5,4,55,polite,5,negative,3,none,2,5,4,2,partially factual,5,5,55,neutral,5,negative,5,low,1.0,2.0,2.0,1.0,unfactual,2.0,1.0,20.0,impolite,3.0,negative,3.0,moderate,3,3,4,2,factual,3,4,60,neutral,4,negative,4,low,4,4,4,4,partially factual,3,3,75,neutral,5,negative,4,low
192,Mirko-Spasić,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include : (1) originality: The paper is not outstandingly original but provides some insights into the benchmarking topics (2) significance of the results: Results presented in the article are almost useless (will be explained later) (3) quality of writing: Quite good, couple of typos General remarks: The paper proposes the new technology-agnostic benchmark that tests fundamental data operations that are of interest for a warehouse scenario, and that should be used for evaluation of the different storage solutions. So, the main feature of this benchmark should be fairness and justice in order to facilitate the developers' selection of the most suitable technology. In order to achieve that goal, the benchmark should not favor any of the storage solution by wrongly chosen performance metric and selected queries that are not equivalent among storage solutions. As the benchmark is designed to evaluate relational DBMSs, No-SQL, and triple stores, so the queries have to be in different languages (SQL, SPARQL), but still equivalent in their semantic and complexity. This is not the case here, and will be explained in details. [Sec 1]: In the Introduction, there is a statement that there is no benchmark that combines 4 mentioned properties. Actually, there is: LDBC Social Network Benchmark. It test fundamental data operations, it is technology agnostic, evaluates relations DBMSes, No-SQL, triple stores, graph database systems, etc, and it operates on synthetic datasets, that mimic all real-world characteristics. [Sec 2.8]: If a computer has 16GB of RAM, it is not good idea to give all of them to the database system. In order to start Virtuoso server, it is necessary to have the virtuoso.ini file in the current directory. If that is not the case, and you start the server in foreground (just like author mentioned with +foreground option), it is not true that there is no error message. You will see: ""There is no configuration file virtuoso.ini"". Some of the parameters are used with '+', but some of them are supposed to be used with '-', e.g. (-f which is the same as +foreground). [Sec 3]: The performance metric doesn't make sense. I don't see the reason why the preparation time will affect performance score in the following equation: performance(database, queryscenario, testseries) = (prepare + execution1[+execution2 + execution3])/3. For example, in the RDBMS, in the preparation step we have creation of the indices, and there is no such use case scenario where we will drop index before execution of each query, and build it over and over again. Usually, these indices are build once, before or after loading the data, and these times should affect loading times, not query execution times. But, on the other side, the preparation phase for triple stores for almost all query scenarios does not exist, and all of these measurements for Fuseki and Virtuoso are almost 0. Building indices will take a lot of time (couple of seconds for MEDIUM test series). This is not fair and it is triple-store biased. This is the reason why author considered Virtuoso as ""the best aggregation performer"" in Section 4.5, and it is not true at all that ""Virtuoso already stores atomic field information instead of complete records"", as the author stated.  For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_ALL Test Series MEDIUM, the query execution times are: SQLite-Xerial 1112.13 ms PostgreSQL    1592.18 ms Virtuoso      3018.93 ms but in figure 4b you presented PostgreSQL as the best performer (1.0), followed by Virtuoso (1.11) and then by SQLite-Xerial (2.18). The reason for this is the preparation time. It is very similar in all the other query scenarios. For example, in AGGREGATE_PUBLICATIONS_PER_PUBLISHER_TOP10, Virtuoso was slightly faster than SQLite-Xerial, and for one order of magnitude faster than ArangoDB, but that cannot be seen from the performance metric: Virtuoso (1.0), SQLite-Xerial (3.63) and ArangoDB (7.05). [Sec 4]: A lot of observations from this section cannot be valid because of the wrongly chosen performance metric. [Sec 4.1]: Errors_Virtuoso_SMALL.txt: This is not a bug in Virtuoso, this is the configuration issue. You should increase max vector length setting in virtuoso.ini file. It is the same problem reported in Errors_Virtuoso_MEDIUM.txt. Virtuoso is well known because of its scalability, so the issue reported in Errors_Virtuoso_LARGE.txt stops it from competition on this scale factor. It would be better to fix the syntax of RDF file, and repeat the experiment than excluding Virtuoso from this part of game. [Sec 4.3]: In the entity retrieval query scenario, there are two main problems. The first one lies in the fact that the SQL queries executed against relational DBMSs are not equivalent to the SPARQL queries, while the second one is the use of DESCRIBE query statement, which is not strictly specified in the W3C specification. DESCRIBE may produce quite different results depending on describe-mode. I would not recommend using constructs that are not strictly defined by the standard. The author uses the following query: describe * where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ##ids## )) } This is similar to: select ?s ?p ?o where {   {     ?s ?p ?o .     ?s  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   }   UNION   {     ?s ?p ?o .     ?o  ?identifier .     FILTER( ?identifier IN ( ##ids## ))   } } which is much more complicated than the relational query: select * from justatable where dcterms_identifier in (?); So, this is unfair against triple stores, and favors relational DBMSs. The equivalent query should be: select ?s ?p ?o where {   ?s ?p ?o .   ?s  ?identifier .   FILTER( ?identifier IN ( ""011363517"" )) } All of these queries will be executed by Virtuoso (on my computer which has similar power to the used one, same configurations, Test Series MEDIUM) in 1-2ms, while the author's proposed SELECT statement in Listing 1, will take about 7s. So, this is very unfair to Virtuoso. In this query scenario, the ordering is not mentioned anywhere, so the Virtuoso's bug referenced in [9] doesn't affect this query at all. [Sec 4.4]: In the Conditional Table Scan scenario, the relational DBMSs are favored at the same way as in the previous section. The needed query should be: select ?s ?p ?o where {   ?s   .   ?s ?p ?o } instead of: describe * where { 	?s ?o ?p . 	optional { ?s  ?type . } 	?s   . } The first query will run by Virtuoso in 300s (on my computer, as explained before), which is comparable to the relational systems. The second conditional query should be: select ?s ?p ?o where {   ?s  ?title .   filter regex(?title, 'stud(ie|y)', 'i') .   ?s ?p ?o. } which will run much faster than the query executed against Virtuoso. Queries executed against Fuseki, are not correct either. The pattern: optional { ?s  ?type . } is not needed at all, while the pattern optional { ?s  ?title . } should not be optional, as there is the following filter: filter regex(?title, 'stud(ie|y)', 'i') . Similar remarks stay in the 3rd conditional query. [Sec 4.5]: In the Aggregation section, queries are comparable, but the conclusions are not (see remarks about performance metric) [Sec 5]: Because all of the aforementioned remarks, this section is quite wrong. The author said that Virtuoso was well in the certain deletion scenarios, e.g. DELETE_LOW_SELECTIVITY_PAPER_MEDIUM - Test Series MEDIUM, but the reason for that lies in the fact that UPDATE_LOW_SELECTIVITY_PAPER_MEDIUM finished with an error, and there was no triple that should be deleted in this scenario. Minor technical issues: page 3: Do not reference pages (e.g. see page 4), instead of that use tables, figures, etc... page 5: rephrase the following: ""Table 3 provides an overview of characteristic properties these databases""",1288,1,1,0.7519,-0.0184439478,0.8718345165,192,46.17,0.0891,semanticweb,0.0,4,5,5,5,factual,4,4,95,polite,4,negative,5,none,4,4,5,4,factual,4,5,85,neutral,5,negative,5,none,3.0,4.0,3.0,2.0,unfactual,1.0,2.0,60.0,neutral,4.0,negative,4.0,moderate,4,4,5,3,factual,3,4,80,neutral,5,negative,5,none,4,4,4,3,partially factual,2,3,75,neutral,5,negative,5,low
5,Alison-Kutywayo,A systematic review: Male engagement in adolescent and young adults’ sexual and reproductive health in the Americas,"Progress towards sexual and reproductive health (SRH) goals for adolescents across the Americas has stagnated. Of all the regions worldwide, Latin America has experienced the slowest decline in adolescent fertility rates. Reports published by the United Nations and multiple nongovernmental organizations demonstrate a growing consensus for a masculinities framework that engages men and boys in public health and social change. Male engagement acts as a complement - and not a replacement - of current SRH. Emerging evidence indicates that Coronavirus disease in 2019  has worsened SRH outcomes, especially related to gender-based violence; new evidence-based interventions are ever more urgent.  This systematic review includes a focus on education-based male engagement, a special consideration of gender equity, and systematic searches by fluent speakers in three most populous languages in the Americas (English, Spanish, and Portuguese). PubMed, EBSCO, SCOPUS, and Google Scholar databases were digitally searched. Publications were excluded if their focus did not align directly with sexual reproductive health, their location was outside the scope of study, its content derived from information collected before 2010, or its study’s population’s age of focus was not between 15-24 years of age. After abstract screening and full-text review, the original 10,721 articles identified were narrowed down to 13 articles whose references were further examined through hand searching, leading us to a total of 32 final articles chosen for analysis. The results were classified by geographic regions of the American continent. The literature emphasized that society often defines masculinity as a hegemonic role grounded in aggressive high-risk sexual behavior. Adolescent males internalize this and hold their peers to these expectations. These beliefs have detrimental SRH consequences that have yet to be fully understood among adolescent boys and males. The efficacy of future interventions will depend on further exploration of these topics, especially among minority populations.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the opportunity to review this systematic review of male engagement in SRH in the Americas. It is an interesting piece of work.  Having reviewed this manuscript, my main comments are related to the structure of the Methods, Results and Discussion. The Results need to be thematically analyzed by theme, rather than by geographical area and there are many things in the Methods that need to be in the Results.  I suggest that the authors please carefully review the following manuscript  ( https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8005924/ ) for guidance on what to include in the respective sections. Table 1 in this PRISMA manuscript provides a clear guide that will help you strengthen your manuscript. In addition to these main comments, I have a 61 editorial comments throughout the manuscript for your consideration. (See attached PDF)  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Not applicable  Are the conclusions drawn adequately supported by the results presented in the review? Partly",255,1,1,0.7795000000000001,0.1264492754,0.8444064856,604,34.76,0.9417,f1000,0.0,1,0,3,1,factual,4,4,20,neutral,2,neutral,3,high,4,4,3,4,factual,4,4,75,polite,4,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,3,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
64,Mohammad-Bodrul-Munir,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The MS ""Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)"" is not written well. For example, writing style of the background in the abstract was found to have grammatical errors. The methods in this section are expressed as future tense.  There was no feed proximate analysis found; however it is necessary to validate the desired protein percentage. How many biological and technical replicates did you take? Superscripts in Table 2 indicated the significance differences among the rows; but the researchers did not express this, therefore it is quiet difficult to understand. Please revise it. In the Table 2, the results of DGR in A and B were not significantly different. It should be same. Please check it. Standard errors of the results are confusing. (how is it possible 0.01, 0.02, etc?) Expression of superscripts in Table 3 have the same problem as Table 2, please re-write. I felt confused the FCR data. Was there any relation of 3% body weight feed provided to fish? How did you calculate this 3% body weight? Overall not at the standard for indexing using the present format.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",338,0,3,0.7796000000000001,0.070045045,0.7844375968,164,40.24,0.4299,f1000,0.0109890109890109,4,4,2,4,factual,3,3,65,polite,4,negative,3,low,4,4,4,4,factual,4,4,75,polite,4,negative,4,moderate,1.0,2.0,2.0,2.0,partially factual,2.0,1.0,40.0,neutral,3.0,negative,3.0,low,4,3,4,3,factual,4,4,70,neutral,5,negative,4,low,4,3,3,3,partially factual,3,3,60,neutral,4,negative,4,low
85,Hannah-R-Frost,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Thank you for the invitation to review the manuscript ""GBS vaccines in the UK: a round table discussion"" by Thorn et al., it was an interesting and informative read. The article provides a concise and well-rounded update to the current status of GBS Vaccines, including an appropriate focus on knowledge gaps and barriers to success with useful recommendations for how to address them. It is particularly interesting to have an update on important ongoing or planned trials, which is not normally available in the literature until >1 year after the completion of the trial. I appreciate the focus on forward planning around vaccine uptake and phase IV trials, and keeping in mind lessons from COVID-19 and other vaccines given in pregnancy.  I have a few minor comments which may improve readability of the manuscript. 1) There is some repetition of points throughout, likely due to the nature of the manuscript as proceedings of a meeting. The authors could clean up the narrative, for example on page four, two subsequent paragraphs have the same conclusion regarding the need for improved surveillance.  2) Different acronyms are used to refer to the same thing (e.g. EOGBS, EOD and EO disease are all used in the first page) and some acronyms are never expanded (e.g. UR when discussing case estimates).  3) It would be good to have references and links provided for the burden of disease data used, acknowledging that some data is as yet unpublished.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",385,0,2,0.7937000000000001,0.1619918699,0.8808193207,112,42.41,0.8514,f1000,0.0,5,5,5,5,partially factual,2,4,73,polite,5,positive,4,low,5,5,4,5,factual,5,5,95,polite,5,positive,5,low,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
57,Reviewer-aWLd,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","This paper improves the methods of utilizing auxiliary outlier data for fine-tuning. Specifically, it synthesizes informative outlier samples close to in-distribution at the decision boundary by adding noise to existing auxiliary data and utilizes them in learning. As a result, it shows improved results by applying the proposed method to various outlier exposure methodologies. 1. This paper's method is simple and clearly explained. This paper's approach is novel in explicitly adding noise to outlier data to synthesize and utilize outlier data. 
2. Theoretically, the proposed method looks a reasonable way to select samples closer to in-distribution data at the decision boundary than existing methods. 
3. The experiments are generally fair and show performance improvements when this method is applied as an add-on to a variety of methods. In particular, this paper suggests a method of synthesizing and utilizing outlier data, which presents the possibility of improving the performance of fine-tuning using outlier data. 
4. Additionally, this paper shows that this method is an effective way to improve the performance of fine-tuning using outlier data by applying it to a variety of outlier exposure methodologies and showing improved results. 1. The novelty and superiority in outlier synthesis against DOE \[Wang et al., 2023\] are not clear against DOE \[Wang et al., 2023\]. 
2. The term ""diversified"" is not well-defined, and it is not clear how it differs from the term ""informative"" used in ATOM \[Chen et al., 2021\]. 
3. The experiments also lack comparison and discussion with the most similar papers MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\] that use outlier synthesis. For example, there is no comparison between the proposed method and DOE \[Wang et al., 2023\] when it is applied as an add-on to a variety of existing methods. 
4. The rationale for using PGD (Projected Gradient Descent) based noise is not well-explained. It is not clear if it has superiority over other attack-based noise. 
5. It is not clear how the proposed method of explicitly synthesizing outlier samples differs from implicitly synthesizing them in terms of new effects or novelty. 1. What is the difference from the most important outlier synthesis methodologies (e.g., MixOE \[Zhang et al., 2023\] and DOE \[Wang et al., 2023\]).
2. The fact that the outlier close to the boundary in the left figure of Figure 2 is a diversified outlier is not clearly explained. It is necessary to explain how the informative and diversified are different.
3.	The process of moving from Equation 4 to Equation 5 is not clear. Additional detailed explanations are needed.
4. Equation 4 synthesizes all outlier data and leverages only loss, while Equation 5 synthesizes and calculates loss for a portion of outlier data. It seems that the two equations are different methods. Please explain how the two equations are connected.
5. It is necessary to discuss direct comparison and difference with DOE in TABLE1.
6. Please add the comparison results of each when combined with DOE and MixOE in TABLE2.
7.	Please add experiments on the ImageNet benchmark \[A\], as discussed in DOE.
8.	Please add experiments on the application of DivOE to OECC \[B\] in TABLE2.

\[A\] Tal Ridnik, Emanuel Ben Baruch, Asaf Noy, and Lihi Zelnik. Imagenet-21k pretraining for the masses. In NeurIPS Datasets and Benchmarks, 2021.

\[B\] Papadopoulos, Aristotelis-Angelos, et al. Outlier exposure with confidence control for out-of-distribution detection. Neurocomputing, 2021, 441: 138-150.
 Yes. The author adequately addressed the limitations",570,0,19,0.7331000000000001,0.1279780564,0.8857254982,230,42.9672,0.2018,neurips,0.0,4,5,5,4,factual,4,3,90,polite,5,neutral,4,low,5,4,4,5,partially factual,4,5,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,neutral,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
113,Reviewer-EAMn,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC \[24\] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet. * The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.

* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.

* The paper is fairly easy to follow (with the exception of some equations and many typos and grammatical errors, see Weaknesses).

* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show small improvements over some existing methods using the CLIP-pretrained model, this can serve as a new strong baseline for future UniDA work. * The paper claims that existing UniDA works overlook the internal intrinsic structure in the categories. 
    * However, \[W1\] aims to resolve the same problem. \[W1\] proposes to learn lower-level visual primitives that are unaffected by the category shift in the higher-level features. And, in their proposed word-prototype-space, different visual primitives can be shared across domains and classes (including unknown classes).
    * There is a significant overlap in the motivation given by this paper and that of \[W1\]. Consequently, the high-level conceptual novelty of this paper is overclaimed. However, I do believe that these conceptual ideas are interesting as well as important for UniDA.
    * Please discuss the similarities and differences (both in terms of motivation and the actual approach) of this paper w.r.t. \[W1\].
    * Another paper with similar conceptual ideas is \[W2\].

* This paper lacks some mathematical rigor.
    * Eq. 1, 2: $\hat{Z}=W\cdot M$ is shown as matrix multiplication (I assume that it is not element-wise multiplication since dimensions of $W$ and $M$ are different), but the expansion of this matrix multiplication contains an arg-max over the elements of $W$. Then, it does not make sense for the overall computation to be a standard matrix multiplication.
    * Eq. 1, 2: the text mentions that $s_i$ is the index of sub-prototypes in the $i^\text{th}$ item but Eq. 2 implies that $s_i$ is a particular dimension found with arg-max. This seems contradictory and is confusing.
    * Eq. 2: Use $\mathop{\arg\max}_{j}$ instead of using `dim=1` since it is a mathematical equation and not the code implementation.
    * Eq. 5: It is unclear which dimension is used for top-$k$
    * Eq. 6: It should be $\max(... , 0)$ instead of just $\max(...)$.

* The requirement of a CLIP-pretrained backbone is very restrictive since the method cannot be extended to other settings (like medical imaging) where the CLIP-pretraining may be suboptimal. While the paper shows comparisons where prior methods use the CLIP-pretrained model, it should also show comparisons when starting from a random initialization as well as the more widely used ImageNet initialization.
    * The paper claims that a CLIP backbone is needed to retrieve sub-prototypes in early iterations. Why not start retrieving sub-prototypes after a few epochs of normal training?

* L135: “eliminates the domain-specific information from the target domain”. This is a very strong claim which does not seem to be backed by evidence. Performing “domain alignment” is not the same as “eliminating” domain-specific information. Further, as we can see from Fig. 3, the sub-prototypes seem to be retaining domain-specific information.

* There are no sensitivity analyses for the several loss-balancing hyperparameters $\lambda_1, \lambda_2, \lambda_3$ (not even in the Supplementary). While the paper claims to have borrowed them from DCC, this approach is vastly different from DCC, and we need to check for sensitivity to these hyperparameters. Further, DCC does not have a reconstruction loss, so it is unclear how that hyperparameter is selected.

* There is no ablation study for the adaptive threshold $\lambda$. It should be compared to various fixed thresholds and the value of the adaptive threshold should also be plotted over the course of training to obtain more insights into its working.

* Other UniDA works, like OVANet \[40\] and \[W1\], study the sensitivity of their methods to the degree of openness (i.e. the number of shared/private classes) which changes the difficulty of the UniDA problem. This analysis is missing in this paper. This should be shown for a better understanding of the capabilities of the proposed method.

* Some more related work \[W3-W4\] on Open-Set DA and UniDA (apart from \[W1, W2\]) that is not discussed in this paper.

* Minor problems (typos):
    * L53: “adaption” → “adaptation”
    * L59: “shifts” → “shift”
    * L92: use `unknown’ i.e. use a backquote in LaTeX for it to properly render the opened and closed quotes like in L102. 
    * L119: use math-mode for K in top-$K$.
    * L124: “varies” → “vary”
    * L126, 179: add space between text and \cite{...}
    * L134: “differenciates $\hat{Z}$ with” → “differentiates $\hat{Z}$ from”
    * L151: “max” → “maximum”
    * L166: “only the $K$” → “only the top-$K$”
    * L181: “$max$” → “$\max$”
    * L244: “fellow” → “following”

* Minor problems (grammatical errors):
    * L32: “aims” → “aiming”
    * L40: “Since such kind” → “Since this type”
    * L41: “almost happens in all the” → “occurs in almost all of the”
    * L59: “embedding give into” → “embedding is passed to” 
    * L125: “sometimes is” → “is sometimes”

### References

\[W1\] Kundu et al., “Subsidiary Prototype Alignment for Universal Domain Adaptation”, NeurIPS22

\[W2\] Liu et al., “PSDC: A Prototype-Based Shared-Dummy Classifier Model for Open-Set Domain Adaptation”, IEEE Transactions on Cybernetics, Dec. 2022

\[W3\] Chen et al., “Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation”, AAAI22

\[W4\] Garg et al., “Domain Adaptation under Open Set Label Shift”, NeurIPS22 Please see the weaknesses section. 

Overall, the technical contributions seem to be novel and intuitive. However, there are significant concerns regarding missing discussions on highly relevant work \[W1\], lack of mathematical rigor, missing sensitivity analyses and ablation studies, and the restrictiveness of requiring a CLIP-pretrained backbone. Hence, my rating is “4: borderline reject” at this time but I am willing to update my rating based on the rebuttal and discussion. I appreciate that the paper provides both limitations and broader societal impact discussions in the Supplementary.",1175,2,6,0.7796000000000001,0.0886058638,0.9329913259,215,40.4277,0.8137000000000001,neurips,0.0106382978723403,4,4,5,4,factual,3,3,70,neutral,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,70.0,polite,5.0,positive,3.0,low,5,4,5,5,factual,5,5,90,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
199,Alejandro-Reyes,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors implement an interactive tool, called iSEE, to perform exploratory analyses for high-throughput experiments. The tool inputs a Bioconductor core structure, the SummarizedExperiment object (coerced into a SingleCellExperiment object) and builds an interactive interphase for data exploration. iSEE provides several tools for data exploration by plotting features of an assay along with sample metadata, feature metadata, and reduced representations of the assays. Furthermore, iSEE enables users to interact with the plots and to dynamically link panels with different representations of the data. The analyses performed using iSEE are reproducible, since the code that was run through the graphic interphase can be downloaded.  Overall, the manuscript presents a very good idea and the code implementation is of great quality. iSEE will be very useful for people without programming background to perform basic analyses. I believe that the success of this tool will depend on whether the authors continue to develop it based on feature requests from users.  I don’t have major concerns. However, I do have some recommendations to increase the interest of potential users. Enable users to select more than one group of samples from the dimensionality reduction plots. Furthermore, it would be very useful to enable users to fill new columns of colData based on the interactive grouping of samples.  Enable users to retrieve an R data object if the initial input was modified during the analysis.  In the context of single-cell or large-scale analyses, it would be helpful to implement tools for differential abundance analyses and gene set enrichment analyses. For instance, one could think of an implementation where users manually define groups of cells from tSNE/PCA plots, retrieve the genes that are differentially expressed between these groups, and extract the pathways that are enriched among the differentially expressed genes.  When grouping samples manually on the tSNE/PCA plots, the violin plots of individual features (for example, genes) could be stratified based on these selections (e.g. plot one violin per group of selected points in the “Feature assay plot” panel). In the current implementation, it is only possible to colors the points within the violin plot, which makes difficult to compare distributions between groups of samples.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",511,0,1,0.7554000000000001,0.1285779221,0.9235043526,13,32.94,0.2025,f1000,0.0,5,5,5,5,factual,4,5,90,neutral,4,positive,4,none,5,5,4,5,factual,5,5,95,polite,5,positive,5,low,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,4,5,4,5,factual,5,4,85,polite,5,positive,5,low,5,5,4,5,factual,4,4,92,polite,5,positive,5,low
46,Anil-Gumber,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study protocol is written well. However, authors need to mention clearly how this protocol is different from the one published in 2018. This has been referred as: Neogi S, Negandhi H, Sharma J, et al.: Diagnostic efficacy of digital hemoglobinometer (TrueHb), HemoCue and non invasive devices for screening patients for anemia in the field settings-a proposal. Indian J Comm Health. 2018; 30(Supp): 86–81. Beside comparing three devices in previously published protocol, what other generic features are included in this protocol. Currently, to me, its just an updated version of previously published protocol and thus should not be indexed again. Instead authors should have mentioned what progress they have made since publishing their previous protocol and how many individuals have been screened until now. In the methods section, canvassing of EQ-5D questionnaire is mentioned but in cost-effectiveness analysis this information has not been used. It is not clear to me why this instrument is required (to measure changes in HRQoL or QALYs between individuals screened by invasive and non-invasive methods, which doesn't make any sense to me when individuals who are screened are not going to be followed-up). More importantly, the very generic concept of Incremental Cost-Effectiveness Ratio (ICER) can't be applied here due to absence of follow-ups (i.e. no data is collected at two points in time). Authors can only compare cost per correctly detected screening outcome between four types of method/equipment instead of computing ICER. Authors have not included details of costing information for enhanced training to field workers/ANMs to be used for various screening equipment in the clinical/community setting. Finally, there is a huge difference in purchase prices between equipment and how these will be used in cost-effectiveness analysis is not clear to me. For instance within invasive method, the cost of Tru Hb equipment is 8-10 times higher than HemoCue. And if one is just comparing the purchase cost per correctly detected screening outcome within invasive method would be a terrible blunder. Therefore, one needs to account for the case loads as well as the mixed-use for varied purposes for an equipment during its lifetime or becoming obsolete.  I think these are serious issues in the published protocol and most things are replicated from the previously published protocol.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Yes  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",488,0,3,0.8182,0.0598577236,0.9207384586,7,34.76,0.0904,f1000,0.0,4,5,4,4,factual,4,4,90,polite,5,neutral,5,none,4,4,4,3,factual,4,5,65,impolite,5,negative,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,neutral,5.0,negative,3.0,low,4,4,4,3,factual,3,4,75,neutral,5,negative,5,low,4,4,4,4,partially factual,4,3,75,neutral,5,negative,3,low
90,Reviewer-yxpU,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","The authors study frequency estimation in a streaming setting using CountMin and CountSketches, both their classic and learning augmented variants. They prove tight theoretical bounds for the expected error when the frequencies follow the Zipf distribution.
They also introduce and analyze a new algorithm with lower error that returns 0 for low frequencies instead of the noisy estimates of classic CountSketch. Furthermore they also introduce a parsimonious version of their algorithm that avoids consulting the potentially much slower machine learned model for each item of the stream using Poisson sampling to provably invoke it a small number of times only. Several experiments with two real world and synthetic data sets support the claims, albeit the implemented algorithm is much simpler than the one analyzed and a simple modification of the classic CountSketch also yields substantial improvements. 1) Problem and techniques studied are extremely well motivated and widely used.
2) Solid theoretical analysis and tight new lower and upper bounds.
3) Introduces multiple new algorithm variants.
4) Substantial error reduction in the experiments.
5) Paper is well written and structured.

 1) No experiments with the theoretically analyzed algorithm, no theory for the simpler variant in the experiments.
2) I would love to see some experiments with the parsimonious algorithm as well.
3) When its truncation threshold is properly tuned the experimentally evaluated simplified algorithm is more accurate than returning max {0, CountSketch's estimate}. However the best threshold is dataset dependent and the wrong threshold underperforms the non-negative CountSketch (i.e. threshold = 0). Section 3.2 proposes a theoretical construction based on the Alon-Matias-Szegedy sketch to adaptively tune and set the threshold, nevertheless this variant is not evaluated in the experiments either. It would be good to evaluate a hyper-parameter free variant that works (well) on any data out of the box or explicitly leave it as future work. Alg 1: What's median of 4? The proof section carefully requires odd number of rows. Could you please clarify, or since it's only for the sake of theory make it 3 (or 5) to keep it simple?

Alg 2: Could you discuss why it's essential (or not) to take median of medians instead of using a single CountSketch with O(T) rows as a filter?

Could you also discuss whether your results hold (strengthen or weaken) for more general power laws where f_i ~ (1/i)^p (or log-normal) beyond f_i ~ 1/i Zipf similarly to Du, Elbert, Franklyn Wang, and Michael Mitzenmacher. ""Putting the “Learning."" ICML, 2021? Probably it's best worked out and discussed after lines 222-223.

Could you also measure and disclose the power law exponent for the CAIDA and AOL datasets?

Figures 2-5: Could you use the same color for best Our (C=..) line in the left and right sub-plots? 

Lines 249-250: three columns and varying number of rows -> 3 rows and varying number of columns (typo). Yes, it's absolutely forthcoming and adequate.",480,0,2,0.7995,0.1289980852,0.8795605302,215,42.4187,0.6631,neurips,0.0099009900990099,5,5,4,5,factual,5,4,90,polite,5,neutral,4,low,5,4,4,5,factual,4,4,85,polite,5,positive,5,moderate,3.0,4.0,5.0,4.0,factual,4.0,4.0,85.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,5,factual,4,4,85,polite,5,neutral,5,low
148,Reviewer-DES2,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","The paper develops three methods for smoothing in state-space models (SSMs). The idea is to assume SSMs that are non-linear and avoid other assumptions like Gaussianity when using variational inference. The drivin gidea is to preserve the temporal structure in the variational proposal. This seems to lead to what is called exponential family dynamical systems, that it a double-looped (forward and backward) chain of markovian conditionals. Having carefully checked the exponential family derivations, the parameterization, as well as the derived ELBOs, I feel that likely they are correct and well-founded on previous related work. The use of exponential families in this context, and particularly to build the factorization into markovian conditionals is definitely a strenght. The work itself is clear and concise on the details, also mentioning limitations and reasoning on why certain decisions are taken. To me the paper has two main weaknesses:

\[w1\] — the paper is in general concise and thorough, but written in a way that the smoothing idea is kind of lost. Particularly, technical details jump in for solving issues of previous technical details (derivations begin at the beginning of pp. 2 and finish at the end of pp. 7). In that way, the paper loses quite a lot of space, and story on the general smoothing idea that authors want to solve (and in which way they want to solve it). 

\[w2\] — the second concern to me is the limited results. Having derived long technical details, the manuscript should at least provide results proportional to the technical development. In my opinion, the evaluation of the model is somehow short (learning of two synthetic systems (pendulum and chaotic scenario) plus analysis on convergence). Not technical questions",282,0,2,0.7676000000000001,0.0099206349,0.7749239206,48,41.0469,0.1262,iclr,0.0246913580246913,2,2,2,2,partially factual,2,3,40,polite,3,negative,3,high,1,5,1,1,unfactual,3,4,20,polite,1,negative,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,2,3,2,2,unfactual,3,3,30,polite,1,positive,3,moderate,3,4,3,4,partially factual,4,4,75,polite,3,neutral,4,low
128,Reviewer-4t5w,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","The paper extends methods for radial basis function (RBF) neural networks to predict time-series to online models---named an ""online sparse residual tree"" (OSRT) model. OSRT involves building a sparse tree in which the RBF networks reside.  To address streaming time-series, the model's online adaptation is done by thresholding the current mean squared residual error as new data arrives. The paper presents several novel ideas, by combining RBF networks, sparse regressison trees, and online updating for time series prediction. The paper lacks a principled approach to model design and evaluation, appearing to have little rationale in the combination of methods used beyond their adaptation from the recent literature, and their apparent heuristic value.  Typically one would expect a cross validation step as part of the algorithm when complexity parameters or thresholds are called for in a model.  

The exposition is hard to follow at best, and at times incomplete, or the symbols are incorrect. 
For instance, in Section 2: 

- Gaussian kernel is designated \theta, but in the approximation the character \phi is used -- are these the same thing? Note that \theta is reused with a different meaning in Equation (17).

- After equation (2) the phase ""Where Nχ is the number of neurons in this node, δl is the shape parameter."" makes no sense since neither variables appear in the previous formula.  The rest of that paragraph has similar problems with reference to variables not introduced in the equations that it attempts to explain. 

In general, one needs a principled method for determining the complexity of the model, e.g. the number of nodes, such as cross validation, or use of complexity penalty terms, e.g. in BIC. Is the maximum tree depth (Section 2.2) something one calculates, or is it a parameter one setd? Simply considering when ""the increase in the number of nodes no longer yields significant improvements in approximation quality"" will lead to overfitting. ""Significant improvement"" is not a principled method. There are many terms introduced in the explanation of the model that are introduced but not explained:  Could you describe the tree in terms of its layers?  What is the ""split rule"" and what is the stopping condition referred in the paragraph following Equation (2)?  In what sense is it sparse? Is there a sparsification step?  What do you mean in Section 2.1 by ""quasi-uniform""?  Are your ""mean points"" C the same as your centers? Honestly this as far as I got in the text.",408,0,1,0.7854,0.0512987013,0.8514997363,49,51.0689,0.1822,iclr,0.0,2,4,4,1,unfactual,4,4,73,impolite,4,negative,4,extreme,3,3,4,4,factual,4,4,65,impolite,4,negative,5,moderate,1.0,2.0,2.0,2.0,unfactual,2.0,1.0,40.0,neutral,3.0,negative,3.0,moderate,3,3,4,3,factual,3,4,70,neutral,5,negative,4,low,2,2,3,3,partially factual,3,3,45,neutral,4,negative,4,moderate
49,Reviewer-4d9L,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","This paper introduces CrossGET, a token reduction-based strategy, to accelerate vision-language transformers. The key contributions of CrossGET can be summarized as follows: 1) CrossGET incorporates cross-modal guided information through cross-modal tokens. 2) CrossGET employs the Complete-Graph Soft Matching (CGSM) strategy, which offers more reliable token-matching results compared to existing bipartite soft matching strategies. Experimental evaluations conducted across multiple models, datasets, and tasks demonstrate the superior performance of the proposed method. The acceleration of VL models is highly relevant for their practical deployment. While this paper presents promising results and extensive evaluations, there are important concerns that should be addressed before publication.
1. Some experimental results are perplexing. Table 1 suggests that ToMe performs worse when equipped with Adapter or ExtraToken. However, Adapter and VPT are parameter-efficient tuning methods that enhance performance with minimal additional parameters. It is unclear how they could instead degrade performance. I suspect there may be errors in the implementations. It is recommended to double-check the results or provide convincing explanations. Additionally, the upper-right subfigure in Figure 4 is also confusing. In my understanding, CrossGET and ToMe have close GFLOPs under the same configuration (as evident from the left subfigure). Therefore, the significant differences in GFLOPs for each data point pair in the upper-right subfigure indicate that they are compared under different configurations. A reasonable explanation should be provided here. Moreover, the down-right subfigure seems to be unusual as well. How is it possible for the model to achieve even better performance (nearly 86) with only 1/10 GFLOPs? Are the settings the same as in other figures?

2. The contribution of the Complete-Graph Soft Matching (CGSM) appears to be minor. For instance, Table 1 suggests that ToMe and CrossGET $\Delta$ perform similarly in different metrics, indicating that the proposed CGSM may have little impact. ToMe employs the bipartite soft matching strategy for its efficiency and simplicity, and the ToMe paper demonstrates that this strategy can approximate optimal matching through extensive combination experiments. This paper should provide more evidence (visualizations, analytical experiments) to justify the effectiveness of the proposed CGSM.

3. Most experiments in this paper focus on Image-Text retrieval tasks. Is the proposed method equally effective in other VL tasks, such as the CoOP benchmark or open vocabulary segmentation?

4. This paper lacks an important comparison. \[1\] proposes reducing the number of tokens through clustering and demonstrates better performance than ToMe in accelerating transformers. However, this paper only briefly mentions it in the introduction without further discussion or comparisons. It is recommended to include more comparisons (\[1\] vs. CrossGET $\Delta$, \[1\] + CGM&CGE vs. CrossGET $\star$, etc., better in dense prediction tasks) with \[1\].

I am glad to increase my rating if my concerns are addressed.

\[1\]. Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Weihong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu. ""Expediting large-scale vision transformer for dense prediction without fine-tuning."" Advances in Neural Information Processing Systems, 35:35462–35477, 2022a. No other questions.",489,5,6,0.8278000000000001,0.1423076923,0.9290834665,76,31.5291,0.1507,iclr,0.0,5,4,4,4,factual,3,5,90,polite,5,positive,5,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,5,4.0,4.0,80.0,3,5.0,-1,5.0,2,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
139,Ehab-Mohamed-Abd-El-Kaf,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript makes a valuable contribution to understanding parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study highlights the local importance and relevance of this issue and provides useful insights for healthcare providers seeking to improve service quality. Overall, this manuscript provides a comprehensive overview of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. Enhancing the introduction with additional references, clarifying secondary objectives, and providing more details on the sampling process and subgroup analyses would further improve the manuscript. Here are a detailed review of the sections. 1. The introduction is clear and effectively sets the stage for the study, emphasizing the importance of patient satisfaction in healthcare within the UAE's evolving landscape. While the background information on patient satisfaction is comprehensive, adding recent studies on similar settings would enhance this section. 2. The goals and objectives of the study are well-stated and align with the introduction. The aim to investigate parents' satisfaction with physiotherapy services for neuropediatric patients is clear. However, clarifying any secondary objectives would provide a more complete picture of the study's scope. 3. The methods section is detailed and well-organized, outlining the study design, participant recruitment, data collection, and analysis procedures. The use of the Patient Satisfaction Questionnaire (PSQ-III) is well-justified, and ethical considerations are thoroughly addressed. More details on the sampling process, including selection criteria and potential biases, would improve transparency and replicability. 4. Results are presented clearly with tables that effectively illustrate key findings. The mean satisfaction scores for different service domains are well-documented, and the statistical analysis is sound. Including more detailed demographic data and subgroup analyses would provide additional context and highlight factors influencing parental satisfaction. 5. The discussion interprets the results well, relating them to existing literature and emphasizing the study's local significance. Identifying areas for improvement, such as access, technical quality, and economic elements, is valuable. The discussion could be enriched by exploring strategies for addressing these areas and discussing the implications for policy and practice in more detail. 6. Comparing the findings with similar studies in other regions would offer a broader perspective. 7. The conclusion succinctly summarizes the main findings and their implications, emphasizing the need for ongoing assessment and improvement of physiotherapy services. Including specific recommendations for future research and practice would strengthen the conclusion.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",532,0,8,0.792,0.1634672619,0.9228382707,26,19.06,0.0999,f1000,0.01010101010101,1,4,5,2,unfactual,3,2,35,polite,2,neutral,3,high,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,4.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,4,5,5,5,factual,5,5,90,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
129,Reviewer-efhf,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The paper explores whether parameterized quantum models can achieve comparable training efficiency to classical neural networks. From the perspective of reusing quantum information, the paper demonstrates that achieving backpropagation scaling in quantum models is not feasible without access to multiple copies of a state. With access to multi-copies assumption, the authors propose an algorithm that achieves backpropagation scaling using gentle measurement and online learning while reducing classical auxiliary computational costs. These findings shed light on reusing quantum information for the challenges of training large quantum models.
 The paper investigates the backpropagation in quantum models which is interesting and of general interest to the community of QML. It combines online learning and shadow tomography to achieve $O(polylog(M))$ sample complexity for gradient estimation.  1. Even though the proposed method achieves $O(polylog(M))$ of sample complexity, it also requires exponential classical resources which is not practical for handling a large system.
2. It only provides the theoretical analysis and does not give some proof-of-principle numerics.
3. Some necessary details and the related brief introduction of the proposed methods should be listed in the manuscript instead of supplementary.
 1. In proposition 7(193), as the variational model is defined as the trace of a quantum state, i.e. $tr\[U_\theta \rho U_\theta^\dagger\]$, the loss will always be constant 1, so no matter what $\theta$ we choose the gradient will always be 0, so it is confusing that what's the contribution of the gradient in such setting and whether in the general case, it also achieves $O(\frac{\log(M)}{\epsilon^4})$ backpropagation scaling. 
2. In the proof of theorem 12(281), As in definition 8, $\mathcal{U}(\theta)=e^{-i\theta_M P_M}\dots U_1$, why each parameter $\theta_i, i\in\[M\]$ associated with the same $P_i=Y_0\otimes Z_1$ and whether the first term in the Pauli string $P_i$ should not be $X_0$ when observable set as $Z_0$?
3. when we choose random Pauli strings $P_j$ and the initial setting of $\theta$ is NOT 0, whether the theorem 12 still holds?
  ",317,0,5,0.7623000000000001,0.0468944099,0.9299380779,216,29.1641,0.1303,neurips,0.0206185567010309,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,4,4,4,partially factual,5,5,80,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
37,Zhi-Ning,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  General Comments: The authors have investigated the oxidative potential of POA and SOA from two different sources namely alpha-pinene and gasoline engine exhaust. The experimental setup included an UV chamber (oxidation flow reactor), to mimic the sun light’s UV rays, to compare primary and secondary organic aerosol-induced radical generation under light and in dark. The comparison could contribute great value to the manuscript if additional parameters as listed below are included in it: Page 3: Right column: Line 4: The authors can address why they have selected only Hydroxyl radicals in the investigations. In some experiments, where UV rays are used to excite the organic aerosols, the elicitation of superoxide radicals is also possible.  Page 3: Right column: Lines 28-30: The statement “aerosol stream was sampled while a UV lamp was on, following a 90-minute reaction period.” is not clear. Does that mean the whole sample streaming is done for a continuous 90 minutes? Was it the same for the aerosol sampling done in dark OFR?  Page 3: Right column: Line 33: The information of the control sample needs to be included here.  Page 4: Left column: Line 23: The analysis part has some information missing such as incubation time for cell growth, and are the same generation (life cycle) used for analysis?  Page 4: Left column: Line 25: The cell exposure study has some basic information missing - PM dose, route of exposure (directly on filters or on PM extracts), number of times analysed (duplicate or triplicate). Please include for clarity.  Page 5: Figure 4: The biogenic organic compounds (for example: the alpha-pinene) are believed to be more hydrophilic compared to engine exhaust organics. Please include a discussion if the water solubility of samples is also driving the difference in oxidative stress.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",436,0,1,0.7897000000000001,0.1238702624,0.8962137699,231,33.54,0.1376,f1000,0.0210526315789473,4,4,4,4,factual,3,3,75,polite,4,positive,4,low,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
35,Reviewer-Bsgy,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","The paper presents a novel, brain-inspired modulatory feedback circuitry (long-range modulation, LRM) for regular feedforward DNNs. The multiplicative modulatory pathways can be conditioned on a) higher-level activations (computed in the initial forward pass or subsequent modulatory passes, Sec 2.1, 2.2) to improve the network’s ImageNet accuracy and adversarial robustness (Fig 2) with brain-like dynamics (Fig 3), or on b) steering signals (derived from labels, instance/class activations, CLIP embeddings, etc., Sec 3.3, 3.5) to perform the composite image recognition task (Fig 5, 6). The evaluation is based on AlexNet, with the small-scale composite task created using Imagenette images (side-by-side or overlay, Sec 3.2) following experimental neuroscience protocols. + \[Originality\] The paper is sufficiently novel in my opinion, with key architectural features well motivated by experimental psychology & neuroscience evidence and reasonably different from existing RNN & predictive coding based architectures (Sec 5). - \[Clarity\] Although the overall writing is reasonably clear and easy to follow, the ambiguity in technical details renders accurate understanding of the paper impossible without digging into the source code. Examples are as follows.
1) How exactly are the modulatory pathways (Sec 2.2) and subsequent forward passes executed? E.g., in LRM1, is (Conv4 -> Conv1) executed after or concurrently with (Output -> Conv4)? Is the modulatory signal $f$ applied to e.g. $x$ from the initial pass or $x’$ from the first modulatory pass (i.e. $x’ + x*f$, or $x’(1+f)$)?
2) How exactly does the model (likely trained with 224x224 ImageNet images, Sec 2.3) handle both the overlay setting (same image size as training) and the side-by-side setting (2x image size) at the same time? 
3) AblationCam (Fig 3), output activation unit (Fig 6), $\sigma\pm$ (Sec 2.1), etc. are undefined.

- \[Quality\] Empirical evaluation (soundness) is the main issue of the paper. While the proposed composite task and dataset are likely acceptable in psychology & neuroscience papers, it’s unfortunately not really sufficient for conferences like NeurIPS in my opinion. I strongly suggest the authors include additional experiments on more standard (commonly seen) CV datasets, such as \[72, 73\] or ones from \[48-69\], and comparisons against (some) existing approaches \[48-71\] whether they’re mechanistically similar to this work or not.

- \[Significance\] Although the paper is sufficiently novel, given its non-negligible weaknesses in clarity and quality (soundness), it’s unfortunately hard to conclude that this work is significant (i.e. sufficiently promising). Brain-Score \[74\] could be a different direction to showcase the paper’s significance.

\[70\] mixup: Beyond Empirical Risk Minimization, ICLR, 2018.\
\[71\] CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features, ICCV, 2019.\
\[72\] https://paperswithcode.com/dataset/clevr \
\[73\] https://paperswithcode.com/dataset/multi-dsprites \
\[74\] https://www.brain-score.org/ 1) Why are the 0th-pass results in Fig 2A and 2C better than the AlexNet baseline? Or, results in L216 better than L176? What does the 0th-pass model have in addition to the baseline?
2) How’s the model’s training & inference speed compared to the baseline? How does the model’s accuracy compare to stronger baselines (either AlexNet with more parameters, or newer networks) running at a similar speed? The authors have sufficiently addressed the paper’s limitations in Sec 4.",509,10,2,0.8077000000000001,0.0706666667,0.8916092515,216,28.5196,0.0376,neurips,0.0,4,4,5,4,factual,4,4,95,polite,4,neutral,5,none,4,4,4,4,partially factual,4,4,85,neutral,5,neutral,5,moderate,1.0,2.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,5,low
161,Anne-Thessen,Reuse of the FoodOn Ontology in a Knowledge Base of Food Composition Data,"We describe our work to integrate the FoodOn ontology with our knowledge base of food composition data, WikiFCD. WikiFCD is knowledge base of structured data related to food composition and food items. With a goal to reuse FoodOn identifiers for food items, we imported a subset of the FoodOn ontology into the WikiFCD knowledge base. We aligned the import via a shared use of NCBI taxon identifiers for the taxon names of the plants from which the food items are derived. Reusing FoodOn benefits WikiFCD by allowing us to leverage the food item groupings that FoodOn contains. This integration also has potential future benefits for the FoodOn community due to the fact that WikiFCD provides food composition data at the food item level, and that WikiFCD is mapped to Wikidata and contains a SPARQL endpoint that supports federated queries. Federated queries across WikiFCD and Wikidata allow us to ask questions about food items that benefit from the cross-domain information of Wikidata, greatly increasing the breadth of possible data combinations. ","Overall, this is an interesting paper. I think making the types of connections that are described in this paper will be helpful for my work. I have a few minor suggestions. 1. I find that referring to properties by number can be confusing. This could just be me and is not an important change. 2. When I visited tinyurl.com/28uu3sm5 I got a ""query malformed"" error. 3. page 7 line 51 ""that has"" should be ""that have"" 4. page 8 line 12 ""diaries and"" should be ""diaries are"" 5. If you need an identifier for a taxon that is not in NCBI you would probably have more luck looking in Catalog of Life or Encyclopedia of Life. This is not an important change.",122,0,2,0.7362000000000001,0.15625,0.7357035875,28,77.13,0.3011,semanticweb,0.034090909090909,3,4,3,3,partially factual,3,3,50,neutral,2,positive,3,moderate,4,5,3,5,5,5,5,85,5,5,5,4,0,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,factual,4,3,60,polite,4,positive,2,low,2,4,3,3,partially factual,4,3,70,polite,4,positive,2,low
17,Reviewer-dK5u,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This study introduces GBLM-Pruner, a gradient-based approach for the unstructured pruning of large language models (LLMs). The core idea of this research is centered around a Taylor expansion applied to the loss function. This method estimates the change in loss by employing a combination of first-order gradient and second-order approximation (OBD). Empirical evaluations using LLaMA and LLaMA-2 demonstrate that GBLM-Pruner outperforms other methods such as magnitude pruning, SparseGPT, and Wanda in terms of performance. 1. This paper highlights the significance of gradients in the pruning of large language models (LLMs). The author presents a Taylor-based approach to identify critical parameters, yielding favorable outcomes in comparison to earlier techniques.
2. The work sets robust benchmarks by contrasting the proposed methods with various existing baselines, offering valuable insights for the research community. 1. To my knowledge, SparseGPT is similarly a gradient-based approach, utilizing Taylor expansion and second-order Hessian for estimating parameter importance. In light of this, the contribution of the current work may appear somewhat constrained.
2. As depicted in Figure 2, SparseGPT, Wanda, and the newly introduced GBLM-Pruner exhibit closely comparable results, with only minor differences in Perplexity (PPL). There isn't compelling evidence to suggest that GBLM-Pruner significantly outperforms its predecessors.
3. It would be beneficial if the author could include data on the latency of the pruned LLMs, particularly in the context of 2:4 sparsity acceleration. Please refer to the weaknesses.",231,0,5,0.857,0.1018589254,0.9564601183,56,26.1914,0.1719,iclr,0.0,3,2,2,2,partially factual,4,3,45,polite,3,neutral,2,moderate,4,4,4,4,partially factual,3,4,65,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,partially factual,4,4,75,polite,5,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
23,John-T.-Stoffel,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is an interesting report on a patient with Behcet’s disease and urological symptoms. The neurological description of presentation, progression, and treatment is outstanding.  The case report nicely reviews Behcet’s disease for the reader in a clear and concise way.  The authors should be commended for highlighting this uncommon disease.  The presentation of the urological symptoms and the relationship to Behcet’s disease in this patient are not as clear. My comments:  The introduction notes that the patient was treated with an ileocecal augment/continent stoma for neurogenic detrusor overactivity and DSD.The description, however, notes that she was in retention and could not void after a MMK procedure.Presenting fluro images would be helpful for the reader to better understand how the diagnosis of DSD was reached versus post procedural obstruction. By the history, she could not void after the MMK making it more likely that this is contributing to her retention. The discussion notes that the patient was not properly diagnosed by her urologists.Is it possible that she did have mixed incontinence prior to MMK and then developed complications from this procedure rather than a missed diagnosis of neurogenic DO? More data could be presented to highlight educational opportunities on what the authors feel the work up could have included prior to MMK to avoid the complication and to better work up neurogenic bladder patients. The authors could also touch on the role of Botox in treating neurogenic DO.",306,0,0,0.7937000000000001,0.1962643678,0.8283587694000001,661,32.33,0.1695,f1000,0.0,2,4,3,1,partially factual,4,3,50,polite,3,positive,3,low,5,5,4,5,partially factual,5,5,85,polite,5,positive,5,low,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,3,4,partially factual,4,3,75,polite,5,neutral,4,low
23,Fereydoun-Davatchi,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  First, the authors have to prove that this patient has Behcet’s Disease. For that, they have give the exact medical history of the patient. We know approximately when the patient started to have Neuro-psychiatric manifestations, but we don’t know when the oral ulcers started and how long after the genital ulcers appeared. Second, we have to know how long each attack of oral ulcer took to disappear. Then we have to know how long the duration between the two attacks was. Then after, we have to know how many ulcers appeared in each attack. Finally we have to know the exact clinical manifestations of the ulcers and their progression until their disappearance. The same has to be given for genital ulcers. It is primordial to remember that not any oral or genital ulcer is an aphthous ulcer, and only an aphthous ulcer can be used as a diagnostic criterion. There are many oral or genital ulcers that may resemble an aphthous lesion, to the eyes of a non-expert. It is why for case reports like this, a high definition picture of the lesion is essential to be sure of the nature of the lesion. Once it is accepted that the oral lesion is an aphthous lesion, the authors have to prove that the genital ulcers were also aphthous ulcers. Once the presence of oral and genital aphthous ulcers is proved, one can say that the patient may have a Behcet’s Disease, because the patient fulfills the International Criteria for Behcet’s Disease (the ICBD). However, as said before, the patient may not have Behcet’s Disease. To be sure, one has to not find any other reason for the presence of the symptoms together. When it is sure that the patient has Behcet’s Disease, one has to show that the neurological manifestations are related to Behcet’s Disease. A patient can have Behcet’s Disease and another neurological disease like Multiple Sclerosis. In this case, the patient refused an examination of the Cerebrospinal fluid (CSF).  Is the background of the case’s history and progression described in sufficient detail? No  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? No  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? No",467,0,1,0.6792,0.0910683761,0.7335828543,787,42.41,0.0709,f1000,0.0,3,4,4,4,factual,4,4,65,polite,4,neutral,4,low,4,4,4,4,partially factual,4,4,75,neutral,4,negative,5,low,1.0,2.0,2.0,1.0,unfactual,2.0,1.0,40.0,impolite,3.0,negative,3.0,moderate,5,4,4,5,factual,4,5,85,neutral,5,negative,5,none,4,4,3,3,partially factual,3,2,65,neutral,5,negative,4,low
106,Reviewer-nNZN,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","The paper considers the problem of learning to select branching strategies while solving mixed integer programs via branch and bound algorithm. The key idea is to collect offline training dataset using full strong branching as behavior policy and learn an offline RL algorithm to generate the learned branching policy. Improvement of the dual bound is chosen as the reward function. Experiments are performed on four synthetic and two real world problems. - Using offline RL for branching policies seems like a natural idea that should do better than pure imitation learning. I am surprised that this wasn't tried earlier and commend the paper for making this simple but natural idea work well. 

- The description of the problem and solution is written clearly and easy to understand.

- The proposed approach performs well on multiple benchmarks. - A large part of the paper talks about sub-optimality of the FSB policy. For example, this statement ""Although FSB generally achieves high-quality branching, it could still become sub-optimal when the linear programming relaxation is uninformative or there exists dual degeneracy"" Is there more justified argument for this backed by some evidence?

- why choose the proposed algorithm over any existing offline RL algorithm like CQL\[1\], IQL etc.?

\[1\] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33, 1179-1191. - What are connections of equation 6 to reward weighed regression?",240,3,2,0.8317,0.1780952381,0.9082451463,49,39.297,0.12,iclr,0.009090909090909,2,3,3,2,partially factual,3,4,70,neutral,2,positive,3,low,4,5,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,5,positive,4,low
106,Reviewer-9gri,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","The authors propose an offline Reinforcement Learning (RL) framework for learning to branch (L2B) which reportedly exhibits superior performance with a sub-optimal dataset compared to existing methods that require extensive, high-quality datasets. This advantage is particularly notable in reducing the time to collect datasets for training the models. The reported performance on the MIP instances also indicates the effectiveness of the framework. 1. **Innovative Formulation:** The novel formulation of L2B as an Offline RL approach using a sub-optimal dataset is a significant departure from traditional methods.
2. **Efficiency in Data Collection:** The framework requires significantly less time to collect its dataset, enhancing its practicality.
3. **Performance:** The proposed framework improved performance compared to the GGCN framework on smaller dataset sizes, which is commendable. Despite the novelty of the work, I have reservations about the robustness of its results. These concerns are expanded upon in this section and further detailed in the questions that follow. 

1. **Lack of Scaling-Generalization Results:** A key aim of collecting datasets on smaller instances is to develop policies that excel on larger, more complex instances. It would be beneficial to see how various models perform on scaled-up versions of instances in various problem categories like SC, MIS, CA, or CFL. How do these policies perform on Medium or Hard instances (scaled-up versions) in SC, MIS, CA, or CFL? Does RCAC retain its performance advantage on scaling up to larger instances?

2. **Insufficient Comparison with Existing Methods:** 
- The paper lacks a thorough comparison with recent advancements in the GGCN framework, particularly the augmented loss function introduced in ""Lookback for Learning to Branch"" (Gupta et al. 2022, https://arxiv.org/abs/2206.14987). It would be insightful to see how RCAC compares to this improved GGCN variant. 
 - If I understand correctly, RCAC (S) and GGCN (S) primarily differ in their approach to training despite similarities in other aspects, such as dataset collection. Specifically, GGCN (S) employs a Cross-Entropy loss function, while RCAC (S) is focused on learning a Q-function (and a corresponding policy). The distinctiveness of the RCAC framework lies in its utilization of rewards instead of directly using FSB selections, as is the case with GGCN. However, an alternative comparison could involve integrating rewards into the GGCN framework as an additional signal. This could be achieved, for instance, by employing rewards to modulate the Cross-Entropy loss at each node, similar to how node depth might be used. Demonstrating RCAC's superior performance in this modified context would further reinforce the effectiveness of its RL-based approach as formulated in the study. 
    - It would be valuable to have the values of \( k \) specified for each model. I am particularly curious to know whether \( k > 1 \) for RCAC(S).
- Comparisons with other RL methods, especially in terms of dataset size and time efficiency, would also be valuable. Clarifications:

1. **Section 3.3:** Should ""representation of the B&B tree"" be replaced with ""representation of the B&B node"" for accuracy? 
2. **Training Dataset for GGCN (H) and RCAC (H):** Are these models trained on the same dataset? Is GGCN (H) trained on a separate dataset collected as specified in the Appendix?
3. **VHB Dataset Transitions:** Could the authors clarify what constitutes a 'transition' in this context? Does the transition include (s,a,s’) even when FSB is not employed in VHB, which is 0.05 times? Do you discard any transition? How is it ensured that you explore a wide array of instances before 100K transitions are collected?
4. **S Method Training:** Is the S method trained with only 5K transitions? 
5. **Reward Distribution:** Could the authors provide details on the distribution of reward values in the dataset, perhaps in the Appendix? Information on how this varies with tree depth and how normalization is handled would be valuable.
6. **Figure 3 Clarity:** What is the specific problem family represented in Figure 3?
7. **Practicality of H dataset collection:** Given that VHB takes longer than FSB (as indicated in column 2), is it still a practical choice since the performance is worse than S?
8. **GGCN Expansion:** Could the authors clarify the abbreviation GGCN? It seems to be a variation of GCNN (Graph Convolutional Neural Networks) as used in Gasse et al. 2019.
9. **Inference Procedure in RCAC:** Are there two forward passes $G_\omega\$ and $\pi_\phi$ during inference in RCAC? How does this differ from the inference process in GGCN?
10. **Hyperparameter \(k\):** Figure 3 suggests that \(k\) has a significant impact on RCAC's performance. Could the authors provide the \(k\) values used for each model and dataset?

11. **Aggregation in Table 4:** How are scores aggregated across 20 instances in Table 4? Assuming this is a cumulative sum, RCAC appears to outperform in WA but not against RPB in AP. Can the authors speculate on which problem types might be more amenable to improvement by RCAC?

12. **Reward Ablation:** Could the authors discuss the rationale behind choosing dual bound improvement over primal-dual gap improvement? Understanding the preference for one metric over the other would be enlightening.


Suggestions:
1. **Dataset Comparison:** I think it will be pretty helpful to have a section or a figure demonstrating the difference (transition vs. individual nodes) between the dataset collected using the standard IL methods and the one proposed in this work. 
2. **Statistical Significance:** Please include p-values to indicate the statistical significance of differences in Tables 2 and 3.
3. **Evaluation Methodology:** Given that 20 seems a relatively small sample size for testing, it's common practice to evaluate each instance with multiple seeds, as demonstrated in Gasse et al. 2019. Could the authors clarify whether a similar approach can be employed in their study?",936,1,23,0.7754000000000001,0.0670068027,0.8958138227,49,40.0849,0.3021,iclr,0.0,4,4,4,4,factual,4,4,80,neutral,4,neutral,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
96,Hedi-Peterson,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  KEGGViewer is a BioJS component for easy visualization of KEGG pathways. Although the article is quite short it provides all the essential information about the BioJS component for KEGG pathway visualization and points interested users to the source code for its implementation.  We do however have some minor comments about the text:The emphasis of signaling pathways is not reasoned enough in the text. KEGG has signaling pathways, but it is so much more (e.g. metabolism, information processing, diseases, etc). For the usage of the given component it makes no difference between pathway classification, this should be clarified.KEGG also has information about metabolites but this has not been mentioned in the text nor in the documentation of the component. I would assume that KEGGViewer is capable of handling metabolite data as well, but it would be nice to have it specified in the text and/or in the documentation of the component.Although KEGGViewer is an easy plugin for visualizing KEGG pathways it is not a unique way for visualizing user data and alternative options could be mentioned in the Introduction section. KEGG itself allows for user data mapping, for example, KEGGanim is a special web tool for mapping metabolite and gene expression data to the pathways. Other alternatives that could be mentioned include Reactome, which allows expression analysis from user provided data.Although the BioJS KEGGViewer component page has enough information to create working examples of the component, not all the requirements are self-explanatory (missing UI icons, display problems on certain mac chrome versions, expression range setup bar is confusing and it could be set to a default state at 0,0, the proxy setup is confusing and needs better documentation).Currently, the description of parameters and options allows only basic usage. To make the component usable for a wider range of users and to display it's full power, the authors will have to considerably update the component description with additional details and 3-4 use cases.",388,0,1,0.7763,0.1035533911,0.9303927422,12,28.27,0.216,f1000,0.010752688172043,5,5,5,5,factual,5,5,100,polite,5,negative,5,none,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
105,Reviewer-fJQP,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","The general goal of the paper is to leverage CLIP for zero-shot segmentation. For this, in contrast to prior work, an CLIP-inspired IP-CLIP encoder is trained to enable mask-level encodings for segmentation. The core of the approach is the so-called IP-encoder that uses as input a frozen mask generator, as well as two losses, a mask-aware loss and a distillation loss The general idea is good and on a high-level the components (IP-CLIP encoder, mask aware loss and CLIP distillation) make sense to me

The reported results are good across three prior baselines and different datasets In my view the paper is not well written and important details are either not well motivated or even unclear (see below)

The paper reads to a large extend like an engineering paper with a few changes here and there to adapt to the task as hand. I would assume that to be not so interesting for the majority of NeurIPS Here are my main questions about writing

1) A^c is defined to be of dimension NxC (line 119) - with N the number of mask proposals
in the self-distillation loss however, figure 2 is showing a cx1 dimensional vector?
in any case I am confused as standard CLIP (here used as teacher) would not generate a NxC dimensional 
map - and thus A^C_{fro} in equation (7) does not seem to make sense to me - can you please explain?

2) related to that: in figure 2 it seems that the final projection from IP-CLIP encoder is used without biases? (w.o. B) - that seems quite obscure to me actually - what is meant? 

3) the mask-aware loss seems sensible on a high-level - but the details are not really motivated well. E.g. the reason for normalizing the IoU scores is unclear (equation 5)  - similarly the reason behind the exact formulation of equation 6 remain unclear. 

4) the paper uses L layers prior to condition on the masks, and 12-L after that. While there is an experimental ablation about this why would that make sense intuitively?

if the authors can clarify these points I will consider upgrading my score. 

detail:
- line 125 ""wildly"" -> ""widely""


post rebuttal
thanks for addressing my questions - I have upgraded mu review as mentioned in my initial review. Please make sure that the improvements are promised are implemented - thanks ok",395,0,2,0.7952,0.1193650794,0.9075968862,215,47.4684,0.933,neurips,0.0,3,4,5,3,partially factual,3,4,76,neutral,4,neutral,5,none,5,4,3,5,partially factual,4,4,75,neutral,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,4,4,partially factual,4,3,85,polite,5,neutral,4,low
145,Reviewer-GFGi,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","This paper presents a pretty interesting idea called rehearsal, which is able to **initialize or warm up a generalizable policy with zero interaction data or limited mismatched offline data**. Concretely, the proposed method, *ReDM*, takes as input a reward function and a termination function and generates a set of transition functions or models. Imaginary trajectories can thus be generated by rolling out these transition models and used to warm up the policy. As some of the models may produce data close to the target environment dynamics, the policy warmed up with these data can have a good initialization when deployed to the target environment, which is helpful for subsequent fine-tuning. Additionally, the method can be modified for offline-RL settings, allowing it to learn a robust and generalizable policy even with a small amount of offline data mismatched with target environment dynamics. 

The method is motivated theoretically and contains lots of analysis like performance bound, laying foundations for future study in this new direction. Besides, the experiments on the standard gym and D4RL environment empirically prove the effectiveness of the method for both online and offline policy learning. 1. The idea is novel unlike traditional model-based RL, this new idea suggests learning a bunch of transition models from reward function and termination functions, exempting the need for interaction data. 
2. In terms of soundness, it proves empirically and theoretically that the transition models learned in this way can help warm up the policy and improve its performance when deployed in environments with diverse transition dynamics. 1. The paper writing is not attractive. In my perspective, the main paper contains too much tedious content regarding the theoretical analysis and lacks an explanation for the rehearsal framework. My suggestion would be to move some theoretical content to the appendix and include at least one figure to explain the procedures of this new rehearsal framework and what it can achieve or why we need it. People don't care about the theoretical stuff until they are attracted by the idea and want to dive into it. Thus I suggest making some figures to explain the idea or the method.
2. No standard deviation is included for experiments in Table 1. Also, there is no error bar in Figure 7. 
3. What is the $D_{TV}$ should be explained in the main paper. It is strongly related to your main theorem but without definition.
4. What is relative performance? Is it calculated through minus the baseline performance?
5. The axis *Number of models* in Figure 3 should be \[0, 10, 20, 30, 40\], right? 1. How about replacing the random model for calculating the eligible reward with a human-crafted planner? It is supposed to be helpful for improving the performance as well. I guess this can be a good direction for exploration and to make this method more practical. A simple rule-based planner is also as easily accessible as a reward function in most practical settings like robotics. 
2. In the zero interaction data setting, the method indeed works well in three simple gym environments. I wonder if the method still works well in the more complex Mujoco environment without any pre-collected interaction data. I am curious about its performance on high-dimensional control tasks.",537,1,9,0.7805000000000001,0.1227907962,0.9027240276,47,39.5944,0.2889,iclr,0.0208333333333333,4,3,4,4,factual,4,4,75,neutral,4,negative,2,moderate,5,4,4,5,factual,4,4,85,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,4,3,4,4,partially factual,4,3,85,polite,5,neutral,3,low
196,Reviewer-FpNc,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper proposes a novel ZSQ framework, leveraging publicly available data instead of  synthetic data.
It offers a promising solution for achieving high-performance low-bit networks without relying on original training data. 1. Leveraging open-world or public dataset for ZSQ is interesting and reasonable.
2. Comparing to ZSQ relying on synthetic data, ZeroP is more efficient and easier to implement. 1. The core contribution is somewhat limited. In my opinion, ZSQ is just a sub-area of data-free KD, and any data-free KD methods can be extend to ZSQ. \[1\]\[2\] are two data-free KD methods that utilize proxy data for distillation. They can also be applied on ZSQ.
2. There is lack of more details. e.g. image number, of the proxy datasets. And there is not ablation on the numbers of proxy data. 

## ref
\[1\] Sampling to Distill: Knowledge Transfer from Open-World Data, 2307.16601
\[2\] Learning Student Networks in the Wild, CVPR 2021 Please refer to Weaknesses.",156,4,5,0.7688,0.2548701299,0.8334491253,54,51.7318,0.127,iclr,0.0,3,5,3,3,partially factual,5,4,60,neutral,4,neutral,5,none,3,4,3,3,partially factual,3,3,65,polite,4,neutral,4,moderate,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
48,Rekha-Pillai,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article is novel. The rationale for creating the aforesaid data set is clearly outlined. The authors have collected Individual information from websites of the stablecoin projects and a crypto-data aggregator and they have clearly mentioned about the limited availability of stablecoin related information available on the public domain. It can be considered as an exploratory study as it unearths the stable coin dimensions, a less researched topic but one of high significance.  Future studies can build on the same and this is the main contribution of the paper. The data set is clearly presented in a useable and accessible format. It is clearly evident that no other cross- sectional studies of a similar nature has been conducted till date. However, as a suggestion, you may also justify the rationale behind why only 30 stable coins were selected, although the attempt is highly appreciated. You have clearly highlighted the rationale in excluding certain stable coins but you may elaborate on the total available, ones included and those excluded for providing a comprehensive picture.  As a recommendation to improve the paper, a brief literature review in a tabular form which only contains author names, year and key findings can add value. The paper may include a concluding paragraph, wrapping up the study with some future research/practical implications. Limitations of the study can be highlighted and suggest potential use of aforesaid data collected as recommendations for future research.  Finally thank you for giving this opportunity to review the paper and I hope the comments will be taken positively.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",370,0,1,0.7936000000000001,0.098659113,0.8915177584,109,33.65,0.6119,f1000,0.0,5,5,5,4,factual,5,5,75,polite,5,positive,4,low,5,5,4,5,factual,5,5,92,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
6,Reviewer-ALvG,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","The paper introduces ANet, a scalable path-based approach for reasoning on extensive knowledge graphs (KGs). In contrast to embedding techniques, path-based methods exhibit inductive capabilities but encounter challenges in terms of scalability due to the exponential growth of paths. ANet addresses this issue by incorporating a priority function, inspired by the A\* algorithm for shortest-path problems, which enables the selection of crucial nodes and edges during each iteration. This novel approach effectively reduces the time and memory requirements for both training and inference processes. S1: This paper proposes an efficient GNN called A\*Net for link prediction with good scalability.

S2: A\*Net shows impressive results on various KGs. W1: Although the method proposed in this article has better scalability, the contributions from theoretical perspectives are incremental compared to NBFNet.

W2: The introduction of the parameter sharing between the priority function and predictor is somewhat unclear, and the reason why the reasoning task can be regarded as weak supervision for the priority function is not well explained. Q1: The priority function in A\*Net is similar to the attention used in RED-GNN except that A\*Net selects the nodes and edges according to the attention score. In the case where memory allows, how does the performance of A\* Net change when Top operation is disabled in Algorithm 1 (line 5 & line 7)?

Q2: If some nodes and edges are discarded in the early phase of model training, it may introduce incorrect inductive biases and prevent the model from training effectively. How do you address this issue to avoid such problems or why is this not an issue in A\*Net? See **Weaknesses** and **Questions**.",270,0,1,0.798,0.1941176471,0.9508162141,218,36.1312,0.1163,neurips,0.0,2,2,2,2,partially factual,3,3,40,neutral,4,neutral,3,moderate,4,4,4,4,partially factual,5,5,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
6,Reviewer-2kB4,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","This paper presents a scalable path-based method for knowledge graph reasoning, which is inspired by the A* algorithm for shortest path problems. 1. The intriguing approach of applying the A$^*$ algorithm's principle to path reasoning in KG is proposed in this paper, along with the introduction of novel methods for crafting the priority function.

2. The paper achieves state-of-the-art results on the large-scale KG reasoning dataset, ogbl-wikikg2.

3. There's a substantial enhancement in efficiency, considering both time and memory usage, as opposed to the top-performing baseline, NBFNet. The proposed method performs slightly worse than NBFnet as shown in Table 1, and no results of NBFnet are reported on tail prediction in Table 2. 1. In the context of KG reasoning, a crucial question is, how many steps are typically required for a query? According to the vanilla path reasoning in Equation 1, the number of paths increases exponentially with respect to path length. However, if the path length is typically small, this might not pose a significant problem? Moreover, when dealing with a large-scale KG, the BF algorithm would need to visit $|\mathcal{V}|$ nodes and $|\mathcal{E}|$ edges for each step, which can be quite computationally intensive. Given these considerations, it leads to the question: If the path length is usually small, could vanilla path reasoning be a more efficient choice compared to BF?

2. Another question is, can we simply leverage the idea of beam search into vanilla path reasoning? For example, we keep top-K ranked paths for each step, which may also avoid the exponential growth of the number of paths. Yes",263,0,6,0.7921,0.0608333333,0.8937489986,218,45.3052,0.1303,neurips,0.0,3,4,2,3,partially factual,4,3,60,polite,4,neutral,4,low,2,3,3,2,partially factual,4,4,55,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
8,Reviewer-aJpk,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","LLMs have shown success as autonomous agents that make and execute plans in sequential decision problems. Existing methods either make open-loop plans, limiting adaptability to the environment, or closed-loop plans. Existing closed-loop methods, apart from DEPS, keep the plan static but simply modify immediate actions according to environment feedback, leading to potentially sub-optimal policies. The authors introduce AdaPlanner, a closed-loop LLM planner that additionally allows for *plan* refinement during the episode. The success of their method not only relies on this, but additionally code-style prompts and a skill-discovery mechanism for few-shot exemplars. AdaPlanner outperforms existing works while relying on far fewer demonstration examples from similar tasks.  - Empirically the authors show strong results with respect to sample efficiency and asymptotic performance.

- Many ablations make it easy to understand which components of the model lead to overall success. 

- Conceptually simple approach.
 - In the evaluation section, the baselines are glossed over. This makes it hard to comprehend the distinction between their approach and the baselines. 
   - I’d recommend adding some of the Appendix descriptions to the evaluation section, and potentially referencing Table 1 more often.

- The authors use the term ‘hallucination’ a lot but do not define it.

- The authors discuss in- and out-of- plan refiners a lot before providing intuitive examples for when either would be necessary. Could the authors provide more examples earlier on in the paper?

- DEPS appears to be a relevant baseline. Could the authors include it or at least delve deeper into its limitations and why it is not appropriate?

- It appears that the largest contributor to the success of AdaPlanner, over existing approaches, is code style prompts and skill prompts. Wouldn’t it be worthwhile to apply those modifications to existing approaches, like Reflextion (Fig 4), and contrast?

- AdaPlanner prompts the LLM to correct any syntax errors. How important is this? Would be nice to include this ablation.
 - Line 80, could you define the output of pi, in the same way that you did for the planner?
- Line 81, shouldn’t it be P_t rather than P_{t - 1}?
- Lines 114 - 144 I think you’ve repeated the sentence twice.
- Line 216, what are the 6 task types?
- Line 132, how is N chosen and what’s its effect on performance?
 AdaPlanner still requires demonstrations for learning. Would be worthwhile comparing with RL agents trained directly on the task, without any expert demonstrations.",407,0,1,0.8075,0.19765625,0.8522759080000001,215,45.2435,0.464,neurips,0.0117647058823529,3,4,4,4,partially factual,4,3,77,neutral,3,neutral,3,low,4,4,4,4,factual,4,4,85,polite,4,neutral,5,moderate,4.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,5,4,4,5,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
8,Reviewer-GDYQ,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","The paper presents AdaPlanner, a closed-loop planning method that uses a large language model (LLM) to solve tasks in text-based environments. AdaPlanner operates by decomposing a complex task into manageable sub-goals and predicting environmental feedback for each. During execution, it refines its actions based on the feedback received from the environment. AdaPlanner operates solely via prompting, eliminating the need for a dedicated training phase and reducing its computational cost. The paper demonstrates that AdaPlanner consistently outperforms existing baselines, achieving state-of-the-art performance in ALFWorld tasks and MiniWoB++ tasks. - AdaPlanner introduces a novel approach to task-solving in text-based environments using a large language model. It stands out for its closed-loop planning method and its ability to decompose tasks into manageable sub-goals.
- The paper is well-written and clear. The authors have done a good job of explaining complex concepts and methodologies in an understandable manner.
- The work presents a new way of leveraging large language models for task-solving in text-based environments. The results show that AdaPlanner can effectively leverage feedback to refine its plans and enhance its performance. - The part about skill discovery is not described very clearly, and I still cannot understand the details of the skill discovery module well.
- The author compared the version without a code interface in the experiment, but it seems that they did not specifically show the prompt after removing the code interface. At the same time, as an ablation experiment, it is also necessary to analyze the effects of specific components in the code interface.
- The phenomenon that GPT-3 performs better than GPT-3.5 is interesting, but it seems that the paper only compares GPT-3 and GPT-3.5 in Alfworld, without conducting the same experiments in MiniWoB++ to further support the conclusion. And the author's hypotheses about this phenomenon (the smaller scale of GPT3.5) lacks specific analysis or literature references to support it. - In the experiment, what is the proportion of in-plan and out-of-plan occurrences? How will this proportion change over time? This should be a necessary indicator for understanding the two refiners.
- On MiniWoB++, will there be better performance from GPT-3 than GPT-3.5?
- Is there still a necessity for AdaPlanner in larger-scale LLMs, such as models like GPT4 with better self-refining capabilities? - As mentioned above, this paper still needs more experiments and analysis to further validate the rationality of its methods, as well as the observed phenomena and corresponding hypotheses.",403,0,0,0.8062,0.159257885,0.9227041602,215,34.8105,0.0468,neurips,0.0,2,3,3,3,partially factual,4,3,78,neutral,4,neutral,3,low,4,4,3,4,partially factual,4,4,75,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,4,4,3,4,partially factual,3,3,78,polite,4,neutral,4,low
66,Reviewer-U9CF,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","This paper proposes Adaptive Feature Transfer (AFT) to transfer from an arbitrary set of pre-trained models into a single downstream model. When fine-tuning the downstream model, AFT introduces an informative prior favoring low mutual information between the downstream inputs and features given the pre-trained features. It then efficiently optimizes it by exploiting a kernel formulation of the objective. This paper conducts experiments on multiple vision, language, and multi-modal datasets, and AFT outperforms standard transfer learning and knowledge distillation methods. 1 This paper explores an interesting problem of efficient transfer learning from arbitrary pre-trained models. 
 
2 The proposed AFT method is efficient and easy to implement. It is evaluated on multiple datasets on various tasks, including vision, language, and multi-modal, and outperforms standard fine-tuning and knowledge distillation methods.
 
3 This paper is clearly written and presented, and the proposed method is easy to follow. 1 Compared with the knowledge distillation mentioned in this paper (KD), the authors emphasize the contribution that KD transforms the downstream (student) features, while the proposed AFT transforms the pre-trained (teacher) features. However, in the general feature-based knowledge distillation framework \[1\], both teacher and student features can be transformed before minimizing their distances. This makes the proposed method a simple variant in the feature-based knowledge distillation framework and thus lack novelty.  

2 Some related works are missing in this paper, including those improving standard transfer learning and those considering transfer learning from multiple pre-trained models. For example, \[2\] also proposes to match pre-trained features and downstream features during transfer learning. \[3\] and \[4\] also consider transfer learning from multiple pre-trained models and propose to use features or knowledge distillation from pre-trained models. More related works in these two topics should be discussed in the paper. In experiments, some of these more advanced transfer learning methods should be compared, instead of only comparing AFT with standard transfer learning or knowledge distillation.

3 Some issues in the experiments. 

(1) It seems that in this paper, the pre-trained models are stronger than downstream models. Figures 2(c) and 3(c) also show that transfer learning by directly using pre-trained models leads to better results than AFT. This makes the problem setting in the experiments less convincing, especially considering that the linear probe from pre-trained models is also efficient.
 
(2) It is good to see experiments from vision, language, and multi-modal tasks, but in each task, only a few datasets are evaluated, and most of them seem to be easy.
 
(3) Transfer learning from multiple models is interesting, but currently, the number of models in the experiments is still small, and the improvements by using more pre-trained models are not clear from the results.

\[1\] Knowledge Distillation: A Survey. 2021

\[2\] Delta: Deep learning transfer using feature map with attention for convolutional networks. ICLR 2019

\[3\] Knowledge flow: Improve upon your teachers. ICLR 2019

\[4\] Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs. JMLR 2022 1 What are the exact results before normalization in Figure 2(b)?

2 Could the kernel method in Section 3.2 still improve the performance if the downstream datasets have more training data? It would be better to have more experiments on more datasets or situations to validate the efficacy of such a design.",538,8,0,0.7688,0.1518897769,0.9437077045,47,40.6613,0.1647,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,4,4,4,4,partially factual,4,4,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
66,Reviewer-QqHJ,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The paper proposes Adaptive Feature Transfer (AFT) to extract information from the (multiple) pre-trained model to the downstream model by minimizing the mutual information between pre-trained and downstream features. The paper at the end uses a stronger regularized loss by only minimizing the feature distance in the downstream and pre-trained space to make the training more robust. The results show that AFT outperforms KD on vision and language tasks and architectures. 1. The paper observes that the stronger regularization (using kernels) on the regularization term can further improve the results. 

2. The proposed approach outperforms KD on various tasks and architectures. 1. I do not fully understand what is the main difference between AFT with $\rho$ and KD, namely, the equation (7) and (8). Is the main difference that in equation (7) you downsample the pre-trained features and in equation (8) you upsample the downstream features? If yes, is there mathematical proof (or visualization, other experiments, etc) that this difference really makes the model learn the essential information of downstream tasks and discard useless information?

2. Some parts of Section 3.2 are unclear. 

(1) There is a missing $\prime$ in the first kernel definition, the definition of applying the kernel function to vector is undefined in equation (9), $X$ and $X^{\prime}$ should be the same according to Algorithm 1 but not mentioned in the text.

(2) Why the $\rho$ in Section 3.2 does not downsample the feature to the shape of the downstream features ($d_{\phi}$)?

(3) How to optimize U to make sure it is orthogonal?

(4) In Algorithm 1, the definition of $\hat{L}(\theta)$ is missing and $\hat{Y}_{batch}$ is not used.

3. The evaluation is conducted only on small subsets of benchmarks. Using more datasets and reporting the average results would make the results more convincing (like datasets used in few-shot experiments in CLIP, GLUE, SuperGLUE, Winogrande, etc). Why choose Eq (7) as the starting point to develop AFT rather than equation (8), as in Figure 4, the results of AFT w/o kernel (optimizing Eq 7 only) are not better than STL (maybe KD either).",345,0,5,0.7134,0.107183908,0.9178090692,60,47.8521,0.069,iclr,0.0,3,4,3,3,factual,3,4,70,polite,4,negative,3,low,5,4,4,5,factual,5,5,88,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,neutral,5,neutral,5,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
114,Reviewer-n4fn,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","The paper introduces a rehearsal-based method called AGILE to tackle the class-incremental learning setting in continual learning. Specifically, the paper leverages learnable task embedding vectors and shared task-attention module for better mitigating task interference. Experimental results on benchmark datasets demonstrate the effectiveness of the method. - The paper reads well and is easy to follow.
- Class-incremental learning is indeed a more challenging setting than task-incremental learning. - The idea of using task-attention or task embedding vector is not quite novel. For example, DyTox \[1\] also has a task attention module, L2P \[2\] leverages task-specific prompts. 
- Following the first one, I think the paper misses several recent competitive methods to compare against. For example, I understand both DyTox and L2P are based on transformers. However, if the proposed method AGILE is generalizable enough, it should be compatible with transformer architectures as well, making comparison with more advance methods like DyTox, L2P possible.
- 
- The contents in middle and right subfigures in figure 3 seems missing?

\[1\] Douillard, Arthur, et al. ""Dytox: Transformers for continual learning with dynamic token expansion."" CVPR 2022
\[2\] Wang, Zifeng, et al. ""Learning to prompt for continual learning."" CVPR 2022 - I understand the method is based on rehearsal, what if the rehearsal part is removed. Will the remaining design lead to improvement upon the baselines without rehearsal as well?
- See weaknesses for the rest questions.",233,4,2,0.7893,0.2149470899,0.8692247868,48,41.8675,0.1262,iclr,0.0119047619047618,3,4,3,4,factual,3,3,65,polite,4,neutral,4,low,3,4,3,4,partially factual,3,3,65,polite,4,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,5,3,4,partially factual,3,3,75,polite,4,neutral,4,low
75,Reviewer-wEMM,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","In order to distinguish between human-generated text and machine-generated text, the authors propose the use of the periodicity of cross entropy for discrimination. More specifically, they suggest analyzing cross entropy through the Fourier transform. 1. This paper is well-written and easy to follow.
2. The experimental section of this paper is fairly comprehensive.  The authors' experimental objects have broadly encompassed the latest open-source large models. Although it lacks large language models like GPT-3.5 (the cross entropy can still be obtained through APIs).

 1. Motivation. The motivation of the paper is not clear, as the authors do not clearly explain why the CE of human language would exhibit periodicity. In the related work section, they briefly mention previous works, but in my view, dialogue tasks are just a specific case of text generation. Overall, skipping the motivation part significantly reduces the soundness of this paper.

2. Method. The authors' method simply involves applying a FFT to the CE sequences, which I believe lacks substantial novelty. Why haven't the authors considered using the information in the frequency domain as input to a deep neural network to incorporate a powerful NN? Why only analyze information in the frequency domain using spectral similarity metrics? Additionally, most of these metrics have already been presented in \[1\]. Which method would better utilize this information for discrimination? In conclusion, the proposed method by the authors lacks both sufficient contribution and profound insight.

3. Experiments.  In the experimental section, the authors did not compare against sufficient baselines. For instance, could we achieve good results by only training a contrastive model using human-generated text and LLM-generated text? How helpful is the frequency domain information in discriminating texts?

\[1\] Y. Xu and D. Reitter. Spectral analysis of information density in dialogue predicts collaborative task performance. ACL see weakness  the authors adequately addressed the limitations",304,2,8,0.8037000000000001,0.1702938988,0.8712091446,216,32.5148,0.1199,neurips,0.011111111111111,3,4,3,4,factual,3,3,70,polite,4,negative,4,low,3,3,3,3,partially factual,3,3,65,neutral,4,neutral,4,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,3,partially factual,3,3,65,neutral,4,negative,4,low
75,Reviewer-mMGf,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a set of metrics based on Fourier Analysis of the estimated Cross-Entropy (FACE) of language. The main idea is to compute the similarity between the spectra of cross-entropy in model-generated texts and human-written texts. Experimental results show that FACE as a computationally efficient metric can scale with model size and reflect the outcomes of different sampling methods for decoding. 1. The idea to introduce the spectra of cross-entropy into the evaluation task of open-ended text generation is interesting since it may include some patterns (e.g. periodical patterns) to identify the difference between model-generated texts and human-written texts.

2. This paper is overall well-written and easy to follow. 1. The proposed method lacks deeper analysis on the spectrum of cross entropy in the evaluation task. The authors only use the spectrum of cross entropy as a feature vector of texts to compute similarities without clearly describing the characteristics of texts it can reflect. This seems like an empirical try without definite intuitions or theoretical supports. In comparison, the features which are commonly used in the existing metrics such as n-gram statistics (in BLEU) and contextual hidden vectors (in BERTScore) intuitively indicate the surface-level and semantic-level representation of texts, respectively.

2. From Table 5, the performance of SO is still worse than that of MAUVE proposed in 2021. I understand that pursuing SOTA is not necessary for each paper. But the authors should provide more insights into the advantages of SO over MAUVE in other aspects.

3. In Section 4.4, the authors mention that they use GPT-2 of different scales to compute the spectra of GPT-2 output data. I wonder whether this setting can introduce potential bias because the cross entropy may be exceptionally low when using GPT-2 to evaluate its own output data from my experience.
 I have included my questions in the weaknesses part. The authors have adequately addressed the limitations.",314,0,5,0.8096,0.0682098765,0.9098261595,216,36.0945,0.11,neurips,0.0112359550561798,4,4,4,5,factual,4,4,80,polite,4,negative,4,none,3,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_f1000 = pd.read_csv('f1000research.csv')\n",
    "df_sw = pd.read_csv('semantic-web-journal-analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the mapping for review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision',\n",
    "    'Accept': 'Accept',\n",
    "    'Reject': 'Reject'\n",
    "}\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "# Apply the mapping and filter to the three categories for both dataframes\n",
    "for df_name in ['df_f1000', 'df_sw']:\n",
    "    df = globals()[df_name].copy()\n",
    "    df['review_suggestion'] = df['review_suggestion'].replace(review_mapping)\n",
    "    df = df[df['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "    globals()[df_name] = df\n",
    "\n",
    "# Compute per-sample weights based on the new categories\n",
    "for df in (df_f1000, df_sw):\n",
    "    counts = df.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "    df['weight'] = 1.0 / counts  # Each group sums to 1\n",
    "\n",
    "# Updated color map for the three categories\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define the plotting function with the new color map\n",
    "def plot_politeness_distribution(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for suggestion in df['review_suggestion'].unique():\n",
    "        subset = df[df['review_suggestion'] == suggestion]\n",
    "        kwargs = {'color': color_map[suggestion]} if suggestion in color_map else {}\n",
    "        sns.kdeplot(\n",
    "            data=subset,\n",
    "            x='Politeness',\n",
    "            label=suggestion,\n",
    "            weights=subset['weight'],\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3,\n",
    "            **kwargs\n",
    "        )\n",
    "    plt.title(f\"Politeness Score Distribution – {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Politeness Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "plot_politeness_distribution(df_f1000, 'F1000')\n",
    "plot_politeness_distribution(df_sw, 'Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the mapping for review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision',\n",
    "    'Accept': 'Accept',\n",
    "    'Reject': 'Reject'\n",
    "}\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "# Apply the mapping and filter to the three categories for both dataframes\n",
    "for df_name in ['df_f1000', 'df_sw']:\n",
    "    df = globals()[df_name].copy()\n",
    "    df['review_suggestion'] = df['review_suggestion'].replace(review_mapping)\n",
    "    df = df[df['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "    globals()[df_name] = df\n",
    "\n",
    "# Compute per-sample weights based on the new categories\n",
    "for df in (df_f1000, df_sw):\n",
    "    counts = df.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "    df['weight'] = 1.0 / counts  # Each group sums to 1\n",
    "\n",
    "# Updated color map for the three categories\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define the plotting function with the new color map\n",
    "def plot_len_distribution(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for suggestion in df['review_suggestion'].unique():\n",
    "        subset = df[df['review_suggestion'] == suggestion]\n",
    "        kwargs = {'color': color_map[suggestion]} if suggestion in color_map else {}\n",
    "        sns.kdeplot(\n",
    "            data=subset,\n",
    "            x='Comprehensiveness',\n",
    "            label=suggestion,\n",
    "            weights=subset['weight'],\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3,\n",
    "            **kwargs\n",
    "        )\n",
    "    plt.title(f\"Comprehensiveness Score Distribution – {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Comprehensiveness Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xlim(0, 2000)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "plot_len_distribution(df_f1000, 'F1000')\n",
    "plot_len_distribution(df_sw, 'Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the mapping for review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision',\n",
    "    'Accept': 'Accept',\n",
    "    'Reject': 'Reject'\n",
    "}\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "# Apply the mapping and filter to the three categories for both dataframes\n",
    "for df_name in ['df_f1000', 'df_sw']:\n",
    "    df = globals()[df_name].copy()\n",
    "    df['review_suggestion'] = df['review_suggestion'].replace(review_mapping)\n",
    "    df = df[df['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "    globals()[df_name] = df\n",
    "\n",
    "# Compute per-sample weights based on the new categories\n",
    "for df in (df_f1000, df_sw):\n",
    "    counts = df.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "    df['weight'] = 1.0 / counts  # Each group sums to 1\n",
    "\n",
    "# Updated color map for the three categories\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define the plotting function with the new color map\n",
    "def plot_len_distribution(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for suggestion in df['review_suggestion'].unique():\n",
    "        subset = df[df['review_suggestion'] == suggestion]\n",
    "        kwargs = {'color': color_map[suggestion]} if suggestion in color_map else {}\n",
    "        sns.kdeplot(\n",
    "            data=subset,\n",
    "            x='Clarity_and_Readability',\n",
    "            label=suggestion,\n",
    "            weights=subset['weight'],\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3,\n",
    "            **kwargs\n",
    "        )\n",
    "    plt.title(f\"Clarity and Readability Score Distribution – {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Clarity_and_Readability Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xlim(0, 80)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "plot_len_distribution(df_f1000, 'F1000')\n",
    "plot_len_distribution(df_sw, 'Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the mapping for review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision',\n",
    "    'Accept': 'Accept',\n",
    "    'Reject': 'Reject'\n",
    "}\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "# Apply the mapping and filter to the three categories for both dataframes\n",
    "for df_name in ['df_f1000', 'df_sw']:\n",
    "    df = globals()[df_name].copy()\n",
    "    df['review_suggestion'] = df['review_suggestion'].replace(review_mapping)\n",
    "    df = df[df['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "    globals()[df_name] = df\n",
    "\n",
    "# Compute per-sample weights based on the new categories\n",
    "for df in (df_f1000, df_sw):\n",
    "    counts = df.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "    df['weight'] = 1.0 / counts  # Each group sums to 1\n",
    "\n",
    "# Updated color map for the three categories\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define the plotting function with the new color map\n",
    "def plot_len_distribution(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for suggestion in df['review_suggestion'].unique():\n",
    "        subset = df[df['review_suggestion'] == suggestion]\n",
    "        kwargs = {'color': color_map[suggestion]} if suggestion in color_map else {}\n",
    "        sns.kdeplot(\n",
    "            data=subset,\n",
    "            x='Relevance_Alignment',\n",
    "            label=suggestion,\n",
    "            weights=subset['weight'],\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3,\n",
    "            **kwargs\n",
    "        )\n",
    "    plt.title(f\"Relevance Alignment Score Distribution – {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Relevance Alignment Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.xlim(0.5, 1)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the plots\n",
    "plot_len_distribution(df_f1000, 'F1000')\n",
    "plot_len_distribution(df_sw, 'Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#drop columns starting with 'llm'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.startswith('llm')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.startswith('llm')]\n",
    "\n",
    "#drop columns with names ends with '.1'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('.1')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('.1')]\n",
    "\n",
    "#drop columns with names weight\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('weight')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('weight')]\n",
    "\n",
    "###################\n",
    "# create new column named hedge_score as follow: 1 - hedge_C / (hedge_E + hedge_D + hedge_C + hedge_N + hedge_I)\n",
    "df_f1000['hedge_score'] = 1 - df_f1000['hedge_C'] / (df_f1000['hedge_E'] + df_f1000['hedge_D'] + df_f1000['hedge_C'] + df_f1000['hedge_N'] + df_f1000['hedge_I'])\n",
    "df_sw['hedge_score'] = 1 - df_sw['hedge_C'] / (df_sw['hedge_E'] + df_sw['hedge_D'] + df_sw['hedge_C'] + df_sw['hedge_N'] + df_sw['hedge_I'])\n",
    "\n",
    "# drop hedge_E, hedge_D, hedge_C, hedge_N, hedge_I\n",
    "df_f1000 = df_f1000.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "df_sw = df_sw.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "\n",
    "# drop flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index\n",
    "df_f1000 = df_f1000.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_sw = df_sw.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "\n",
    "# rename flesch_reading_ease to readability_score\n",
    "df_f1000 = df_f1000.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "df_sw = df_sw.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "\n",
    "#drop columns end with date and submission_number\n",
    "df_f1000 = df_f1000.drop(columns=[col for col in df_f1000.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "df_sw = df_sw.drop(columns=[col for col in df_sw.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "\n",
    "\n",
    "# df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "# df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment'})\n",
    "\n",
    "\n",
    "# Function to plot correlation map\n",
    "def plot_correlation_map(df, title):\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "    correlation_matrix = numerical_columns.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', cbar=True, vmax=1, vmin=-1, center=0)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation maps for each dataframe\n",
    "plot_correlation_map(df_f1000, 'Correlation Map - F1000')\n",
    "plot_correlation_map(df_sw, 'Correlation Map - Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_frequency(df, column, title, bins=50):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=column,\n",
    "        bins=bins,\n",
    "        kde=False\n",
    "    )\n",
    "    # if this is the readability plot, limit x from 0 to 40\n",
    "    if column == 'automated_readability_index':\n",
    "        plt.xlim(0, 40)\n",
    "\n",
    "    plt.title(f\"{column.replace('_', ' ').title()} Frequency - {title}\",\n",
    "              fontsize=16)\n",
    "    plt.xlabel(column.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df, label in [(df_f1000, 'F1000'), (df_sw, 'Semantic Web Journal')]:\n",
    "    for col in ['automated_readability_index', 'days_to_submit']:\n",
    "        plot_frequency(df, col, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_frequency(df, column, title, bins=50):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=column,\n",
    "        bins=bins,\n",
    "        kde=False\n",
    "    )\n",
    "    # if this is the readability plot, limit x from 0 to 40\n",
    "    if column == 'question_count':\n",
    "        plt.xlim(0, 30)\n",
    "    elif column == 'citation_count':\n",
    "        plt.xlim(0, 30)\n",
    "    \n",
    "    plt.title(f\"{column.replace('_', ' ').title()} Frequency of {title}\",\n",
    "              fontsize=16)\n",
    "    plt.xlabel(column.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for df, label in [(df_sw, 'Semantic Web Journal'), (df_f1000, 'F1000')]:\n",
    "    for col in ['question_count', 'citation_count']:\n",
    "        plot_frequency(df, col, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Approved': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Not Approved': 'red',\n",
    "    'Minor Revision': 'blue',\n",
    "    'Major Revision': 'orange',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "def plot_polarity_distribution(df, title):\n",
    "    df = df.copy()\n",
    "    df['decision'] = df['review_suggestion']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].dropna().unique():\n",
    "        subset = df[df['decision'] == decision]\n",
    "        sns.kdeplot(\n",
    "            data=subset,\n",
    "            x='sentiment_polarity',\n",
    "            label=decision,\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3,\n",
    "            color=color_map.get(decision, None)\n",
    "        )\n",
    "    plt.title(f\"Sentiment Polarity Score Distribution – {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Sentiment Polarity Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(-0.2, 0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for Semantic Web Journal\n",
    "plot_polarity_distribution(df_sw, 'Semantic Web Journal')\n",
    "\n",
    "# Plot for F1000\n",
    "plot_polarity_distribution(df_f1000, 'F1000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sw_reviewer = pd.read_pickle('/home/ali/Review_Quality_Benchmark/Soroush/processed/sw_reviewers_similarity_info.pkl')\n",
    "\n",
    "# 1. Turn your nested dict into a flat DataFrame\n",
    "records = []\n",
    "for paper_id, reviewers in df_sw_reviewer.items():\n",
    "    for reviewer, metrics in reviewers.items():\n",
    "        records.append({\n",
    "            'paper_id':             paper_id,\n",
    "            'reviewer':             reviewer,\n",
    "            'max_similarity':       metrics.get('max_similarity'),\n",
    "            'avg_similarity':       metrics.get('avg_similarity'),\n",
    "            'avg_recent_similarity':metrics.get('avg_recent_similarity')\n",
    "        })\n",
    "df_sim = pd.DataFrame(records)\n",
    "\n",
    "# 2. Merge onto your df_sw\n",
    "df_sw = df_sw.merge(\n",
    "    df_sim,\n",
    "    on=['paper_id','reviewer'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Force Anonymous rows to stay null\n",
    "anon_mask = df_sw['reviewer'] == 'Anonymous'\n",
    "df_sw.loc[anon_mask, ['max_similarity','avg_similarity','avg_recent_similarity']] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Preprocess review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision'\n",
    "}\n",
    "\n",
    "# Apply mapping and filter to target categories\n",
    "df_sw['review_suggestion'] = df_sw['review_suggestion'].replace(review_mapping)\n",
    "df_sw = df_sw[df_sw['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "\n",
    "# 2. Calculate weights for density normalization\n",
    "counts = df_sw.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "df_sw['weight'] = 1.0 / counts\n",
    "\n",
    "# 3. Set up visualization parameters\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "metric = 'avg_similarity'\n",
    "\n",
    "# 4. Create single figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 5. Plot distributions\n",
    "for suggestion in df_sw['review_suggestion'].unique():\n",
    "    subset = df_sw[\n",
    "        (df_sw['review_suggestion'] == suggestion) &\n",
    "        (df_sw[metric].notna())\n",
    "    ]\n",
    "    \n",
    "    if subset.empty:\n",
    "        continue\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=subset,\n",
    "        x=metric,\n",
    "        label=suggestion,\n",
    "        color=color_map[suggestion],\n",
    "        weights=subset['weight'],\n",
    "        fill=False,\n",
    "        alpha=0.9,\n",
    "        linewidth=3\n",
    "    )\n",
    "\n",
    "# 6. Configure plot aesthetics\n",
    "plt.title(f\"Average Similarity Distribution by Review Decision - Semantic Web Journal\", fontsize=16)\n",
    "plt.xlabel(\"Average Similarity Score\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.legend(title=\"\", fontsize=12, title_fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xlim(0.5, 0.9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- load your nested reviewer info for F1000 ---\n",
    "df_f1000_reviewer = pd.read_pickle('/home/ali/Review_Quality_Benchmark/Soroush/processed/f1000_reviewers_similarity_info.pkl')\n",
    "\n",
    "df_f1000 = pd.read_csv('f1000research.csv')\n",
    "\n",
    "# 1. flatten into a lookup DataFrame\n",
    "records = []\n",
    "for paper_id, reviewers in df_f1000_reviewer.items():\n",
    "    for reviewer, metrics in reviewers.items():\n",
    "        records.append({\n",
    "            'paper_id':              paper_id + '/v1',\n",
    "            'reviewer':              reviewer,\n",
    "            'max_similarity':        metrics.get('max_similarity'),\n",
    "            'avg_similarity':        metrics.get('avg_similarity'),\n",
    "            'avg_recent_similarity': metrics.get('avg_recent_similarity')\n",
    "        })\n",
    "df_f1000_sim = pd.DataFrame(records)\n",
    "\n",
    "# 2. merge onto your df_f1000\n",
    "df_f1000 = df_f1000.merge(\n",
    "    df_f1000_sim,\n",
    "    on=['paper_id','reviewer'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. force Anonymous rows to stay null\n",
    "anon_mask = df_f1000['reviewer'] == 'Anonymous'\n",
    "df_f1000.loc[anon_mask, ['max_similarity','avg_similarity','avg_recent_similarity']] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Preprocess review suggestions\n",
    "review_mapping = {\n",
    "    'Approved': 'Accept',\n",
    "    'Not Approved': 'Reject',\n",
    "    'Approved With Reservations': 'Revision',\n",
    "    'Minor Revision': 'Revision',\n",
    "    'Major Revision': 'Revision'\n",
    "}\n",
    "\n",
    "# Apply mapping and filter to target categories\n",
    "df_f1000['review_suggestion'] = df_f1000['review_suggestion'].replace(review_mapping)\n",
    "df_f1000 = df_f1000[df_f1000['review_suggestion'].isin(['Accept', 'Reject', 'Revision'])].copy()\n",
    "\n",
    "# 2. Calculate weights for density normalization\n",
    "counts = df_f1000.groupby('review_suggestion')['review_suggestion'].transform('count')\n",
    "df_f1000['weight'] = 1.0 / counts\n",
    "\n",
    "# 3. Set up visualization parameters\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "metric = 'avg_similarity'\n",
    "\n",
    "# 4. Create single figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 5. Plot distributions\n",
    "for suggestion in df_f1000['review_suggestion'].unique():\n",
    "    subset = df_f1000[\n",
    "        (df_f1000['review_suggestion'] == suggestion) &\n",
    "        (df_f1000[metric].notna())\n",
    "    ]\n",
    "    \n",
    "    if subset.empty:\n",
    "        continue\n",
    "    \n",
    "    sns.kdeplot(\n",
    "        data=subset,\n",
    "        x=metric,\n",
    "        label=suggestion,\n",
    "        color=color_map[suggestion],\n",
    "        weights=subset['weight'],\n",
    "        fill=False,\n",
    "        alpha=0.9,\n",
    "        linewidth=3\n",
    "    )\n",
    "\n",
    "# 6. Configure plot aesthetics\n",
    "plt.title(f\"Average Similarity Distribution by Review Decision - F1000\", fontsize=16)\n",
    "plt.xlabel(\"Average Similarity Score\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.legend(title=\"\", fontsize=12, title_fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xlim(0.5, 0.9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "df_f1000 = pd.read_csv('f1000research.csv')\n",
    "df_sw = pd.read_csv('semantic-web-journal-analysis.csv')\n",
    "\n",
    "\n",
    "#drop columns starting with 'llm'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.startswith('llm')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.startswith('llm')]\n",
    "\n",
    "#drop columns with names ends with '.1'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('.1')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('.1')]\n",
    "\n",
    "#drop columns with names weight\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('weight')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('weight')]\n",
    "\n",
    "###################\n",
    "# create new column named hedge_score as follow: 1 - hedge_C / (hedge_E + hedge_D + hedge_C + hedge_N + hedge_I)\n",
    "df_f1000['hedge_score'] = 1 - df_f1000['hedge_C'] / (df_f1000['hedge_E'] + df_f1000['hedge_D'] + df_f1000['hedge_C'] + df_f1000['hedge_N'] + df_f1000['hedge_I'])\n",
    "df_sw['hedge_score'] = 1 - df_sw['hedge_C'] / (df_sw['hedge_E'] + df_sw['hedge_D'] + df_sw['hedge_C'] + df_sw['hedge_N'] + df_sw['hedge_I'])\n",
    "\n",
    "# drop hedge_E, hedge_D, hedge_C, hedge_N, hedge_I\n",
    "df_f1000 = df_f1000.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "df_sw = df_sw.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "\n",
    "# drop flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index\n",
    "df_f1000 = df_f1000.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_sw = df_sw.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "\n",
    "# rename flesch_reading_ease to readability_score\n",
    "df_f1000 = df_f1000.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "df_sw = df_sw.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "\n",
    "#drop columns end with date and submission_number\n",
    "df_f1000 = df_f1000.drop(columns=[col for col in df_f1000.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "df_sw = df_sw.drop(columns=[col for col in df_sw.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "\n",
    "\n",
    "# df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "# df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment',\n",
    "'max_similarity': 'In-depth Topical Alignment', 'avg_similarity': 'General Topical Alignment', 'avg_recent_similarity': 'Recency-Based Topical Alignment', 'reviewer_citations': 'Reviewer\\'s Citation', 'reviewer_experience_years': 'Reviewer’s Academic Tenure'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment',\n",
    "'max_similarity': 'In-depth Topical Alignment', 'avg_similarity': 'General Topical Alignment', 'avg_recent_similarity': 'Recency-Based Topical Alignment', 'reviewer_citations': 'Reviewer\\'s Citation', 'reviewer_experience_years': 'Reviewer’s Academic Tenure'})\n",
    "\n",
    "\n",
    "'''\n",
    "max_similarity --> In-depth Topical Alignment\n",
    "avg_similarity --> General Topical Alignment\n",
    "avg_recent_similarity --> Recency-Based Topical Alignment\n",
    "reviewer_citations --> Reviewer's Citation\n",
    "reviewer_experience_years --> Reviewer’s Academic Tenure\n",
    "'''\n",
    "\n",
    "# Function to plot correlation map\n",
    "def plot_correlation_map(df, title):\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "    correlation_matrix = numerical_columns.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', cbar=True, vmax=1, vmin=-1, center=0)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation maps for each dataframe\n",
    "plot_correlation_map(df_f1000, 'Correlation Map - F1000')\n",
    "plot_correlation_map(df_sw, 'Correlation Map - Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "df_f1000 = pd.read_csv('f1000research.csv')\n",
    "df_sw = pd.read_csv('semantic-web-journal-analysis.csv')\n",
    "\n",
    "\n",
    "#drop columns starting with 'llm'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.startswith('llm')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.startswith('llm')]\n",
    "\n",
    "#drop columns with names ends with '.1'\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('.1')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('.1')]\n",
    "\n",
    "#drop columns with names weight\n",
    "df_f1000 = df_f1000.loc[:, ~df_f1000.columns.str.endswith('weight')]\n",
    "df_sw = df_sw.loc[:, ~df_sw.columns.str.endswith('weight')]\n",
    "\n",
    "###################\n",
    "# create new column named hedge_score as follow: 1 - hedge_C / (hedge_E + hedge_D + hedge_C + hedge_N + hedge_I)\n",
    "df_f1000['hedge_score'] = 1 - df_f1000['hedge_C'] / (df_f1000['hedge_E'] + df_f1000['hedge_D'] + df_f1000['hedge_C'] + df_f1000['hedge_N'] + df_f1000['hedge_I'])\n",
    "df_sw['hedge_score'] = 1 - df_sw['hedge_C'] / (df_sw['hedge_E'] + df_sw['hedge_D'] + df_sw['hedge_C'] + df_sw['hedge_N'] + df_sw['hedge_I'])\n",
    "\n",
    "# drop hedge_E, hedge_D, hedge_C, hedge_N, hedge_I\n",
    "df_f1000 = df_f1000.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "df_sw = df_sw.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "\n",
    "# drop flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index\n",
    "df_f1000 = df_f1000.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_sw = df_sw.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "\n",
    "# rename flesch_reading_ease to readability_score\n",
    "df_f1000 = df_f1000.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "df_sw = df_sw.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "\n",
    "#drop columns end with date and submission_number\n",
    "# df_f1000 = df_f1000.drop(columns=[col for col in df_f1000.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "# df_sw = df_sw.drop(columns=[col for col in df_sw.columns if col.endswith('submit') or col.endswith('submission_number')])\n",
    "\n",
    "\n",
    "# df_f1000 = df_f1000.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "# df_sw = df_sw.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment'})\n",
    "\n",
    "df_f1000 = df_f1000.rename(columns={'length_words': 'Review Length', 'mattr': 'Lexical Diversity', 'question_count': '#Raised Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment',\n",
    "'max_similarity': 'In-depth Topical Alignment', 'avg_similarity': 'General Topical Alignment', 'avg_recent_similarity': 'Recency-Based Topical Alignment', 'reviewer_citations': 'Reviewer\\'s Citation', 'reviewer_experience_years': 'Reviewer’s Academic Tenure', 'explicit_reference': '#Section-specific Comments', 'days_to_submit': 'Timeliness'})\n",
    "df_sw = df_sw.rename(columns={'length_words': 'Review Length', 'mattr': 'Lexical Diversity', 'question_count': '#Raised Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment',\n",
    "'max_similarity': 'In-depth Topical Alignment', 'avg_similarity': 'General Topical Alignment', 'avg_recent_similarity': 'Recency-Based Topical Alignment', 'reviewer_citations': 'Reviewer\\'s Citation', 'reviewer_experience_years': 'Reviewer’s Academic Tenure', 'explicit_reference': '#Section-specific Comments', 'days_to_submit': 'Timeliness'})\n",
    "\n",
    "\n",
    "def plot_combined_correlation_maps(df1, df2, title1, title2):\n",
    "    # Columns to move to bottom\n",
    "    bottom_group = [\n",
    "        'General Topical Alignment',\n",
    "        'Recency-Based Topical Alignment',\n",
    "        'In-depth Topical Alignment',\n",
    "        \"Reviewer's Citation\",\n",
    "        \"Reviewer’s Academic Tenure\"\n",
    "    ]\n",
    "    \n",
    "    top_group = [\n",
    "        'Review Length', '#References', '#Section-specific Comments', 'Semantic Alignment', 'Timeliness', 'Politeness', \n",
    "        'Readability', 'Lexical Diversity', '#Raised Questions', 'Sentiment Polarity', 'Hedging']\n",
    "    \n",
    "    # Create figure with 2 rows, 1 column (stacked vertically)\n",
    "    fig, (ax1, ax2) = plt.subplots(\n",
    "        2, 1,\n",
    "        figsize=(10, 22),                   # flip to a tall figure\n",
    "        gridspec_kw={\n",
    "            'height_ratios': [1, 1],       # equal row heights\n",
    "            'hspace': 0.075                  # small vertical gap\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    def process_df(df):\n",
    "        # Reorder columns with bottom group last\n",
    "        # all_cols = [c for c in df.columns if c not in bottom_group]\n",
    "        # return df[all_cols + bottom_group]\n",
    "        return df[top_group + bottom_group]\n",
    "\n",
    "    # Process both dataframes\n",
    "    df1_processed = process_df(df1.select_dtypes(include=['float64', 'int64']))\n",
    "    df2_processed = process_df(df2.select_dtypes(include=['float64', 'int64']))\n",
    "    \n",
    "    # Calculate correlations with new order\n",
    "    corr1 = df1_processed.corr()\n",
    "    corr2 = df2_processed.corr()\n",
    "    \n",
    "    # Find global min/max\n",
    "    vmin = -1  #min(corr1.min().min(), corr2.min().min())\n",
    "    vmax = 1  #max(corr1.max().max(), corr2.max().max())\n",
    "    \n",
    "    # Plot first heatmap\n",
    "    sns.heatmap(corr1, ax=ax1, cmap='coolwarm', cbar=False,\n",
    "                vmin=vmin, vmax=vmax, center=0, xticklabels=False, yticklabels=True)\n",
    "    ax1.set_title(title1, fontsize=16, pad=15)\n",
    "    \n",
    "    # Plot second heatmap\n",
    "    sns.heatmap(corr2, ax=ax2, cmap='coolwarm', cbar=False,\n",
    "                vmin=vmin, vmax=vmax, center=0, xticklabels=True, yticklabels=True)\n",
    "    ax2.set_title(title2, fontsize=16, pad=15)\n",
    "    \n",
    "    # Add bold separation lines\n",
    "    sep_position = len(corr1.columns) - len(bottom_group)\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.axhline(sep_position, color='black', linewidth=0.5, linestyle='--')\n",
    "        ax.axvline(sep_position, color='black', linewidth=0.5, linestyle='--')\n",
    "    \n",
    "    # Shared colorbar\n",
    "    cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(ax1.collections[0], cax=cbar_ax, shrink=0.5)\n",
    "    \n",
    "    # Adjust ticks and labels\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right', fontsize=16)\n",
    "    plt.setp(ax1.get_yticklabels(), fontsize=16)\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=16)\n",
    "    \n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=16)\n",
    "    plt.setp(ax2.get_yticklabels(), fontsize=16)\n",
    "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right', fontsize=16)\n",
    "    \n",
    "    # Final adjustments\n",
    "    ax2.tick_params(left=False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('reviewer_dep_corr.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_combined_correlation_maps(df_f1000, df_sw,\n",
    "                              'F1000',\n",
    "                              'Semantic Web Journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_correlation_map(df, title):\n",
    "    # same bottom/top grouping logic…\n",
    "    bottom_group = [\n",
    "        'General Topical Alignment',\n",
    "        'Recency-Based Topical Alignment',\n",
    "        'In-depth Topical Alignment',\n",
    "        \"Reviewer's Citation\",\n",
    "        \"Reviewer’s Academic Tenure\"\n",
    "    ]\n",
    "    top_group = [\n",
    "        'Review Length', '#References', '#Section-specific Comments', 'Semantic Alignment',\n",
    "        'Timeliness', 'Politeness', 'Readability', 'Lexical Diversity',\n",
    "        '#Raised Questions', 'Sentiment Polarity', 'Hedging'\n",
    "    ]\n",
    "\n",
    "    def process_df(df):\n",
    "        return df[top_group + bottom_group]\n",
    "\n",
    "    # process only the first df\n",
    "    df_processed = process_df(df.select_dtypes(include=['float64', 'int64']))\n",
    "    corr = df_processed.corr()\n",
    "\n",
    "    # make a single tall figure\n",
    "    fig, ax = plt.subplots(\n",
    "        1, 1,\n",
    "        figsize=(16, 16),\n",
    "        gridspec_kw={'hspace': 0.0}  # no extra vertical gap needed\n",
    "    )\n",
    "\n",
    "    # draw the heatmap\n",
    "    sns.heatmap(\n",
    "        corr,\n",
    "        ax=ax,\n",
    "        cmap='coolwarm',\n",
    "        vmin=-1, vmax=1,\n",
    "        center=0,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar=True,\n",
    "        cbar_kws={\n",
    "            'label': 'Correlation Coefficient',\n",
    "            'orientation': 'horizontal',  # Key parameter\n",
    "            'location': 'top',            # Position\n",
    "            'pad': 0.02,                  # Distance from heatmap\n",
    "            'shrink': 0.95,                # Size relative to heatmap\n",
    "            'aspect': 40                   # Length vs thickness ratio\n",
    "        },\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        annot_kws={\"size\": 10}\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=20, pad=15)\n",
    "\n",
    "    # add the separation line\n",
    "    sep = len(corr.columns) - len(bottom_group)\n",
    "    ax.axhline(sep, color='black', linewidth=0.5, linestyle='--')\n",
    "    ax.axvline(sep, color='black', linewidth=0.5, linestyle='--')\n",
    "\n",
    "    # Access colorbar and modify labels\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.xaxis.label.set_size(24)      # Changed from yaxis to xaxis\n",
    "    cbar.ax.tick_params(axis='x', labelsize=18)  # Changed axis to 'x'\n",
    "    cbar.ax.xaxis.labelpad = 15\n",
    "\n",
    "\n",
    "    # Adjust layout to make space for top colorbar\n",
    "    # plt.subplots_adjust(top=0.85)  # Increase top margin\n",
    "\n",
    "    # tweak tick labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=20)\n",
    "    plt.setp(ax.get_yticklabels(), rotation=0, fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'corr_{title}_qmetric.pdf', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Usage for only the F1000 map:\n",
    "plot_single_correlation_map(df_f1000, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_qmetric = pd.read_csv('/home/ali/Review_Quality_Benchmark/Qmetrics-vs-Human/human_llms_qmetrics.csv')\n",
    "df_human_qmetric\n",
    "\n",
    "# drop all the columns that start with 'Qwen', 'Llama', 'GPT', 'Phi'\n",
    "df_human_qmetric = df_human_qmetric.loc[:, ~df_human_qmetric.columns.str.startswith(('Qwen', 'Llama', 'GPT', 'Phi'))]\n",
    "# drop paper_id, reviewer, title, abstract, review_text\n",
    "df_human_qmetric = df_human_qmetric.drop(columns=['paper_id', 'reviewer', 'title', 'abstract', 'review_text', 'venue'])\n",
    "# rename the columns \n",
    "df_human_qmetric = df_human_qmetric.rename(columns={'length_words': 'Review Length', 'mattr': 'Lexical Diversity', 'question_count': '#Raised Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'flesch_reading_ease': 'Readability', 'similarity_score': 'Semantic Alignment',\n",
    "'days_to_submit': 'Timeliness', 'hedging': 'Hedging'})\n",
    "df_human_qmetric\n",
    "\n",
    "CATEGORY_MAP = {\n",
    "    'Human_Factuality': ['unfactual', 'partially factual', 'factual'],\n",
    "    'Human_Politeness': ['impolite', 'neutral', 'polite'],\n",
    "    'Human_Sentiment_Polarity': ['negative', 'neutral', 'positive'],\n",
    "    'Human_Vagueness': ['none', 'low', 'moderate', 'high', 'extreme']\n",
    "}\n",
    "\n",
    "# change the values of columns in CATEGORY_MAP to numerical values\n",
    "def map_categories(df, category_map):\n",
    "    for category, values in category_map.items():\n",
    "        df[category] = df[category].apply(lambda x: values.index(x) if x in values else None)\n",
    "    return df\n",
    "\n",
    "# Apply the mapping to the DataFrame\n",
    "df_human_qmetric = map_categories(df_human_qmetric, CATEGORY_MAP)\n",
    "# Convert columns to float\n",
    "df_human_qmetric = df_human_qmetric.astype({\n",
    "    'Human_Factuality': 'float',\n",
    "    'Human_Politeness': 'float',\n",
    "    'Human_Sentiment_Polarity': 'float',\n",
    "    'Human_Vagueness': 'float'\n",
    "})\n",
    "df_human_qmetric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_human_qmetric.copy()\n",
    "\n",
    "# Separate columns into two groups\n",
    "non_human_columns = [\n",
    "        'Review Length', '#References', 'Semantic Alignment', 'Timeliness', 'Politeness', \n",
    "        'Readability', 'Lexical Diversity', '#Raised Questions', 'Sentiment Polarity', 'Hedging']\n",
    "\n",
    "human_columns = ['Human_Comprehensiveness', 'Human_Usage_of_Technical_Terms', 'Human_Factuality', 'Human_Sentiment_Polarity',\n",
    "                 'Human_Politeness', 'Human_Vagueness', 'Human_Objectivity', 'Human_Fairness', 'Human_Actionability', 'Human_Constructiveness',\n",
    "                 'Human_Relevance_Alignment', 'Human_Clarity_and_Readability', 'Human_Overall_Quality']\n",
    "\n",
    "# Create a subset DataFrame for correlation analysis\n",
    "correlation_df = df[human_columns + non_human_columns]\n",
    "\n",
    "# Calculate correlations\n",
    "corr_matrix = correlation_df.corr()\n",
    "\n",
    "# Slice the correlation matrix to show only Human vs Non-Human correlations\n",
    "human_vs_rest = corr_matrix.loc[human_columns, non_human_columns]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(human_vs_rest, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            fmt=\".2f\", \n",
    "            vmin=-1, \n",
    "            vmax=1, \n",
    "            center=0,\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            annot_kws={\"size\": 12})\n",
    "\n",
    "# Formatting\n",
    "plt.title('', pad=20, fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)\n",
    "plt.yticks(rotation=0, fontsize=16)\n",
    "# plt.xlabel('Quantifiable Metrics', fontsize=16)\n",
    "# plt.ylabel('Human Evaluations', fontsize=16)\n",
    "\n",
    "# # Access colorbar and modify labels\n",
    "# cbar = ax.collections[0].colorbar\n",
    "# cbar.ax.xaxis.label.set_size(18)      # Changed from yaxis to xaxis\n",
    "# cbar.ax.tick_params(axis='x', labelsize=14)  # Changed axis to 'x'\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = df_human_qmetric.copy()\n",
    "\n",
    "# Original column names (keep unchanged)\n",
    "non_human_columns = [\n",
    "    'Review Length', '#References', 'Semantic Alignment', 'Timeliness', 'Politeness',\n",
    "    'Readability', 'Lexical Diversity', '#Raised Questions', 'Sentiment Polarity', 'Hedging'\n",
    "]\n",
    "\n",
    "human_columns = [\n",
    "    'Human_Comprehensiveness', 'Human_Usage_of_Technical_Terms', 'Human_Factuality',\n",
    "    'Human_Sentiment_Polarity', 'Human_Politeness', 'Human_Vagueness', 'Human_Objectivity',\n",
    "    'Human_Fairness', 'Human_Actionability', 'Human_Constructiveness',\n",
    "    'Human_Relevance_Alignment', 'Human_Clarity_and_Readability', 'Human_Overall_Quality'\n",
    "]\n",
    "\n",
    "# Create display labels without \"Human_\" and with spaces instead of underscores\n",
    "human_labels = [col.replace('Human_', '').replace('_', ' ') for col in human_columns]\n",
    "non_human_labels = [col.replace('_', ' ') for col in non_human_columns]\n",
    "\n",
    "# Create subset and calculate correlations\n",
    "correlation_df = df[human_columns + non_human_columns]\n",
    "corr_matrix = correlation_df.corr()\n",
    "human_vs_rest = corr_matrix.loc[human_columns, non_human_columns]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(18, 12))\n",
    "ax = sns.heatmap(\n",
    "    human_vs_rest, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    fmt=\".2f\", \n",
    "    vmin=-1, \n",
    "    vmax=1, \n",
    "    center=0,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\n",
    "        'label': 'Correlation Coefficient',\n",
    "        'orientation': 'horizontal',  # Key parameter\n",
    "        'location': 'top',            # Position\n",
    "        'pad': 0.02,                  # Distance from heatmap\n",
    "        'shrink': 0.8,                # Size relative to heatmap\n",
    "        'aspect': 40                   # Length vs thickness ratio\n",
    "    },\n",
    "    annot_kws={\"size\": 12}\n",
    ")\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.yaxis.label.set_size(20)  # Colorbar title/label\n",
    "\n",
    "# Customize labels using original column names but display cleaned labels\n",
    "ax.set_yticklabels(human_labels, rotation=0, fontsize=18)\n",
    "ax.set_xticklabels(\n",
    "    [label.replace('_', ' ') for label in non_human_labels],  # Extra safety for underscores\n",
    "    rotation=45, \n",
    "    ha='right', \n",
    "    fontsize=18\n",
    ")\n",
    "\n",
    "# Axis labels\n",
    "plt.xlabel('', fontsize=18, labelpad=15)\n",
    "plt.ylabel('', fontsize=18, labelpad=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Figs/corr_human_vs_qmetric.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

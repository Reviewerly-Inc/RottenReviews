{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# df_ha = pd.read_json('../final_data/HA_ALL_nonllm.json', lines=True)\n",
    "df_iclr = pd.read_json('../final_data/iclr2024_ALL_nonllm.json')\n",
    "df_neurips = pd.read_json('../final_data/neurips2023_ALL_nonllm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iclr.shape, df_neurips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform df_iclr['mattr'] to float for each row, if error raised, set 0\n",
    "def transform_mattr(row):\n",
    "    try:\n",
    "        return float(row)\n",
    "    except ValueError:\n",
    "        return 0.0\n",
    "    \n",
    "#apply the function to the column\n",
    "df_iclr['mattr'] = df_iclr['mattr'].apply(transform_mattr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ha = df_ha.drop(columns=['llm_overall_score_100'])\n",
    "df_iclr = df_iclr.drop(columns=['llm_overall_score_100'])\n",
    "df_neurips = df_neurips.drop(columns=['llm_overall_score_100'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop review_creation_to_review_submission_days\n",
    "# df_ha = df_ha.drop(columns=['review_creation_to_review_submission_days'])\n",
    "df_iclr = df_iclr.drop(columns=['review_creation_to_review_submission_days'])\n",
    "df_neurips = df_neurips.drop(columns=['review_creation_to_review_submission_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename paper_submission_to_review_creation_days to submission_to_revision_days\n",
    "# df_ha = df_ha.rename(columns={'paper_submission_to_review_creation_days': 'submission_to_revision_days'})\n",
    "df_iclr = df_iclr.rename(columns={'paper_submission_to_review_submission_days': 'submission_to_revision_days'})\n",
    "df_neurips = df_neurips.rename(columns={'paper_submission_to_review_submission_days': 'submission_to_revision_days'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column named hedge_score as follow: 1 - hedge_C / (hedge_E + hedge_D + hedge_C + hedge_N + hedge_I)\n",
    "# df_ha['hedge_score'] = 1 - df_ha['hedge_C'] / (df_ha['hedge_E'] + df_ha['hedge_D'] + df_ha['hedge_C'] + df_ha['hedge_N'] + df_ha['hedge_I'])\n",
    "df_iclr['hedge_score'] = 1 - df_iclr['hedge_C'] / (df_iclr['hedge_E'] + df_iclr['hedge_D'] + df_iclr['hedge_C'] + df_iclr['hedge_N'] + df_iclr['hedge_I'])\n",
    "df_neurips['hedge_score'] = 1 - df_neurips['hedge_C'] / (df_neurips['hedge_E'] + df_neurips['hedge_D'] + df_neurips['hedge_C'] + df_neurips['hedge_N'] + df_neurips['hedge_I'])\n",
    "\n",
    "# drop hedge_E, hedge_D, hedge_C, hedge_N, hedge_I\n",
    "# df_ha = df_ha.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "df_iclr = df_iclr.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])\n",
    "df_neurips = df_neurips.drop(columns=['hedge_E', 'hedge_D', 'hedge_C', 'hedge_N', 'hedge_I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index\n",
    "# df_ha = df_ha.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_iclr = df_iclr.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_neurips = df_neurips.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "\n",
    "# rename flesch_reading_ease to readability_score\n",
    "# df_ha = df_ha.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "df_iclr = df_iclr.rename(columns={'flesch_reading_ease': 'readability_score'})\n",
    "df_neurips = df_neurips.rename(columns={'flesch_reading_ease': 'readability_score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns end with date and submission_number\n",
    "# df_ha = df_ha.drop(columns=[col for col in df_ha.columns if col.endswith('_date') or col.endswith('submission_number')])\n",
    "df_iclr = df_iclr.drop(columns=[col for col in df_iclr.columns if col.endswith('_date') or col.endswith('submission_number')])\n",
    "df_neurips = df_neurips.drop(columns=[col for col in df_neurips.columns if col.endswith('_date') or col.endswith('submission_number')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop submission_to_revision_days\n",
    "# df_ha = df_ha.drop(columns=['submission_to_revision_days'])\n",
    "df_iclr = df_iclr.drop(columns=['submission_to_revision_days'])\n",
    "df_neurips = df_neurips.drop(columns=['submission_to_revision_days'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns as follow: length_words to Comprehensiveness, mattr: Usage_of_Technical_Terms, question_count: Factuality_Q, citation_count: Factuality_C, \n",
    "#sentiment_polarity: Sentiment_Polarity, politeness_score: Politeness, readability_score: Clarity_and_Readability, hedge_score: Vagueness, similarity_score: Relevance_Alignment,\n",
    "#review_rating: Overall_Quality, review_confidence: Confidence, review_soundness: Soundness, review_presentation: Presentation, review_contribution: Contribution\n",
    "# df_ha = df_ha.rename(columns={'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "# 'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "# 'review_rating': 'Overall_Quality', 'review_confidence': 'Confidence', 'review_soundness': 'Soundness', 'review_presentation': 'Presentation', 'review_contribution': 'Contribution'})\n",
    "df_iclr = df_iclr.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment'})\n",
    "df_neurips = df_neurips.rename(columns={'length_words': 'Reviw Length', 'mattr': 'Lexical Diversity', 'question_count': '#Questions', 'citation_count': '#References',\n",
    "'sentiment_polarity': 'Sentiment Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Readability', 'hedge_score': 'Hedging', 'similarity_score': 'Semantic Alignment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot correlation map\n",
    "def plot_correlation_map(df, title):\n",
    "    # ignore columns starting with 'review_'\n",
    "    df = df.loc[:, ~df.columns.str.startswith('review_')]\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "    correlation_matrix = numerical_columns.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', cbar=True, vmax=1, vmin=-1, center=0)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation maps for each dataframe\n",
    "# plot_correlation_map(df_ha, 'Correlation Map for df_ha')\n",
    "plot_correlation_map(df_iclr, 'Correlation Map - ICLR')\n",
    "plot_correlation_map(df_neurips, 'Correlation Map - NeurIPs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iclr = pd.read_json('../final_data/iclr2024_1000_llama.json')\n",
    "df_neurips = pd.read_json('../final_data/neurips2023_1000_llama.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot LLM-based feature correlations with improved readability\n",
    "def plot_llm_feature_correlation(df, title):\n",
    "    llm_columns = [col for col in df.select_dtypes(include=['float64', 'int64']).columns if col.startswith('llm_')]\n",
    "    llm_correlation_matrix = df[llm_columns].corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(llm_correlation_matrix, annot=False, cmap='coolwarm', cbar=True, vmin=-1, vmax=1,\n",
    "                linewidths=0.5, linecolor='black', annot_kws={\"size\": 12})\n",
    "    plt.title(title, fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# Plot LLM-based feature correlations for each dataframe\n",
    "# plot_llm_feature_correlation(df_ha, 'LLM Feature Correlation Map for df_ha')\n",
    "plot_llm_feature_correlation(df_iclr, 'LLM Feature Correlation Map for df_iclr')\n",
    "plot_llm_feature_correlation(df_neurips, 'LLM Feature Correlation Map for df_neurips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes and their corresponding titles\n",
    "dataframes = [(df_iclr, 'ICLR'), (df_neurips, 'NeurIPS')]\n",
    "\n",
    "for df, title in dataframes:\n",
    "    # Select review-related features\n",
    "    review_features = ['Overall_Quality', 'Confidence', 'Soundness', 'Presentation', 'Contribution']\n",
    "    \n",
    "    # Calculate descriptive statistics\n",
    "    numerical_columns = df.select_dtypes(include=['float64', 'int64'])\n",
    "    review_stats = numerical_columns[review_features].describe()\n",
    "    print(f\"Descriptive Statistics for Review-Related Features ({title}):\")\n",
    "    print(review_stats.round(2))\n",
    "    \n",
    "    # Visualize distributions of review-related features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, feature in enumerate(review_features, 1):\n",
    "        plt.subplot(2, 3, i)\n",
    "        sns.histplot(numerical_columns[feature], bins=20, color='blue')\n",
    "        plt.title(f\"Distribution of {feature} ({title})\", fontsize=14)\n",
    "        plt.xlabel(feature, fontsize=12)\n",
    "        plt.ylabel(\"Frequency\", fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Submission-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze aggregated metrics for a given dataframe\n",
    "def analyze_reviews(df, title):\n",
    "    # Group by submission_number and calculate aggregated metrics\n",
    "    grouped = df.groupby('submission_number').agg({\n",
    "        'review_rating': ['mean', 'var'],\n",
    "        'review_confidence': ['mean', 'var'],\n",
    "        'politeness_score': 'mean',\n",
    "        'citation_count': 'mean',\n",
    "        'similarity_score': 'mean',\n",
    "        'sentiment_polarity': 'mean'\n",
    "    })\n",
    "\n",
    "    # Rename columns for better readability\n",
    "    grouped.columns = [\n",
    "        'avg_review_rating', 'var_review_rating',\n",
    "        'avg_review_confidence', 'var_review_confidence',\n",
    "        'avg_politeness_score', 'avg_citation_count',\n",
    "        'avg_similarity_score', 'avg_sentiment_polarity'\n",
    "    ]\n",
    "    grouped.reset_index(inplace=True)\n",
    "\n",
    "    # Display the aggregated metrics\n",
    "    print(f\"Aggregated Metrics for {title}:\")\n",
    "    print(grouped.head())\n",
    "\n",
    "    # Separate features into two groups based on their value ranges\n",
    "    features_0_1 = ['avg_similarity_score', 'avg_sentiment_polarity', 'avg_politeness_score']\n",
    "    features_other = ['avg_citation_count']\n",
    "\n",
    "    # Visualize features with values between 0 and 1\n",
    "    grouped[features_0_1].plot(kind='box', figsize=(10, 6), title=f\"Features (0-1 Range) for {title}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize features with values outside the 0-1 range\n",
    "    grouped[features_other].plot(kind='box', figsize=(10, 6), title=f\"Features (Other Range) for {title}\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze df_iclr\n",
    "analyze_reviews(df_iclr, 'ICLR')\n",
    "\n",
    "# Analyze df_neurips\n",
    "analyze_reviews(df_neurips, 'NeurIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze temporal trends\n",
    "def analyze_temporal_trends(df, title):\n",
    "    # Convert timestamps to datetime for better readability\n",
    "    df['submission_creation_date'] = pd.to_datetime(df['submission_creation_date'], unit='ms')\n",
    "    df['creation_date'] = pd.to_datetime(df['creation_date'], unit='ms')\n",
    "    df['last_modification_date'] = pd.to_datetime(df['last_modification_date'], unit='ms')\n",
    "\n",
    "    # Calculate average review timelines\n",
    "    avg_submission_to_review_days = df['paper_submission_to_review_submission_days'].mean()\n",
    "    avg_review_creation_to_submission_days = df['review_creation_to_review_submission_days'].mean()\n",
    "    print(f\"Average Submission to Review Days ({title}): {avg_submission_to_review_days:.2f}\")\n",
    "    print(f\"Average Review Creation to Submission Days ({title}): {avg_review_creation_to_submission_days:.2f}\")\n",
    "\n",
    "    # Plot trends in review quality over time\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(df['creation_date'], df['review_rating'], label='Review Rating', marker='o', linestyle='-', alpha=0.7)\n",
    "    plt.plot(df['creation_date'], df['review_confidence'], label='Review Confidence', marker='s', linestyle='--', alpha=0.7)\n",
    "    plt.title(f\"Trends in Review Quality Over Time ({title})\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Review Creation Date\", fontsize=12)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze temporal trends for df_iclr\n",
    "analyze_temporal_trends(df_iclr, 'ICLR')\n",
    "\n",
    "# Analyze temporal trends for df_neurips\n",
    "analyze_temporal_trends(df_neurips, 'NeurIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Review Dataset Statistics\n",
    "\n",
    "* Number of Papers\n",
    "* Number of Reviews in total \n",
    "* #review per paper on average \n",
    "* Number of non-anonymous Reviewers\n",
    "* Review Length min/mean/ std /max \n",
    "* Time Taken to Submit Reviews (days)\n",
    "\n",
    "Table with all the stats\n",
    "\n",
    "Plot 1 : review length \n",
    "\n",
    "Time to submit review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique paper IDs in each dataframe\n",
    "unique_papers_iclr = df_iclr['submission_id'].nunique()\n",
    "unique_papers_neurips = df_neurips['submission_id'].nunique()\n",
    "\n",
    "print(f\"Number of unique papers in df_iclr: {unique_papers_iclr}\")\n",
    "print(f\"Number of unique papers in df_neurips: {unique_papers_neurips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows (total reviews) in each dataframe\n",
    "total_reviews_iclr = len(df_iclr)\n",
    "total_reviews_neurips = len(df_neurips)\n",
    "\n",
    "print(f\"Total reviews in df_iclr: {total_reviews_iclr}\")\n",
    "print(f\"Total reviews in df_neurips: {total_reviews_neurips}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average number of reviews per paper\n",
    "avg_reviews_per_paper_iclr = total_reviews_iclr / unique_papers_iclr\n",
    "avg_reviews_per_paper_neurips = total_reviews_neurips / unique_papers_neurips\n",
    "\n",
    "print(f\"Average number of reviews per paper in ICLR: {avg_reviews_per_paper_iclr:.2f}\")\n",
    "print(f\"Average number of reviews per paper in NeurIPS: {avg_reviews_per_paper_neurips:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print statistics for df_iclr\n",
    "iclr_stats = df_iclr['Comprehensiveness'].agg(['min', 'max', 'mean', 'std'])\n",
    "print(\"Review Length Statistics for ICLR:\")\n",
    "print(iclr_stats)\n",
    "\n",
    "# Calculate and print statistics for df_neurips\n",
    "neurips_stats = df_neurips['Comprehensiveness'].agg(['min', 'max', 'mean', 'std'])\n",
    "print(\"\\nReview Length Statistics for NeurIPS:\")\n",
    "print(neurips_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized frequency of review lengths for both conferences\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.kdeplot(df_iclr['Comprehensiveness'], label='ICLR', color='blue', fill=False, alpha=0.9, linewidth=3)\n",
    "sns.kdeplot(df_neurips['Comprehensiveness'], label='NeurIPS', color='orange', fill=False, alpha=0.9, linewidth=3)\n",
    "plt.title('Normalized Frequency of Review Lengths (ICLR vs NeurIPS)', fontsize=16)\n",
    "plt.xlabel('Review Length (Words)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate averages for ICLR\n",
    "avg_citation_count_iclr = df_iclr['Factuality_C'].mean()\n",
    "avg_question_count_iclr = df_iclr['Factuality_Q'].mean()\n",
    "\n",
    "# Calculate averages for NeurIPS\n",
    "avg_citation_count_neurips = df_neurips['Factuality_C'].mean()\n",
    "avg_question_count_neurips = df_neurips['Factuality_Q'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Citation Count (ICLR): {avg_citation_count_iclr:.2f}\")\n",
    "print(f\"Average Question Count (ICLR): {avg_question_count_iclr:.2f}\")\n",
    "print(f\"Average Citation Count (NeurIPS): {avg_citation_count_neurips:.2f}\")\n",
    "print(f\"Average Question Count (NeurIPS): {avg_question_count_neurips:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# Define the columns of interest\n",
    "columns_of_interest = ['Usage_of_Technical_Terms', 'Sentiment_Polarity', 'Relevance_Alignment', 'Politeness', 'Clarity_and_Readability', 'Vagueness']\n",
    "\n",
    "# Convert columns to numeric, coercing errors to NaN\n",
    "df_iclr[columns_of_interest] = df_iclr[columns_of_interest].apply(pd.to_numeric, errors='coerce')\n",
    "df_neurips[columns_of_interest] = df_neurips[columns_of_interest].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate mean and std for ICLR\n",
    "iclr_stats = df_iclr[columns_of_interest].agg(['mean', 'std'])\n",
    "print(\"ICLR Statistics (Mean and Std):\")\n",
    "print(iclr_stats)\n",
    "\n",
    "# Calculate mean and std for NeurIPS\n",
    "neurips_stats = df_neurips[columns_of_interest].agg(['mean', 'std'])\n",
    "print(\"\\nNeurIPS Statistics (Mean and Std):\")\n",
    "print(neurips_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns of interest\n",
    "# columns_of_interest = ['hedge_C', 'hedge_D', 'hedge_E', 'hedge_I', 'hedge_N']\n",
    "\n",
    "# # Convert columns to numeric, coercing errors to NaN\n",
    "# df_iclr[columns_of_interest] = df_iclr[columns_of_interest].apply(pd.to_numeric, errors='coerce')\n",
    "# df_neurips[columns_of_interest] = df_neurips[columns_of_interest].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# # Calculate mean and std for ICLR\n",
    "# iclr_stats = df_iclr[columns_of_interest].agg(['mean', 'std'])\n",
    "# print(\"ICLR Statistics (Mean and Std):\")\n",
    "# print(iclr_stats)\n",
    "\n",
    "# # Calculate mean and std for NeurIPS\n",
    "# neurips_stats = df_neurips[columns_of_interest].agg(['mean', 'std'])\n",
    "# print(\"\\nNeurIPS Statistics (Mean and Std):\")\n",
    "# print(neurips_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# Define the columns of interest\n",
    "columns_of_interest = ['Overall_Quality', 'Confidence', 'Soundness', 'Presentation', 'Contribution']\n",
    "\n",
    "# Calculate mean and std for df_neurips\n",
    "neurips_stats = df_neurips[columns_of_interest].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"\\nNeurIPS Statistics (Mean and Std):\")\n",
    "print(neurips_stats)\n",
    "\n",
    "# Calculate mean and std for df_iclr\n",
    "iclr_stats = df_iclr[columns_of_interest].agg(['mean', 'std', 'min', 'max'])\n",
    "print(\"ICLR Statistics (Mean and Std):\")\n",
    "print(iclr_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot correlation map for specific columns with a black border on all sides\n",
    "def plot_selected_correlation_map(df, columns, title):\n",
    "    # Create a mapping for renaming columns in the figure\n",
    "    renamed_columns = {\n",
    "        'review_rating': 'Overall_Quality',\n",
    "        'review_confidence': 'Confidence',\n",
    "        'review_soundness': 'Soundness',\n",
    "        'review_presentation': 'Presentation',\n",
    "        'review_contribution': 'Contribution'\n",
    "    }\n",
    "    \n",
    "    # Rename columns for the correlation matrix\n",
    "    correlation_matrix = df[columns].rename(columns=renamed_columns).corr()\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xticks(fontsize=10, rotation=45)\n",
    "    plt.yticks(fontsize=10, rotation=0)\n",
    "    \n",
    "    # Add black border to all four sides\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('black')\n",
    "        spine.set_linewidth(1.5)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot correlation map for df_iclr\n",
    "plot_selected_correlation_map(df_iclr, columns_of_interest, 'Correlation Map - ICLR')\n",
    "\n",
    "# Plot correlation map for df_neurips\n",
    "plot_selected_correlation_map(df_neurips, columns_of_interest, 'Correlation Map - NeurIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "columns_of_interest = ['Overall_Quality', 'Confidence', 'Soundness', 'Presentation', 'Contribution']\n",
    "# Normalize the columns for each conference\n",
    "for df in [df_iclr, df_neurips]:\n",
    "    for col in columns_of_interest:\n",
    "        df[f\"{col}_normalized\"] = 1 + 4 * (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "\n",
    "# Separate figures for each conference\n",
    "# Plot for ICLR\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df_iclr[[f\"{col}_normalized\" for col in columns_of_interest]])\n",
    "plt.title(\"Normalized Ratings - ICLR\", fontsize=14)\n",
    "plt.xticks(\n",
    "    ticks=range(len(columns_of_interest)),\n",
    "    labels=[col.replace('review_', '').capitalize() for col in columns_of_interest],\n",
    "    rotation=45,\n",
    "    fontsize=10,\n",
    ")\n",
    "plt.ylabel(\"Normalized Value\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for NeurIPS\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df_neurips[[f\"{col}_normalized\" for col in columns_of_interest]])\n",
    "plt.title(\"Normalized Ratings - NeurIPS\", fontsize=14)\n",
    "plt.xticks(\n",
    "    ticks=range(len(columns_of_interest)),\n",
    "    labels=[col.replace('review_', '').capitalize() for col in columns_of_interest],\n",
    "    rotation=45,\n",
    "    fontsize=10,\n",
    ")\n",
    "plt.ylabel(\"Normalized Value\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define a function to plot politeness_score per decision distribution\n",
    "def plot_politeness_distribution(df, title):\n",
    "    # Add a decision column based on review_rating [1, 4) reject, [4, 6) revision, [6, 10] accept\n",
    "    df['decision'] = pd.cut(df['Overall_Quality'], bins=[0, 4, 6, 10], labels=['Reject', 'Revision', 'Accept'], right=False)\n",
    "    \n",
    "    # Plot politeness_score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].unique():\n",
    "        sns.kdeplot(\n",
    "            data=df[df['decision'] == decision],\n",
    "            x='Politeness',\n",
    "            label=decision,\n",
    "            color=color_map.get(decision, 'black'),\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3\n",
    "        )\n",
    "    plt.title(f\"Politeness Score Distribution - {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Politeness Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for df_neurips\n",
    "plot_politeness_distribution(df_neurips, 'NeurIPS')\n",
    "\n",
    "# Plot for df_iclr\n",
    "plot_politeness_distribution(df_iclr, 'ICLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define a function to plot politeness_score per decision distribution\n",
    "def plot_len_distribution(df, title):\n",
    "    # Add a decision column based on review_rating [1, 4) reject, [4, 6) revision, [6, 10] accept\n",
    "    df['decision'] = pd.cut(df['Overall_Quality'], bins=[0, 4, 6, 10], labels=['Reject', 'Revision', 'Accept'], right=False)\n",
    "    \n",
    "    # Plot politeness_score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].unique():\n",
    "        sns.kdeplot(\n",
    "            data=df[df['decision'] == decision],\n",
    "            x='Comprehensiveness',\n",
    "            label=decision,\n",
    "            color=color_map.get(decision, 'black'),\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3\n",
    "        )\n",
    "    plt.title(f\"Comprehensiveness Score Distribution - {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Comprehensiveness Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(0, 2000)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for df_neurips\n",
    "plot_len_distribution(df_neurips, 'NeurIPS')\n",
    "\n",
    "# Plot for df_iclr\n",
    "plot_len_distribution(df_iclr, 'ICLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define a function to plot politeness_score per decision distribution\n",
    "def plot_len_distribution(df, title):\n",
    "    # Add a decision column based on review_rating [1, 4) reject, [4, 6) revision, [6, 10] accept\n",
    "    df['decision'] = pd.cut(df['Overall_Quality'], bins=[0, 4, 6, 10], labels=['Reject', 'Revision', 'Accept'], right=False)\n",
    "    \n",
    "    # Plot politeness_score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].unique():\n",
    "        sns.kdeplot(\n",
    "            data=df[df['decision'] == decision],\n",
    "            x='Clarity_and_Readability',\n",
    "            label=decision,\n",
    "            color=color_map.get(decision, 'black'),\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3\n",
    "        )\n",
    "    plt.title(f\"Clarity and Readability Score Distribution - {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Clarity and Readability Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(0, 80)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for df_neurips\n",
    "plot_len_distribution(df_neurips, 'NeurIPS')\n",
    "\n",
    "# Plot for df_iclr\n",
    "plot_len_distribution(df_iclr, 'ICLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define a function to plot politeness_score per decision distribution\n",
    "def plot_len_distribution(df, title):\n",
    "    # Add a decision column based on review_rating [1, 4) reject, [4, 6) revision, [6, 10] accept\n",
    "    df['decision'] = pd.cut(df['Overall_Quality'], bins=[0, 4, 6, 10], labels=['Reject', 'Revision', 'Accept'], right=False)\n",
    "    \n",
    "    # Plot politeness_score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].unique():\n",
    "        sns.kdeplot(\n",
    "            data=df[df['decision'] == decision],\n",
    "            x='Relevance_Alignment',\n",
    "            label=decision,\n",
    "            color=color_map.get(decision, 'black'),\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3\n",
    "        )\n",
    "    plt.title(f\"Relevance Alignment Score Distribution - {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Relevance Alignment Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(0.5, 1)  # Set x-axis limit to 2000\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for df_neurips\n",
    "plot_len_distribution(df_neurips, 'NeurIPS')\n",
    "\n",
    "# Plot for df_iclr\n",
    "plot_len_distribution(df_iclr, 'ICLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_frequency(df, column, title, bins=50):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=column,\n",
    "        bins=bins,\n",
    "        kde=False\n",
    "    )\n",
    "    plt.title(f\"{column.replace('_', ' ').title()} Frequency of {title}\",\n",
    "              fontsize=16)\n",
    "    plt.xlabel(column.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for df, label in [(df_iclr, 'ICLR'), (df_neurips, 'NeurIPs')]:\n",
    "    for col in ['Clarity_and_Readability']:\n",
    "        plot_frequency(df, col, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_frequency(df, column, title, bins=50):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=column,\n",
    "        bins=bins,\n",
    "        kde=False\n",
    "    )\n",
    "    plt.title(f\"{column.replace('_', ' ').title()} Frequency of {title}\",\n",
    "              fontsize=16)\n",
    "    plt.xlabel(column.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for df, label in [(df_iclr, 'ICLR'), (df_neurips, 'NeurIPs')]:\n",
    "    for col in ['question_count', 'citation_count']:\n",
    "        plot_frequency(df, col, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "'length_words': 'Comprehensiveness', 'mattr': 'Usage_of_Technical_Terms', 'question_count': 'Factuality_Q', 'citation_count': 'Factuality_C',\n",
    "'sentiment_polarity': 'Sentiment_Polarity', 'politeness_score': 'Politeness', 'readability_score': 'Clarity_and_Readability', 'hedge_score': 'Vagueness', 'similarity_score': 'Relevance_Alignment',\n",
    "'review_rating': 'Overall_Quality'\n",
    "'''\n",
    "\n",
    "# color map for suggestions/decisions\n",
    "color_map = {\n",
    "    'Accept': 'green',\n",
    "    'Reject': 'red',\n",
    "    'Revision': 'blue'\n",
    "}\n",
    "\n",
    "# Define a function to plot politeness_score per decision distribution\n",
    "def plot_polarity_distribution(df, title):\n",
    "    # Add a decision column based on review_rating\n",
    "    df['decision'] = pd.cut(df['Overall_Quality'], bins=[0, 4, 6, 10], labels=['Reject', 'Revision', 'Accept'], right=False)\n",
    "    \n",
    "    # Plot politeness_score distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for decision in df['decision'].unique():\n",
    "        sns.kdeplot(\n",
    "            data=df[df['decision'] == decision],\n",
    "            x='Sentiment_Polarity',\n",
    "            label=decision,\n",
    "            color=color_map.get(decision, 'black'),\n",
    "            fill=False,\n",
    "            alpha=0.9,\n",
    "            linewidth=3\n",
    "        )\n",
    "    plt.title(f\"Sentiment Polarity Score Distribution - {title}\", fontsize=16)\n",
    "    plt.xlabel(\"Sentiment Polarity Score\", fontsize=12)\n",
    "    plt.ylabel(\"Density\", fontsize=12)\n",
    "    plt.legend(title=\"\", fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.xlim(-0.2, 0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for df_neurips\n",
    "plot_polarity_distribution(df_neurips, 'NeurIPs')\n",
    "\n",
    "# Plot for df_iclr\n",
    "plot_polarity_distribution(df_iclr, 'ICLR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

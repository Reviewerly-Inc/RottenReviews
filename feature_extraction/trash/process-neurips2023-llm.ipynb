{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "\n",
    "input_file = '/home/ali/Review_Quality_Benchmark/data/processed/neurips2023_1000_papers.json'\n",
    "# Load data\n",
    "df = pd.read_json(input_file)\n",
    "\n",
    "llm_fields = [\n",
    "    \"llm_Comprehensiveness\", \"llm_Vagueness\", \"llm_Objectivity\", \"llm_Fairness\", \"llm_Actionability\", \n",
    "    \"llm_Constructiveness\", \"llm_Relevance Alignment\", \"llm_Clarity and Readability\", \"llm_Usage of Technical Terms\",\n",
    "    \"llm_Factuality\", \"llm_Overall Quality\", \"llm_overall_score_100\", \"llm_Sentiment Polarity\", \"llm_Politeness\", \n",
    "]\n",
    "\n",
    "\n",
    "# Check for missing fields and add them if not present\n",
    "for field in llm_fields:\n",
    "    if field not in df.columns:\n",
    "        df[field] = pd.NA\n",
    "\n",
    "# Pattern to extract JSON block\n",
    "pattern = re.compile(r\"<review_assessment>\\s*(\\{.*?\\})\\s*</review_assessment>\", re.DOTALL)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"# REVIEW-QUALITY JUDGE\n",
    "\n",
    "## 0 — ROLE\n",
    "\n",
    "You are **ReviewInspector-LLM**, a rigorous, impartial meta-reviewer.\n",
    "Your goal is to assess the quality of a single peer-review against a predefined set of criteria and to provide precise, structured evaluations.\n",
    "\n",
    "## 1 — INPUTS\n",
    "\n",
    "Title: {title}\n",
    "Abstract: {abstract}\n",
    "Review: {review_text}\n",
    "\n",
    "## 2 — EVALUATION CRITERIA\n",
    "\n",
    "Return **only** the scale value or label at right (no rationale text).\n",
    "\n",
    "| #  | Criterion                    | Allowed scale / label                       | Description                                                                |\n",
    "| -- | ---------------------------- | ------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| 1  | **Comprehensiveness**        | integer **0-5**                             | Extent to which the review covers all key aspects of the paper.            |\n",
    "| 2  | **Usage of Technical Terms** | integer **0-5**                             | Appropriateness and frequency of domain-specific vocabulary.               |\n",
    "| 3  | **Factuality**               | **factual / partially factual / unfactual** | Accuracy of the statements made in the review.                             |\n",
    "| 4  | **Sentiment Polarity**       | **negative / neutral / positive**           | Overall sentiment conveyed by the reviewer.                                |\n",
    "| 5  | **Politeness**               | **polite / neutral / impolite**             | Tone and manner of the review language.                                    |\n",
    "| 6  | **Vagueness**                | **none / low / moderate / high / extreme**  | Degree of ambiguity or lack of specificity in the review.                  |\n",
    "| 7  | **Objectivity**              | integer **0-5**                             | Presence of unbiased, evidence-based commentary.                           |\n",
    "| 8  | **Fairness**                 | integer **0-5**                             | Perceived impartiality and balance in judgments.                           |\n",
    "| 9  | **Actionability**            | integer **0-5**                             | Helpfulness of the review in suggesting clear next steps.                  |\n",
    "| 10 | **Constructiveness**         | integer **0-5**                             | Degree to which the review offers improvements rather than just criticism. |\n",
    "| 11 | **Relevance Alignment**      | integer **0-5**                             | How well the review relates to the content and scope of the paper.         |\n",
    "| 12 | **Clarity and Readability**  | integer **0-5**                             | Ease of understanding the review, including grammar and structure.         |\n",
    "| 13 | **Overall Quality**          | integer **0-100**                           | Holistic evaluation of the review's usefulness and professionalism.        |\n",
    "\n",
    "## 3 — SCORING GUIDELINES\n",
    "\n",
    "For 0-5 scales:\n",
    "\n",
    "* 5 = Outstanding\n",
    "* 4 = Strong\n",
    "* 3 = Adequate\n",
    "* 2 = Weak\n",
    "* 1 = Very weak\n",
    "* 0 = Absent/irrelevant\n",
    "\n",
    "## 4 — ANALYSIS & COMPUTATION (silent)\n",
    "\n",
    "1. Read and understand the review in the context of the paper title and abstract.\n",
    "2. Extract quantitative and qualitative signals (e.g., term usage, factual consistency, tone, clarity).\n",
    "3. Map observations to the corresponding scoring scales.\n",
    "\n",
    "## 5 — OUTPUT FORMAT (strict)  \n",
    "Return **exactly one** JSON block wrapped in the tag below — **no comments or extra text**.\n",
    "\n",
    "```json\n",
    "<review_assessment>\n",
    "{{\n",
    "  \"paper_title\": \"{title}\",\n",
    "  \"criteria\": {{\n",
    "    \"Comprehensiveness\":       ...,\n",
    "    \"Usage of Technical Terms\":   ...,\n",
    "    \"Factuality\":    ...,\n",
    "    \"Sentiment Polarity\":      ...,\n",
    "    \"Politeness\":  ...,\n",
    "    \"Vagueness\":          ...,\n",
    "    \"Objectivity\":             ...,\n",
    "    \"Fairness\":         ...,\n",
    "    \"Actionability\":        ...,\n",
    "    \"Constructiveness\":    ...,\n",
    "    \"Relevance Alignment\":    ...,\n",
    "    \"Clarity and Readability\":    ...,\n",
    "    \"Relevance Alignment\":    ...,\n",
    "    \"Overall Quality\":     ...\n",
    "  }},\n",
    "  \"overall_score_100\": ...\n",
    "}}\n",
    "</review_assessment>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each row\n",
    "# Set the temperature parameter for the llama model\n",
    "temperature = 0\n",
    "seed = 42\n",
    "\n",
    "\n",
    "# Process each row\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Scoring with LLM\"):\n",
    "    # Skip if all llm fields are already filled\n",
    "    if all(pd.notna(row.get(field, pd.NA)) for field in llm_fields):\n",
    "        continue\n",
    "    # if idx >= 30:\n",
    "    #     break\n",
    "\n",
    "    prompt = template.format(\n",
    "        title=row['submission_title'],\n",
    "        abstract=row['submission_abstract'],\n",
    "        review_text=row['total_review']\n",
    "    )\n",
    "    \n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = chat(\"qwen3:8b\", messages=[{'role': 'user', 'content': prompt}], options={'temperature': temperature, 'seed': seed})\n",
    "            content = response['message']['content']\n",
    "            match = pattern.search(content)\n",
    "            if not match:\n",
    "                raise ValueError(\"No JSON block found\")\n",
    "\n",
    "            parsed = json.loads(match.group(1))\n",
    "            print(parsed[\"overall_score_100\"])\n",
    "            for key, val in parsed[\"criteria\"].items():\n",
    "                df.at[idx, f\"llm_{key}\"] = val\n",
    "            df.at[idx, \"llm_overall_score_100\"] = parsed[\"overall_score_100\"]\n",
    "\n",
    "            # Save after every successful row\n",
    "            # df.to_csv(input_file, index=False, quoting=csv.QUOTE_ALL)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error at row {idx}, attempt {attempt + 1}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('/home/ali/Review_Quality_Benchmark/data/processed/neurips2023_1000_qwen.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the first 50 rows and check for rows without null values in \"llm_\" columns\n",
    "non_null_count = df.iloc[:][[col for col in df.columns if col.startswith(\"llm_\")]].dropna().shape[0]\n",
    "print(non_null_count)\n",
    "print(df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

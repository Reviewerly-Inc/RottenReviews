{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_reviews = pd.read_json('../Human_Annotation/merged_200_papers.json', orient='records', lines=True)\n",
    "df_reviews = df_reviews.iloc[:, :31]\n",
    "df_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_reviews(folder_path):\n",
    "    rows = []\n",
    "    # find all JSON files in the folder\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        paper_id = data.get('paper_id')\n",
    "        assessor = data.get('assessor')\n",
    "        metrics = data.get('metrics', {})\n",
    "        \n",
    "        # group metrics by reviewer name\n",
    "        reviewer_metrics = {}\n",
    "        for key, value in metrics.items():\n",
    "            # only process keys that start with \"review_\"\n",
    "            if not key.startswith('review_'):\n",
    "                continue\n",
    "            parts = key.split('_')\n",
    "            reviewer = parts[1]                          # e.g. \"Palwinder-Singh\"\n",
    "            metric_name = '_'.join(parts[2:])            # e.g. \"Comprehensiveness\"\n",
    "            \n",
    "            reviewer_metrics.setdefault(reviewer, {})\n",
    "            reviewer_metrics[reviewer][metric_name] = value\n",
    "        \n",
    "        # turn each reviewer’s metrics into a row\n",
    "        for reviewer, mdict in reviewer_metrics.items():\n",
    "            row = {\n",
    "                'paper_id': paper_id,\n",
    "                'assessor': assessor,\n",
    "                'reviewer': reviewer\n",
    "            }\n",
    "            row.update(mdict)\n",
    "            rows.append(row)\n",
    "    \n",
    "    # build the final DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "folder = '../Human_Annotation_Data'\n",
    "df_human = load_reviews(folder)\n",
    "\n",
    "# show the first few rows\n",
    "df_human = df_human[df_human['Overall_Quality'] > 10]\n",
    "df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human['reviewer'] = df_human['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_human['reviewer'] = df_human['reviewer'].str.replace(' ', '-', regex=False)\n",
    "df_reviews['reviewer'] = df_reviews['reviewer'].str.replace('_', '-', regex=False)\n",
    "df_reviews['reviewer'] = df_reviews['reviewer'].str.replace(' ', '-', regex=False)\n",
    "\n",
    "\n",
    "# merge two df_reviews on df_human based on paper_id and reviewer\n",
    "df_human['paper_id'] = df_human['paper_id'].astype(int)\n",
    "df_reviews['paper_id'] = df_reviews['paper_id'].astype(int)\n",
    "\n",
    "# transform paper_id column in all dfs to int\n",
    "df_human['reviewer'] = df_human['reviewer'].astype(str)\n",
    "df_reviews['reviewer'] = df_reviews['reviewer'].astype(str)\n",
    "\n",
    "df_human_vs_metric = (\n",
    "    df_human\n",
    "    .merge(df_reviews, on=['paper_id', 'reviewer'], how='inner')\n",
    ")\n",
    "\n",
    "# filter the df_human_vs_metric up to first 16 columns\n",
    "# df_human_vs_metric = df_human_vs_metric.iloc[:, :16]\n",
    "df_human_vs_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with Anonymous reviewers\n",
    "df_human_vs_metric = df_human_vs_metric[~df_human_vs_metric['reviewer'].str.contains('Anonymous')]\n",
    "df_human_vs_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column named hedging compute as follows: 1 - (hedge_C / (hedge_C + hedge_D + hedge_E + hedge_I + hedge_N))\n",
    "df_human_vs_metric['hedging'] = 1 - (df_human_vs_metric['hedge_C'] / (df_human_vs_metric['hedge_C'] + df_human_vs_metric['hedge_D'] + df_human_vs_metric['hedge_E'] + df_human_vs_metric['hedge_I'] + df_human_vs_metric['hedge_N']))\n",
    "# drop columns with 'hedge_' prefix\n",
    "df_human_vs_metric = df_human_vs_metric.drop(columns=[col for col in df_human_vs_metric.columns if col.startswith('hedge_')])\n",
    "# drop following columns: flesch_kincaid_grade, gunning_fog, smog_index, automated_readability_index\n",
    "df_human_vs_metric = df_human_vs_metric.drop(columns=['flesch_kincaid_grade', 'gunning_fog', 'smog_index', 'automated_readability_index'])\n",
    "df_human_vs_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human_vs_llm = pd.read_csv('human_vs_llm.csv')\n",
    "df_human_vs_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate paper_id and reviewer pairs from both DFs\n",
    "df_human_vs_metric = df_human_vs_metric.drop_duplicates(subset=['paper_id', 'reviewer'])\n",
    "df_human_vs_llm = df_human_vs_llm.drop_duplicates(subset=['paper_id', 'reviewer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = (\n",
    "    df_human_vs_metric\n",
    "    .merge(df_human_vs_llm, on=['paper_id', 'reviewer'], how='inner')\n",
    ")\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop following columns: assessor, Comprehensiveness, Usage_of_Technical_Terms, Factuality, Sentiment_Polarity, Politeness, Vagueness, Objectivity, Fairness, Actionability, Constructiveness, Relevance_Alignment, Clarity_and_Readability, Overall_Quality, authors, review_date, review_rating, review_confidence, review_soundness, review_presentation, review_contribution\n",
    "df_merge = df_merge.drop(columns=['assessor', 'Comprehensiveness', 'Usage_of_Technical_Terms', 'Factuality', 'Sentiment_Polarity', 'Politeness', 'Vagueness', 'Objectivity', 'Fairness', 'Actionability', 'Constructiveness', 'Relevance_Alignment', 'Clarity_and_Readability', 'Overall_Quality', 'authors', 'review_date', 'review_rating', 'review_confidence', 'review_soundness', 'review_presentation', 'review_contribution'])\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv('human_llms_qmetrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load df_human_llms_qmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "df_all = pd.read_csv('human_llms_qmetrics.csv')\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 1) Load your DataFrame however you like.\n",
    "#    For example, if it’s already in memory:\n",
    "# df = your_dataframe\n",
    "\n",
    "# 2) Set up 10-fold splitter\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 3) Make the base folder\n",
    "os.makedirs('Folds', exist_ok=True)\n",
    "\n",
    "# 4) Loop and save\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(df_all), start=1):\n",
    "    train_df = df_all.iloc[train_idx]\n",
    "    test_df  = df_all.iloc[test_idx]\n",
    "    \n",
    "    train_df.to_csv(f'Folds/f{fold}_train.csv', index=False)\n",
    "    test_df.to_csv( f'Folds/f{fold}_test.csv',  index=False)\n",
    "\n",
    "print(\"Saved 10 train/test pairs in the ‘Folds’ folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import kendalltau\n",
    "import krippendorff  # pip install krippendorff\n",
    "\n",
    "# the features to use\n",
    "features = [\n",
    "    'length_words', 'citation_count', 'question_count', 'mattr',\n",
    "    'sentiment_polarity', 'similarity_score', 'flesch_reading_ease',\n",
    "    'politeness_score', 'hedging'\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i in range(1, 11):\n",
    "    # load train/test for fold i\n",
    "    train = pd.read_csv(f\"Folds/f{i}_train.csv\")\n",
    "    test  = pd.read_csv(f\"Folds/f{i}_test.csv\")\n",
    "    \n",
    "    # train\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(train[features], train['Human_Overall_Quality'])\n",
    "    \n",
    "    # predict on test\n",
    "    y_true = test['Human_Overall_Quality']\n",
    "    y_pred_model = clf.predict(test[features])\n",
    "    \n",
    "    # collect metrics\n",
    "    fold_key = f\"Fold{i}\"\n",
    "    results[fold_key] = {}\n",
    "    \n",
    "    for name, y_pred in [\n",
    "        ('Qwen', test['Qwen_Overall_Quality']),\n",
    "        ('GPT',  test['GPT_Overall_Quality']),\n",
    "        ('Phi',  test['Phi_Overall_Quality']),\n",
    "        ('Output', y_pred_model)\n",
    "    ]:\n",
    "        tau   = kendalltau(y_true, y_pred).correlation\n",
    "        alpha = krippendorff.alpha([y_true, y_pred])\n",
    "        results[fold_key][name] = {\n",
    "            \"kendall_tau\":       tau,\n",
    "            \"krippendorff_alpha\": alpha\n",
    "        }\n",
    "\n",
    "# print results\n",
    "for fold, metrics in results.items():\n",
    "    print(f\"{fold}:\")\n",
    "    for name, m in metrics.items():\n",
    "        print(f\"  {name} {{Kendall's Tau: {m['kendall_tau']:.3f}, \"\n",
    "              f\"Krippendorff's Alpha: {m['krippendorff_alpha']:.3f}}}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import kendalltau\n",
    "import krippendorff\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'length_words', 'citation_count', 'question_count', 'mattr',\n",
    "    'sentiment_polarity', 'similarity_score', 'flesch_reading_ease',\n",
    "    'politeness_score', 'hedging'\n",
    "]\n",
    "target = 'Human_Overall_Quality'\n",
    "all_fold_metrics = []\n",
    "\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold in range(1, 11):\n",
    "    print(f\"Fold{fold}:\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(f\"Folds/f{fold}_train.csv\")\n",
    "    test_df = pd.read_csv(f\"Folds/f{fold}_test.csv\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train = train_df[features]\n",
    "    y_train = train_df[target]\n",
    "    X_test = test_df[features]\n",
    "    y_test = test_df[target]\n",
    "    \n",
    "    # Train model\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Groups to compare with Human_Overall_Quality\n",
    "    groups = {\n",
    "        \"Qwen\": test_df[\"Qwen_Overall_Quality\"],\n",
    "        \"GPT\": test_df[\"GPT_Overall_Quality\"],\n",
    "        \"Phi\": test_df[\"Phi_Overall_Quality\"],\n",
    "        \"Output\": y_pred\n",
    "    }\n",
    "    \n",
    "    # Compute metrics for each group\n",
    "    results = {}\n",
    "    for name, scores in groups.items():\n",
    "        # Kendall's Tau\n",
    "        tau, _ = kendalltau(y_test, scores)\n",
    "        \n",
    "        # Krippendorff's Alpha (requires 2D array of shape [raters, items])\n",
    "        data = [y_test.tolist(), scores.tolist()]\n",
    "        alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "        \n",
    "        results[name] = {\n",
    "            \"Kendall Tau\": round(tau, 3),\n",
    "            \"Krippendorff Alpha\": round(alpha, 3)\n",
    "        }\n",
    "    \n",
    "    # Print results\n",
    "    for model in [\"Qwen\", \"GPT\", \"Phi\", \"Output\"]:\n",
    "        metrics = results[model]\n",
    "        print(f\"{model} {{Kendall Tau: {metrics['Kendall Tau']}, Krippendorff Alpha: {metrics['Krippendorff Alpha']}}}\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    all_fold_metrics.append(results)\n",
    "    \n",
    "\n",
    "# After processing all folds, calculate averages\n",
    "average_metrics = {\n",
    "    model: {\n",
    "        \"kendall\": sum(fold[model][\"Kendall Tau\"] for fold in all_fold_metrics) / 10,\n",
    "        \"alpha\": sum(fold[model][\"Krippendorff Alpha\"] for fold in all_fold_metrics) / 10\n",
    "    }\n",
    "    for model in [\"Qwen\", \"GPT\", \"Phi\", \"Output\"]\n",
    "}\n",
    "\n",
    "# Print final averages\n",
    "print(\"\\nAverage across all folds:\")\n",
    "for model, metrics in average_metrics.items():\n",
    "    print(f\"{model}: {{Kendall Tau: {metrics['kendall']:.3f}, Krippendorff Alpha: {metrics['alpha']:.3f}}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import kendalltau\n",
    "import krippendorff\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "features = [\n",
    "    'length_words', 'citation_count', 'question_count', 'mattr',\n",
    "    'sentiment_polarity', 'similarity_score', 'flesch_reading_ease',\n",
    "    'politeness_score', 'hedging'\n",
    "]\n",
    "feature_importances = {}\n",
    "target = 'Human_Overall_Quality'\n",
    "\n",
    "models = {\n",
    "    # \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    # \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    # \"XGBoost\": XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    # \"SVM\": SVC(random_state=42),\n",
    "    # \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "    # \"Neural Network\": MLPClassifier(random_state=42, hidden_layer_sizes=(50,))\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    \"SVR\": SVR(kernel='linear'),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Neural Network\": MLPRegressor(random_state=42, hidden_layer_sizes=(54, 108, 108, 54)),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# ================== METRIC STORAGE ==================\n",
    "results = {\n",
    "    model_name: {\n",
    "        'Kendall': {'Qwen': [], 'GPT': [], 'Phi': [], 'Output': []},\n",
    "        'Alpha': {'Qwen': [], 'GPT': [], 'Phi': [], 'Output': []}\n",
    "    }\n",
    "    for model_name in models\n",
    "}\n",
    "\n",
    "# ================== MAIN PIPELINE ==================\n",
    "for fold in range(1, 6):\n",
    "    print(f\"\\n{'='*40}\\nFold {fold}\\n{'='*40}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(f\"Folds/f{fold}_train.csv\")\n",
    "    test_df = pd.read_csv(f\"Folds/f{fold}_test.csv\")\n",
    "    \n",
    "    X_train, y_train = train_df[features], train_df[target]\n",
    "    X_test, y_test = test_df[features], test_df[target]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Clone model to prevent parameter leakage\n",
    "        cloned_model = clone(model)\n",
    "        \n",
    "        # Train and predict\n",
    "        cloned_model.fit(X_train, y_train)\n",
    "        \n",
    "        # ======== Feature Importance Calculation ================================\n",
    "        if hasattr(cloned_model, 'feature_importances_'):\n",
    "            # Tree-based models\n",
    "            fold_imp = cloned_model.feature_importances_\n",
    "        elif hasattr(cloned_model, 'coef_'):\n",
    "            # Linear models\n",
    "            fold_imp = np.abs(cloned_model.coef_.flatten())\n",
    "        else:\n",
    "            # For models without inherent importance (SVR, MLP)\n",
    "            result = permutation_importance(\n",
    "                cloned_model, X_test, y_test,\n",
    "                n_repeats=10, \n",
    "                random_state=42\n",
    "            )\n",
    "            fold_imp = result.importances_mean\n",
    "        \n",
    "        # Normalize and store\n",
    "        fold_imp = fold_imp / fold_imp.sum()  # Normalize to sum=1\n",
    "        \n",
    "        if model_name not in feature_importances:\n",
    "            feature_importances[model_name] = {\n",
    "                'features': features,\n",
    "                'importances': {f: [] for f in features}\n",
    "            }\n",
    "        \n",
    "        for f, imp in zip(features, fold_imp):\n",
    "            feature_importances[model_name]['importances'][f].append(imp)\n",
    "        \n",
    "        # Print fold-level importance\n",
    "        print(f\"\\n{model_name} Feature Importance (Fold {fold}):\")\n",
    "        sorted_idx = np.argsort(fold_imp)[::-1]\n",
    "        for idx in sorted_idx:\n",
    "            print(f\"  {features[idx]}: {fold_imp[idx]:.4f}\")\n",
    "\n",
    "        # ======== Feature Importance Calculation ================================\n",
    "        \n",
    "        y_pred = cloned_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        groups = {\n",
    "            'Qwen': test_df['Qwen_Overall_Quality'],\n",
    "            'GPT': test_df['GPT_Overall_Quality'],\n",
    "            'Phi': test_df['Phi_Overall_Quality'],\n",
    "            'Output': y_pred\n",
    "        }\n",
    "\n",
    "        for group_name, scores in groups.items():\n",
    "            # Kendall's Tau\n",
    "            tau, _ = kendalltau(y_test, scores)\n",
    "            results[model_name]['Kendall'][group_name].append(tau)\n",
    "            \n",
    "            # Krippendorff's Alpha\n",
    "            data = [y_test.tolist(), scores.tolist()]\n",
    "            alpha = krippendorff.alpha(data, level_of_measurement='ordinal')\n",
    "            results[model_name]['Alpha'][group_name].append(alpha)\n",
    "\n",
    "        # Print fold results\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"Output Kendall: {tau:.3f}, Alpha: {alpha:.3f}\")\n",
    "\n",
    "\n",
    "print('#############################################')\n",
    "# ================== FINAL RESULTS ==================\n",
    "print(\"\\n\\nAverage Metrics Across All Folds:\")\n",
    "for model_name in models:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric in ['Kendall', 'Alpha']:\n",
    "        print(f\"  {metric}:\")\n",
    "        for group in ['Output']:  # 'Qwen', 'GPT', 'Phi', \n",
    "            avg = np.mean(results[model_name][metric][group])\n",
    "            std = np.std(results[model_name][metric][group])\n",
    "            print(f\"    {group}: {avg:.3f} ± {std:.3f}\")\n",
    "            \n",
    "\n",
    "# ================== FINAL FEATURE IMPORTANCE ==================\n",
    "print(\"\\n\\nAverage Feature Importance Across All Folds:\")\n",
    "for model_name, data in feature_importances.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    avg_imp = {f: np.mean(vals) for f, vals in data['importances'].items()}\n",
    "    sorted_imp = sorted(avg_imp.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for feature, importance in sorted_imp:\n",
    "        print(f\"  {feature}: {importance:.4f} ± {np.std(data['importances'][feature]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================== PLOTTING ==================\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.labelsize': 16,    # X/Y axis labels\n",
    "    'xtick.labelsize': 16,   # X-axis ticks\n",
    "    'ytick.labelsize': 14,   # Y-axis ticks\n",
    "    'legend.fontsize': 14,   # Legend\n",
    "    'axes.titlesize': 16     # Title\n",
    "})\n",
    "\n",
    "# Data from your results\n",
    "data = {\n",
    "    'Qwen-3': {'kendall Tau': 0.272, 'krippendorff Alpha': 0.164},\n",
    "    'Phi-4': {'kendall Tau': 0.254, 'krippendorff Alpha': 0.191},\n",
    "    'GPT-4o': {'kendall Tau': 0.372, 'krippendorff Alpha': 0.401},\n",
    "    \n",
    "    r'LLaMA-3-FT$\\mathregular{^*}$': {'kendall Tau': 0.406, 'krippendorff Alpha': 0.454},\n",
    "    \n",
    "    'Random Forest': {'kendall Tau': 0.451, 'krippendorff Alpha': 0.551},\n",
    "    'Linear Regression': {'kendall Tau': 0.459, 'krippendorff Alpha': 0.530},\n",
    "    'MLP': {'kendall Tau': 0.426, 'krippendorff Alpha': 0.567},\n",
    "    # 'XGBoost': {'kendall Tau': 0.380, 'krippendorff Alpha': 0.510},\n",
    "    # 'SVR': {'kendall Tau': 0.454, 'krippendorff Alpha': 0.559},\n",
    "}\n",
    "\n",
    "std_devs = {\n",
    "    'Qwen-3': {'kendall Tau': 0.079, 'krippendorff Alpha': 0.109},\n",
    "    'Phi-4': {'kendall Tau': 0.116, 'krippendorff Alpha': 0.149},\n",
    "    'GPT-4o': {'kendall Tau': 0.100, 'krippendorff Alpha': 0.119},\n",
    "    \n",
    "    r'LLaMA-3-FT$\\mathregular{^*}$': {'kendall Tau': 0.035, 'krippendorff Alpha': 0.035},\n",
    "    \n",
    "    'Random Forest': {'kendall Tau': 0.071, 'krippendorff Alpha': 0.060},\n",
    "    'Linear Regression': {'kendall Tau': 0.107, 'krippendorff Alpha': 0.118},\n",
    "    'MLP': {'kendall Tau': 0.036, 'krippendorff Alpha': 0.038},\n",
    "    # 'SVR': {'kendall Tau': 0.089, 'krippendorff Alpha': 0.093},\n",
    "    # 'XGBoost': {'kendall Tau': 0.075, 'krippendorff Alpha': 0.085},\n",
    "}\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, (ax1) = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Configure style\n",
    "plt.rcParams['font.size'] = 18\n",
    "# colors = plt.cm.tab10.colors  # Get colors from tab10 colormap\n",
    "\n",
    "colors = {\n",
    "    'GPT-4o': '#C0C0C0',    # Lightest Gray\n",
    "    'Qwen-3': '#08306B',   # Medium Blue\n",
    "    'Phi-4': '#4F81BD',   # Very Dark Blue (Navy)\n",
    "    'Random Forest':      '#A5D6A7',  # Medium-light Green\n",
    "    'Linear Regression':  '#FFCC80',  # Medium-light Orange\n",
    "    'MLP':                '#EF9A9A',  # Medium-light Red\n",
    "    r'LLaMA-3-FT$\\mathregular{^*}$':         '#ADD8E6',  # Medium-light Purple\n",
    "}\n",
    "\n",
    "patterns = {\n",
    "    'GPT-4o':  '////',\n",
    "    'Qwen-3': '****',\n",
    "    'Phi-4':  'xxxx',\n",
    "    'Random Forest': '',\n",
    "    'Linear Regression': ''\n",
    "}\n",
    "\n",
    "# Plot parameters\n",
    "bar_width = 0.75\n",
    "index = np.arange(len(data))\n",
    "model_names = list(data.keys())\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Define grouping parameters\n",
    "group1_size = 4  # First 4 columns\n",
    "group2_size = 3  # Next 3 columns\n",
    "intra_group_space = 0.1    # Space within groups\n",
    "inter_group_space = 0.8   # Space between groups\n",
    "\n",
    "# Create positions for each group\n",
    "group1_pos = np.arange(group1_size)\n",
    "group2_pos = np.arange(group1_size + inter_group_space, \n",
    "                      group1_size + inter_group_space + group2_size)\n",
    "all_positions = np.concatenate([group1_pos, group2_pos])\n",
    "\n",
    "# Modified plotting code\n",
    "for i, (model, values) in enumerate(data.items()):\n",
    "    ax1.bar(\n",
    "        all_positions[i] + intra_group_space/2,  # Center bars in their slot\n",
    "        values['kendall Tau'], \n",
    "        width=bar_width - intra_group_space + 0.1,\n",
    "        yerr=std_devs[model]['kendall Tau'],\n",
    "        color=colors[model],\n",
    "        label=model,\n",
    "        capsize=5,\n",
    "        edgecolor='black', \n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "# Set x-axis labels and ticks\n",
    "ax1.set_xticks(all_positions + (bar_width - intra_group_space)/2)\n",
    "ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "\n",
    "# Add visual separation between groups\n",
    "ax1.axvline(x=group1_size + inter_group_space/2 - 0.45, \n",
    "           color='gray', \n",
    "           linestyle='--', \n",
    "           linewidth=0.8,\n",
    "           alpha=1)\n",
    "\n",
    "\n",
    "# Adjust x-axis limits\n",
    "ax1.set_xlim(-0.5, all_positions[-1] + bar_width)\n",
    "############################################################\n",
    "\n",
    "# # Plot Kendall's Tau\n",
    "# for i, (model, values) in enumerate(data.items()):\n",
    "#     ax1.bar(i, values['kendall Tau'], bar_width,\n",
    "#             yerr=std_devs[model]['kendall Tau'],\n",
    "#             color=colors[model],\n",
    "#             label=model,\n",
    "#             # patterns=patterns[model],\n",
    "#             capsize=5\n",
    "#             , edgecolor='black', linewidth=1)\n",
    "\n",
    "ax1.set_title(\"\")\n",
    "ax1.set_ylabel('Kendall Tau')\n",
    "# ax1.set_xticks(index)\n",
    "# ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "ax1.set_ylim(0, 0.6)\n",
    "ax1.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot Krippendorff's Alpha\n",
    "# for i, (model, values) in enumerate(data.items()):\n",
    "#     ax2.bar(i, values['krippendorff Alpha'], bar_width,\n",
    "#             yerr=std_devs[model]['krippendorff Alpha'],\n",
    "#             color=colors[model],\n",
    "#             # patterns=patterns[model],\n",
    "#             capsize=5)\n",
    "\n",
    "# ax2.set_title(\"Krippendorff's Alpha\")\n",
    "# ax2.set_ylabel('Score', fontsize=16)\n",
    "# ax2.set_xticks(index)\n",
    "# ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "# ax2.set_ylim(0, 0.7)\n",
    "# ax2.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Create a single legend for both plots\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "# fig.legend(handles, labels, loc='center left', bbox_to_anchor=(0.85, 0.6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.85)  # Make space for legend\n",
    "plt.show()\n",
    "\n",
    "# Save as high quality PNG\n",
    "fig.savefig('model_comparison.pdf', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Data from your feature importance results\n",
    "data = {\n",
    "    \"Random Forest\": [0.5129, 0.1051, 0.0885, 0.0738, 0.0724, 0.0637, 0.0423, 0.0292, 0.0121],\n",
    "    # \"SVR\": [0.0046, 0.0249, 0.3210, 0.1085, 0.4155, 0.0145, 0.0443, 0.0147, 0.0520],\n",
    "    \"Linear Regression\": [0.0002, 0.0010, 0.0762, 0.0393, 0.0368, 0.0710, 0.7731, 0.0010, 0.0014],\n",
    "    \"MLP\": [0.7906, 0.1967, -0.0004, 0.0018, -0.0018, 0.0019, -0.0001, -0.0355, 0.0467],\n",
    "    # \"XGBoost\": [0.3093, 0.1228, 0.1033, 0.0841, 0.0959, 0.0642, 0.1252, 0.0508, 0.0444]\n",
    "}\n",
    "\n",
    "features = [\n",
    "    'length_words', 'readability', 'similarity_score', 'sentiment_polarity', 'politeness_score', 'mattr',\n",
    "    'hedging', 'question_count', 'citation_count'\n",
    "]\n",
    "\n",
    "models = list(data.keys())\n",
    "\n",
    "# Create custom blue colormap\n",
    "cmap = mpl.colors.LinearSegmentedColormap.from_list(\"blue\", [\"#f0f8ff\", \"#0047ab\"])\n",
    "\n",
    "# Create figure with adjusted size and ratios\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Create matrix for visualization\n",
    "matrix = np.array([data[model] for model in models])\n",
    "\n",
    "# Normalize each row (handling negative values)\n",
    "normalized_matrix = np.array([\n",
    "    [max(0, x)/sum(max(0, x) for x in row) for x in row] \n",
    "    for row in matrix\n",
    "])\n",
    "\n",
    "# Display heatmap with adjusted cell size\n",
    "im = ax.imshow(normalized_matrix, cmap=cmap, aspect='equal')\n",
    "\n",
    "# Set axis labels and ticks\n",
    "ax.set_xticks(np.arange(len(features)))\n",
    "ax.set_yticks(np.arange(len(models)))\n",
    "ax.set_xticklabels(features, rotation=45, ha=\"right\", fontsize=14)\n",
    "ax.set_yticklabels(models, fontsize=14)\n",
    "\n",
    "# Add text annotations with 3 decimal places\n",
    "for i in range(len(models)):\n",
    "    for j in range(len(features)):\n",
    "        value = matrix[i, j]\n",
    "        color = 'black' if normalized_matrix[i, j] < 0.5 else 'white'\n",
    "        ax.text(j, i, f\"{value:.3f}\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=color, fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.5)\n",
    "cbar.set_label('Feature Importance Intensity', rotation=270, labelpad=20, fontsize=14)\n",
    "\n",
    "# Adjust layout to remove whitespace\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.subplots_adjust(left=0.15, right=0.85, bottom=0.2, top=0.95)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama Finetuning results\n",
    "# Experiment 1 (llama_mse.txt) - MSE-based training\n",
    "experiment1_metrics = {\n",
    "    'F1': {'Kendall Tau': 0.381, 'Krippendorff Alpha': 0.422},\n",
    "    'F2': {'Kendall Tau': 0.347, 'Krippendorff Alpha': 0.385},\n",
    "    'F3': {'Kendall Tau': 0.412, 'Krippendorff Alpha': 0.453},\n",
    "    'F4': {'Kendall Tau': 0.394, 'Krippendorff Alpha': 0.436},\n",
    "    'F5': {'Kendall Tau': 0.365, 'Krippendorff Alpha': 0.408},\n",
    "    'Average': {'Kendall Tau': 0.380 ± 0.028, 'Krippendorff Alpha': 0.421 ± 0.027}\n",
    "}\n",
    "\n",
    "# Experiment 2 (llama.txt) - BCE-based training\n",
    "experiment2_metrics = {\n",
    "    'F1': {'Kendall Tau': 0.428, 'Krippendorff Alpha': 0.479},\n",
    "    'F2': {'Kendall Tau': 0.392, 'Krippendorff Alpha': 0.437},\n",
    "    'F3': {'Kendall Tau': 0.353, 'Krippendorff Alpha': 0.402},\n",
    "    'F4': {'Kendall Tau': 0.415, 'Krippendorff Alpha': 0.461},\n",
    "    'F5': {'Kendall Tau': 0.441, 'Krippendorff Alpha': 0.493},\n",
    "    'Average': {'Kendall Tau': 0.406 ± 0.035, 'Krippendorff Alpha': 0.454 ± 0.035}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

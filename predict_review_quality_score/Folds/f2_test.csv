paper_id,reviewer,title,abstract,review_text,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,politeness_score,venue,hedging,Human_Actionability,Human_Clarity_and_Readability,Human_Comprehensiveness,Human_Constructiveness,Human_Factuality,Human_Fairness,Human_Objectivity,Human_Overall_Quality,Human_Politeness,Human_Relevance_Alignment,Human_Sentiment_Polarity,Human_Usage_of_Technical_Terms,Human_Vagueness,Qwen_Actionability,Qwen_Clarity_and_Readability,Qwen_Comprehensiveness,Qwen_Constructiveness,Qwen_Factuality,Qwen_Fairness,Qwen_Objectivity,Qwen_Overall_Quality,Qwen_Politeness,Qwen_Relevance_Alignment,Qwen_Sentiment_Polarity,Qwen_Usage_of_Technical_Terms,Qwen_Vagueness,Llama_Actionability,Llama_Clarity_and_Readability,Llama_Comprehensiveness,Llama_Constructiveness,Llama_Factuality,Llama_Fairness,Llama_Objectivity,Llama_Overall_Quality,Llama_Politeness,Llama_Relevance_Alignment,Llama_Sentiment_Polarity,Llama_Usage_of_Technical_Terms,Llama_Vagueness,GPT_Actionability,GPT_Clarity_and_Readability,GPT_Comprehensiveness,GPT_Constructiveness,GPT_Factuality,GPT_Fairness,GPT_Objectivity,GPT_Overall_Quality,GPT_Politeness,GPT_Relevance_Alignment,GPT_Sentiment_Polarity,GPT_Usage_of_Technical_Terms,GPT_Vagueness,Phi_Actionability,Phi_Clarity_and_Readability,Phi_Comprehensiveness,Phi_Constructiveness,Phi_Factuality,Phi_Fairness,Phi_Objectivity,Phi_Overall_Quality,Phi_Politeness,Phi_Relevance_Alignment,Phi_Sentiment_Polarity,Phi_Usage_of_Technical_Terms,Phi_Vagueness
74,Reviewer-itVg,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","The authors introduced a view sampling strategy for novel view synthesis, grounded in the perspective of causal representation learning. They identified three key metrics to assess sampling performance: the fitting term, the consistency term, and the uniformity term. Additionally, they presented a novel theoretical framework addressing the sampling challenge within NeRF. 1. The introduction of the causal perspective in the view sampling algorithm holds significant potential and could serve as a foundational approach for future research in this domain.
2. The authors meticulously lay out a comprehensive mathematical framework that not only elucidates the underlying problem but also leads to the derivation of the three pivotal terms central to their methodology.
3. The paper stands out for its clarity and coherence, ensuring that readers, regardless of their expertise level, can grasp the concepts and findings presented."" 1. The rationale behind the view-sampling task raises questions. In certain scenarios, acquiring additional view images can be challenging. However, when a substantial number of dense views are already available, the motivation to devise a sampling strategy for training the neural rendering model with sparse views appears insufficient. Specifically, the activeNeRF model's primary objective is to identify the most optimal camera view for capturing the training image, rather than selecting from a plethora of pre-existing images.
2. The paper's primary contribution seems to be the introduction of a metric or loss function to evaluate the selected views. However, the absence of an ablation study that separately assesses the impact of each of these three terms is a missed opportunity for deeper understanding. As a result, the contribution feels somewhat lacking in depth.
3. The proposed loss function presents challenges in differentiability with respect to 't'. The sampling proposal, derived from the farthest sampling strategy, may not be the most efficient approach. It appears to demand significant training resources, resulting in elevated training costs. The potential enhancements in model performance might not justify the trade-off in terms of the increased training time and resource allocation. Please see the weakness above.",335,0,6,0.7966000000000001,0.1848602484,0.9041278958,52,29.0988,0.1431,iclr,0.0098039215686274,2,4,3,4,factual,4,4,85,polite,4,neutral,3,none,4,5,4,4,partially factual,5,5,75,5,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
74,Reviewer-bNPg,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","This paper studies the view sampling strategies of Nerf reconstruction from a causal perspective. The authors try to solve the problem using a small subset of photos from a total of K potential views, to achieve the best reconstruction. To solve this, the authors propose to use causal represntation learning using loss by Identification Treatment Effect. They propose three terms, a normal fitting term as reconstruction loss, a consistency term to ensure consistency between visible views and invisible views and a uniformity term requires the samples to be distributed evenly. The results show the proposed strategy can provide slightly better reconstruction compared to alternative baselines in the proposed setting. * The paper proposes a novel perspective to study the view sampling problem in volumetric reconstruction using NeRF as an example. This take-away can potentially also generalize other multiview reconstruction algorithms. 
* Given its current setting, the hypothesis is validated on nerf reconstruction datasets, with small improvement compared to its baselines. * The presentation of this paper could be greatly improved. I may not have understand a lot of details correctly given its current presentation. 
  * It is very hard to read without being very familiar with ActiveNeRF and casual representation learning. Have to trace to original papers for more details. This could be added to the preliminary parts. 
  * Too many notations which makes things more complicated than needed. I don't think I found how exactly the loss of consistency term and uniformity term were calculated in (8) at runtime. As I understand, the method should be as simple as calculating the reconstruction loss using different groups of input samples. Provide an algorithm chart of how of how P^{F}, P^{hat}^{CF} and P^{CF} will greatly help. 
  * There are some notations introduced in 4.1 (e.g. P(Y|do(d))) are not explained until 4.2. 
* Overall I am not sure I understand the real-world impact of this paper using the proposed strategy. Maybe I had some misunderstanding in the details given my concern on its presentation. Please correct me if I am wrong here. The goal of this paper to find ""optimal sampling strategy for training set"", ""K_s corresponding photos as sparse sample inputs among K_d total potential views"" is hardly a real problem statement for its real-world use case, which is my biggest concern for this proposed application of causal representation learning. From sampling perspective, we can use all the K_d potential views as long as they are available. As I understand, the evaluation of the counter factual distribution will require using the non-selected but captured images as supervision, which is not how active learning is executed in real-world case. Given this setting, it makes the results also less appealing in contrast to alternative baselines (which learns to predict next-best unknown view) given the fact all images from that particular datasets are used in evaluating the sampling strategy. 1. My major question is around how the clarity of the sampling process in training time. Confirm any places I misunderstood about this paper, as I highlighted in the weakness part. 
2. I am also curious how the views are sampled finally for different groups in the final results. Provide some visualization and discussions about them can be very helpful to guide the view-sampling process in real world applications. I wonder how that indicate the connection of uniformity term and consistency term are correlated to the camera FoV and ray distributions.",566,0,2,0.8138000000000001,0.1125,0.8472209573,52,40.8228,0.33,iclr,0.0,3,3,4,4,factual,4,4,90,polite,4,neutral,3,none,5,4,4,5,partially factual,4,4,65,polite,5,negative,5,moderate,2.0,3.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,5,neutral,4,moderate,2,2,3,4,partially factual,3,3,65,polite,4,neutral,4,low
194,Reviewer-cMiu,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The authors studied fine-tuning LLMs with limited memory. As the increased scale of current LLMs, the memory cost during fine-tuning is of great importance when adapting the pretrained LLMs to down-streaming tasks. In contrast to the existing work that mainly focus on the number of updated weights, this paper proposed to reduce the number of stored activations, also the inputs to each layer. Given the widely used stochastic gradient descent optimization pipeline, the authors proposed to store a subset of activations that can generate an unbiased gradient estimation. This way, the training memory and the training time decreased significantly. The authors provide both theoretical and experimental analysis on their CRS methods.  - This paper studied an important problem in LLM fine-tuning, i.e., how to fine-tuning LLMs with less memory consumption without increasing the computation cost. The authors provided solid quantitative results to show that the main memory consumption is from storing the intermediate activations. 
- The authors provided a general solution for fine-tuning LLMs under memory constraints. The solution can be applied in most transformer-based network architectures.  
- The authors provided solid mathematical proof on the unbiased gradient estimation, which is especially encouraged. 
- The extensive experiments on different network architectures showed the efficacy of the methods.
- The released code can benefit the following researchers studying efficient LLM fine-tuning.  - I am not fully convinced by the comment made in Line241-244, i.e., the methods in the paper is orthogonal to the activation quantization. When activation is quantized into a lower bit width, it is very possible that the number of less important activations will decrease. This way, the selection on the top-k columns in activation matrices with the proposed methods may hurt the training accuracy or convergence. It would be great if the authors can provide some theoretical analysis or experimental results on this combination. Otherwise, it would be necessary to provide some comparison results w.r.t. the activation quantization.
- It would be great if the authors can discuss the main difference of their paper w.r.t. \[Randomized Automatic Differentiation, ICLR2021\].	  Overall, I think this paper has a relatively high quality in both writing and scientific contribution. Yes",358,0,2,0.7825000000000001,0.1275074405,0.8477004170000001,215,33.3958,0.1262,neurips,0.0,4,4,4,3,factual,4,4,80,polite,4,positive,4,low,5,5,4,5,partially factual,5,5,88,polite,5,positive,5,low,3.0,4.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
13,Joseph-philipraj,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This systematic review is appropriate for the journal with a global problem of Mets and Urolithiasis. The introduction part clearly explains the motivation. The manuscript is clear and balanced. The manuscript stays focused on the subject. Authors have gone through the process of searching relevant articles from all websites and of sufficient duration. The inclusion and exclusion criteria in the analysis have been clearly stated. The impact of the analysis is clearly stated. The statistical analysis supports the paper well. The interpretation of the results, visualisation are well presented. The tables and figures are clear, relevant and correct. The authors demonstrate the knowledge of basic composition skills, including word choice, sentence structure, paragraph development and grammar. Limitations:  The studies included in the meta-analysis have cross-sectional nature and hence ascertainment of temporal association is not possible which also dictates need for further prospective studies. The specific type of stone formation is not correlated with studies. Despite these limitations all studies included in the meta-analysis showed the same directionality in the association between urolithiasis and Mets.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",292,0,1,0.7415,0.1145833333,0.8487246633000001,39,30.46,0.0999,f1000,0.01,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,3,5,5,3,factual,5,5,85,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,85.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,5,4,4,factual,4,5,92,polite,5,positive,5,low
13,Muhammad-Faruk,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assessed the association between metabolic syndrome and its components with the risk of developing nephrolithiasis by conducting systematic review, Bayesian random-effects meta-analysis, meta-regression and dose-response analysis. This study was done appropriately based on PRISMA flowchart. Risk of bias was also conducted of the included studies. This study has successfully presented the proper meta-analysis for this design. However, to complete this study for indexing, I personally recommended several revisions: 1. Abstract: Introduction section: It is better to address meta-regression as the analysis to assess the correlation of association along with dose-response analysis  Conclusion section: In reporting the association between predictors and nephrolithiasis, state only the predictors in which its coefficient was statistically significant 2. R language was not considered as a statistical software for data analysis. The software for data analysis should be written as ""R"" (Please refer to methods section in statistical analysis subsection). 3. Please update the PRISMA flowchart (refer to PRISMA guideline 2009). 4. Give the numbering of each Forrest plot in Figure 3 and numbering of each meta-regression plot in Figure 4. Design these figures so that it could be well presented. 5. Uniformly decide the word choice of ""traits"" or ""components"", choose whether to use traits or components in the whole text, use one of these words consistently to avoid any misunderstanding. 6. It is better to provide the meta-regression of hypertension in systolic blood pressure and diastolic blood pressure as it is important to explain the relationship to nephrolithiasis in differentiation for these two types of blood pressure. 7. Meta-regression of body mass index  was sufficient in this study thus waist circumference meta-regression was not necessary to be included. 8. Provide the value of coefficient and confidence interval of each meta-regression analysis in the result section so that better understanding of predictors-outcome relationship could be reached clearly.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",422,0,7,0.7554000000000001,0.1982758621,0.9299524426,270,24.68,0.2302,f1000,0.0108695652173913,5,5,5,5,factual,5,5,93,polite,5,neutral,5,moderate,4,4,4,4,factual,4,4,85,polite,4,neutral,4,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,85.0,polite,5.0,positive,4.0,none,5,4,4,5,factual,5,5,90,polite,5,positive,5,low,4,4,4,4,factual,4,5,88,polite,5,neutral,5,low
181,Reviewer-HLbG,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","This work aims to build a foundation model for knowledge graph reasoning tasks, where the authors explore the setting of generalization to any edges and nodes, including unseen, of any multi-relational knowledge graphs without using node and edge features. To this end, the authors first construct a view of a relation-centric graph from an original graph where edges become nodes of this new relation graph, and then, based on this view, the authors represent the relation (node) relative to and conditioned on the query relation. Then, based on this relative relation representation, the authors use existing inductive link prediction methods to perform knowledge graph reasoning. The authors conduct link prediction experiments on various knowledge graphs considering both inductive and transductive settings, and show that the proposed method, namely ULTRA, outperforms other SOTA baselines sometimes without further fine-tuning on target knowledge graphs (i.e., zero-shot). * This work studies the very important, challenging, and practical setups of building a foundation model for knowledge graph reasoning, which aims to be generalizable to any other knowledge graphs involving unseen nodes and unseen edges, without leveraging features of nodes and edges. 
* The proposed method works well with different knowledge graphs, on zero-shot transfer learning setups without further fine-tuning on target knowledge graphs, and further shows the boosted performance with task-specific further fine-tuning on them, on most experiment setups.
* This paper is very well-written and easy to follow. * I would like to note that I don't see any major weakness, and below is the minor.
* In Section 4.2, the explanation about the indicator function with variables $u$ and $v$ is a bit unclear to me. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?
* Text-based methods (e.g., LM-based methods) can be generalizable to any knowledge graphs including unseen nodes and unseen edges, as long as their nodes and edges are represented with texts. In this vein, I think one potential direction for building a foundation model for knowledge graph-related tasks might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features; meanwhile, given the framing of this work (""Towards Foundation Models for Knowledge Graph Reasoning""), this point should be carefully explained. * I would like to suggest emphasizing the performance differences between inductive and transductive setups when explaining Table 1. The proposed method w/ 0-shot settings are strong on inductive graphs; meanwhile, previous methods are superior to it on transductive graphs, which are worthwhile to discuss.
* It may be beneficial to show the results of the ULTRA fine-tuned on the knowledge graphs used for pre-training the ULTRA. I am wondering if there are further performance improvements when further fine-tuning the model on the data used for pre-training.",496,0,0,0.7682,0.1549267161,0.9152074456,49,38.4364,0.3761,iclr,0.0,4,4,3,4,factual,3,3,80,neutral,4,neutral,3,moderate,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,3.0,4.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,4,low
9,Reviewer-y5kB,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to ""encourage"" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors. The proposed rule is straight forward.  

Computational complexity of the evaluation of true utility is well addressed making the method practical.

Empirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.

Decomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.

Proposed measures of plasticity and forgetting seem sensible.

The paper is well written. Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.

Fundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below). Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?

Why are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?

This is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?",730,0,0,0.7435,0.0644808927,0.8415006399,47,41.9389,0.0501,iclr,0.0,4,4,4,4,factual,3,4,85,polite,5,neutral,3,low,3,4,4,3,partially factual,3,3,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
187,Reviewer-2CBB,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","This paper introduce a novel unsupervised feature selection methods called GRSSLFS, which combine matrix factorization with self-representation subspace learning and apply graph regularization to preserve the geometric structure of the feature vectors. This method is proved to be effective in both theory and experiments. In this paper, the author introduce the problem of the redundant data in traditional self-representation, and then apply matrix factorization self-representation problem to achieve the goal to reduce the dimension of basis matrix. Here are some strengths of this article:

1. This paper introduce a novel problem of redundant data in self-representation problems and then propose a method to solve this problem.

2. Plenty of theoretical proof are given in the paper and appendix, the convergence analysis indeed increase the persuasiveness of the article.

3. The proposed method was compared with a variety of comparison algorithms on multiple data sets, demonstrating the effectiveness of the method. However, there are still some weaknesses in this paper.

1. In the end of Introduction section, the second and third contribution points is not sufficient, as these constraints of regularization are not proposed in this article. 

2. In the methodology section, some formula calculations are confusing and not very convincing. Such as the multiplication in formula (6) and the optimization target in the optimization goal (formula 7). These issues will be described in detail in subsequent questions 1 and 2.

3. In the methodology section, the description of the algorithm is not complete enough. The specific process of selecting features according to the matrix U in Algorithm 1 has not been described in detail. 1. In the section of methodology, the equation (6) is confusing and not so clear. It seems impossible to subtract the matrix XUV of shape m*m from the matrix X with the shape m*n? 

2. As the feature matrix B is fixed by the VBE method proposed in section 3.3, it is unclear why the basis coefficient matrix G in equation (7) is a parameter to be optimized. Why the matrix G can not be determined by equation (4) directly and reduce the number of parameters.

3. In section 3.1, subspace learning that introduces graph regularization seems to be existing methods. Should this part of the content be moved to related work?",376,0,8,0.679,-0.048046398,0.9640573859,51,40.0877,0.0751,iclr,0.0,4,4,4,4,factual,4,3,70,polite,4,neutral,4,low,3,3,4,3,partially factual,4,4,70,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
33,Torill-Sauer,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assess the FNAC method combined with cell blocks for immunostaining of ER, PgR and HER-2 in breast cancer cell material compared with CNB. As such I consider it a validation study and the number of cases as sufficient. The introduction part about the cell block technique and immunostaining is brief, and could be expanded. There is quite a number oF articles on the topic, both in cytology journals and others. There are also a number of articles on ER, PgR and HER-2 on FNAC materiale from breast cancer, AND you should confer with and refer to chapters 8 and 9 in ""The International Academy of Cytology Yokohama System for Reporting Breast Fine Needle Aspiration Biopsy Cytopathology"" by Andrew Field and coworkers. ISBN 978-3-030-268824. A validation study should tell us if the two methods we validate are equal. As such the concordance should be high, > 90 %. About 1/3 of your HER-2 positives were missed by the cell block method. ER and PgR have divergent results in both directions: positive and negative. Your results will have treatment implications, and divergent results must be minimal. You use the same IHC protocol both for cell block and CNB. That is common, BUT they are not equal. The preanalytical handling of FNAC material is not the same as for CNB. From your description it seems that all your aspirated material is an ethanol cell suspension. Ethanol is a good fixative, but it is a precipitating/coagulating type of fixative that changes the tertiary structure of the cell molecules in a quite a different way as the cross-linking formalin. The time in alcohol is the primary fixation. Your material is brought to the laboratory when you have finished your out patient clinic, which can be from 30 minutes up to more than one hour. Your cells are fully fixed in ethanol when they reach the lab. You use formalin as post-fixation, which is a good thing, but your epitope presentation will be determined by the alcohol fixation. That means you need to modify your protocol, because the demasking should not be equal to a primary formalin fixed tissue. I suggest that this is the reason for the significant discrepancy of HER-2 positivity. I disagree with your conclusion then, that the two are equal, but with a protocol modification and optimisation, I think you could achieve it. The subtype of BC is hardly relevant as a cause of discrepancy, nor the number of cells as long as the number is sufficient for evaluation. You mix methodology, screening and clinical issues in your discussion.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",574,0,2,0.7688,0.1598682477,0.9022762775,850,44.24,0.3172,f1000,0.0106382978723403,5,5,5,5,factual,5,5,100,neutral,5,negative,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
112,Agnieszka-Lawrynowicz,Machine Learning for the Semantic Web: Lessons Learnt and Next Research Directions,"Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed jointly with an analysis of their main peculiarities and drawbacks, afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.","The paper surveys methods of machine learning as solutions developed for the Semantic Web, dividing them into symbolic ones and numeric ones. Machine learning methods proved efficient in supporting Semantic Web tasks, and there have been an icreasing interest in their application in the Semantic Web, especially regarding the numeric approaches, which is what the paper also discusses. Besides of their strenghts, the paper also points to drawbacks of current numeric machine learning approaches such as non-interpretability or lack of reasoning capabilites with respect to standard languages (especially OWL). The paper also points to next research directions in the development of machine learning solutions for the Semantic Web, and I fully agree with the author when it comes to these directions.  Below I provide some suggestions for improving the manuscript: 1) Overall, the manuscript contains several technical words (ILP, propositionalization, embeddings etc.), which may be not known to a reader not knowledgeable in machine learning. I suggest to explain those which are not explained to make the paper self-contaied, e.g. by injecting phrases with explanations, similarly, like it is already done in some places in the paper, e.g.: ""latent attributes (i.e. attributes not directly observable in the data)"". 2) The paper surveys methods developed by researchers active in the field, including the author. It would be much nicer to mention their names along with the citations, when suitable. 3) It would be valuable to summarize the main, recurring peculiarities and drawbacks of the methods discussed in Sections 2-3, maybe even using some table or graphics?  4) Regarding definitions, they are in an informal style (which is perfectly OK for a position paper), but still there is some care needed: * ""embedding models (also called energy-based models)"" -> are energy-based embedding models a class of embedding models or they are equivalent to each other? * ""In this context, link prediction is also referred to as knowledge graph completion."" -> in what context, in the context of KGs? Are there other tasks of knowledge graph completion, beyond link prediction?  5) Numeric methods are described for one major task: link prediction. Are there any other tasks that have been tackled by numeric machine learning methods for the Semantic Web?  6) References: It would be also nice to include a book within the topic, but of course this is up to the author: Agnieszka Lawrynowicz, Semantic Data Mining - An Ontology-Based Approach. Studies on the Semantic Web 29, IOS Press 2017. There is also a highly cited survey that deals with the topic of knowledge graph completion: Heiko Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic Web 8(3): 489-508 (2017) Minor issues, typos:  *** Section 1. Introduction *** Page 1: it would be valuable to provide a reference to OWL Page 1: ""and assertion"" -> ""assertions"" Page 1: ""some these gaps"" -> ""some of these gaps"" Page 2: ""are illustrated is Sect. 4"" -> ""are illustrated in Sect. 4""  *** Section 2. Symbol-based Methods for the Semantic Web ** Page 2: ""One of the first problem"" -> ""One of the first problems"" Page 3: ""by the the employment"" -> ""by the employment"" *** Section 3. Numeric-based Methods for the Semantic Web ** Page 4: ""Almost any reasoning"" -> ""Almost no reasoning"" *** Section 4. Machine Learning for the SemanticWeb: Next Research Directions *** Page 5: ""As a first step, the integration of numeric and symbolic approaches should be focused.""->""The first step should focus on the integration of numeric and symbolic approaches""? Page 5: ""The main the conclusion""-> ""The main conclusion"" Page 5: ""how representing expressive logics within neural networks"" -> ""how  to represent expressive logics within neural networks"" Page 6: ""background knowledges"" -> ""background knowledge"" Page 6: ""and and makes it understandable"" -> ""and makes it understandable"" *** Section 5. Conclusions *** ""their main peculiarities and drawback"" -> ""their main peculiarities and drawbacks""",643,1,3,0.7094,0.144527027,0.9203350544,99,43.43,0.2025,semanticweb,0.0,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,5,4,4,5,factual,4,4,85,polite,5,positive,4,low,5,4,4,5,factual,4,4,88,polite,5,positive,3,low
191,Reviewer-qQw6,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The authors present Video Prediction Rewards, an algorithm that leverages transformer-based video prediction models as action-free reward signals for reinforcement learning. The reward induced by this video prediction model incentivizes the agent to find the most likely trajectory under the expert video distribution. By further incorporating some exploration rewards, such as RND, the proposed method obtains good performance across a wide range of DMC, Atari, and RLBench tasks. The paper is well written and easy to read.
The authors aim to address a crucial problem in reinforcement learning, i.e., the reward function design. The authors propose a concise method, and experimental results also validate the effectiveness of the approach.
 I'm concerned about the problem of out-of-distribution. Can the pre-trained video prediction models accurately evaluate unseen behaviours? See the weakness NA",130,0,1,0.7995,0.1777777778,0.9095652103,218,32.2492,0.1633,neurips,0.0109890109890109,2,2,1,2,partially factual,3,2,20,polite,3,neutral,2,high,3,4,4,4,partially factual,4,4,75,polite,4,positive,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,2,factual,3,3,60,polite,4,positive,4,low,2,5,3,3,partially factual,3,3,75,polite,4,positive,4,low
191,Reviewer-BsvD,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The paper proposes to use a large video-prediction model for learning a reward model for RL. The agent's performance is evaluated on a total of 8 envs from 3 benchmarks: DMC, Atari, and RLBench. The paper argues that the proposed model also generalizes to different environments for which no training data was provided, enabling cross-embodiment generalization for tabletop manipulation. * The paper is well-written and easy to follow.
* Section 4 is very well organized. It starts by asking base questions like ""can VIPER provide an adequate learning signal for solving a variety of tasks?"" before jumping on to the evaluation of the performance of the RL algorithm. Limited baselines: The paper compares with just 1 baseline (the second ""baseline"" is more of an ablation of the first baseline). e.g. there is https://sites.google.com/view/vip-rl that claims to provide ""dense visual reward"". The paper itself lists a bunch of baselines (in the related work) but does not compare to them.

Limited ablations: See questions.

Overall, I think the paper is interesting but I want to see performance improvement over a bunch of baselines and some ablations. I would encourage the authors to engage during the rebuttal period. 1. In line 128, the paper states ""For example, when flipping a weighted coin with p(heads = 0.6) 1000 times, typical sequences will count roughly 600 heads and 400 tails, in contrast to the most probable sequence of 1000 heads that will basically never be seen in practice"". Could the authors explain why is the sequence of 1000 heads the most probable one ?
2. Does the algorithm work with good trajectories as well or does it need access to expert trajectories? e.g. in line 172, what if they were using the top 650 to top 550 episodes, in place of the top 100 episodes. This would make for an useful ablation experiment.
3. Arent the video datasets ""too small""? Given that the video models are trained for hundreds of thousands of updates, I wonder if the video models are drastically overfitting, leading to (i) the learned policies not showing any diverse behaviours and (ii) the learned policies failing with stochastic envs. This would make for another useful ablation experiment.
4. Line 201 states that TPUs were used for training while 226 states that GPUs were used for training. Which is it :) NA",389,1,7,0.7881,0.1882653061,0.9128586054,218,56.4134,0.2025,neurips,0.0,3,3,2,3,partially factual,3,3,40,neutral,3,positive,2,low,4,5,3,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,5,neutral,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
147,Reviewer-NBus,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper studies the problem of injecting new entity knowledge in LLMs, such that these knowledge can be propagated and utilized when LLMs make inference on related queries. The paper proposes a context distillation method that consists of two steps to inject entity knowledge in a definition sentence: 1) Use a LLM to generate a set of continuations (a.k.a transfer set) for the definition sentence. 2) Fine-tune a student model such that its output distribution without conditioning on the definition sentence is close to the output distribution of a teacher model that conditions on the definition sentence.
They conduct experiments on two datasets about entity knowledge and show that the proposed method outperforms several baselines including standard fine-tuning and previous knowledge editing methods. 1. This paper studies an important question of knowledge injection and propagation of injected knowledge. The proposed method is novel in this context.
2. Some of the conducted analyses are insightful, such as the NLL with/without definition sentence for analyzing the supervision from the teacher model. 1. On Entity Inferences dataset, the conclusion that the proposed method improves the model ability to make inference using the injected knowledge is suspected. The reported performance improvement might due to the overlap between the generated transfer set and the probe sentence in the evaluation set. Without reporting (1) the level of overlap, and (2) a baseline that simply fine-tunes on the transfer set, the possibility of this overlap cannot be ruled out.
2. How does the method perform compared to a baseline that simply prepends the transfer set to the query? 1. In Table 2, the Target for GPT2-XL should be 64.3 instead of 65.3 (based on the $\Delta$ value)?
2. In Table 2, why would using GPT3.5 to generate transfer set result in worse specificity for GPT2-XL?
3. I'm not sure why most of the analyses are done on the ECBD dataset, as I thought Entity Inferences dataset concerns more about injected knowledge propagation. Limitations are discussed.",328,0,7,0.752,0.0371685606,0.8985326290000001,215,40.4891,0.1507,neurips,0.0,5,5,4,4,factual,5,5,75,polite,5,neutral,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
125,Nicola-Maffulli,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article bears witness to how much we fall in love with novelties, and how much we, as a scientific community, do not know yet about a fashionable autologous blood product.This case report is now one year old, and the situation in this field remains unchanged: randomised controlled trials show in a fairly unequivocable fashion that PRP use is at best dubious, and nevertheless case series report success.This should make us think, and use strict stringent scientific methods to plan and evaluate new technologies.",153,0,0,0.8462000000000001,0.2045900178,0.6052519083,368,15.68,0.1355,f1000,0.0,1,2,1,2,unfactual,2,0,40,polite,3,positive,1,extreme,2,4,3,2,partially factual,2,2,45,impolite,4,negative,4,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,1,3,2,1,partially factual,3,2,40,neutral,3,neutral,3,moderate,1,4,3,2,partially factual,3,2,55,neutral,4,negative,4,low
124,Dharma-Varapula,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this report, Barbitoff, Y. and Predeus, A. have described a study investigating if read trimming, specifically adapter trimming, affects variant calling accuracy using commonly employed variant callers. The authors find this investigation to be of significant value citing there is no prior systematic study exploring the impact of read trimming on variant calling accuracy. In the study, WES and WGS datasets from seven GIAB samples were processed using six different variant callers (DeepVariant, GATK HaplotypeCaller, Freebayes, Strelka2, Octopus, and Clair3) to measure the effect of read trimming performed prior to the variant calling. The authors show comparative metrics (differences between trimmed and untrimmed variant caller performance metrics – recall, precision and F1 scores) and find no substantial differences in variant calling performance, except in the case of 200x coverage WES. Subsequently, the authors downsampled the data to produce a simulated 80x WES dataset expecting a greater likelihood for an  increased impact of read trimming on variant calling accuracy. This simulated dataset too did not show significant impact due to read trimming. Further, the authors found no correlation between extent of adapter base contamination and impact of read trimming on variant caller performance metrics. Additionally, the authors ran the pipelines with different variant callers and found minimal impacts due to read trimming upstream. My comments below: The adapter base percentage variation ranged from 8.1% to 35.2%. Please comment if this is an expected range for WES datasets. Also, please mention the coverage of the WES dataset in the caption for Fig 1. How does one assess the changes in performance metrics to be significant or not (Fig 1b and 1c)? Recall and precision score metrics in Figure 1b for Indels in WES datasets show deviations from the mean and these are not explained thoroughly. If this variance is to be expected, is it likely that the sample set n of 7 is too low? Or is the data heteroscedastic? In my view, the observations made on data presented in Figure 1e are not sufficiently explained. Discussion section on this aspect is a rehash of the content in the Results section. Read trimming is often a lower time-cost step compared to the variant calling step. It would benefit the reader (and the authors) greatly if there was a more detailed explanation why this is an important decision to make, which this study is aimed to inform us better for. Data redundancy and potential loss of raw data (if only single copy retained) appear to be valid reasons on the surface, a more complete justification is need in my view. Review of prior literature work can be more exhaustive.  I was unable to access or review the Supplementary information, so it has not been included in my review. Please update in revised version  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",606,0,1,0.7883,0.0970054945,0.9262039661,96,35.37,0.2663,f1000,0.0108695652173913,5,4,4,5,partially factual,5,4,75,polite,5,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,5,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
168,Reviewer-hsiV,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. However, the novelty is not enough. This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. They show various results to examine their methods. The novelty is not enough. The joint error based triplet alignment is not new, which is an extension of maximum cross margin discrepancy to three subsets, source, labeled target and unlabeled target. Eventual model is also very complicated. 

The model performance is not good enough. Especially compared with DECOTA in Table 1 & 2, it is very comparable. Also for semi-supervised setting, the selected target samples are very essential. There is no standard variance. Also t-test is needed to examine the significance. The clarification of model novelty.
The performance improvement.",174,0,0,0.7846000000000001,0.0049242424,0.9568377137,49,38.6381,0.0999,iclr,0.0,1,4,3,4,partially factual,3,2,45,neutral,3,negative,2,low,4,3,3,3,partially factual,4,3,55,neutral,5,negative,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,4,moderate,2,4,3,3,partially factual,3,3,60,neutral,4,negative,4,low
168,Reviewer-TnGf,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This work introduces a Triplet Alignment approach for semi-supervised domain adaptation. It simultaneously minimizes the joint error among different domains and the error rate on labeled data. 1.	The motivation for this work is clear. It aims to address the challenge of semi-supervised domain adaptation, particularly when only a limited number of annotated examples are available in the target domain. The proposed method optimizes both the classification loss and the joint error across source, labeled, and unlabeled target domains simultaneously.
2.	The proposed models are presented in a clear and comprehensible manner. 1.	The proposed model, to the best of my knowledge, lacks significant novelty as it closely resembles the approach in \[2\]. It would be helpful to explicitly identify the main difference.
2.	The choice of baseline methods in this work appears to be less competitive. Given the recent progress in semi-supervised domain adaptation (SSDA), including \[1\]\[2\], it is advisable to compare the proposed method with these contemporary approaches. Furthermore, while the use of t-SNE for feature space visualization is commendable, the comparisons are made with older methods like ENT (Grandvalet & Bengio, 2005), MJE (Zhang & Harada, 2019), and MME (Saito et al., 2019). It is imperative to include comparisons with more recent methods to provide a comprehensive evaluation.
\[1\]  Yu, Yu-Chu, and Hsuan-Tien Lin. ""Semi-Supervised Domain Adaptation with Source Label Adaptation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
\[2\] Rahman, Md Mahmudur, Rameswar Panda, and Mohammad Arif Ul Alam. ""Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Please see ""Weaknesses""",270,6,7,0.776,0.1943277311,0.9432914257,49,34.3667,0.1719,iclr,0.0,3,4,3,3,factual,3,3,55,neutral,4,neutral,2,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
115,Reviewer-sFvB,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The motivation of this paper is that people constantly make lots of causal and moral judgments to reason about why did what things and why. This paper contributes a dataset of stories compiled from cog sci papers, with detailed annotation of the factors that contributes to the human judgment. Then, the paper looks at how LLMs make judgments, and check the alignment with humans. - The paper addresses an important topic to check the causal and moral reasoning and the alignment of LLMs with humans
- The proposed dataset looks solid and well-annotated
- The analysis provides insights to the community to develop safer and more aligned LLMs. - The size of the dataset is a bit limited, 144 causal stories and 62 moral stories, making the insights drawn upon them be not extensive enough
- The yes/no binary answer is reasonable, but analyzing LLMs behavior using a binary classification task might have a little signal-noise ratio. There needs to be lots of human annotation to evaluate the reasoning quality of LLMs, and whether any misalignment or unsafe reasoning was provided apart from the binary answer. 1. Are there domain experts in moral psychology / philosophy involved in the design process of this paper? How do you make sure the factors in 2a and 2b are comprehensive and can explain for all the judgment decisions? I saw the appendix A.1, but I would like to see one dedicated paragraph for each of Table 2a and 2b, describing the rationale behind each factor and how they correlate with human intuitions in the main text in the next version of the paper.

2. Can the authors let LLMs to output its reasoning, and then annotate what type of tendencies LLMs show in its reasoning (maybe doing it on a subset, e.g., 50 samples)?

\[I have read the rebuttal, and acknowledge the author's effort into it. I'm supportive of the acceptance of this paper.\] N/A",322,0,3,0.7616,0.0912608225,0.9197995663,215,45.357,0.1229,neurips,0.0,4,4,2,3,factual,3,3,62,neutral,4,negative,4,moderate,5,5,5,5,factual,5,5,95,polite,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,3,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,3,low
154,Reviewer-MiyQ,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","This work studies stochastic block models where blocks/clusters can have different sizes. It proposed a simple SVD algorithm which recovers communities in this setting. The main technical improvement of this work is that the assumption is removed which requires there to be a ‘size interval’ where no clusters appear. 
A secondary result is a efficient clustering algorithm with sublinear query complexity. 
 -	This work is a clear improvement over the previous state-of-the-art. As I understand it, a key technical contribution of this work that might influence future work is instead of finding $k$ clusters as is done using the SVD approach, the algorithm first aims to find large clusters one-by-one. Although these are not perfect (they form a so-called plural set), using some non-trivial techniques perfect recovery can be obtained. 
- Experiments on synthetic data indicate that the algorithm not only works well in theory but also in practice.
- The write-up of this work is excellent. -	Given that the aim of the studied setting is to look at more realistic settings, I would have expected to find experimental results on real-world datasets as well. Although this work does provide better bounds for SBMs generated with differently sized clusters, SBMs still have a highly symmetric structure compared to real-world graphs. It would be interesting to see the performance of the proposed algorithm on some real-world graphs.  -	How does the algorithm compare with respect to the previous work in terms of running time?
- In practice the Spectral Clustering algorithm performs well in practice on graphs with clusters of unbalanced size. Even though not many bounds are known of spectral clustering with respect to SBMs, did you try to compare your algorithm experimentally with Spectral Clustering? none",288,0,1,0.795,0.1212698413,0.9042724371,221,46.453,0.1969,neurips,0.02,3,4,3,3,partially factual,4,4,70,polite,4,neutral,3,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
151,Reviewer-27ee,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper introduces a technique for rare event sampling that combines normalizing flows with importance sampling. The authors refer to this technique as NOFIS (NOrmalizing Flows assisted Importance Sampling). They justify their work by highlighting the limitations of standard sampling algorithms, such as MCMC, in sampling regions of low probability, where the density, denoted as $p$, is approximately $10^{-X}$, with X being an integer greater than 4. In this context, known as the regime of rare event sampling, algorithms like MCMC would require an impractical number of samples, rendering these approaches highly inefficient. The authors propose that employing normalizing flows-aided importance sampling holds promise as a solution to this problem. - The paper flows smoothly and is enjoyable to read.
- The authors provide great level of details and do not take anything for granted, which I appreciate. - **Novelty**: I don’t find much novelty in the proposed paper. The technique presented by the authors has already been explored in many prior works in different fields, particularly in physics, where rare event sampling is often a challenging problem (see below).

- **Related Works**: Despite many prior works combining normalizing flows with importance sampling, and beyond, exist, this paper lacks a dedicated *Related Work* section. Several seminal works have been completely overlooked despite their significant contributions to the field of normalizing flow-aided importance sampling in statistical physics \[1\], chemistry\[2\], and quantum field theory\[3,4,5\].

- **Annealed Importance Sampling**: There is no reference to *annealed importance sampling* \[6\], which I believe is highly tight to the idea of the paper. Besides \[6\], several relevant works \[7,8,9\] perform annealed importance sampling within the context of normalizing flows, falling within the same category as the CRAFT method referenced in the paper, though only marginally. What these methods do closely aligns with what the authors propose in the paper: instead of learning the target distribution in one step, they 'anneal' towards that distribution by learning and sampling from intermediate distributions, ensuring that the final learned probability density has as much support as possible, including regions where the target density is small enough to fall within the rare event regime. I believe it is crucial for this paper to be published in this or any other venue to highlight the connection to these (and the previously referenced works).

- **Rare Event Sampling**: A recent paper \[10\] discusses similar behaviors in training normalizing flows and combining them with importance sampling to ensure full support over the target density, including rare event regions. I would find it interesting if the authors commented on this work within the context of their findings. Some of the metrics and tools proposed in \[10\], such as the mode-dropping estimator, could also be used to assess the performance of a sampler in approximating regions of low probability where a shallow sampler is likely to lose some of the probability mass.

- **Idea of Anchor Points**: The notion of *anchor points* has implicitly been explored in some of the prior works mentioned above, albeit with a slightly different connotation that may have escaped the authors' attention. For instance, in the paper by Kanwar et al. \[4\] (Fig. 4), the authors use a technique very similar to what is suggested in this paper, although with slightly different connotations (e.g., they use previously trained flow-based models as starting (anchor) points to sequentially train more challenging distributions).

- **Additional Related Works**: Other closely related works, such as \[11\], are not mentioned in the manuscript despite having similar titles. This may cause confusion for potential readers.

- **Experiments**: I find the results presented in the paper not entirely convincing. Although the authors compared their approach to a large set of baselines, this alone does not seem sufficient to claim the superiority of the proposed method. I am surprised that the proposed approach is not compared against prior works, such as Annealed Importance Sampling with Normalizing Flows \[7\], and naive RealNVP training with a sufficiently large number of couplings and no anchor points.

As a side note, I strongly recommend that the authors conduct an extensive literature search to include and acknowledge existing prior works, and eventually, compare and discuss potential differences and similarities - I'd like to see how the author would compare their work (and its corresponding novelty) to previous works. In particular, I'd like to see comparisons with Refs. \[6-9\] for the annealing aspect and Ref. \[10\] for the theoretical discussion regarding low-support regions (e.g., the rare event regime). Furthermore, discussing the differences concerning Ref. \[11\] would be helpful for the readers.

- I'd appreciate if the authors could perform an extensive literature search and create a Related Work section to place their paper in the context of existing prior works. Please refer to Refs. \[1-11\].

- I found the last paragraph in Section 3.1 and the discussion in Appendix B to be a bit unintuitive. It has been shown in the literature that using Forward KL, instead of Reverse KL, generally results in larger support and, therefore, has some benefits when combined with importance sampling. In that sense, I am surprised by the author's claim that training using Forward KL deteriorates performance. Do the authors consider the case where NO samples are given from the target density? If so, then I may understand this point. Otherwise, when a sample set from the target density, even if small, is available, it should be possible to show that training with Forward KL is feasible.

- It would be informative to see the density plot from Figure 4 for the other baselines as well.

- On page 8, referring to Figure 4, the authors write ""\[…\] the right part further reveals that when increasing $N_{IS}$, the estimation could become even more accurate."" This result does not seem neither novel nor unexpected. Indeed, it was already demonstrated in prior works, as seen in \[1,5\], that the variance of the importance sampling estimators scales with $N^{-1}$, with N being the number of samples. Could maybe the authors comment on this?

**Minor**

- The quality of the plots on pages 7-8 is quite poor. The axis labels are missing, and the font size for the x-y tick labels is too small.

- As a side note, I sometimes find the MK notation a bit confusing. However, I understand that it would require a substantial effort to rewrite the manuscript and adapt to a clearer notation. Nevertheless, this my be a feedback worth keeping in mind for the authors for future iterations of the manuscript. 

- I find it somewhat unintuitive to completely relegate the discussion of the datasets to the appendix. Perhaps the authors could add corresponding references in the main text when mentioning the datasets and also refer to the Appendix for further details.

- In the conclusion, statements like *using nested subset events as bridges* agains strongly reminds of annealed importance sampling. I believe that a discussion comparing the present method to AIS, highlighting potential differences, or connecting them through their analogies is an essential element currently missing in the manuscript.


**References:**


- \[1\] \[Nicoli, Kim A., et al. ""Asymptotically unbiased estimation of physical observables with neural samplers."" Physical Review E 101.2 (2020): 023304.\](https://link.aps.org/accepted/10.1103/PhysRevE.101.023304)
- \[2\] \[Noé, Frank, et al. ""Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning."" Science 365.6457 (2019): eaaw1147.\](https://www.science.org/doi/10.1126/science.aaw1147)
- \[3\]\[Albergo, Michael S., Gurtej Kanwar, and Phiala E. Shanahan. ""Flow-based generative models for Markov chain Monte Carlo in lattice field theory."" Physical Review D 100.3 (2019): 034515.\](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.100.034515)
- \[4\]\[Kanwar, Gurtej, et al. ""Equivariant flow-based sampling for lattice gauge theory."" Physical Review Letters 125.12 (2020): 121601.\](https://link.aps.org/pdf/10.1103/PhysRevLett.125.121601)
- \[5\] \[Nicoli, Kim A., et al. ""Estimation of thermodynamic observables in lattice field theories with deep generative models."" Physical review letters 126.3 (2021): 032001.\](https://link.aps.org/pdf/10.1103/PhysRevLett.126.032001)
- \[6\]\[Neal, Radford M. ""Annealed importance sampling."" Statistics and computing 11 (2001): 125-139.\](https://arxiv.org/abs/physics/9803008)
- \[7\] \[Midgley, Laurence Illing, et al. ""Flow annealed importance sampling bootstrap."" arXiv preprint arXiv:2208.01893 (2022).\](https://arxiv.org/pdf/2208.01893)
- \[8\] \[Wu, Hao, Jonas Köhler, and Frank Noé. ""Stochastic normalizing flows."" Advances in Neural Information Processing Systems 33 (2020): 5933-5944.\](https://proceedings.neurips.cc/paper/2020/hash/41d80bfc327ef980528426fc810a6d7a-Abstract.html)
- \[9\] \[Caselle, Michele, et al. ""Stochastic normalizing flows as non-equilibrium transformations."" Journal of High Energy Physics 2022.7 (2022): 1-31.\](https://arxiv.org/pdf/2201.08862.pdf)
- \[10\] \[Nicoli, Kim A., et al. ""Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories."" arXiv preprint arXiv:2302.14082 (2023).\](https://arxiv.org/pdf/2302.14082)
- \[11\] \[Falkner, Sebastian, et al. ""Conditioning normalizing flows for rare event sampling."" arXiv preprint arXiv:2207.14530 (2022).\](https://arxiv.org/pdf/2207.14530.pdf)",1395,47,22,0.8111,0.0786587302,0.9124334455,55,43.1599,0.8084,iclr,0.011111111111111,4,4,5,4,factual,4,4,90,neutral,4,negative,4,none,5,5,5,5,factual,5,5,100,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
151,Reviewer-9Yao,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper proposes to use normalizing flows to sample rare events. The neural networks learn the proposal distribution for the importance sampling and then use importance sampling to estimate the rare event probability. The numerical experiments show that the proposed method uses fewer function calls and has smaller errors in the average of the estimation. 1. The motivation and the problem statement are clear. The paper is also easy to follow.
2. The implementation details about the algorithm are well-explained and the math of the method is also well-written.
3. The numerical section shows experiments with synthetic data and real-world data with multiple dimensions. The paper also compares the proposed method with five other baselines. 1. The experiments only contain up to dimension 62, and the paper does not explain why sampling rare events at this dimension is difficult. How the comparison may look like if we compare the method with traditional sampling methods, like metropolis sampling.
2. The method's speedup and precision improvement are not clear from the languages used in the text. 
3. The experiments in Figure 2 and Figure 3 look unrelated to rare event sampling but show the effectiveness of the method approximating a given distribution. It will be beneficial to get more ideas on what these figures tell us. 1. Does the number of anchors matter in your experiments? 
2. How do you determine the training is complete?
3. For Tables 1 and 2, do you have the measurement of time in seconds? When you say function call, does it always take the same time for different methods? If the numbers include the time of training the neural networks, would the proposed method still be faster than other methods, especially non-ML methods?
4. It would also be useful to see the confidence interval from the 20 estimations. Do you have them?",306,0,10,0.7398,0.0801587302,0.9148631692,55,51.1349,0.1509,iclr,0.0,3,4,3,3,partially factual,3,4,65,neutral,4,neutral,3,moderate,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
198,Reviewer-mUwv,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper presents a new method called iGraphMix for node classification in graph neural networks. The method addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. iGraphMix generates virtual graphs that serve as inputs for GNNs training, leading to better generalization performance compared to training without augmentation. The authors evaluate iGraphMix on several benchmark datasets and show that it outperforms existing state-of-the-art methods. The contributions of this paper include a novel approach to graph augmentation, a comprehensive evaluation of the proposed method, and insights into the effectiveness of virtual graph generation for GNNs training. This paper presents a novel method, iGraphMix, for addressing the challenges of irregularity and alignment in generating virtual nodes and edges for graph neural networks. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear and comprehensive description of the method, including theoretical analysis and experimental validation of its effectiveness. The evaluation is thorough and includes comparisons to existing state-of-the-art methods on several benchmark datasets. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. 

Overall, the paper is well-written and easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. The experimental results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of originality, iGraphMix is a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method builds on existing work in Input Mixup for other domains but is specifically designed for node classification in the graph domain. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. 

In terms of quality, the paper is well-written and well-organized, with clear explanations of the technical details and experimental setup. The authors provide a thorough evaluation of the proposed method, including comparisons to existing state-of-the-art methods on several benchmark datasets. The results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of clarity, the paper is easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. 

In terms of significance, the paper presents a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. Overall, the paper is well-written and presents a novel approach to graph augmentation for node classification in GNNs. However, there are a few weaknesses that could be addressed to improve the paper:

1. Limited analysis of the impact of hyperparameters: The authors do not provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix. It would be useful to see how the performance of iGraphMix varies with different hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient.

2. Lack of ablation study: The authors do not provide an ablation study to analyze the contribution of each component of iGraphMix. It would be useful to see how the performance of iGraphMix varies when different components are removed or modified.

3. Limited discussion of limitations: The authors do not provide a detailed discussion of the limitations of iGraphMix. It would be useful to see a discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate.

4. Lack of analysis of computational complexity: The authors do not provide an analysis of the computational complexity of iGraphMix. It would be useful to see how the computational cost of iGraphMix compares to other graph augmentation methods and how it scales with the size of the graph.

Addressing these weaknesses would strengthen the paper and provide a more comprehensive evaluation of the proposed method. How sensitive is the performance of iGraphMix to the choice of hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient? Can the authors provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix?

Can the authors provide an ablation study to analyze the contribution of each component of iGraphMix? This would help to better understand the importance of each component and how the performance of iGraphMix varies when different components are removed or modified.

What are the limitations of iGraphMix? Can the authors provide a detailed discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate?

Can the authors provide an analysis of the computational complexity of iGraphMix? How does the computational cost of iGraphMix compare to other graph augmentation methods, and how does it scale with the size of the graph?

How does iGraphMix perform on larger and more complex graphs? Can the authors provide an analysis of the scalability of iGraphMix to larger graphs with more nodes and edges?

Can the authors provide a discussion of the potential applications of iGraphMix beyond node classification, such as link prediction or graph classification?

How does iGraphMix perform on graphs with different characteristics, such as sparsity or degree distribution? Can the authors provide an analysis of the robustness of iGraphMix to different graph properties?

Can the authors provide a discussion of the potential limitations of the theoretical analysis presented in the paper? How well does the theoretical analysis capture the behavior of iGraphMix in practice?

Can the authors provide a discussion of the potential ethical implications of using graph augmentation methods like iGraphMix? How can we ensure that these methods are used responsibly and do not perpetuate biases or inequalities in the data?",1055,0,3,0.7009000000000001,0.1379187281,0.9172924757,51,28.6703,0.0948,iclr,0.0,5,5,5,5,factual,3,4,95,polite,5,positive,4,none,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
95,Reviewer-mRm5,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","This paper views graphs as meta-nodes and constructs a super graph, which then enables semi-supervised graph classification learning, akin to semi-supervised node classification learning. Specifically:

1. First, a GNN is used to learn a representation for each graph, serving as the initial node representation of the supergraph,
2. Next, the WL kernel is employed to determine the similarity between graphs, forming the edges of the supergraph,
3. Finally, another GNN is used for semi-supervised learning on the supergraph.

The experiments implied that this method can achieve SOTA or comparable to SOTA results on several datasets. 1. Compared to other methods based on contrastive learning, utilizing a supergraph for semi-supervised learning eliminates the need to construct negative samples, simplifying the whole framework.

2. It achieves SOTA results on smaller datasets and comes close to SOTA on medium-sized datasets. 1. The datasets used for experiments are relatively small, and it seems that the advantages are not as pronounced on larger datasets, necessitating validation on larger datasets.

2. A comparison is needed with the following two papers:

    \[1\]. **Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures**

    \[2\]. **PRODIGY: Enabling In-context Learning Over Graphs**

In paper \[a\], a supergraph is constructed for Few-Shot graph classification, while in paper \[b\], a supergraph is built for In-context few-shot node and *edge classification*. 1. This paper mentions that the two GCNs are optimized jointly, implying that during training, all graphs in the dataset must be inputted into the hardware simultaneously. Does this limit the model's ability to be trained on large-scale datasets?

2. If KDGCN only supports the Transductive setting, while the compared methods MVGRL, SimGRACE, and GLA can support the Inductive setting?

3. If it is the Transductive setting, must the entire dataset be inferred together during inference? Please describe the inference budget, including platform, memory usage, and inference time.

4. Is this paper the first to perform semi-supervised graph classification by constructing a supergraph? The core innovative point of the article needs to be re-emphasized.",333,2,9,0.7496,0.0476851852,0.9002113938,51,36.8953,0.1431,iclr,0.0109890109890109,4,4,4,4,factual,4,4,75,polite,3,neutral,4,low,4,4,4,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
102,Reviewer-wEPT,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This paper proposes LaughingHyena - an improvement to the Hyena model that can perform long-convolutions instead of attentions in transformers to avoid the quadratic scaling issues. One of the issues with the convolution sequence models is that they incur significant cost due to autoregressive inference. To avoid this, this paper seeks to come up with a techinique to have constant memory recurrent inference to increase generation thoroughput. This is achieved using the use of compact linear SSM and weight tying the filters across heads in Hyena architecture. The resulting performance improvements are impressive - 1.5x throughput improvement compared to Hyena. The model also achieves SOTA in the PILE dataset. - The perplexity on small-scale models on Table 1 and Table 2 outperform GPT, Hyena and establishes a new SOTA.
- The peak memory usage is also constant for different sequence lengths in Table 5.4 - The writing is a bit hard to follow.
- The performance of the model is still lacking compared to full transformer baseline such as Pythia in Table 5.3. Can the authors comment on this? Any idea on how much the performance degradation will be on very large scale models and datasets? - What assumptions do you use for the state-space model in Eq 3.1 to yield a good distillation results (d<<L)?
 I think the writing can be improved to provide a simple explanation of the method for readers who don't have a strong understanding of state-space models. Else, the text is hard to follow.",249,0,0,0.7824,0.1728084416,0.9072981477,215,51.1531,0.195,neurips,0.0,4,4,3,4,factual,4,4,85,polite,4,positive,3,low,4,3,4,4,factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,polite,5,positive,4,low,2,2,3,4,partially factual,3,3,65,polite,4,neutral,4,low
153,Glen-Tillotson,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,0,0,0,0,unfactual,0,0,0,neutral,0,neutral,0,extreme,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,3,0,0,factual,1,1,20,polite,1,neutral,0,extreme
70,Sarah-Elizabeth-West,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The abstract results section could be clearer, in particular the sentence starting ‘ Even remembering’. I think it would be useful in the introduction to give the geographical context for your work, and figures about the size of the environmental volunteering sector in that country. I assumed UK, and it seems like the bulk of responses were from the UK, but I note that your survey was completed by people in 11 countries. It also needs some definition of environmental volunteering I think. I guess this includes things like practical conservation, environmental CS surveys, but what about someone delivering leaflets promoting Friends of the Earth activities for example? This example highlights why definition is important. And in your results, you talk about Biodiversity monitoring volunteers – is this your definition of environmental volunteers? Some justification of why PERMA was used as opposed to other multidimensional well-being measures would be useful. Some more info on why managers’ perceptions of their volunteers’ motivations is important is needed, I think this is missing. ‘Worldwide responses’ – how do you know that any difference in responses is due to the factors you are interested in, not due to the fact that they are in a different part of the world? Some justification for including these (relatively small number of responses) would be useful. The results text is very dense, and it is hard for those not very familiar with factor analysis (like me!) to understand what the key parts of the text are. I guess it’s the bottom of page 9 is it? I think some explanatory text at the beginning of results about what factor analysis is would be helpful. The 'External factors and volunteer well-being' section is clearer as you’ve said what the results are and then gone into the detail of how you came to that result, and means that people who are not au fait with statistics (as I guess will be many of your readers) can skip over it. Discussion – how did your volunteers and non volunteers compare to others using your well-being index? Or compared to other well-being indices? This would help to give your results more context.  Some of your sentences are a little long which makes them a bit hard to read, for example, the one starting However, this positive…on page 19.  Should your figures be in the discussion section, or would they be better placed in the results? It breaks the text up a bit too much I feel.",482,0,0,0.7688,0.0870438856,0.9067310691,13,50.36,0.1733,f1000,0.0194174757281553,3,5,4,2,partially factual,4,4,65,polite,4,positive,3,low,5,4,4,5,factual,5,5,88,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,3,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,3,low
126,Reviewer-qyRc,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","# Problem Statement
The paper addresses the challenge of neural policy learning methods struggling in long-horizon tasks, particularly in open-ended environments with multi-modal observations, such as the game NetHack. It was observed that symbolic agents significantly outperformed neural approaches in the NeurIPS 2021 NetHack Challenge.

# Main Contribution
The paper's main contribution is an extensive study on neural policy learning for NetHack. The authors analyzed the winning symbolic agent and extended its codebase to generate one of the largest available demonstration datasets. They examined the advantages of an action hierarchy, enhancements in neural architecture, and the integration of reinforcement learning with imitation learning. Their investigations resulted in a state-of-the-art neural agent that surpassed previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, they also demonstrated that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.

# Methodology and Experiments

## The Hierarchical HiHack Dataset
The authors create the HiHack dataset, which is a hierarchically-informed version of the NetHack Learning Dataset (NLD-AA), containing 3 billion recorded game transitions from over a hundred thousand games played by the AutoAscend agent.

## Hierarchical Behavioral Cloning
The authors extend the ChaoticDwarvenGPT5 (CDGPT5) model, a top-performing open-source neural model for NetHack, by introducing a hierarchical decoding module. The model consists of three separate encoders for different types of observations and an LSTM core module. The hierarchical extension replaces the linear decoder of the CDGPT5 model with a hierarchical decoder that predicts the strategy label and selects the appropriate low-level MLP for action prediction. The hierarchical LSTM policy and the baseline non-hierarchical LSTM CDGPT5 policy are trained using a simple cross-entropy loss. The results show that the introduction of hierarchy significantly improves the performance of LSTM policies trained with behavioral cloning, yielding a 40% gain over the baseline in mean NLE score and a 50% improvement in median score across seeds. The authors confirm that this improvement is due to hierarchy and not simply a result of the increased parameter count of the hierarchical LSTM policy.

## Architecture and Data Scaling
The authors explored scaling as a potential solution to improve the performance of the model, which was significantly behind the symbolic policy used to generate the HiHack demonstrations. They developed a novel base policy architecture for NetHack that introduces a Transformer module into the previous CDGPT5-based architecture. They also conducted data scaling experiments using subsets of the HiHack dataset to examine the relationship between dataset size and the test-time performance of BC policies. The results showed that both the non-hierarchical and hierarchical variants of the combined transformer-LSTM policy architecture yielded gains, but the larger model performed worse than the smaller one due to overfitting. This suggested that scaling of model capacity alone would not be sufficient to close the neural-symbolic gap. Additionally, brute force scaling of the dataset alone could not viably close the gap to symbolic methods.

## Combining Imitation with Reinforcement Learning
The authors explored combining imitation learning with reinforcement learning (RL) to bridge the performance gap with AutoAscend. They used a combination of behavioral cloning (BC) and asynchronous proximal policy optimization (APPO) for training. The results showed that RL fine-tuning significantly improved the performance of all models. The best-performing approach was APPO + BC using the hierarchical LSTM model, which achieved a new state-of-the-art for neural policies on NLE, surpassing the previous best result by 48% in mean NLE score and 25% in median NLE score. The Transformer-LSTM models performed worse due to their slower training speed and the fixed training time budget. The authors also observed that fine-tuning with RL improved the error-correction capability of models across all model classes compared to their purely offline counterparts. # Originality
The problem is interesting and the approaches are insightful.

# Quality
The analysis and experiments are comprehensive.

# Clarity
The article is overall well written and clear. 1. The current focus of the study is quite narrow, being primarily centered on the application of imitation learning for NetHack, limiting its influence. In the context of mastering the game, while this approach is interesting, it is unlikely to exceed the performance of experts that generate demonstrations, not to mention that the experts are already algorithms that can scale well. Furthermore, NetHack, despite being an excellent game, is somewhat niche and its real-world implications are relatively minimal. The techniques proposed in this study are specifically tailored for this game, which limits their potential for inspiring more universally applicable methods that could have a broader impact.
  - The availability of hierarchical labels is a strong assumption that does not often hold, which further limits the applicability of the proposed methods.

2. Even just for bridging the performance gap between neural models and AutoAscend, there is no promising direction revealed by the work as the various augmenting components seem to contradict each other. 1. When introducing Transformer to augment the capacity of the neural model, why did authors choose the architecture as shown in the article? Specifically, transformers are best known for their NLP and CV capacity, which could make them good replacement for the CNN and MLP encoders.
2. Why do the authors enforce the 48 hour training time cap instead of training all models till convergence? Given that this study does not appear to prioritize data efficiency or training efficiency, the necessity of such a computational time constraint is unclear. It would be beneficial to understand the rationale behind this choice, as it may not directly align with the study's primary objectives. The authors note that possible avenues for future exploration include: (a) methods for increasing the Transformer context length to give the agent a longer memory to aid exploration; (b) addressing the multi-modal nature of the demonstration data (i.e. quite different trajectories can lead to the same reward), which is a potential confounder for BC methods. Some forms of distributional BC (e.g. GAIL, BeT) could help alleviate this issue.

The aforementioned two points do not address the limitations raised in the ""Weakness"" section.",1010,0,4,0.7969,0.0384225531,0.9721859097,232,26.3989,0.0751,neurips,0.0,4,4,5,5,factual,4,4,90,polite,4,neutral,4,none,4,5,5,5,factual,5,5,88,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,5,5,4,factual,5,5,90,polite,5,neutral,5,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
126,Reviewer-PtAe,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","The paper improves the existing solutions in the NetHack Learning Environment (NLE). This is done by taking earlier solutions from a competition around NLE, collecting more data with the best available (symbolic) agent, and using that data to improve a neural only solution. The paper provides experiments with imitation learning (with or without RL tuning), larger models, hierarchical memory setup (LSTM + Transformers) as hierarchical behavioural cloning setup, using labels of the newly collected dataset. While there are improvements, it is still below the demonstrator results, which is then studied by scaling the model sizes and amount data collected. Paper concludes by providing the state of the art results in the task, but also noting that scaling alone is not enough to reach the expert demonstrator level (symbolic agent). - Provides more detailed dataset than the previous works (with hierarchical action labels)
- Sets an interesting premise/task for trying to reach the demonstrators' (AutoHack agent) performance with neural solutions.
- Different ablations to try to answer questions (data/model scaling, model architecture with hierarchy)
- Proposed hierarchical approach to imitate the demonstrator agent. While I enjoyed reading the paper, overall I think the results are interesting or applicable to most of the NeurIPS audience, even in the limited scope. The paper presents many results and provides some explanations for them, but does not verify these explanations with further experiments. I think proper answers to these issues would be insightful to many, and others could then use these insights in their work (e.g., where the trained agent failed to imitate the demonstrator? What was the cause of poorer performance? Why did bigger model perform worse?). Creating such insight in one environment would be sufficient, as by focusing on a single environment, you can create very specific scenarios to tease out these answers. 

- Limited scope of the work: experiments done in a single environment. Most of the paper is framed in a way that this is not a huge issue (e.g., ablations), but proposing new method just for playing NLE has limited impact. If a new method is proposed to generally improve RL/IL performance, it should be tested at least in two distinct environments.
- Limited improvement in the context of SOTA solutions: 2x over the baseline used in the paper with RL and proposed architecture included, but other neural agents in the NetHack Challenge had higher score. To be interesting in terms of performance, it should at least outperform the NetHack Challenge Neural solutions.
- Proposed method is limited in novelty, as evident by the previous work listed in the paper. If the hierarchical BC figured out the hierarchy automatically (or, if it was an emergent behaviour of the model), that would be more interesting.
- Paper outlines some assumptions on why things failed (e.g., ""model overfitted"" or ""learned to self-correct""), but these claims were not verified with results. The paper would be much stronger if you can give solid, verified answer that indeed, overfitting was to blame or that RL trained the model to ""self-correct"". Questions:
1) In multiple occasions paper says that the lower performance of bigger model is due to overfitting (e.g., line 229). However there are no results/experiments to show that this indeed was the case. A simple way to find this out is to do train-validation (or even train/validation/test) split, and testing on held out data as training progresses.
2) Regarding data scaling experiments: did you change any other settings of the training setup when increasing data amount? Previous work has demonstrated that the optimal model size and/or training compute depends on the amount of data (Hoffmann et al. 2020).
3) Regarding model scaling experiments: I assume only the number of layers in the transformer was changed? The bottleneck of the network may be elsewhere, e.g., one of the input layers or output layers. I would recommend scaling the whole network, similar to what OpenAI VPT work did, where ResNet blocks were ""widened"" in terms of filters, as well as increasing transformer size (Baker et al. 2022). Also, Hoffmann et al. (2020) changed number of layers, number of attention heads and transformer dimensionality when scaling models. This might be something you want to try.
4) Instead of LSTM + Transformer model, did you experiment with transformer model only? E.g., akin to VPT work (Baker et al. 2022), embed all inputs into one vector, stack vectors over timesteps, apply causal transformer, and predict actions from the transformer outputs. This type of model might scale better, as it reduces the amount of components that might interfere.

#### Comments (not questions)
- Fig1 right: weird scale. Any chance to get more points?
- Line 205: grammar error at the start of the line
- Explain/rename ""Dlvl"" and why ""Turns"" is good metric
- Figure 3: ""LSTM + XXL Dec"" is bit confusing naming, since ""decoder"" is not commonly used term in the paper. I'd recommend using something like ""LSTM (bigger)"" to simply reflect that it is the LSTM baseline but with bigger network
- Figure 3 (and others): add explanation to caption what is the error bar of the bar plots. Is it standard deviation or standard error (or something else)?
- Table 2 caption: starts with weird ""\[V4\]""

#### References

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas et al. ""Training compute-optimal large language models."" arXiv preprint arXiv:2203.15556 (2022).
- Baker, Bowen, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. ""Video pretraining (vpt): Learning to act by watching unlabeled online videos."" Advances in Neural Information Processing Systems 35 (2022): 24639-24654. No explicit sections for limitations or broader/societal impact was given. Authors bring up the future work ideas in the conclusion. While I think the work does not require societal impact section (no immediate impact), I urge authors still think through of any cases where the work or the insights could impact others. Or alternatively, what impact would _not_ including some results do (e.g., skipping some analysis).


## Rebuttal acknowledgement

I have read authors' rebuttal which did address my concerns, and I increased my rating from 4 to 7 to signal my vote to accept this paper (change was done before discussion period closed).",1043,3,5,0.8198000000000001,0.0860855389,0.8613269329000001,232,48.5199,0.1278,neurips,0.0,5,4,5,5,factual,4,4,95,neutral,4,positive,5,none,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
150,Reviewer-MvTD,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","The paper proposes to replace the optimal transport formulation in DDGAN with Unbalanced optimal transport formulation. 1. The paper proposes a way to address the noisy dataset generative problem with the unbalanced optimal transport.

2. The paper is well-organized and well written. 1. The novelty. The method replaces the extsing optimal transport loss with the unbalanced optimal transport.  The technical novelty is limited.  Or authors may consider adding more in-depth analysis about the unbalanced optimal transport in diffusion model. section 4.3.1 is a good example.  

2. The noisy datasets are synthetic. Authors combines digits and CIFAR dataset, which are usually unlikely to happen in the real world. It would be better if authors could add experiments on some more real-world noisy datasets.

3. Some noisy-learning baselines need to be included. For instance, can we apply some noisy sample detection (a simplest way would be clustering and I think it should be easy to cluster the cifar images and digit image into two different groups.) before learning the datasets instead of using unbalanced optimal transport?  

Minor:

1. It would be better to introduce motivation to learning the generative model under noisy samples. as above",193,0,8,0.7317,0.2086080586,0.8585108519,48,39.8612,0.0866,iclr,0.0113636363636363,2,4,3,3,unfactual,3,3,65,neutral,3,positive,3,low,5,4,4,5,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,3,4,partially factual,4,3,75,polite,5,neutral,4,low
63,Reviewer-Qg2K,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the learning dynamics in the special case of two-layer ReLU neural network with small initialization. The results of this paper extend prior results from infinitesimal initialization to finitely small initialization.

The relation and difference with prior results are clearly presented. The paper is clearly written and easy to follow. The figures are helpful for understanding the argument. The setting of the neural network is unconventional. It requires the second layer weights to depend on the first layer weights in a way as shown in Eq.(3), instead of independently initialized.  This setting is used neither in practice nor in most theoretical analysis. According to the analysis,  I doubt that the results of this paper hold without this restriction on the second layer weights. 

> The paper has some discussion on this setting. However, it does not justify the validity of this setting. That it is commonly assumed in other papers does not directly justify. I would like to see some analysis, or at least some intuition, on why the results would hold on the natural setting.

The assumption on the data (Assumption 1) is strong, as it is not met by almost all real data. 

The significance of the results is limited, as it is an extension of similar results from $\epsilon \to 0$ to the finite but small $\epsilon$. In Eq.(5), why the R.H.S. is independent of the network output $f(x_i)$? Or, why there is no such term $y_i-f(x_i)$ which usually appears in the expression of gradients.",250,0,1,0.7221000000000001,0.0232363316,0.8568468094,49,53.8922,0.11,iclr,0.0,1,4,2,1,unfactual,4,1,55,neutral,3,negative,4,extreme,5,5,4,5,partially factual,5,5,92,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,3,partially factual,3,4,75,polite,5,neutral,4,low
63,Reviewer-5S56,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper improves on previous theoretical analysis of the early alignement phase of the neurons of a shllow neural network initialized with small weights. This allows them to prove quantitative bounds in terms of the initialization scale, time, number of neurons and number of datapoints required to guarantee convergence. The paper is easy to follow and explains well the previous issues and how they are solved. It is nice that the results apply to deterministic initialization of the weights, and do not require a random initialization (though they can of course be applied to this case). The assumption of positively correlated labels and balancedness are very strong, and usually are not true in practice. The description of Assumption 2 before the statement of the assumption does not match the statement of the assumption.",133,0,0,0.7155,0.0346338384,0.8429466486,49,43.7599,0.0999,iclr,0.0096153846153845,0,4,1,0,unfactual,3,1,20,polite,2,positive,0,extreme,2,5,4,4,partially factual,5,5,80,polite,5,neutral,5,none,2.0,5.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,3,low
63,Reviewer-Qhf5,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the problem of training a two-layer ReLU network classifier via gradient flow under small initialization. Training dataset assumes well-separated input vectors. Analysis of the neurons’ directional dynamics establishes an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on the MNIST dataset validate the theoretical findings. + Interesting alignment behavior of the gradient flow for training two-layer ReLU networks under small initialization and separable data
+ Rate analysis for the after-alignment-phase convergence - The results only hold for correlated data. 
- Only ReLU activation functions are analyzed.
- Some of the results have been previously known or observed, e.g., the solution of (stochastic) gradient flow finds in training two-layer ReLU networks on separable data is (almost) low-rank.
- How small the initialization shall be to ensure the two-phase convergence is not qualitiatively discussed? 1) Do the results/analysis extend to other activation functions? 
2) How small the initialization shall be to ensure the two-phase convergence? In general, since the training is nonconvex even with correlated/separable data due to the nonlinear relu activation function in the los. SGD/GD converges to a local minimum and indeed, this has been observed and numerically validated in the literature; see also \[R1\] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR. \[R2\] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP. In \[R1\], SDG for two-layer ReLU networks under separable data converges to local minimum; yet for leaky ReLU networks, it finds global minimum. In \[R2\], it also shows that plain SGD on ReLU networks using separable data converges to local minimum numerically. Yet, a bit modification on the SGD helps SGD converge to a global minimum but with random initialization. It would be great if a comparison can be made between the approach in \[R2\] and the small-initialization SGD for training two-layer ReLU networks on e.g., MINIST. Moreover, is there any transition for such initialization value to go from the two-phase to single-phase gradient flow convergence to local minimum?
3) It would be great if more numerical tests are provided to demonstrate the two-phase convergence and provide the plots. 
4) If the second-layer weights are initialized not in a balanced manner (although not initialized according to (3)), I understand it would also work and guess that the imbalance between positive v_j and negative v_j values only influences the time it takes for the two-phases. More balanced initalization, faster convergence. It would be interesting to numerically validate if this is the case. 
5) There are some grammar issues and typos. Please correct.",445,0,8,0.7757000000000001,0.0692361402,0.9179714322,69,34.7577,0.5423,iclr,0.0106382978723403,1,3,3,2,unfactual,3,1,66,polite,4,positive,3,extreme,5,4,4,5,factual,5,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,3,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
64,Wing-Yin-Mo,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ​​1. In the abstract, please revise the Methods from future tense to past tense, as the authors have already finished the experiment.  2. Professional editing is needed to improve the overall quality of the manuscript.  3. In the introduction, I think it will be more meaningful if the following information is provided: Production volume of the species, exported vs. domestic consumption.  Natural diet composition: Is the fish carnivorous fish? What are the natural preys?  4. One of the major issues of the manuscript is the feed formulation (Methods). More explanation should be provided to polish the manuscript. How did the authors produce the feed? Home-made or produced by manufacturers?  Where did the authors get the ingredients? Home-made or purchased?  Do the fish feed pellet float or sink? Does the diet fit the preference of fish?  The design seems like the authors were testing the suitability of blood meal (I prefer “meal” instead of “flour”) rather than testing the effects of protein levels. I did a quick check. There are many studies to replace fishmeal with blood meal. For omnivorous fish, 50% replacement is suggested (Kirimi et al. (20161)). For carnivorous species, such as Murray cod, partial replacement is possible (Abery et al. (20022)). Would it be true that high levels of blood meal affect growth of fish?  There are too many variables in the diets. To my best knowledge, high levels of carbohydrate is harmful to carnivorous fish. The author might check the paper published by Stone et. al. (20033), for more information about the effects of carbohydrates on different fish species. Thus, it would be possible that fed with Diet A and B resulted in inferior growth is related to the high levels of carbohydrate.  The diets were tested using juveniles and that should be reflected on the title.  5. The authors suggested fish mortality of groups D and E were related to feces accumulation and poisoning. However, the authors didn’t mention the depth of the experimental pond. In the pond used in the experiment, the authors suggested nets were used. Could the fish feed accumulate inside the cage and kill the fish because of that? Did the fish show any sign of intoxication? Also, is that pond equipped with any aerators? Is there any water treatment facility? 6. Amino acids and proximate compositions of the diets: The authors calculated the protein content of fish feeds. Did the authors measure the exact protein concentration? Would it be possible that the high level of blood meal resulted in inferior growth, because of insufficient amino acid(s)?  In addition to protein, other proximate compositions i.e. lipid, ash, moisture and carbohydrate contents should also be measured and presented.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",589,0,9,0.7455,0.1621666667,0.8303694725,150,49.72,0.2777,f1000,0.0,5,4,4,5,factual,4,4,85,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,65,polite,5,negative,5,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,5,4,5,5,factual,5,5,90,neutral,5,negative,5,low,5,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
85,Lisa-Hanson,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  *Very well written article. Statistics and references are up to date and appropriate. Tables are very effective. A few suggestions for clarity. *""more crowded pregnancy vaccine space"" is unclear. *In the GBS3 trial description, more clarity is needed as to why participants in the routine testing arms receive either rapid PCR IP (versus 35-37 weeks). A reference here about the sensitivity and utiliy of rapid IP testing is needed-as this is not a usual strategy in culture-based EOGBS prevention approach recommended by the CDC and now ACOG (2019). *Table 3. The points about midwives having hesitancy to offer vaccines was interesting, as this is not the case in the USA. *Table 4 is redundant of the text on Potential Phase IV study designs.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",270,1,2,0.774,0.2182758621,0.8594522476,98,37.4,0.0999,f1000,0.0206185567010309,5,5,5,5,factual,4,4,86,neutral,5,neutral,5,low,4,5,4,4,factual,5,5,85,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
57,Reviewer-vD9G,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The paper tries to solve the problem that sampled auxiliary informative outliers may not be sufficient and diverse enough to recover the data boundary in the OOD detection setting. To achieve this, the authors propose a new learning objective with information extrapolation, where second term expands the surrogate OOD distributions towards a more diversified one. The authors have provided theoretical analysis to show that for Gaussian mixture model and binary classification problem, DivOE can extrapolate the
outlier boundary towards ID data. In the experiments, the authors adopt image ID datasets such as CIFAR10, CIFAR100, and results show that DivOE achieves better performances over several evaluation metrics. S1. The paper targets an important research problem within the OOD detection research community, and proposes a new auxiliary outlier generation and learning objective to target the research problem that surrogate auxiliary outliers are not sufficient and diverse enough.

S2. The learning objective is simple but adaptable to different post-hoc scoring functions, augmentation techniques and sampling techniques to auxiliary outliers.

S3. The paper has provided a solid theoretical analysis that shows the effectiveness of DivOE within a simplified binary classification setting. The result of Theorem 3.1 is consistent with the authors' explanations and observations in the previous sections.

S4. The paper provides good experiment comparison to exciting methods. They have used several evaluation metrics to demonstrate the effectiveness of the approach.

S5. The writing is very clear and presentation is easy to understand. W1. In Theorem 3.1, The hypothesis class of the binary classification problem is overly simplified to only linear decision boundary. It would be nicer if the authors can provide theoretical results that generalize to more complex hypothesis class.

W2. As Figure 4 shows, the extrapolation ratio and diversified strength are two variables that affect the OOD detection results. If extrapolation ratio is between 0.9~1.0, the performance may degrade and become worse than OE. However, the authors have not mentioned any general guidance to tune those two variables, especially for unseen OOD detection problems.

W3. The authors should consider to include a table comparing the number of outliers generated by DivOE and other benchmark methods in order to achieve similar detection performance scores during experiments.

W4. The authors should also include bold numbers for the best performing algorithms for Table 11, Table 12 and Table 13. Q1. The experiments are only performed on two simple image classification tasks with CIFAR10 and CIFAR100. The reviewer is wondering whether the authors have used other image datasets or tabular datasets for evaluation. 

Q2. Is there a general guideline in how to choose the extrapolation ratio, augmentation techniques, diversified strength, OOD scores when using DivOE on unseen OOD detection task? It seems that the four factors play an important role in detection performance.

Q3. How would $\eta$ and $\alpha$ play an effect in the training steps of DivOE? 

Q4. Could different data augmentation techniques be used in combination with the inner-maximization function? Would the augmentation function boost up the performance? The authors have adequately addressed the limitations and potential negative societal impact of their work, as listed in the NeurIPS checklist. The reviewer would appreciate if the authors can address the question and weakness section.",529,0,13,0.7985,0.1007839262,0.9079990983,230,29.1727,0.0751,neurips,0.0105263157894737,5,5,5,5,factual,4,4,95,polite,5,positive,5,none,4,4,4,4,3,4,4,75,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
113,Reviewer-QTwQ,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings. Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see. Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6). All weaknesses listed above should be well addressed to improve the paper. The authors have shown some limitations of the proposd method, but more should consider other that the method itself.",505,0,5,0.7445,-0.0014520202,0.8663344979000001,215,41.2356,0.2383,neurips,0.0,4,4,4,4,factual,3,3,75,neutral,3,negative,4,low,5,4,4,5,5,4,4,85,polite,5,neutral,5,moderate,1.0,4.0,4.0,3.0,partially factual,2.0,2.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,5,negative,4,low,4,4,4,4,partially factual,3,3,78,neutral,5,negative,4,low
113,Reviewer-S9DQ,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases. S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section. W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as \[1, 2, ..\] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like \[3\]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
\[1\] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
\[2\] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
\[3\]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022 Please refer the weaknesses section for the related questions that need more clarification. A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.",695,4,0,0.8047000000000001,0.1297619048,0.9349661469,215,17.6354,0.1939,neurips,0.0,4,4,5,5,partially factual,3,4,80,polite,4,negative,5,low,5,4,4,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
42,Reviewer-GAW7,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper presents a convergence analysis of Actor Critic method with multiply layer networks.  The convergence analysis of AC with multi-layer networks is important, as AC methods with neural networks plays the core role of the success of DRL.  The paper is difficult to follow and the writing can be significantly improved. (More in Questions) I would ask the authors to clarify the following questions, which I believe can significantly improve the paper if addressed:
1. What are the differences for AC methods with single hidden layer networks and multiple layer networks, especially for proving the convergence? In the current version, these differences are not well explained. To improve the clarity and strengthen the paper's contribution, the authors should provide a more detailed comparison of these two methods, highlighting the specific challenges that arise when proving the convergence for each architecture. This will enable readers to better understand the significance of the proposed approach in tackling the convergence problem and its relevance in the context of existing research.
2. The proof of convergence presented in the paper is challenging to follow, and its integration within the main context is inadequate. To improve the overall readability and accessibility of the paper, I recommend including a sketch of the proof in the main body, i.e., Section 3. This will allow readers, especially those unfamiliar with previous work on the convergence analysis of AC methods, to grasp the high-level idea behind the proof. Providing a concise outline of the proof in the main paper will enhance the paper's accessibility and make it more appealing to a broader audience.
3. Expanding on the suggestions mentioned in point 1, once the key differences between AC methods with single hidden layer networks and multiple layer networks are clearly stated, it would greatly benefit the readers if the authors could elaborate on the key techniques utilized to address these differences and overcome the associated challenges (Section 4 in the current version is far from satisfactory). By doing so, the authors can provide valuable insights into the novelty and contributions of the proposed approach. Understanding the techniques employed to tackle specific obstacles will enable the readers to evaluate the significance of the paper more effectively and appreciate its potential impact on the field.


-------------------
After rebuttal, as the authors address most of my concerns, I would increase my score to 6. I would still suggest the authors to carefully revise the paper to make it more readable if accepted.  No. The limitation is not discussed. As there are many assumptions, I suggest that the authors should discuss whether these assumptions can be relaxed, as well as the cases where these assumptions cannot hold. ",445,0,3,0.7604000000000001,0.1671732523,0.8855220079,215,35.8282,0.1507,neurips,0.0283018867924528,2,4,3,3,factual,4,3,65,neutral,4,neutral,4,low,5,4,4,5,factual,4,5,85,polite,5,negative,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,5,3,4,5,factual,4,4,85,polite,5,neutral,4,low,5,3,4,5,factual,4,4,85,polite,5,neutral,3,low
108,Reviewer-euBm,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper presents a new approach called Merlin for goal-conditioned reinforcement learning, which is inspired from diffusion models. The authors introduce a new approach to construct a ""forward process"" by stitching trajectories if there are two states from different trajectories are close. The forward process outputs a augmented dataset, and authors propose to learn the corresponding backward process. They validate their method on offline goal-reaching tasks and show competitive results with state-of-the-art methods. Overall, the paper proposes a new class of goal-conditioned RL algorithms, 1. I like the high level idea of this work which is inspired from diffusion models: constructing a simple forward process to enlarge the training set by injecting noise, and learning the reverse process. Specifically, they use the Nearest-neighbor Trajectory Stitching to generate more data. The algorithm is somewhat novel and might work well on some tasks. 

2. Competitive results: The authors validate their approach on offline goal-reaching tasks and show competitive results with state-of-the-art methods. This demonstrates the effectiveness of their approach and its potential for real-world applications. 1. Weak theoretical justification: diffusion models enjoy strong theoretical foundations, the forward and the backward process are proven to share the same marginal distribution. However, it is not clear to me whether the backward process of  Nearest-neighbor Trajectory Stitching still has similar theoretical guarantees.

2. Limited range of applications: Nearest-neighbor Trajectory Stitching seems to be designed for some specific applications. The generalizability remains unclear.

3. Misleading title: Diffusion models have a relatively clear definition now. While there are ""forward"" and ""backward"" processes in this paper, this algorithm does not fall into the class of diffusion models. See weaknesses.",271,0,5,0.8161,0.0717140796,0.9331908226,47,30.1209,0.1695,iclr,0.0113636363636363,2,5,3,3,factual,4,4,75,polite,4,neutral,4,none,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
108,Reviewer-5ke8,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper models the offline GCRL problem in offline data in a diffusion process-like paradigm called merlin. The authors consider three choices for the noise model to replace Gaussian noise in diffusion including reverse play from the buffer, reverse dynamics model, and a novel non-parametric trajectory stitching. This is an improved behavioural cloning paradigm without the need to learn an additional value function, which achieves excellent results in offline control tasks. 1.Novel perspective of framing goal-reaching as a diffusion process. 
2.Trajectory stitching technique seems useful for generating diverse state-goal pairs from offline data.
3.Strong empirical results on offline goal-reaching tasks compared to prior methods. 1.Although the paper seems to describe a feasible diffusion-like process to model the GCRL problem, I think merlin is essentially a variant of constrained GCSL. From this perspective, merlin has only limited novelty. Start with the cleanest method, merlin build policy upon $s, g, h$ instead of $s, g$ by GCSL. Although the merlin shows better results in the motivation example, I think it's because of the inclusion of a more stable time guide.
2.I observe that Merlin-NP and Merlin-P show better results in the experiments, but they can be considered as GCSL + temporal constraints + reverse dynamics model (Wang et al.) + trajectory stitching (a commonly used data augmentation method in OfflineRL). These other components can be easily combined with the universal GCRL approach, so the performance gains are no surprise. 
3.The approach seems sensitive to hyperparameters like time horizon and hindsight ratio. I'm not sure that good performance comes from hyperparameter tuning. 1.What metric and distance threshold works best for the trajectory stitching? Is there a principled way to set this?
2.In appendix table 5, I have observed that there is not much difference in success rate between Merlin and DQL, GCSL and other methods, whereas there is a bigger difference using reward metric, why is that? Success rate should be a common metric for evaluating a GCRL algorithm.

Overall this paper proposes interesting ideas for offline goal-conditioned RL as diffusion process. The empirical results are strong but there are some open questions (see above weakness and questions). Addressing some of the weaknesses and questions raised would strengthen the paper further. I think the central problem is that the article overclaimed the design of the approach to solving the GCRL problem by a diffusion process and I vote reject for current version.",399,0,0,0.8320000000000001,0.1665223665,0.9278346896,47,43.6593,0.11,iclr,0.0114942528735632,4,4,5,5,factual,5,4,85,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,75,polite,4,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,3,3,4,3,factual,3,3,65,neutral,4,negative,4,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
52,Reviewer-qzzG,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This work presents a framework, called Decompose Novel Into Known parts (DNIK), that addresses the challenge of 3D Novel Class Discovery (NCD) – identifying new classes from an unlabeled dataset using the knowledge of known classes. Current methods, heavily biased towards known classes, struggle to generalize to novel classes. By leveraging more generalizable geometric parts across different classes, DNIK mitigates this issue. It constructs a part concept bank encoding rich geometric patterns from known classes, which is used to represent novel 3D shapes as part concept compositions, thus facilitating cross-category generalization. DNIK also leverages part-wise spatial relations for improved recognition. The method has been tested through three 3D NCD tasks, consistently outperforming state-of-the-art baselines.


---- after rebuttal ----

As the author's rebuttal resolved some of concerns, I raised my score to 5. However, I still feel the studied task is a bit simple, and also there are several spaces to improve for the current manuscript. I will not fight for its acceptance.  The studied direction is important as we need to understand parts well to play with 3D objects generalizable. This paper takes a step towards open 3D object recognition via part understanding. Overall, the components used in the proposed framework are sound and reasonable. The paper is easy to follow. 


Extensive results shown in Table 1 & 2 demonstrate the strength of the proposed method. The proposed DNIK generally achieved state-of-the-art performance. Some detailed ablation studies are included in Table 4. The cross-domain task is interesting to see the transfer performance.  Utilizing the part are sharable across different 3D object categories are studied in the previous literature \[1,2\]. In those paper, they exploited ""harder"" task, such as segmentation. As the proposed framework can address novel class classification via known part concepts. Can the framework be extended to ground where is those known parts in novel object? Or other applications beyond object recognition?

\[1\] Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories
\[2\] 3D Compositional Zero-Shot Learning with DeCompositional Consensus

How the framework handle two different object categories with limited shared parts, such as airplane and chair? Will the framework train on multiple object categories benefit novel object discovery? It would be good to include some failure cases to analyze and provide readers a sense for the limitation of the proposed framework. 

L46~47 said the framework can help use part relation features. Can the framework be extended to discovery part relationship?  Please address the concerns raised above.  Line 319~320 analyzed one minor limitation. I feel there are potential more:

1. if the part are not sharable between different categories, such as lamp -> chair, table -> faucet, can the framework still handle?

2. the only shown application is recognition which limits the practical use of the proposed framework.",463,3,1,0.8174,0.100393028,0.9493124485,220,43.6879,0.1933,neurips,0.0,2,4,3,2,partially factual,2,2,50,neutral,3,neutral,3,low,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low
22,Reviewer-Qwcz,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper proposes CO2, a new approach that enables efficient distributed training of large language models on clusters with limited bandwidth. CO2 introduces local-updating and asynchronous communication to the distributed data-parallel training, allowing for full overlap of communication with computation. The approach achieves 100% scalability even on clusters with limited communication bandwidth. The paper also introduces staleness gap penalty and outer momentum clipping techniques to improve convergence and training stability. The proposed approach is validated through extensive experiments on computer vision and natural language processing tasks as well. + The paper is well-written and comprehensible.
+ The code is available in this work.
+ The utilization of local updating and asynchronous communication makes a full overlap of computation and communication. 
+ The paper provides enough theoretical explainability and empirical validation. 
+ The experimental results are sound and promising. I do not have much to comment on the weakness, as this work goes beyond my acceptance threshold. How many runs for each task? I understand that training a Language Learning Model from scratch can be quite costly. However, conducting the experiment only once may not yield persuasive results.",187,0,0,0.8098000000000001,0.1653896104,0.9581924677,51,23.0455,0.145,iclr,0.0,1,4,3,1,partially factual,3,2,29,polite,3,positive,2,high,3,5,4,3,partially factual,4,4,85,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,3,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
22,Reviewer-m91N,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","To address the communication problem in large-scale distributed training of deep neural networks, the paper proposes a combination of local-SGD and asynchronous communication to derive a new distributed training algorithm named CO2. In CO2, two novel approaches are developed to ensure that CO2 aligns the convergence performance with conventional distributed data-parallel algorithms. Experiments are conducted on a 64-GPU testbed, showing that CO2 outperforms existing methods significantly. The studied problem is timely and important. The paper is also well-written. - Propose a new distributed training algorithm, CO2, using local updates and asynchronous communication to alleviate the communication problem in conventional synchronous data-parallel distributed training. 
- New tricks to address the convergence problem in stale gradients.
- Comphesive experiments to show the effectiveness of CO2. - Some stale parallel algorithms (e.g., SSP \[ref1\]), whose key ideas are quite similar to CO2, were not included in the discussion and comparison. The survey paper \[ref2\] may help find SSP-like methods for comparison.
- It seems that 100% scaling efficiency is over-claimed. The scaling efficiency highly depends on $\tau$. Higher $\tau$ has better scaling efficiency but has worse convergence performance. Thus, achieving 100% scaling efficiency with a $\tau>1$ while sacrificing the convergence performance cannot conclude the algorithm has true 100% scaling efficiency.

\[ref1\] More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server, NeurIPS 2013.
\[ref2\] Communication-efficient distributed deep learning: A comprehensive survey, arXiv 2020. - How about comparing with SSP-like algorithms in terms of theoretical convergence bound and empirical scaling efficiency? 
- How $\tau$ is set in Table 1?
- How about the end-to-end training performance (i.e., time to accuracy)?
- How to choose $\tau$ in a new distributed GPU cluster?",278,0,0,0.8098000000000001,0.0605264378,0.8219593763,51,24.2808,0.0751,iclr,0.0,4,4,4,4,factual,3,4,80,polite,4,neutral,4,low,5,4,4,5,5,5,5,85,5,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
12,Reviewer-7M7p,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper presents a new generative model called Composable Diffusion (CoDi). This model is capable of generating any combination of output modalities from any combination of input modalities, including language, image, video, or audio. Unlike other models that are limited to a subset of modalities like text or image, CoDi can generate multiple modalities in parallel.

The authors have designed CoDi to align modalities in both the input and output space. This allows the model to condition on any input combination and generate any group of modalities, even if they are not present in the training data.

A key feature of CoDi is its novel composable generation strategy. This involves building a shared multimodal space by bridging alignment in the diffusion process. This feature enables the synchronized generation of intertwined modalities, such as temporally aligned video and audio.

The paper reports that CoDi achieves strong joint-modality generation quality. It either outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. 1. Originality: The paper introduces Composable Diffusion (CoDi), a new model in multimodal generation. This model is designed to process and generate modalities across text, image, video, and audio simultaneously. This is a novel contribution as it enables the generation of various output modalities from different combinations of input modalities.

2. Quality: The authors have conducted extensive experiments to demonstrate the capabilities of CoDi. The results show that CoDi can generate single or multiple modalities from a wide range of inputs. The model's performance is competitive with state-of-the-art models in tasks such as image and video generation, video captioning, and image synthesis from multiple input modalities.

3. Clarity: The paper is well-structured and provides clear explanations of the model's architecture and its generation strategy. The use of figures and tables helps to understand the model's capabilities and performance.

4. Significance: This work represents a step towards more comprehensive human-computer interactions by enabling the generation of multiple modalities in parallel. CoDi has potential applications in various areas, from content creation to human-computer interaction. The authors also provide a basis for future research in generative artificial intelligence.

In summary, the paper presents a significant and original contribution to the field of multimodal generation, demonstrating high-quality research and clear presentation. The paper presents a novel approach to multimodal generation, but there are several areas where it could be improved:

1. Evaluation Metrics: The evaluation of the model's performance is primarily based on quantitative metrics such as Frechet Inception Distance (FID) and CLIPSIM. These metrics, while useful, may not fully capture the perceptual quality or coherence of the generated outputs. Incorporating user studies or other qualitative evaluations could provide a more comprehensive understanding of the model's performance.

2. Quality of Generated Results: The quality of the generated results could be improved. The generated videos are relatively short, the quality of the images is perceptually low, and the generated text is often short and discontinuous. These factors could limit the practical utility of the generated outputs.

3. Preservation of Input Modality: The model primarily focuses on understanding between modalities, but it does not always preserve the faithfulness of the input modality. For instance, the output video and images do not consistently preserve the identity of the input image. This could limit the model's ability to generate accurate and coherent outputs across different modalities.

4. Cross-Modality Benefits: The paper does not convincingly demonstrate that the generation results benefit from cross-modality conditions. For example, Table 8 shows that the quality of image generation can even degrade when using conditions from two modalities. Similarly, Table 9 shows only marginal improvements in video quality when using multiple modalities. The authors should establish a benchmark that clearly demonstrates the benefits of using multiple modalities for generation. Without such evidence, the necessity of the proposed architecture could be questioned.

5. Omission of Baselines: In Table 2, the authors omit the StableDiffusion v1.5 baseline, which is the image Latent Diffusion Model (LDM) they used. Including this baseline could provide a more comprehensive comparison of the model's performance. 1. Evaluation Metrics: Could you provide more details on why you chose FID and CLIPSIM as the primary evaluation metrics? Have you considered incorporating user studies or other qualitative evaluations to assess the perceptual quality and coherence of the generated outputs?

2. Quality of Generated Results: Could you elaborate on the factors that might be contributing to the short and discontinuous text, short video length, and perceptually low-quality images? Are there potential improvements or modifications to the model that could address these issues?

3. Preservation of Input Modality: How does the model ensure the preservation of the identity or characteristics of the input modality in the generated outputs? Are there specific mechanisms in place to ensure this, or is it an area for future work?

4. Cross-Modality Benefits: Could you provide more evidence or a clearer explanation of how the generation results benefit from cross-modality conditions? The results in Tables 8 and 9 suggest that the benefits might be marginal or even negative in some cases. Could you clarify this?

5. Omission of Baselines: Why was the StableDiffusion v1.5 baseline omitted from the comparisons in Table 2? Including this baseline could provide a more comprehensive view of the model's performance relative to existing methods.  The authors have adequately addressed the limitations and potential negative societal impact of their work.",886,0,13,0.7738,0.0881843647,0.9791465998,215,19.9989,0.3617,neurips,0.011111111111111,5,4,5,5,factual,3,5,95,polite,5,neutral,5,none,5,5,5,5,factual,5,5,100,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
0,Reviewer-HFRa,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","This paper introduces ν-ensembles, a novel deep ensemble algorithm that achieves both efficiency and conceptual simplicity. When presented with an unlabeled dataset, ν-ensembles generate distinct labelings for each ensemble member and subsequently fit both the training data and the randomly labeled data. The strength of ν-ensembles lies in their ability to enhance deep ensemble diversity and calibration without significantly increasing computational demands. Key strengths include improved calibration in both in-distribution and out-of-distribution settings, achieved without complex implementation or extensive hyperparameter tuning. This method maintains the efficiency of standard deep ensembles, ensuring diversity through a straightforward process of assigning random labels to unlabeled data points. The theoretical grounding via PAC-Bayesian analysis provides a guarantee of diversity, accuracy, and calibration on test data, making ν-ensembles a promising and efficient technique for enhancing deep neural network ensembles. 1. The paper lacks the related works of other calibration method such as train time calibration loss, and post hoc calibration which is very important in this domain.
2. From my experience, the ECE measurement could be very unstable when classification accuracy is low. For experiments in table 1 for CIFAR100, the accuracy is very low, and the results may not reliable.
3. The experiments lack the comparison with SOTA methods such as Focal Loss Calibration and Adaptive Label Smoothing. In table 1, how many times does the author run the experiments? Since the ECE measurement can be very stable among low prediction accuracy models, the ECE reported in Table can have very large variance. Please report the variance of multiple runs to verify the effectiveness of your method.

The experiment is limited to CIFAR10 datasets. Since the authors mention that the small dataset regime often happens in medical area. It is better to verify your algorithm on the small medical datasets.",296,0,3,0.7848,0.0529183673,0.9382253885,47,22.8589,0.2519,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,5,5,4,5,factual,5,5,92,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,4,neutral,4,low,4,4,3,4,partially factual,3,4,78,polite,5,neutral,4,low
92,Reviewer-QsMj,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","To represent the continuous dynamic network, authors provide the framework based on the intensity profile. First, the intensity between nodes is estimated, which produces the intensity profile. Low dimension reduction via SVD is applied on the intensity, and then each node embedding is obtained by the low dimensional subspace.
Author also provide various theoretical analysis about the error bound and the bias-various trade-off. Theoretical analysis as well as empirical analysis on the simulated data demonstrates that the proposed method capture structural preserving and temporally coherent properties. Case study on the real data is conducted to explain the outcome of the proposed framework qualitatively - Simple but powerful method is proposed
- Based on the mathematical model, theoretical bound is analyzed and explained.
- IPP can capture the behavior of a bifurcating block model. - The proposed method is not novel enough. SVD decomposition is a very common technique for the reduction of dimensions, and it often suffers from the long-tailed singular values. 
- Comparison is too limited. The analysis has been made only for the simulated data with figures. More experiments as well as some qualitative results would be great to have.
- SVD decomposition does not prevent producing negative values at the reconstruction.
- The proposed projection space is very dependent on the fixed dataset. At least, how to leverage the given embeddings for predictions is not straightforward. Given this, the potential application value is not very clear. - Figure numbers are all wrong. 
- Section 4 is true for any global subspace projection. Also, both properties could be debatable, not necessarily ideal. For instance, when \Labmda_{i}(s) = \Lambda_{i}(t), X_{i}(s) = \alpha * X_{i}(t) could be more ideal, depending on the interactions among the other nodes. 
- It would be great if authors compare the embedding trajectory for more real data, beyond the specific simulated ones.  Often, the meaning of each dimension from the SVD decomposition is not clear. This interpretability is not necessarily required for the representation, but this should be addressed when presenting the case study.",339,0,0,0.7807000000000001,0.0777465827,0.8251838684,215,34.1479,0.0999,neurips,0.0,4,4,4,4,factual,4,4,88,polite,4,positive,4,low,4,3,4,4,partially factual,3,3,65,neutral,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
92,Reviewer-SnVH,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The authors propose an approach for learning time-varying node embeddings from continuous-time dynamic network data, which consist of a set of instantaneous timestamped relational events between nodes (e.g., messages from one social media user to another). Their proposed approach learns a projection that minimizes reconstruction error of the pairwise intensities between nodes and comes with theoretical guarantees on estimation error. They also show that their approach generates embeddings that both preserve network structure at a given time and is temporally coherent. They demonstrate strong empirical performance on simulated data compared to other dynamic network embeddings. Furthermore, they use their approach to analyze a real network data set on face-to-face interactions of primary school students, which is quite enlightening due to the interpretability of their model.

*After rebuttal:* The authors have clarified the one question I had about the meaning of ""inductive"" in their setting. I continue to strongly support the paper. - Proposed approach learns time-varying node embeddings from continuous-time networks with theoretical guarantees, which is among the first, if not the first, in the literature.
- Proposed embeddings can satisfy two good properties of structure preservation and temporal coherence.
- Very well written and organized paper that provides highlights of theoretical analysis in the main paper followed by details, including proofs, in the supplementary. - There's a large body of related literature on probabilistic generative models for continuous-time networks using point process models such as Hawkes processes that should be discussed. Many of these models are based on stochastic block models or latent space models and are thus also learning node embeddings. See suggested references below.
- No quantitative evaluation. This is only a minor weakness in my opinion because I view the main contribution to be theoretical.

Typos and minor issues:
- Supplementary Section C heading: Visualsation -> Visualisation

References:
- Arastuie, M., Paul, S., & Xu, K. S. (2020). CHIP: A Hawkes process model for continuous-time networks with scalable and consistent estimation. In Advances in Neural Information Processing Systems 33 (pp. 16983-16996).
- Corneli, M., Latouche, P., & Rossi, F. (2018). Multiple change points detection and clustering in dynamic networks. Statistics and Computing, 28(5), 989-1007. doi:10.1007/s11222-017-9775-1
- Huang, Z., Soliman, H., Paul, S., & Xu, K. S. (2022). A mutually exciting latent space Hawkes process model for continuous-time networks. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (Vol. 180, pp. 863-873).
- Junuthula, R. R., Haghdan, M., Xu, K. S., & Devabhaktuni, V. K. (2019). The Block Point Process Model for continuous-time event-based dynamic networks. In Proceedings of the World Wide Web Conference (pp. 829-839).
- Matias, C., Rebafka, T., & Villers, F. (2018). A semiparametric extension of the stochastic block model for longitudinal networks. Biometrika, 105(3), 665-680. doi:10.1093/biomet/asy016
- Yang, J., Rao, V., & Neville, J. (2017). Decoupling homophily and reciprocity with latent space network models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence. 1. The authors mention several times that their approach is inductive, allowing one to obtain a node representation profile outside of the training sample. If the task is to obtain the node representation for the future, how would the Intensity Profile Projection approach handle it? Would it require some data from other nodes at that future time? Limitations are thoroughly discussed in Section 6. I commend the authors for being very forthcoming with these limitations. I don't view the limitations as weaknesses, because they are mostly limitations that apply to all unsupervised problems.",576,8,12,0.8295,0.098241342,0.8879517317000001,215,35.1135,0.1953,neurips,0.0,4,3,4,4,factual,3,4,78,polite,4,neutral,4,none,3,5,4,4,factual,5,5,90,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,92,polite,5,positive,5,low
148,Reviewer-Fii6,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","In Graph Drawing Objective (GDO) and the generalized GDO, the optimization problem in Equation 1 and 3 are used to find the Laplacian representation, but this formulation allows symmetries, which lead to hyper-parameters that can lead to potential issues. The proposed method, Augmented Lagrangian Laplacian Objective (ALLO) in Equation 6, requires no hyper-parameters. In Theorem 1, they show a theoretical result on how there is a guarantee of the stability of the proposed objective function for finding Laplacian representations. The paper concludes with some experiments. - interesting formulation and solution
- motivated problem
- having experiments - some parts (e.g., Section 1 and 2) are hard to follow - How do you compare the complexity of the proposed objective function optimization problem with previous cases?





---------------------------------------------
After the rebuttal: I appreciate the authors for their response. They fully addressed my question and I decided to keep my acceptance score.",149,0,0,0.8237,0.0046296296,0.7480648756,61,30.7324,0.7922,iclr,0.0128205128205127,3,3,1,2,partially factual,4,3,50,polite,3,positive,3,moderate,3,3,3,3,factual,4,4,65,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,2,3,3,2,factual,4,3,60,polite,4,positive,4,moderate,2,3,3,3,partially factual,3,3,65,polite,4,neutral,4,low
148,Reviewer-nARE,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","The authors propose a method to approximate the true eigenvalues and eigenvectors of a graph Laplacian relying on an unconstrained max-min problem solved by gradient-based optimization. This can be used to learn good representations for the states in reinforcement learning problems. In the experiments, the efficiency of the method is demonstrated together with an ablation study. - This is an interesting and novel approach to the challenging problem of unsupervised representation learning.
- The technical part of the paper seems to be solid and reasonable, but I have not verified the theoretical results in detail. 
- Both the theoretical results and the experiments support the claims.
- The paper is relatively well written. I think that the proofs could have been in appendix and instead use the space for more examples, demonstrations, and clarifications. Q1. While in the paper the approach focuses on the eigenvectors of the graph Laplacian, in the experiments it is used for finding eigenfunctions. I think that further information should be provided for the actual formulation/solution of this problem.
Q2. I find Corollary 1 and the paragraph above a bit unclear. Why does an optimum of (2) and (4) imply that the constraint must be violated? 
Q3. Perhaps, an experiment to test the stability of the equilibrium with respect to permutations.
Q4. Why rotated eigenvectors do not provide a good representation?",225,0,4,0.7319,0.2205882353,0.8910561204,48,41.1356,0.1507,iclr,0.0106382978723403,4,5,3,4,factual,4,3,70,polite,4,positive,4,low,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
53,Reviewer-nfKV,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","The authors explore set prediction, or conformal prediction, within the context of out-of-distribution (OOD). In this prediction paradigm, rather than offering a singular classification result, a predictor provides a set of labels. This set is expected to encompass the true label with a high degree of certainty. When dealing with OOD, predicting an empty set becomes a significant indication, suggesting the assignment of an OOD label to the test data point. The authors introduce an algorithm that employs Random Fourier Features, ensuring scalability in relation to sample size. Furthermore, they present the adaptive weighted hinge loss and offset penalization techniques to boost classification efficiency. The paper theoretically investigates the expected prediction set size for their algorithm, showing that it approaches the optimal size as the sample size grows. Experimental outcomes underscore that their algorithm surpasses existing methods. Moreover, the components of the adaptive weighted hinge loss and offset penalization play pivotal roles in enhancing classification efficiency. 1. The paper is well-written and easy to follow.

2. Addressing set-valued classification issues in OOD scenarios is both demanding and imperative. Issues of trustworthiness and OOD can stymie the deployment of machine learning algorithms in real-world applications. Developing an algorithm for set-valued classification within OOD scenarios augments the applicability of machine learning techniques.

3. The proposed elements—adaptive weighted loss and offset penalization—are astutely crafted to evaluate the accuracy constraint more rigorously and to minimize the expected set size.

4. Theoretical insights guarantee that the classifier obtained by the proposed algorithm will attain the optimal expected set size achieved by the ideal classifier. This underscores the rationale behind the algorithm's design.

5. Experimental findings robustly attest to the proposed method's dominance over existing techniques in terms of the metrics evaluated. 1. The rationale behind incorporating Random Fourier Features is ambiguous. Attaining scalability can be realized by merely employing a fixed-width network as the penultimate layer. Resorting to infinite-dimensional kernel features as the penultimate layer seems unnecessary without a clear justification, making the algorithm's design seem somewhat ill-advised.

2. The theoretical findings seem to be direct derivations from the generalization bound established through the Rademacher complexity. Their technical significance remains dubious. Furthermore, given that the core contributions revolve around the introduction of adaptive weighted loss and offset penalization, the impact of these components on generalization error remains unexplored. Consequently, the results offer limited support for the algorithm's design.

3. There is likely an intrinsic trade-off between OOD recall and Efficiency as gauged in the experiments. Thus, assessing this trade-off's efficiency becomes crucial. A comprehensive superiority assertion for the proposed algorithm necessitates comparative analyses of such trade-off efficiencies.

4. It is also vital to assess the trade-off between OOD recall and Efficiency within the ablation studies.

5. The authors seem to incorporate the adaptive weighted loss with an aim to enhance the precision of class-wise error assessments. Therefore, to ascertain the efficacy of this component, evaluations of the precision of class-wise errors should be undertaken. 1. Would the authors shed light on the imperative of integrating the Random Fourier Features into their methodology?",507,0,11,0.7986000000000001,0.0256258503,0.9049091339,48,15.1946,0.1041,iclr,0.0210526315789473,4,4,4,4,partially factual,4,4,80,polite,4,neutral,4,low,5,5,5,5,partially factual,5,5,90,polite,5,neutral,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
32,Denise-Battaglini,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study investigates the predictors of hospital length of stay in COVID-19 patients in Jordan.  The study is well written and interesting. However, it has a lack of novelty and should be improved. I would suggest to add more information: 1) Hospital length of stay is often made by different wards and eventually ICU. I think it is important to understand which patients were admitted to ICU, if some of them were endotracheally intubated, tracheostomize, if some patients had hemorrhage, thrombosis, infections, other complications, which PaO2/FiO2 on admission, if they were non-invasively ventilated (CPAP, NIPPV, High flow), if CPR, D-dimer, previous antibiotic therapy, SOFA on admission, Charlson comorbidity index, steroidal therapy, sedation, analgesia, myorelaxants, etc. and other factors that could have been predictors of hospital stay.  The study aims to investigate only predictors but I believe that there is a lack of some important factors which could have changed patients' clinical course.  I suggest to extend the analysis to other important factors and, if possible to divide between those who survived and those who did not OR those who were admitted to ICU/those who did not.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",330,0,1,0.7733,0.1701041667,0.7571926117000001,174,32.22,0.2025,f1000,0.0,5,4,3,5,partially factual,3,2,50,polite,4,negative,4,moderate,5,5,5,5,partially factual,5,5,75,polite,5,negative,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,4,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
32,Omar-Soliman-Mohamed-El-Masry,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article titled ""Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan” represents an attempt to assess clinical factors that might be associated with COVID-19 patients' hospitalization in Jordan. The article rationale is good, however, it cannot reflect the figure in the whole country as data being taken from five centers and included a limited number of patients. I would suggest revising the title unless the data at this time represents the total number of patients in the whole country. In the introduction, the authors started to recount history of the beginning of COVID-19 observation in China, but the year was not mentioned (December 2019). Please, add 2019. The introduction should include a background section on factors reported in the study that might affect patients’ hospitalizations that were reported, at least, for similar diseases (MERS, for example). In addition, the authors should discuss the other factors that could affect this parameter; such as comorbidities. In the material and methods’ section, the following sentence “It is noteworthy that, in Jordan, all patients diagnosed with COVID-19 were admitted to hospital during the study’s timeframe, regardless of the severity of their illness” needs further clarification; does this mean that those were all COVID-19 patients reported in the whole country? If yes, it would be very early to generalize the findings of the current study and this must be clearly indicated as a limitation. In the results section, smoking status was not found as a predictor for the length of the hospital stay, which is odd knowing that COVID-19 patients suffer from serious lung problems. Did the author investigate confounding factors with smoking status; such as age, for example? Also, I think calculating odd ratio is not suitable for the study design. I think the findings were concluded from a premature study, which was conducted at the very beginning of the Corona crisis; therefore, the conclusions are premature and cannot reflect the logical and the expected conclusions regarding COVID-19. Thus, the study should be revised by including data of a larger sample size to be more representative and provide evidence-based conclusions. Having said that, the study rationale is good and interesting; but when supported with robust design, it will be of more interest to the scientific community and will better reflect the real situation.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",531,0,2,0.7698,0.1330880952,0.8107333779,183,30.09,0.2674,f1000,0.0202020202020202,5,4,4,5,factual,3,2,60,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,80,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,low,4,4,3,4,partially factual,4,3,75,polite,5,neutral,4,low
139,Bhamini-Krishna-Rao,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript titled ""Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study"" presents a valuable exploration of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study design, which utilizes a cross-sectional correlational approach, is appropriate for the research objectives and provides a comprehensive overview of the satisfaction levels among parents and caregivers.  The methods section is detailed and well-structured, clearly outlining the study design, participant recruitment, data collection, and analysis procedures. The choice of the Patient Satisfaction Questionnaire (PSQ-III) is justified and its reliability is well-documented, making it a suitable tool for this study. The ethical considerations are thoroughly addressed, ensuring the integrity and ethical soundness of the study. However, providing more details on the sampling process, including the selection criteria and any potential biases, would enhance the transparency and replicability of the methodology. The results are presented clearly and concisely, with comprehensive tables that effectively illustrate the key findings. The analysis is robust, and the interpretation of the data is logical and consistent with the study's objectives. The sociodemographic characteristics of the participants are well-documented, providing important context for understanding the results. The correlation analysis between demographic variables and satisfaction scores is particularly useful, highlighting the factors that influence parental satisfaction. Including more detailed subgroup analyses could provide additional insights into these factors. The discussion effectively interprets the results in the context of existing literature, highlighting both the strengths and areas needing improvement in the physiotherapy services. The identification of areas requiring improvement, such as access, technical quality, and economic elements, is particularly valuable for informing future service enhancements. The discussion could be further enriched by exploring potential strategies for addressing these areas and by discussing the implications of the findings for policy and practice in more detail. Additionally, a comparison with similar studies in other regions could provide a broader perspective on the findings and underscore the study's relevance in a global context. In conclusion, this study sheds light on the crucial aspect of parents' satisfaction with physiotherapy treatment for neuropediatric outpatients in the UAE. The findings underscore the overall positive satisfaction reported by parents and caregivers regarding various aspects of physiotherapy services, particularly in communication, interpersonal factors, and doctor-patient time. However, it is evident that there are areas in need of improvement, notably access, technical quality, and economic elements. These findings emphasize the importance of continuous assessment and enhancement of healthcare services to meet the evolving needs of patients and their families. Addressing the identified areas of concern is paramount to enhancing patient experiences and ultimately improving clinical outcomes. Therefore, it is imperative for service providers and managers to prioritize these domains in their efforts to optimize the quality of care provided to neuropediatric outpatients and ensure the delivery of patient-centered healthcare in the UAE. Suggestions for Improvement: The abstract can be reorganized to suit the title of the study by giving importance to parents whose children receive long term rehabilitation services. The introduction can emphasize more on how caregiving is difficult in neuropediatric population rather than giving too much importance to general aspects of patient satisfaction Provide more details on the sampling process and potential biases in the methods section. Include more detailed subgroup analyses in the results section to provide additional insights into factors influencing satisfaction. The results section can highlight parents' or caregivers' characteristics and then compare it with the patient satisfaction scores. Explore potential strategies for improving areas of low satisfaction in the discussion. Compare findings with similar studies in other regions to provide a broader context. Include specific recommendations for future research and practice in the conclusion. Recommendation: Approve for indexing with minor revisions.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",763,0,1,0.7771,0.1780205905,0.9188451767,8,7.66,0.0999,f1000,0.011111111111111,1,4,4,1,unfactual,3,1,30,polite,1,neutral,4,extreme,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,5,5,5,5,factual,5,5,95,polite,5,positive,4,low,4,4,4,4,factual,4,5,88,polite,5,positive,4,low
44,Talha-Bin-Emran,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title: Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study Minor comments: Although the article has scientific rigor, several minor flows need to be improved before publication: 1. The abstract section is unsuitable—no focus point in the abstract section. 2. ""Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding."" Is this necessary? 3. Authors are suggested to use the full form when used for the first time throughout the manuscript. 4. The aim of the study should be written as the last paragraph of the introduction. 7. MTX treatment and follow-up: How was this selected? 8. Receiver Operating Characteristics (ROC) analysis: Please describe in further detail. 9. ""Further analysis using the ROC curve showed that MTX-PG3 level…"" needs more insights with relevant references. 10. Presentation of figures is good. 11. Figure legends are appropriate and self-explanatory. 12. The conclusion needs to address future perspectives. 13. Spacing, punctuation marks, grammar, and spelling errors should be reviewed thoroughly.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",317,0,11,0.7843,0.1904411765,0.8207837343000001,15,29.96,0.1939,f1000,0.0121951219512195,2,3,2,3,partially factual,4,3,40,polite,2,positive,2,low,5,4,5,5,factual,5,5,88,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,4,5,factual,4,4,85,polite,5,neutral,5,low
131,Reviewer-yntr,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies group-wise clipping for optimization under differential privacy. The issues discussed in this article regarding optimization under DP are timely and critical. The performance loss caused by DP necessitates urgent solutions for these problems. The paper lacks novelty as the proposed clipping method is an extension of the existing Book-Keeping
technique Bu et al. (2022c). Furthermore, the convergence analysis relies on smoothness assumptions.

I also disagree with the authors' perspective that ""Differentially private (DP) optimization of deep learning models has enjoyed amazing accuracy and rigorous guarantees against privacy risks."" From my knowledge, accuracy loss remains a significant obstacle, which is also the problem this paper aims to address. Are there any hyperparameters that need to be tuned for the proposed clipping methods? If so, do these adjustments come at an additional privacy cost? Has the paper reported these associated costs?",142,1,1,0.8899,0.2458333333,0.8636165857,49,31.5628,0.1858,iclr,0.0,3,4,1,3,partially factual,3,3,50,neutral,4,negative,2,moderate,4,4,3,4,partially factual,4,3,70,impolite,5,negative,4,low,1.0,4.0,4.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,3.0,low,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,moderate,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,low
129,Reviewer-vTRX,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","This paper explores the efficiency of training parameterized quantum models, from the perspective of backpropagation scaling. By leveraging some recent developments in shadow tomography and accessing multiple copies of a quantum state, the authors propose an algorithm that matches backpropagation scaling in quantum resources and reduces additional classical computational costs. The results provide valuable insights into the reusability of quantum information and the results are potentially meaningful for the future of quantum machine learning. - The paper investigates a timely and relevant topic in quantum machine learning, comparing the efficiency of training parameterized quantum models to classical neural networks.
- The authors leverage recent developments in shadow tomography, providing a novel approach to study a meaningful problem on quantum neural networks.
- The proposed algorithm matches backpropagation scaling in quantum resources and reduces classical auxiliary computational costs.
- The angle of this work to study quantum neural networks is novel. - The primary analysis is limited to quantum neural networks based on variational quantum circuits, which restricts the scope of the paper as many other types of quantum neural networks exist.
- The application of the results to general quantum machine learning algorithms is not convincingly demonstrated.
- The paper lacks a clear and well-motivated example demonstrating the application of the proposed methods, making it difficult to assess its practical implications and usefulness. - Could the authors provide more insights into the practical implications of the results and its potential applications?
- How do the results of this work extend to other quantum neural networks?
- There are a certain number of existing works on the gradients of quantum neural networks. How does Proposition 7 advance the known works?
- What is the relationship between this work and the problem of the barren plateau? NA.",295,0,1,0.7624000000000001,0.0951298701,0.9247617126,216,22.886,0.068,neurips,0.010204081632653,3,3,3,3,factual,4,4,65,neutral,4,neutral,4,moderate,4,4,4,4,factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
129,Reviewer-Hoi6,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The authors go over the backpropagation scheme for both classical and quantum machine learning methods. They also propose a novel quantum backpropagation algorithm based on quantum shadow tomography to reuse information and reduce the time complexity.  1. This paper provides a link between quantum backpropagation and quantum shadow tomography, which are both important in quantum computing.
2. This paper provides a thorough background check on quantum backpropagation, information reuse scheme in QST, and backpropagation scaling problem.
3. This paper is technically sound. 1. This paper is more like a report paper than a research paper to me, since the main contribution is to discuss in detail how reusing information can benefit quantum backpropagation, and the proposed algorithm seems quite trivial. 1. Whether this level of space complexity on the classical device is acceptable?
2. Could you be more explicit about and also highlight the potential impact of this paper on the quantum machine learning society?
 The major limitation is whether this paper fits the scope of the research paper in NeurIPS. ",171,0,6,0.7516,0.225462963,0.9025430679,216,34.1816,0.3617,neurips,0.0,3,3,3,3,partially factual,3,3,50,neutral,3,neutral,3,low,3,3,3,3,partially factual,4,4,55,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
35,Reviewer-kmpt,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","This paper explores adding long-range modulatory feedback connections to deep CNNs (specifically AlexNet, evaluated on ImageNet). It explores 2 ways of incorporating the feedback: a default mode and a cognitive steering mode. The results show improvements on ImageNet accuracy, adversarial robustness, and a composite image recognition task. - The paper is high-quality: it's well-written, clear, the visualizations seem to have been very thoughtfully prepared, etc. The motivation for the modulatory connections is well-explained, and the empirical results (ImageNet accuracy, robustness, and cognitive steering effects) are compelling.
- The authors anticipated most of my questions and responded to them in the body of the paper. For example, Section 2.2 has a great description of why certain decisions were made and which options were explored. 
- The experiments are explained clearly, and the visualizations really enhance the presentation.
- I think this is a very significant question (how to incorporate long-range feedback into deep neural networks), which has been studied for many years but hasn't quite become mainstream yet. I applaud the authors for thoughtfully probing this question and taking a step toward bringing long-range feedback connections into modern neural networks, which I expect will be a quite impactful addition to NNs when it finally lands. - In Section 2.1, I would have liked to see either mathematical equations or pseudocode to remove any ambiguity regarding the implementation of the feedback connections. The descriptions are decent, but I'm having to guess what the exact computations are. The implementation is the essence of the paper, in some sense. It would be nice to make this very precise in the body of the paper.
- It would be good to have a thorough discussion of the costs associated with incorporating these connections (time, memory, etc.). Right now, the paper kind of reads like there's no reason *not* to use them, which probably isn't entirely fair. What am I losing if I incorporate these connections?
- Although the Related Research section is nice, I would like to better understand which 1-3 works are most closely related to this one, with a more detailed description of how this implementation differs from these 1-3 most closely related prior works. - What is the precise definition of ""modulatory"" used here? It seems like one could argue that any feedback connections are ultimately modulatory. What's the exact definition you're using such that other types of feedback connections *aren't* considered modulatory?
- This isn't essential, but I'm curious (and I suspect some readers might be) -- is there something more biologically plausible about this version of feedback connections than some of the prior work?
- How important is the fact that the steering is global vs. local? It might be worth discussing this more.
- In Figure 3, what do you mean by ""full branch""?
- In Section 3.2, what label is used?
- At the end of Section 3.1, could you further spell out why relevant features are amplified and irrelevant features are suppressed? It might be helpful to connect this more precisely to the mathematics/implementation, if added to Section 2.1 (as discussed in Weaknesses).
- nit: typo in the second paragraph of Section 3.2 (where --> were)
- In Section 3.3, you're using ""target"" as the label whereas it's previously used differently in the ""source/target"" description, right? If so, this dual use might not be optimal. Could you find a way to use 2 different words?
- Is there possibly a better phrase than ""target absent category""? It took me a little while to parse this. Do you have any ideas to help clear this up? The paper includes a nice description of the limitations of this work, including limited exploration of different architectures and a lack of mechanistic analysis into why the long-range connections help.",628,0,0,0.7887000000000001,0.237199793,0.9079948664,216,47.996,0.2814,neurips,0.0120481927710843,4,4,5,4,factual,4,4,95,polite,4,positive,4,none,5,4,4,5,partially factual,5,5,88,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,90.0,polite,5.0,positive,4.0,none,4,5,5,5,factual,5,4,90,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
80,Reviewer-Vfdi,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The authors describe a new IoT FL benchmark suite based on several datasets they have curated and demonstrate that it can be used to compare FL optimizers. + Benchmark curation work doesn't receive the credit it deserves, given its impact on advances in the field. The authors are doing something important, here. + The writing quality is poor, even in the abstract.

+ The authors claim this is the first IoT FL benchmark but a Google Scholar search turns up ""FLBench: A Benchmark Suite for Federated Learning"" by Yuan Liang, Yange Guo, Yanxia Gong, Chunjie Luo, Jianfeng Zhan, and Yunyou Huang, which includes an AIoT benchmark domain. It isn't clear whether this is competing work, or work by the authors of the submitted paper. In either case, it seems highly relevant but does not appear to be cited. One way to deal with this problem within the blind review process is to cite the work, but use an entry such as ""Redacted for blind review"" in the bibliography during the review process. Another paper, ""FedML: A Research Library and Benchmark for Federated Machine Learning"" claims to support IoT devices. I am not claiming that those papers are identical to this work, but they seem close enough to merit contrasting them with the author's benchmark. I view this as a substantial weakness, but one that might be resolved via rebuttals and simple revision.

+ The modifications to the curated datasets are not well justified. They are reasonable, but explicit justification or basing the approach on well justified approaches from prior work would be best. 1) Does the similarity matrix used for noisy labeling depend on the particular centralized learning approach? If so, does that mean that centralized training and evaluation must be redone to enable noisy labeling whenever an algorithm changes? Or is there something fundamental about the confusion matrix, i.e., is it unlikely to change much when models change?

 2) Why not leave the sounds in raw format instead of converting to the frequency domain with particular parameters? Isn't this sort of raw data to feature conversion part of the approaches your benchmarks will be used to evaluate? If so, why build one particular approach to feature extraction into the benchmarks?

3) What is the purpose of Section 4? To demonstrate that the benchmarks can be used to compare optimizers? Enabling comparison doesn't imply enabling comparison yielding correct ranking of optimizers. If you could demonstrate that the findings using your benchmarks differ from those using the most closely related existing (perhaps even non-IoT) benchmarks, and your benchmarks are more typical of applications in the IoT domain, that would support your claim that your benchmarks are more useful in this domain than prior work.",453,0,0,0.8017000000000001,0.0920518284,0.8552749157,61,50.4888,0.1443,iclr,0.0,5,3,4,5,factual,4,4,85,polite,4,negative,5,none,5,4,4,5,factual,4,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
123,Reviewer-DWom,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This paper proposes a comprehensive library for evaluating text-to-image finetuning methods, typically based on LoRA. In addition to different algorithms, it also provides comprehensive evaluation criteria. Finally, some experimental results provide some insight about different finetuning methods. 1. This is a good engineering paper that provides a library for text-to-image finetuning methods evaluation.
2. It support different matrix factorization techniques such as LoRA, LoHa, LoKr, DyLoRA, GLoRA, GLoKr and so on.
3. This paper also consider comprehensive evaluation metrics, including fieldity, controllability, diversity, base model preservation and image quality. 1. This paper mainly focus on LoRA-based finetuing strategies, can it be expanded to other parameter-efficient finetuning methods such as \[1\] and \[2\]? It doesn't provide a clear explanation.
2. The conclusion about the performance of different finetuning methods is not clearly presented in the experimental section. Maybe some tables can more straightforwardly represent your final conclusions. 

\[1\] Qiu, Zeju, et al. ""Controlling Text-to-Image Diffusion by Orthogonal Finetuning."" arXiv preprint arXiv:2306.07280 (2023).
\[2\] Xie, Enze, et al. ""DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning."" arXiv preprint arXiv:2304.06648 (2023). Please refer to the weakness section.",187,6,9,0.8365,0.0530612245,0.911740303,52,16.9695,0.2131,iclr,0.0,2,4,2,2,partially factual,3,3,50,polite,3,negative,3,moderate,4,4,3,4,5,4,5,75,4,5,neutral,4,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
106,Reviewer-pc5v,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers. This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set Gω for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains. This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning. •	Considering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?
•	In Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -δ, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?
•	I believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in Gω for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in Gω, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.",365,0,0,0.804,0.0780772006,0.9679618478,49,20.8673,0.1507,iclr,0.0,4,4,3,4,partially factual,3,4,70,neutral,3,neutral,4,moderate,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
106,Reviewer-s5Ux,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state. The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations. - The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn’t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?
- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.
- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).
- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.
- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.

Some minor points:
- Abstract: hand-craft\[ed\]
- Intro: The sentence “All of these models are trained…” needs a re-write
- Intro: “To our knowledge, … to apply offline RL to MILP solving” (re-write)
- Sec. 2: typo pseudocsot
- Sec. 2.2. A\[n\] MDP
- Equation 4: one closing brace is too much (after $Q_\theta$)
- Sec. 3.1: when a\[n\] MILP instance
- Sec 3.1: discounted factor $\rightarrow$ discount factor
- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1
- Sec. 4.1: please use cite and citep depending on how you add these citations into the text
- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state - Which set of variables if being selected from?
- What is the performance of other offline-RL algorithms?
- Can you evaluate on a larger testset?
- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?
- In Sec 3.2. “In fact, a good action does no harm to policy optimization even if it is an OOD action” – can you please elaborate on this a bit more?",553,0,4,0.8156,0.0484206349,0.8696163893000001,49,48.758,0.2567,iclr,0.04,4,4,3,4,partially factual,3,3,75,neutral,3,negative,4,low,4,4,4,4,factual,4,4,82,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
106,Reviewer-3oNR,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The paper is easy to follow.
2.	Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.
2.	The authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work \[1\] applies offline RL methods to branching strategies learning as well. 
3.	The authors may want to explain the novelty of their method over the work \[1\] in detail.  
4.	The experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work \[1\]. Third, the authors may want to evaluate the generalization ability of the learned models.

\[1\] Huang, Zeren, et al. ""Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022. Please refer to Weaknesses for my questions.",239,4,8,0.6839000000000001,0.0766666667,0.89818573,49,40.7962,0.1719,iclr,0.0,4,4,3,4,factual,3,4,75,neutral,4,neutral,3,low,5,4,3,5,partially factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
172,Reviewer-LoZH,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This work studies the impact of imposing known conditional independence structure in fully-connected neural network architectures, notably in the setting of autoregressive normalizing flows. The independence is imposed by masking the weight matrices in the linear layers, similar to the MADE approach. The masks are determined by a factorization of the known adjacency matrix that (approximately) maximizes the number of connections (paths) between inputs and outputs. Through several experiments, the authors show generalization improvement over baselines.

**Edit** Most of my concerns were addressed during the rebuttal/discussion period. I am therefore upgrading my score from 4 to 6. * The manuscript is well written, with standard notations.
* The contributions are clearly defined and the assumptions (known adjacency) are explicit.
* The proposed approach to impose conditional independences is sound.
* The claims are supported by the experiments, notably concerning improved generalization. The idea of imposing prior independence knowledge into autoregressive flows was first introduced by Wehenkel and Louppe (2021), cited as \[25\] in this manuscript. As the authors mention (lines 246-247), StrAF only differs from GNF in the approach to impose the independences, but is conceptually identical. Actually, Wehenkel and Louppe (2021) already propose the approach of the present work:

    > An alternative approach would be to use masking scheme similar to what is done by Germain et al. (2015) in MADE as suggested by Lachapelle et al. (2019).
    
In addition, the official \[UMNN repository\](https://github.com/AWehenkel/UMNN), also cited in this work, links to the normalizing flow library \[Zuko\](https://github.com/francois-rozet/zuko), from the same lab. The latter library implements autoregressive flow conditioners as masked multi-layer perceptrons for which the masks are a factorization of the adjacency matrix between the inputs and outputs. The similarities with the proposed StrNN are too strong to be left unaddressed. * Algorithm 1: It is not clear to me how the factorization algorithm is applied when the StrNN has more than one hidden layer.
* Section 5.1: Are the same number of layers/neurons used for StrNN and MADE in this experiment?
* Line 317: ""As GNF permutes variables between flow steps"". I was not able to find a mention of this in \[25\].
* Figure 5/Table 1: I don't understand how ""ARF-10"" and ""GNF-10"" can be so much worse than ""GNF-1"", as they are strictly more expressive. Is it an overfitting issue? Or maybe an invertibility issue? UMNN is not always numerically invertible. What about ""ARF-1""?
* Table 1: The authors make a distinction between ""density estimation"" and ""sample quality"", which does not make sense to me. If a flow perfectly estimates the density, it necessarily generates probable samples, unless the invertibility is not guaranteed.
* Why not studying the use of StrNN in other settings than density evaluation, such as physics-informed machine learning, where it is common to infuse prior knowledge in the structure of the neural networks? * It is never mentioned that a StrNN is a (pruned) fully-connected network with element-wise activation functions. The approach does not apply to convolutional, attention-based or recurrent networks, and does not support skip/residual connections or normalization layers.
* This is not a limitation of this work, but one should be careful not to confuse Bayesian networks and causal graphs. A Bayesian network (or its adjacency matrix) over variables merely indicates independencies between the variables but in no way causalities.",549,8,1,0.7869,0.0911904762,0.8084603548,215,39.2951,0.0376,neurips,0.0,4,4,4,4,factual,4,3,80,polite,4,positive,5,low,5,5,5,5,factual,5,5,90,polite,5,positive,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
105,Reviewer-XVP7,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper proposes a new topic and method for training a mask-aware CLIP, which could serve as a core component for open-vocabulary segmentation. The designed structure could be used as a flexible plug-in but brings significant improvements for existing methods on various benchmarks. 1. This topic is promising.  Previous methods decouple open-vocabulary segmentation into class-agnostic segmentation and CLIP-guided recognition. However,  most of them fail to use CLIP effectively,  I think training a mask-aware CLIP is an ideal way to deal with this problem.  

2. The model design is reasonable, using a mask2former-style network and tasks the masks to perform masked attention sounds reasonable.

3. The experiment results are great with significant improvement. It would be an ideal solution for various open-vocabulary segmentation tasks incorporating strong class-agnostic segmentation models like SAM. 

4. The paper is clearly presented.   The experiment setting is unsatisfactory, which only tackles zero-shot semantic segmentation. 
As this topic and idea are good,  I expect the authors to extend the method into open-vocabulary panoptic settings, and use some large datasets for training. Currently, the datasets used are small. it is hard to distill universal knowledge from CLIP.
 See weakness yes",191,0,5,0.8227,0.2260687229,0.9103056788,215,37.0755,0.1262,neurips,0.0352941176470588,4,4,3,4,factual,4,4,80,neutral,3,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
116,Reviewer-5v8y,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper proposed a mutual learning approach to learn a pair of Bayesian Neural Network(BNN). The posterior of BNN is approximated by Variational Inference using a Gaussian distribution with a diagonal covariance matrix. To make the BNN learn different perspective of the data, the author proposed to increase the diversity in parameter space and intermediate feature space by adding the an estimate of distance between parameter distribution and fused feature distribution of two BNN models into the objective function. Empirically, the proposed method outperform existing mutual learning method and vanilla BNN model in terms of accuracy, negative log likelihood loss and expected calibration error. An ablation study is also provided to investigate the usefulness of each component.  The paper is well written and easy to follow. Increasing the diversity of parameter distribution and intermediate feature distribution of peer BNN models to boost performance is an interesting idea. Experiments and detailed ablation study demonstrate the effectiveness of proposed method. 1. It is mentioned in the abstract and introduction that the BNN model with variational inference may underperform deterministic model or BNN obtained by MCMC, the baseline only involves BNN model trained with(DML) or without(vanilla) mutual learning. Would the proposed method close the gap to some extent? Data augmentation, optimizer may all affect performance, so it is still helpful to include deterministic model results follow with same training setup. I would expect the BNN model to outperform deterministic model at least in NLL and ECE, and with the 50 ensemble, it can outperform the accuracy. 

2. Continue with last point, for MCMC method (e.g. in line 81 of the paper), I agree that traditional MCMC method(e.g. Metropolis Hasting) may not be feasible for large model, and memory storage can be an issue for MCMC method. But I don't think the stochastic gradient MCMC cited in line 81 would require prohibitive computational cost, it behaves like adding a noise to at each step of standard SGD training. 

3. The code is not provided so it may hurt the reproducibility of the paper. 1. To my knowledge, it is not very clear if variational distribution(e.g. Gaussian with diagonal covariance matrix) can approximate the true posterior very well, can the author comment a bit on this, e.g. how would different choice of variational family affect the model?

2. In line 264 and line 6 of algorithm 1, it is mentioned that one BNN model is initialized with a trained model and this lead to better results empirically. Can the author discuss more on why this happened? It is a bit wired for me as it seems in the implementation detail, the pre-trained model and the model from scratch are trained with same optimizer and learning rate schedule.

3. Seems like $\alpha$ $\beta$ are set to 1,2 for CIFAR and 1,1 for Imagenet, these two parameters controls the strength of proposed penalty to the model, can the author comments a bit more on how sensitive are the model to those parameters, It can help to illustrate how diversity helps model performance.

4. As mentioned in line 268, results are average of 3 trials, I think it would be better to include the standard deviation as well to boost the significance of the results.

5. In figure A.3 in supplementary material, looks like a sharp increase of KL divergence between the fused feature distributions at around 30 epochs, but the penalty for feature is only added for last 100 epochs, can the author explain more on this?  The authors addressed the limitations.",585,0,8,0.7803,0.1055805306,0.8540630937,216,40.5795,0.11,neurips,0.0,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
116,Reviewer-xPiJ,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper presents a novel method for enhancing the performance of Bayesian Neural Networks (BNNs) by employing deep mutual learning. The proposed approach aims to enhance the diversity of both network parameter distributions and feature distributions, encouraging individual networks to capture unique characteristics of the input data. The effectiveness of the proposed method is demonstrated on datasets, including CIFAR10, CIFAR100, and ImageNet. The proposed method improves performance and uncertainty estimation while reducing the expected calibration error (ECE).  The technical approach is novel as the method introduces mutual learning in the context of BNNs and first to propose maximizing the distance between feature distributions and parameter distributions. The paper includes large scale data experiments (ImageNet) and ablation studies to demonstrate the effectiveness of each technical contribution introduced in this paper. The previous studies mentioned in the paper utilize alignments on feature maps \[4\] or predictions \[38\], rather than diversifying them. In contrast, the proposed method diversifies both feature distributions and parameter distributions which is an opposite approach to the previous works. Interestingly, both alignment-based and diversification methods improves performance over vanilla BNNs, as indicated in Table 1, 2, 3, and 5. However, the paper does not explicitly explain the reasons behind the performance improvements resulting from these contrasting approaches.

Given the observed contradicting results in the experiments, where the alignment-based method (DML \[38\]) also enhances the performance of BNNs, an important question arises: could combining alignment-based methods with parameter diversification further improve BNN performance? Alternatively, is it necessary to diversify both feature and parameter distributions to achieve significant improvements?

In the experiment section, the proposed method is only compared with \[38\] and not with \[4\]. 

Hyperparameters used for CIFAR experiments and ImageNet experiments are different. However, the paper does not describe details regarding the hyperparameter tuning or determination. 3 block resnet is used for CIFAR experiments while 4 block resnet is used for ImageNet experiments. Why different form of resents are used for different datasets? Limitations are shortly addressed in the supplementary.",331,5,0,0.7494000000000001,0.0582251082,0.8471010923000001,216,15.9032,0.072,neurips,0.0,3,3,3,3,factual,3,3,70,neutral,4,neutral,4,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
26,Rajinder-K.-Sharma,Case Report: The Sausage Technique using Anorganic Bovine Bone Mineral for Horizontal Bone Augmentation at the Crestal Part of a Posterior Mandibular Ridge: A Case Report.,"Following tooth extraction, the alveolar bone goes through a natural remodeling process resulting in a significant bone resorption which may complicate dental implant placement without prior bone augmentation treatment. The sausage technique is a modified guided bone regeneration (GBR) method that has been successfully used for horizontal bone augmentation. This technique was developed to increase the bone growth at the alveolar crest. Although the sausage technique uses a combination of autograft chips and xenograft particles with a native collagen membrane, several studies have questioned whether adding autograft chips is essential for bone formation with guided bone regeneration. Moreover, harvesting the bone graft may increase the donor site morbidity and patient discomfort. This case report aimed to investigate the bone gain radiologically when the sausage technique was applied to treat a healthy, thirty-year-old patient with a horizontal defect in the posterior mandibular region using anorganic bovine bone mineral (ABBM) particles with Jason membrane, assess the implant primary stability in the augmented ridge, and present the surgical procedure steps in details. After nine months of healing, the cone-beam computed tomography (CBCT) revealed approximately 4.32 mm of bone gain at the alveolar crest in the buccal-lingual direction. The graft particles were well integrated into the newly formed bone. Two implants were inserted with an insertion torque of 35 N/cm. The ISQ values were 76 for the most anterior implant and 78 for the posterior implant. Within the limitations of this case report, the sausage technique using ABBM particles without autograft chips was an effective approach in achieving the prerequisite bone width at the crest in cases with horizontal bone defects.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is about horizontal bone augmentation through staged GBR using the sausage technique to facilitate implant placement. Please consider the following points to improve the quality of discussion section. 1. How the surgical procedure is different from the procedure proposed by Istvan Urban and colleagues, except the exclusion of autogenous graft. 2. What are the alternatives to bone augmentation to facilitate implant placement in this case. Please describe briefly the merits and limitations. 3. What are the probable outcomes of attempted bone augmentation in this case? And how the bone augmentation was ascertained? 4. What are the  long-term complications associated with fragmented bone graft materials?  5. Is the procedure described in this case relevant for improving the success of implant placement?  6.Ethical considerations for use of materials with animal origin.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Yes",268,0,6,0.7475,0.0722222222,0.8633317947,164,27.93,0.0515,f1000,0.0,2,3,3,2,partially factual,3,3,50,neutral,3,neutral,2,moderate,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
157,Reviewer-uRwm,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper provides an in-depth analysis of adversarial training with linear models and its relationship to regularized regression methods under the overparametrized regime.
Depending on the value of the perturbation radius, it is revealed that there exist three modes.
When the radius is small, solutions to adversarial training behave as minimum-norm interpolators (Theorem 1).
When the radius is medium, the solutions behave as solutions to the parameter shrinkage regression (Proposition 4).
When the radius is large, the zero solution is necessary and sufficient (Proposition 3).
In addition to these theoretical results, the authors observe the mode change experimentally and discuss how adversarial training is advantageous over parameter shrinkage regression. - A modern extension of theory on robust optimization and regularization: The relationship between robust optimization (somewhat encompassing adversarial training in this work) and regularization has been known in the literature, including Xu et al. (2009). This work contributes to studying what happens when it comes to overparametrization and nicely characterizes the relationship between the perturbation radius and the corresponding modes (as I summarized above).
- Demonstration of the benefit of overparametrization: In the numerical simulation of Figure 2, the authors demonstrate that the robustness radius increases as the model becomes more overparametrized, namely, $p/n$ increases. This clearly indicates the benefits of overparametrization (though the analysis hinges on norm matching, as mentioned in Remark 2).
- Clarity: Despite the thorough theory, the paper is written clearly and easy to follow.

Xu et al. (2009). ""Robustness and Regularization of Support Vector Machines."" (JMLR) One of the main weaknesses would be the restriction to linear models, which is crucial for the current analysis yet needed for further understanding adversarial training.

You may refer to Xu et al. (2009) when you show Theorem 4. Indeed, the equation right after l.322 can be regarded as a generalization of Theorem 3 in Xu et al. (2009) because $\\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}) - \\delta\\|\\beta\_\*\\|) \\le \\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta})) + \\delta\\|\\boldsymbol{\\beta}\\|\_\*$ when $\\ell$ is the hinge loss.

Below, I have other minor comments.

- In Figure 1, can you specify what $\\lambda$ and $\\delta$ are used for each line?
- In the proof of Theorem 1, you may need $-$ (negative) sign in front of either $\\epsilon\_i\boldsymbol{x}\_i$ in Eq. (6) or $n\\delta\boldsymbol{\\alpha}$ in l.132. Otherwise, ""the subderivative contains zero"" (l.132) does not seem to be correct.
- In l.150, the reason of $\\delta\_{\\text{test}} \\propto \mathbb{E}\[\\|\\boldsymbol{x}\\|\]$ is unclear to me. Can you elaborate on it?
- In Figure 4, can you specify what $n$ and $p$ are used?
- In Eq. (8), do you miss the exponent $2$ for the norm?
- In Eq. (9), it might be better to change the notation $\\epsilon$ for the noise because $\\epsilon$ has already been used in the proof of Theorem 1.
- In the equations after l.319 and l.322, should we need $+ \\Delta x$ on the left-hand sides?
- In the appendix, what is referred to as Theorem 3 seems to be Proposition 3. See the weaknesses. Obviously, the analysis is entirely limited to the linear model case. Nonetheless, the analysis provides a fair amount of insights to readers, so I don't think this is a big limitation.",523,4,7,0.7336,0.151984127,0.9391887188,221,42.6844,0.2594,neurips,0.0,4,4,3,4,factual,4,5,85,polite,4,positive,4,none,5,5,5,5,factual,5,5,92,polite,5,neutral,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
45,Elizabeth-A.-Stokes,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper reports a within-trial cost-utility analysis (CUA) and cost-effectiveness analysis (CEA). The paper is clearly written, and appropriate methods have been used to conduct analyses. The text around interventions being cost-effective where findings are reported in terms of cost per QALY lost is very well explained. I have the following comments: A CUA and a CEA were planned, did you pre-specify which was the primary analysis?  Introduction – first sentence – who is at-risk?  Resource use was captured at baseline, 6 and 12 months. Did the questionnaires at each of these time points ask participants to recall their resource use over the previous 6 months? Was resource use captured at baseline solely for the purpose of including baseline costs in the multiple imputation models?  Did you explore the missing at random assumption?  Costs – resource use was captured on day cases, but no unit cost for this is reported in Table 1. Were there no participants who reported a day case admission? Were hospital admissions not captured as there is no chance that this patient group would be admitted for hand OA? In the introduction, surgery is cited as one of the high costs in this patient group.  The mean difference between groups and 95% CI is presented in Table 3 for costs and Table 5 for EQ-5D utilities, but not in Table 2 for resource use? It would help the reader to include this.  Table 3 – did you consider separating medication costs into HCQ and other medications?  The time horizon for the CUA was 12 months but for the CEA was 6 months? While the primary clinical outcome of hand pain severity was measured at 6 months, this was also captured at 12 months. Why was your analysis for this outcome based on a shorter time horizon than the CUA analysis? Was a CEA over 12 months a pre-planned sensitivity analysis?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",457,0,1,0.7362000000000001,0.1312373737,0.8965256214,35,45.96,0.1879,f1000,0.0,4,5,4,4,factual,4,5,90,polite,5,positive,4,none,4,5,4,4,factual,5,5,85,polite,5,neutral,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,neutral,4.0,none,4,5,4,4,factual,5,5,85,polite,5,positive,5,low,3,4,4,4,factual,4,5,85,polite,5,neutral,5,low
45,David-Mark-Epstein,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conduct an economic evaluation alongside the RCT. There were no differences found between the groups in terms of hand pain or quality-of-life and no significant differences in costs.  Although there were no differences, it is nevertheless worthwhile publishing these results, in order to avoid ""publication bias"" and guide future research in this area. The study, in general, is well conducted and I have no comments on technical matters.  Rather than calculate an ICER, which implies some measurable difference in outcomes and costs, personally, I would interpret the results in the abstract and conclusions that there were no meaningful or statistically significant differences in any outcomes or costs at 1 year.  The authors do not discuss other therapies or research in this area and this contextual comparison would be useful.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",274,0,1,0.7818,0.1495833333,0.8935310245,115,24.68,0.1213,f1000,0.01,2,4,2,2,partially factual,3,2,60,polite,3,positive,3,moderate,3,5,3,3,partially factual,4,4,65,polite,4,neutral,4,moderate,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,neutral,4.0,none,2,4,3,3,factual,4,4,70,polite,4,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
87,Setya-Haksama,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. All variables should be written clearly and systematically, first the independent variables should be described, then the dependent variables should be described. 2. Resources of data from World Bank was too old. 3. No data was obtained from 40 countries measured in relation to this research, there should be a ranking for each country that can indicate which countries have good scores and which countries have low scores.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",214,0,3,0.7434000000000001,0.191,0.7335164547,38,34.76,0.1041,f1000,0.0,3,3,2,2,partially factual,2,2,25,neutral,2,negative,2,moderate,4,4,3,4,partially factual,4,4,65,polite,4,negative,3,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,3,3,partially factual,3,3,60,neutral,4,neutral,3,moderate,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
87,Mohamed-Adil-AA,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article highlights important areas in the arena of globalization and the spread of infectious diseases. The article particularly looks into data from a number of countries globally, thus increasing the validity and reliability of the study across continents and also globally.  Could this study be replicated by using longitudinal data to establish causality and stronger inferences? Do the path regression results provide more robust results than OLS analysis? What was the main logic in choosing only a specific set of covariates and not all the possible covariates for tuberculosis?  This a good study and will help in addressing many lacunae in the area of global health research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",251,0,1,0.7655000000000001,0.1954301075,0.8643754721,41,26.51,0.072,f1000,0.0096153846153845,2,3,2,2,partially factual,3,3,35,polite,3,positive,3,low,3,5,4,4,partially factual,4,4,80,polite,5,positive,3,moderate,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,3,2,factual,4,3,60,polite,4,positive,3,moderate,2,4,3,3,factual,3,4,75,polite,5,positive,4,low
6,Reviewer-rgXX,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","This paper proposes a scalable path-based knowledge graph reasoning approach. The idea is to extend only important paths from the exponentially growing set of all possible paths. A heuristic priority function is parametrized by a feed-forward network and is trained to predict the priority of nodes to expand. Experiments show that the proposed approach can significantly improve time and memory efficiency and also achieve good results. - Scalability is an important issue for path-based reasoning approaches. The idea of selecting only important paths is interesting and sounds reasonable
- The proposed approach is effective and supported by extensive experiments. Time and memory efficiency has been significantly improved. Benchmark results are also good. My concern is mainly about the design of the priority function Eq (10)

In Eq (10), the first part $h_q^{(t)}(u, x)$ is already conditioned on q, u, and x, so in principle the second part $g(\[h_q^{(t)}(u, x), q\])$ doesn't provide any additional information. Therefore, the priority function is purely based on the current path from the start and contains no information about the goal. In other words, the prediction of the priority function would be the same even if the goal changes. This is different from the design of the A* algorithm and may lose theoretical guarantees. 

It is not appropriate to present the approach in the manner of A* algorithm Please see Weaknesses properly addressed",228,0,0,0.7526,0.1886904762,0.8858633041,218,38.7064,0.2971,neurips,0.0,3,4,4,3,factual,4,3,65,polite,4,neutral,5,low,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
193,Nitin-Liladhar-Rane,What we know and what should we know about the future of blockchain in finance,"Background In response to the transformative impact of blockchain technology on economic and financial landscapes, there is a critical need for a review study that analyses the knowledge landscape from diverse perspectives.  Methods This research VOSviewer, and Bibliometrix to undertake a bibliometric analysis of the expanding literature related to blockchain technology within the financial sector. Through a examination of 500 published articles, the study identifies insightful trends, patterns, and emerging domains on a global scale.  Results The findings highlight the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Conclusions While affirming the potential of blockchain, the analysis also sheds light on persistent impediments hindering its widespread adoption and utilization. This study not only contributes to the current understanding of blockchain in finance but also serves as a valuable resource for future researchers. It guides systematic reviews by pinpointing prominent journals and influential authors within the dynamic field of blockchain finance, thereby fostering a deeper understanding and facilitating further exploration in this evolving field.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ""What we know and what should we know about the future of blockchain in finance"" Authors' have made a good attempt by highlighting the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",239,0,1,0.763,0.1354691877,0.9726446271,35,21.84,0.1303,f1000,0.010204081632653,1,1,2,3,unfactual,3,2,50,neutral,1,neutral,2,high,2,5,4,2,factual,4,3,70,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,1,4,3,2,factual,4,4,60,polite,4,positive,3,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
107,Reviewer-X6yV,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This paper addresses the challenge of enabling RL agents to comprehend and act based on complex language input. The proposed framework, Dynalang, enhances agent performance by incorporating language signals into the prediction of future states. Notably, Dynalang builds upon DreamerV3 by introducing text tokens into observations at each step. Experimental results demonstrate its effectiveness across various games, such as Homegrid, Messenger, Habbit, and LangRoom, outperforming previous language-conditioned RL baselines. 1. The paper addresses a compelling problem by enabling RL agents to understand intricate human language, expanding beyond straightforward task instructions, which is an understudied but important area in RL research.

2. The paper's writing, especially in the introduction, effectively highlights the core problem and how Dynalang provides a solution.

3. The study includes experiments across multiple game environments and consistently demonstrates improvements over existing language-conditioned RL methods. 1. The technical contribution is somewhat limited, primarily differing from DreamerV3 by adding text tokens to observations. A deeper exploration of Dynalang's components and their significance is needed. For example, an ablation study could help clarify the role of the language token in the world model.

2. The paper lacks a detailed ablation study that could validate the importance of each component in Dynalang. Explaining why the language token is necessary, particularly if it only serves as input for the policy network, would provide valuable insights.

3. While the paper explores various game environments, they appear simplistic. Evaluating the method on more challenging games, such as Crafter or Minecraft, would enhance the paper's credibility.

Overall, the paper presents an intriguing idea but requires further validation and clarification to strengthen its foundation. I look forward to discussing these points further in the rebuttal stage. 1. How does the paper ensure that the agent can effectively follow language corrections in the Homegrid environment? Are auxiliary reward signals used to guide agent learning?

2. Could you provide more details on the training process? Is the network trained from scratch, or is the world model pre-trained?

3. Have you considered using an LLM as the core of the world model, given its strong language modeling capabilities?",349,0,9,0.8536,0.1234353741,0.9215202332,49,30.7053,0.4104,iclr,0.0,4,5,4,4,factual,3,4,80,polite,5,positive,4,low,4,4,4,4,factual,4,4,82,polite,5,neutral,5,low,3.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
107,Reviewer-bqw2,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The paper proposes Dynalang, an agent that grounds language to visual experience via future prediction. The writing of this paper is clear, and the descriptions and justifications of the methods are comprehensible. This paper appears to have limited novelty, seeming more like a combination of existing techniques. What are the primary challenges addressed by the article? And what are its main contributions?",62,0,0,0.7567,0.1869047619,0.9115282297,49,40.0587,0.038,iclr,0.032258064516129,1,2,1,0,unfactual,1,1,20,neutral,2,negative,2,extreme,3,5,3,3,partially factual,3,3,65,polite,5,neutral,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,1,3,2,1,partially factual,2,2,2,neutral,3,neutral,2,high,2,4,3,2,partially factual,4,3,60,polite,4,neutral,2,moderate
66,Reviewer-ocxo,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The motivation that inspired the work is important:  the pre-trained models are highly complex and difficult to fine-tune. However when you must train a model on a downstream task it could be that all the features that were learned on the source task are not necessary. So, if one had a way to select which features are relevant, then one could reduce the number of features needed to solve the downstream task and thus deploy smaller models. To address this task, they propose to  impose an L2 regularization which forces to find the most relevant features for the downstream task among all the features of the pre-trained models. The idea of using the features learned in different models on the same data points and merge them together to represent the input is nice and, to the best of my knowledge, novel. It also makes sense performing an automatic feature selection in that space, in order to select the feature combination which is more informative. The experimental result presented in support of the idea are partially convincing. I understand the attempt of providing a justification of the regularization loss using information theoretical bonds, but the way the authors arrive to the final form of the regularization they use, which is just a kernel version of the L2, eq. 9, is in my opinion unnecessarily involved and might create confusion. I suggest moving the text from eq 2 to eq 9 to the appendix.


MAJOR:  It is  not very clear why this form of R should help avoiding redundancy: the sigmoid function can set to zero irrelevant features, but if several features are simultaneously relevant (but correlated), I expect the solution  will not be sparse, but it will contain weights contributions from all. I suspect that this might lead to overfitting in data-scarce scenarios (see below).  

Given the topic of the article I would have expected a comparison with LORA (https://arxiv.org/abs/2106.), where the features for the downstream task are selected by multiplying the original features by a low rank matrix before downstream fine tuning. Since the focus of the paper is transfer learning, which typically happens towards data-scarce tasks, I would have liked to see if the procedure is robust with respect to aggressive decimation of the target task. What happens if one attempts to use ~100 examples for category, as typical in clinical image analysis applications?

The last paragraph of page 4 is not very clear. The variational parameters are the components of the vector s, which, via the sigmoidal function, set the weight of the corresponding psi component in the rhoPsi kernel?

Minor: when they present the setting on page 3, the labels seem to be linear regression labels, while the experiments are on classification datasets. Please clarify.",458,1,1,0.7757000000000001,0.0912545788,0.8472784758,47,43.831,0.174,iclr,0.0272727272727272,4,4,4,4,factual,4,4,85,polite,4,negative,4,low,4,4,3,4,partially factual,4,4,80,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
114,Reviewer-pnnH,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","This paper introduces a novel rehearsal based continual learning approach which use a shared task-attention module to mitigate the task interference. The shared task-attention module compresses the task specific information to some trainable parameters. 1. The framework achieves fairly good results compared with baselines.
2. The paper is written clearly and easy to follow. 1. Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works \[*\] \[**\]. L2P has shown its effectiveness in continual learning areas in recent years. 

2. Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.

\[*\] Learning to prompt for continual learning, CVPR 2022.

\[**\] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022. Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?",159,0,5,0.776,0.2238095238,0.8488740325,48,32.7117,0.1695,iclr,0.0,3,4,3,3,factual,4,3,60,polite,4,neutral,3,moderate,5,5,4,5,5,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low

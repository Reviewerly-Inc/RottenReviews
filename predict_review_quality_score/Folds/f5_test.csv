paper_id,reviewer,title,abstract,review_text,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,politeness_score,venue,hedging,Human_Actionability,Human_Clarity_and_Readability,Human_Comprehensiveness,Human_Constructiveness,Human_Factuality,Human_Fairness,Human_Objectivity,Human_Overall_Quality,Human_Politeness,Human_Relevance_Alignment,Human_Sentiment_Polarity,Human_Usage_of_Technical_Terms,Human_Vagueness,Qwen_Actionability,Qwen_Clarity_and_Readability,Qwen_Comprehensiveness,Qwen_Constructiveness,Qwen_Factuality,Qwen_Fairness,Qwen_Objectivity,Qwen_Overall_Quality,Qwen_Politeness,Qwen_Relevance_Alignment,Qwen_Sentiment_Polarity,Qwen_Usage_of_Technical_Terms,Qwen_Vagueness,Llama_Actionability,Llama_Clarity_and_Readability,Llama_Comprehensiveness,Llama_Constructiveness,Llama_Factuality,Llama_Fairness,Llama_Objectivity,Llama_Overall_Quality,Llama_Politeness,Llama_Relevance_Alignment,Llama_Sentiment_Polarity,Llama_Usage_of_Technical_Terms,Llama_Vagueness,GPT_Actionability,GPT_Clarity_and_Readability,GPT_Comprehensiveness,GPT_Constructiveness,GPT_Factuality,GPT_Fairness,GPT_Objectivity,GPT_Overall_Quality,GPT_Politeness,GPT_Relevance_Alignment,GPT_Sentiment_Polarity,GPT_Usage_of_Technical_Terms,GPT_Vagueness,Phi_Actionability,Phi_Clarity_and_Readability,Phi_Comprehensiveness,Phi_Constructiveness,Phi_Factuality,Phi_Fairness,Phi_Objectivity,Phi_Overall_Quality,Phi_Politeness,Phi_Relevance_Alignment,Phi_Sentiment_Polarity,Phi_Usage_of_Technical_Terms,Phi_Vagueness
166,Reviewer-FAWm,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","This paper presents the Blended Exploitation and Exploration (BEE) operator, which addresses the issue of value underestimation during the exploitation phase in off-policy actor-critic methods. The paper highlights the importance of incorporating past successes to improve Q-value estimation and policy learning. The proposed BAC and MB-BAC algorithms outperform existing methods in various continuous control tasks and demonstrate strong performance in real-world robot tasks. - The paper addresses an important issue in off-policy actor-critic methods and proposes a novel approach to improve Q-value estimation and policy learning. 
- The BEE operator is simple yet effective and can be easily integrated into existing off-policy actor-critic frameworks.
- The experimental results demonstrate the superiority of the proposed algorithms in various continuous control tasks and real-world robot tasks. 1. The novelty of the proposed approach is limited. 
2. The choice of $\lambda$ is largely empirical and requires extra manipulation in new tasks.
3. The paper only provides basic theoretical analysis, such as the accurate policy evaluation and the guarantee of policy improvement. The benefit of linearly combining two Q-value functions is not discussed theoretically.
4. The experiments are conducted in continuous control tasks with dense rewards. The exploration ability can be better evaluated in environments with sparse rewards.
5. There is a lack of discussions with related papers (See Question 3). 1. Emprically, the BAC algoithm will only be more efficient in exploiting the replay buffer. The exploration still rely on the maximum-extropy formulation in SAC. Then why can BAC perform significantly better than SAC in failure-prone scenarios such as HumanoidStandup, as if BAC can better explore the unknown regions?
2. Can you discuss or exhibit the performance of BAC in some tasks with sparse rewards? This can demonstrate the generalizability of the proposed approach.
3. What are the advantages of BAC compared with prioritized replay methods \[1,2\] or advantage-based methods \[3\]? These methods are related to BAC in that they also exploit the replay buffer with inductive bias, so they should be mentioned in the paper.

\[1\] Sinha, S., Song, J., Garg, A. &amp; Ermon, S.. (2022). Experience Replay with Likelihood-free Importance Weights. Proceedings of The 4th Annual Learning for Dynamics and Control Conference.

\[2\] Liu, X. H., Xue, Z., Pang, J., Jiang, S., Xu, F., & Yu, Y. (2021). Regret minimization experience replay in off-policy reinforcement learning. Advances in Neural Information Processing Systems, 34, 17604-17615.

\[3\] Nair, A., Gupta, A., Dalal, M., & Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.


I am willing to raise my score if my concerns for weaknesses and questions are adequately discussed.",432,8,14,0.7996000000000001,0.1588311688,0.8829935789000001,60,31.9755,0.1565,iclr,0.0,5,4,4,5,factual,4,4,86,polite,5,neutral,4,none,4,5,4,4,factual,5,5,88,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
38,Reviewer-3sWQ,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper proposes a method that can be used to infer conditional independencies in a Gaussian model. These conditional independencies are related to zeros in the precision matrix. Typically, sparse enforcing norms are used to estimate the precision matrix while enforcing zeros in the elements outside of the diagonal. In this paper a Bayesian approach is considered. For this a pseudo-distribution for the data is considered by taking the exponential to the p-norm. The method is trained via variational inference combined with normalizing flows to increase the accuracy of the posterior approximation. The variational distribution is tuned via simulated annealing and a temperature parameter allows to interpolate between the Bayesian and the Map solution. - Well written paper.

        - Illustrative toy experiments. - The proposed method is a combination of already known techniques.

        - The experimental section is weak as only a single real problem is considered.

        - Although the proposed method is a generalization of several known techniques, I have found in the experimental section a lack of comparisons with other related methods.

        My main point of criticism is the weak experimental section which only considers a single real problem and no comparisons with other related methods are carried out in real problems.

        Another point of criticism is that, for some particular values of the p parameter one does not actually observe sparsity in the Bayesian solution. For example, when sampling from the Laplace distribution one never observes zeros in practice. Spike and slab priors (a mix between a Gaussian and a point of mass center at zero) are the ones that actually lead to zeros. None The authors have not commented on the limitations of their approach.",279,0,2,0.7414000000000001,-0.007047619,0.9233116508,215,36.096,0.0989,neurips,0.0,2,3,3,1,unfactual,3,3,60,neutral,3,negative,3,low,3,4,4,3,partially factual,3,4,65,neutral,5,negative,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,positive,3.0,none,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
181,Reviewer-c3Sg,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","Paper claims to propose a foundation model, named Ultra, for knowledge graph representation learning. The proposed model can handle full inductive graphs in which new entities and relations may appear in the test set. To do so, the authors propose to lift the graph to a one with relations as the nodes and design 4 different edge types (head2head, head2tail, tail2head, tail2tail). The relational representations are then learnt using message passing on this graph. The learnt relation embeddings are then used in the original graph to perform inductive link prediction. For the experiments, the authors pre-train their method on 3 KGs and further evaluate in a zero-shot setting and also by fine-tuning the downstream tasks. - The paper proposes a transductive model that works in settings of new relations and entity nodes.
- The method obtains good zero-shot pretraining results. - The authors have not explicitly stated the computational complexity of the method. From the paper, it seems that the forward pass is run on the entire relational graph to obtain relation representations. This is then used to initialize the node embedding from the query triple and the process is repeated for every triple. Thus it seems that the entire graph is being used for link prediction every triple making the computational complexity O(E^2). This seems limiting for large graphs that have not been explored in the paper (such as wikidata-5m etc.).
- From Table 2 we can see that finetuning over the pre-trained models helps the results significantly over the 0-shot setting. Also, the fine-tuning steps are too large to claim few shot results. This weakens the claim of the ""foundation model"" for KGs. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.
- Another limitation is that of scale. Since the current model has fewer parameters, this would limit learning over larger pretraining datasets as can be seen in Figure 6 and also reported by the authors.
- SoTA results for transductive models are better than the pre-trained Ultra model in many datasets. Thus the Ultra model seems to work well for the inductive setting rather than transductive. Thus the claim of the ""foundation model"" seems broader in scope.
- We see that in the metafam dataset, the pretraining results are poor but on finetuning the results are improved drastically. This shows that the method works well in cases where the relational patterns of the downstream datasets are similar to the pre-trained one but when the data distribution changes the results suffer. Moreover, due to limited capacity, the model may not be able to handle such cases by increasing the pretraining datasets calling for downstream finetuning. Thus domain adaptation is not a problem which can be easily overcome by scaling the current model and this further weakens the claim of a ""foundation model"" for KGs. - For weakness point 2: Any reason why this was not done by the authors?
- For weakness point 3: How would this be addressed in future works for the model?
-  For weakness point 4: Could the authors comment on why this would be the case and how would the model be improved to handle the transductive setting?
- For weakness point 5: Any reason why the results on this dataset are not good?
- Considering KGs are a rich source of textual/semantic data along with graph/structured data and the current model does not use this rich source of context information, how can we extend Ultra to incorporate the KG ontology? 
- Considering weaknesses 2,4,5 the claim of the foundation model seems a bit broad as of now and at best the model could be said to be a good inductive learner.",625,0,0,0.7487,0.1723163098,0.9095818996,49,51.6761,0.1355,iclr,0.0104166666666666,4,4,4,3,factual,3,3,70,neutral,3,neutral,3,high,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,5.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
181,Reviewer-ebFz,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","The key limitation of designing the foundation models for dealing with the Knowledge Graphs (KGs) is that the KGs have different entities and relations that generally do not overlap. To address this issue, this paper proposes ULTRA, which positively transfers the information of source KG to unseen KG. It constructs relation representations based on the interactions between the relations by introducing the graph of relations. The proposed approach has shown good performance on various tasks. - From their experiments, the proposed methods have shown good performance on various tasks.
- Research topics about the foundational models on graph-structured datasets is really interesting and important.
- The paper is well written and easy to follow. - The authors first pretrain the ULTRA model with the mixture of 3 standard KGs and then fine the model for the downstream task. But, the other supervised SOTA model only uses dataset of the downstream tasks without employing the pre-training datasets. If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks. However, if the SOTA models are the models for the inductive setting, I think they may be possible to be pretrained like the ULTRA. So, could you measure the performance of the ""pretrained"" SOTA models on the inductive if possible? Please refer to the weaknesses.",222,0,0,0.7001000000000001,0.1619617225,0.9080925584,49,47.3913,0.1823,iclr,0.0,3,4,2,3,factual,3,3,65,neutral,3,neutral,2,high,4,5,4,4,partially factual,4,4,85,polite,5,positive,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,2,4,3,3,factual,3,3,65,polite,4,positive,4,low,1,4,2,2,partially factual,3,2,45,polite,4,positive,3,low
171,Magdalena-Czlapka-Matyasik,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I commend the authors to contribute to this body of literature regarding the socio-demographic and lifestyle factors associated with understanding fast food consumption in Cambodia adults. The paper brings quantified information about the factors influenced by understanding fast food consumption. The authors revealed that poor and fair knowledge, insufficient exercise levels, and not getting enough sleep were predictors of inadequate understanding of the impact of fast food on health. Such conclusions do not bring entirely new knowledge to the literature, on this matter. Across the whole world population, the problems related to fast food consumption have been discussed. Nevertheless, I consider the work to be original, well designed and contribute knowledge to this field of public health research. My suggestions concern: In the introduction, the authors indicate the system of fast-food restaurants; it would be more attractive to explain, how it was developed in Cambodia directly?  What would be very interesting is information concerning the real take away or fast food intake in those groups. It must or might be in direct relation to this matter?  My main concern about the validation of the ""Level of knowledge of fast food consumption"": Could the authors explain the procedure? How were the questions selected and validated?  I regret that the authors discussed the interesting results in such a concise way.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",361,0,1,0.7773,0.2011784512,0.89415133,137,35.27,0.1507,f1000,0.01,0,4,1,0,unfactual,3,3,35,neutral,3,neutral,1,high,4,4,4,4,partially factual,4,4,85,polite,4,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,3,4,3,3,factual,4,4,70,polite,4,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
141,Reviewer-xxEb,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","The paper introduces a novel model, PatchSynth, in the Patch-Text Pre-training (PTP) domain, aiming to improve software patch representation and description generation. Through a blend of patch understanding and generation, PatchSynth	 addresses the limitations of prior models. Empirical evaluations reveal its superior performance in patch description generation, with an ablation study further underscoring the importance of generating task training. Novelty and Importance: The work is first to propose a unimodel for patch-text understanding and related tasks. And the topic is very important in this domain.

	Melds patch understanding and generation, addressing prior models' specialization limitations.

	The work provides a good representation and good results Unclear adaptability across diverse programming languages or coding standards. What are the considerations for deploying PatchSynth in real-world software development environments, and what infrastructure would be required for efficient and secure operation?",136,0,0,0.7967000000000001,0.3063636364,0.944865346,50,11.6712,0.0364,iclr,0.0,3,4,2,3,partially factual,3,3,50,polite,4,neutral,3,low,3,4,4,3,factual,4,4,75,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,2,3,3,2,factual,4,3,60,polite,4,positive,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,positive,4,low
28,Reviewer-FRnw,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper studies the problem of individual fairness in supervised learning. The focus is on studying how to certify distributional individual fairness (IF) (individual fairness over a set of distributions close to the observed empirical data distribution) in neural networks. Prior work has focused largely on certifying global IF, which is more expensive and thus can only be applied to smaller neural networks than the proposed certification/debiasing technique. The contributions of the paper are in showing how to certify distributional IF in neural networks and then using these bounds in the training process as regularizers to debias NNs. 

The main methodology for certifying IF is presented in Section 5. The first step is to certify local IF by over-approximating the similarity ball to find a conservative estimate of the IF violation. They can then use this bound to certify distributional IF around the empirical data distribution and apply finite sample guarantees to give an estimate of the true distributional IF. 

The authors then show how to use the bounds on distributional fairness as regularizers in the training procedure as a way to debias neural networks. They then provide experimental evaluation on a few benchmark datasets that demonstrates that their proposed training method indeed improves distributional individual fairness, at relatively modest degradations in accuracy.  The main advantage is a relatively lightweight way to certify and train NNs for IF, in a way that requires little additional computation, compared to previous methods which are not able to scale to large NNs. 

The experimental evaluation seems to confirm that DIF training as proposed by the regularization method does in fact improve significantly improve IF at modest degradation in classification accuracy.  Section 5 is a little dense and it would be helpful for the reader if there was a little more discussion of the optimization procedure, particularly in Section 5.3. Theorem statements here might also be helpful for the reader to understand what the final guarantees are.  What is the purpose of Table 2? It is a little difficult to interpret the punchline - it just seems to indicate that DIF training does not have a consistent effect on group fairness measures, either positively or negatively.  -",363,0,1,0.7939,0.0286027569,0.9569676518,215,26.5652,0.0354,neurips,0.0,4,4,3,3,factual,3,4,60,neutral,3,negative,4,high,4,4,4,5,5,5,5,85,5,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,4,low
41,Reviewer-viiH,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper proposes contrastive introspection (ConSpec), an algorithm for learning a set of prototypes for critical states via the contrastive loss. ConSpec works by delivering intrinsic rewards when the current states match one of the prototypes. This paper also conducted experiments in various environments. The intuition of learning the critical states is natural and easy to follow. The experimental results in this paper look solid and promising. Despite the empirical performance, the reviewer finds the ConSpec algorithm itself hard to follow.

The largest weakness is: the insufficient discussion on how the prototypes $h_i$ are learned. Hence, the reviewer cannot understand the detailed on how $h_i$ are used (see detailed in Questions). 

Besides insufficient discussion on the prototypes, some minor issues are: (1) the title in the pdf (Contrastive Introspection:. ..) seems to mismatch with the one appear in openreview (ConSpec: …). (2) The font of citations appears to be confusing. E.g., from line 19-20 in the introduction, the manuscript uses (number) to address some key points, and the citation also appears as (number) – it would be nice if the citation can be changed to something that is not (number; number).
 Per the major weaknesses:
1. How are the prototypes $h_i$ actually learned? If the reviewer understands correctly, in line 7 of the abstract, the manuscript says “ConSpec learns a set of prototypes…”. While in Algorithm 1, it seems that the prototypes $h_i$ are given to the algorithm as inputs. Maybe the author can clarify why this inconsistency in learning the prototypes happens?
2. How are the $h_i$ learned/chosen in each experiment? The reviewer has looked into the detail of the experiments in the appendix, but cannot clearly understand how the presented experiments actually utilize the $h_i$. It would be nice that the authors can provide more details of all the $h_i$ in all the present experiments (Sec. 4.1-4.5).  
 See questions and weaknesses.",313,0,2,0.7000000000000001,0.0809294872,0.8764749765000001,216,48.5775,0.0354,neurips,0.0,2,4,3,2,factual,4,3,60,polite,3,neutral,5,none,4,4,4,4,partially factual,4,3,75,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
41,Reviewer-fj2y,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","The paper noticed that in real-world MDP, success is often contingent upon a small set of steps. While Bellman equation can theoretically do credit assignment over long-horizon, reward is hard to propagate under Bellman-based methods in practice. The authors therefore propose a novel algorithm that uses contrastive learning to identify critical states that final success relies on. The method uses a memory like system that can give agents intrinsic reward during training. The paper then evaluates the proposed method on a wide variety of domains and shows performance gain when the proposed method is added to RL algorithms.
 The paper is based on an interesting and important insight about long-horizon credit assignment and reward learning. The proposed method is designed to explicitly improve long-term credit assignment and have shown empirical success in the evaluation. 

The writing and figures are clear. The paper is easy to follow.

The evaluation covers a wide variety of RL tasks are benchmarked to back the claim of the paper.  1. The method assumes additional access to a ""success"" indicator at the end of episode. While this is commonly obtainable in gym environments, this doesn't fit into the general MDP setting and thus might limit when the algorithm can be applied. 

2. The assumption about access to ""success"" seems privileged compared to baselines. I am wondering they will catch up with the performance of the proposed method when a success bonus is added.

3. The evaluation has #mini batches / # gradient steps as x-axis, unlike the environment steps in common RL benchmarks. I am wondering why this is the case. If this is necessary, I'd like to see convincing justifications. 

4. The proposed method relies on a memory system, which may hurt generalization and might have problem when scaling up.

5. CURL+PPO doesn't seem to be a strong baseline to ablate in figure 3. I hope the authors could benchmark against RAD\[https://arxiv.org/abs/2004.14990\], a much stronger baseline in pixel space.  I am wondering whether adding the intrinsic reward can degrade the performance of RL algorithms on common environments (aka, those environments where the final success does not depend on just a few critical steps). This shall be justified the experiments.

When the observation is partial, is the proposed method still reasonable?

 1. The method requires privileged information about success of an episode. 
2. I cannot see how the method can be applied to RL that has partial observation that requires recurrent policies.",406,1,7,0.7978000000000001,0.1046052632,0.8880720139,216,45.7357,0.2025,neurips,0.0,5,5,5,5,factual,3,3,90,polite,5,positive,5,none,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
41,Reviewer-t9VM,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper introduces ConSpec, a reinforcement learning (RL) algorithm designed to identify critical steps and improve performance in continuous control tasks. ConSpec utilizes contrastive learning to learn prototypes of critical steps and employs a contrastive loss to differentiate successful and failed experiences. It addresses the challenges of long-term credit assignment and generalization in RL tasks. This article presents an interesting idea of learning to match key states in a task through contrastive learning. The writing of this paper is clear and Figure 1 is well-drawn, making it easy to quickly grasp the details of ConSpec. And the experimental results effectively demonstrate that the learned prototypes indeed match the key states in 3D Orange-Tree task and gridworld. The cosine similarity measures the similarity between the prototype and the hidden state, both of which are learnable vectors. However, optimizing the contrastive learning loss with updates to both vectors may lead to extremely unstable training. In related literature on representation learning, it is common to use the stop gradient approach and optimize only one of the learnable vectors. 
 - ConSpec achieved a return of only 400 in the Montezuma's Revenge task compared to RND, which reached a return of 7500 in its original paper. It appears that ConSpec is not as effective as RND in this regard. Both the multi-key room environment and Montezuma's Revenge involve similar logic, but the former is much simpler. So why does ConSpec outperform RND in Fig.4?
- It appears that ConSpec has significantly higher variance than the baseline algorithm in all tasks. What could be the reason for this? Further, prototype learning is crucial and can every seed match the key states?
- What does ""softmaxed over t"" mean in line 152? Why is it necessary to introduce softmax operation?
- How is success and failure defined in Atari and MuJoCo tasks? I think this is important, but the article lacks details in this aspect. As presented in Section 5, the number of prototypes is task-specific and definations of  success and failures need human design.  Furthermore,  The proposed method introduces hyperparameters, such as the Diversity measure and the hyperparameter $\lambda$, that require careful tuning. Furthermore, the algorithm exhibits a significant variance in its actual performance.",368,0,0,0.7889,0.0998903509,0.8961703181,216,40.6298,0.0649,neurips,0.0,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,4,5,4,4,partially factual,4,5,85,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
179,Akira-Endo,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study considers the effectiveness of contact tracing focused on variants in reducing the reproduction number. Focusing contact tracing efforts on variants is an interesting approach and may be relevant to the current situation of variant circulations worldwide. The model and the analysis themselves seem well constructed and implemented. However, the authors’ analysis only focuses on a single variant essentially, and does not account for some important aspects that need to be considered to estimate the effect of real-world contact tracing in the presence of multiple variants. As a result, I am not sure if this study provides new insights that are distinct from existing studies on contact tracing for a single-pathogen outbreak. In addition, it should be noted that given a fixed capacity for contact tracing, the reduction in the reproduction number would not be permanent if the outbreak continues to grow. I believe these issues, along with other comments detailed below, need to be addressed for this study to be truly of epidemiological and public health interest. Major comments: Please clarify how this study is distinct from existing studies on contact tracing considering a single-pathogen outbreak (including the authors’ own study cited here).  There seems to be a mismatch between the study motivation/context and the modelling approach. One of the points the authors are trying to make is that the contact tracing efforts should be focused on variants because they are of more epidemiological importance (due to potentially higher transmission or immunoescape). I do not disagree with this point, but there are several major issues regarding how it was handled in the manuscript.  The reproduction number R is used as an objective variable to measure the effect of contact tracing. This is useful to connect interventions and the dynamic evolution of the epidemic, but essentially assumes that the same level of tracing can continue everywhere long-term, regardless of the epidemic size. This is obviously not true as the authors also state in the manuscript. In conditions where R is above 1, transmission of variants would continue and overwhelms the tracing capacity at some point, pushing R back to the original value eventually. Focusing on R may be useful in identifying conditions required to control the outbreak (i.e. R<1), but it is unrealistic to consider that the tracing can keep R lower than the original value in a long term if the resulting value exceeds 1.  Variants are no longer minor in many places now (see for example: https://covid.cdc.gov/covid-data-tracker/#variant-proportions), and I am not sure how much this assumption of ‘minor variants’ is relevant to the actual situation. Moreover, even in places where the variants are still minor, if the (effective) transmissibility of the variants is higher than the existing virus, they would rapidly replace the existing viruses, potentially in a few weeks/months. Exclusion of existing strains. The main argument regarding the tracing capacity is that the variants account for a small proportion of cases and thus can be handled if tracing focuses on these variants. However, even if such focused intervention is possible by tests that can distinguish variants, existing non-variant viruses may continue spreading if their R is above 1. Although such a situation may still have some benefit, e.g. if preventing the spread of immunoescaping variants would ensure the success of the vaccination program, such contexts should be clarified and discussed.  Cost and capacity. As discussed above, contact tracing would work as estimated here only until the capacity is reached. However, I feel efforts associated with tracing is not seriously considered in the analysis. For example, if all contacts of cases within the tracing period are traced, extending the tracing period from 2 days to 6 days would incur substantial additional effort for tracing. I believe it is important to discuss to what extent contact tracing might be sustainable for each setting because the presented results become invalid once the capacity is reached.  Given the points above, I would recommend the authors reconsider what outcome measure to use and how to present them; e.g. consideration of the growth of ""non-targeted"" viruses, conditions required to keep R below 1, whether tracing can “buy time” until achieving a sufficient level of vaccination before reaching the capacity, optimising the intensity of other NPIs (e.g. lockdowns) in the presence of contact tracing, etc., such that the results are relevant to what may actually happen. The Introduction looks lightweight and lacking necessary details or contexts. There are a lot of concepts that may not be familiar enough to every reader but are not sufficiently explained (e.g. TTI, backward contact tracing, bidirectional tracing, why TaqPath test can distinguish B.1.1.7… etc.) and thus may require a succinct clarification. Please also note that this paper may be read in 20 years from now, when the reader may not have the same level of recognition of the current situation. In this light, for example, I feel the first paragraph of Introduction may sound a bit abrupt to the reader who is less aware of the overall timeline of the pandemic. Also see some of the specific comments in the Minor comments section.  The Methods section is too simple and does not contain sufficient information for the reader to comprehend the overall structure of the analysis. Although it does not need to contain every technical detail of the model and analysis as the supplementary methods can be found in the repository (but please include a link and description in the paper so that the reader can easily find it), I feel more information from the supplementary methods should be extracted and summarised in the main text. For example, from the current Methods section I cannot interpret how the course of transmission was characterised, what is the assumed procedure of tracing (Is it always bidirectional tracing? I feel 2-day window is too short for backward tracing), how environmental transmission was assumed to work, how R was calculated, etc.  I believe additional sensitivity analysis would be necessary. For example, the overdispersion parameter (0.11 used in the current analysis) is estimated to be slightly higher (0.3-0.5) in some studies where interventions were in place (Adam et al., 20201). As the authors assume that interventions may be affecting R during contact tracing, possible changes in overdispersion should also be considered. Delay from secondary transmission to quarantine of contacts (defined as a sum of various delay distribution) would also affect the effectiveness of contact tracing in a nontrivial manner.  Is the effect of vaccines not considered, although as in Introduction it was one of the major motivation for considering controlling variants? Vaccines may affect different viruses similarly or differently, depending on the type of variants.  Supplementary Methods, “Identified contacts are quarantined, …isolated, tested, and traced as described above”: what is the difference between quarantining and isolation of traced contacts? Does this mean all traced contacts of a case are put under quarantine regardless of their true infection status, but only tested if they are symptomatic (which changes the label from quarantine to isolation)? If so, it is expected that as the epidemic grows there would be a substantial number of quarantined individuals, and at some point this might be impossible (e.g. due to depletion of essential workers) and the Reff control could collapse.  Minor comments: Throughout: please spell out acronyms at their first appearance, including SARS-CoV-2 and COVID-19.  Introduction, protection against B.1.351 and P.1: now the evidence is not limited to in-vitro studies (e.g. Madhi et al., 20212 and Kustin et al., 20213). Please update and include clinical findings. Also summarise what we know about protection against B.1.1.7.  “All three variants share…; B.1.1.7 can also be…”: I would suggest that the authors first describe B.1.1.7 that can be detected by TaqPath tests (with some more background context, as this is primarily happening in UK and not necessarily recognized by the wider audience) and then go on to a discussion of potential detectability of other variants (because this is only a hypothetical scenario so far in my understanding, as opposed to detection of B.1.1.7). Also, would there be any data on the rollout of these variant-distinguishable tests worldwide?  “Samples testing positive…”: This needs more context. Why is authorisation going to be an issue and why can re-screening bypass it?  “as is true for SARS-CoV-2 – but not yet the variants – in many regions”: I feel this is unclear. TTI capacity would be overwhelmed when the overall caseloads are high, even if the variants account for a very small fraction of them. It should be made clear if this indicates contact tracing would only target variants distinguished by the (variant-specific) tests.  Method, “child cases” may be interpreted as cases that are children. Secondary transmissions?  Results, “In the absence of contact tracing, identification and isolation of symptomatic cases alone reduced Reff by 0.2 to 0.3…”: I couldn’t read this from the top rows of Figure 1. This may correspond to 0% of cases sharing data or 0% trace success probability, but Reff for such a scenario cannot be read from the figure because there is no colour scales or numbers.  “When identification and isolation…substantial effects.”: I am not sure how “moderate levels” and “substantial effects” are defined.  “Due to the exponential growth of uncontrolled epidemics…over a given timespan”: As stated above, this is only the case if contact tracing can continue without hitting the capacity. If R goes back to the original level after tracing is overwhelmed, there may be only a marginal difference in the final epidemic size.  Discussion, “Higher rates of cooperation…quarantine and isolation”: related to the first major comment, these efforts would make tracing more effective but require a substantial amount of effort and cost, and warrant discussion.  Please update references. Many of the preprints cited here have now been published in peer-reviewed journals, which might include more up-to-date information.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",1775,1,3,0.8027000000000001,0.1066198131,0.8703778982,18,35.17,0.2522,f1000,0.0096153846153845,4,4,5,1,factual,3,5,84,neutral,5,negative,5,none,5,4,5,5,factual,5,5,95,polite,5,neutral,5,low,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,moderate,5,5,5,5,factual,5,5,5,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
179,Tim-C.-D.-Lucas,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this study the authors use established and previously published models of contact tracing to examine whether targeted test and trace systems could suppress novel variants. The premise is sound; contact tracing scales poorly, so while it is not necessarily effective at control SARS-CoV-2 at large once national prevalence is high, the numbers of certain variants are still low in a number of countries and therefore contact tracing might be able to control those new variants as they are seeded into a country. Whether this approach would work or not is not trivially obvious and so this study is asking an important question with policy implications globally. The analytical approach taken is quite simple in that the authors assume (and back up with some literature) that the variants can be identified easily and that therefore contact tracing of a new variant can continue without any reference to the dominant variant.  Comments: Most of my comments relate to this assumption that contact tracing of new variants can be modelled by simply ignoring the dominant variant.  First, I would like to see this assumption explicitly stated in the methods just to make it completely clear to the reader.  There are a number of further considerations with this assumption that I think should be discussed.  Given the high rate of vaccination and previous infection with the original SARS-CoV-2 strain, many countries are now in a state where immunity cannot be ignored. This is all handled by Reff, but I think it needs to be mentioned that Reff is combining NPIs, immunity or partial immunity from vaccination (depending on whether there's vaccine escape in the variant)  and partial immunity from previous infection with other strains.  The authors state that new variants can be detected with RT-PCR and TaqPath. However, does this extra step create no extra delay in the process? I imagine this would depend on the specific organisation but might be worth considering and mentioning.  Furthermore, is this identification of variants 100% accurate? The false negative rate (someone is infected with a new variant but the test says they are infected with the original variant) can be just included as part of the test sensitivity and I wouldn't be surprised if the difference is fairly small. More worrying for me is the false positive rate (someone is infected with the original variant but the tests says they are infected with a new variant). This is important because the rationale for the study relies entirely on the fact that there are not many cases with the new variant in a country but if, say, the false positive rate (as defined above) is even 1% then the large number of original variant cases in a country will quickly lead to the targeted test-trace-isolate system being swamped. This effect will obviously vary with the prevalence of original variant SARS-CoV-2.  I only know the literature for the UK, but even the lowest compliance rates used here are much higher than those measured (I wouldn't be surprised if some countries have much high compliance rates though). I am taking my values from the reference below (Smith et al., 20201),  but there might be more up-to-date surveys in the UK and I don't know at all about other countries.  From self-reported behaviour (past behaviour, not intentions) in the UK, about 12% of people with symptoms requested a test. This relates to the 50% of symptomatic cases identified without tracing parameter. Some details of how you selected 50% from ref 32 would be useful, as the values in that paper range from 5% to 100% depending on the country and time. In the UK, of those contacted by track and trace, 11% of people fully complied with 2 weeks self isolation (this relates to the 50%-90% comply with isolation parameter). So at the very least I think it might be useful to state that these values might be quite optimistic in some settings.  Finally, a minor and subjective point, but it might be useful to present Figure 1 with a diverging colour palette that clearly distinguishes Reff < 1 and Reff > 1.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",828,0,1,0.7912,0.1146087604,0.9262154102,28,37.64,0.1733,f1000,0.0194174757281553,1,4,3,0,partially factual,4,4,80,polite,3,negative,4,moderate,5,5,4,5,partially factual,4,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
33,Anil-Mukund-Limaye,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This work is a comparison of paraffin embedded FNABs versus tissue blocks for the assessment of HR, and HER2/neu expression. This is an important piece of work. However, the study lacks depth. I have the following comments and suggestions. The manuscript requires language correction. Some sections need to be re-written as they were not clear to me.  Routine immunohistochemical method for assessment of HR, and HER2/neu has been used for a long time now. It appeared to me that invasive nature of core biopsies is a problem and the authors seek to evaluate the performance of paraffin embedded FNAB. The authors could have given an account of similar work upfront in the introduction. That could be followed by what this study particularly has to offer to provide new or additional insights. The literature on others' work could have been discussed.  Sample size is small. Hence the authors should be cautious in making conclusions about the outcome of the study, with particular reference to the data in tables 2 and 3.  The scoring method for HR and HER2/neu expression should be clearly stated.  Discrepancy between Table 1 and data described in the first paragraph of the Discussion section. Figure 1 shows 78% ductal carcinoma, 16% lobular, 4% mixed and 2% mucinous. However, in the cell block data discussed in the first paragraph of the discussion mentions- 74% ductal, 16% lobular, 4% mixed and 6% mucinous.  Similar data for the tissue block results were 64% ductal, 16% lobular and 14% mixed. However, I don't see any raw data for the same.  The expression of HR and HER2/neu was concluded as false positive or false negative based on the assumption that the data from the tissue blocks provided the true picture. However, the tissue block method may itself have a small but a certain rate of false positives and false negatives. Although, I do understand that specificity and sensitivity are evaluated based on certain truth, which the tissue block data is assumed to represent.  The authors have mentioned that "" the patients with lobular breast cancer tended to be younger"". What is the statistical basis for that? The F value (0.356) and the p value (0.785) does not suggest that this is true.  I did not find the relevance or utility of the second and third paragraphs in the discussion part of the manuscript.  How is the discrepancy (Table 3) calculated?  The authors have determined the specificity and sensitivity of the cell block method in reference to the tissue block method. However, there is no clear picture as to how the levels of expression been scored. In table 2, what is the cut-off score for each marker (HRs and HER2/neu) for categorizing the samples as positive or negative.  Having used the Cohen's kappa for assessment of the agreement between the data from two methods, what is the need for Spearman's correlation coefficient? Having good correlation is one thing, but whether the methods are agreeing well with the levels of expression of a particular marker is another. In this context the protocol for scoring is important. These issues need to be scrutinized by an expert statistician.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",674,0,2,0.7646000000000001,0.0853282682,0.8795065284,752,47.79,0.1507,f1000,0.0106382978723403,4,5,5,5,factual,5,5,100,neutral,5,negative,5,none,4,4,4,4,partially factual,4,4,80,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
144,Reviewer-4nHF,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",This article proposes a method for training physics informed neural networks (PINNs) when the distribution of measurement noise is unknown. The key idea is to learn noise distribution using an energy-based model on top of training of PINNs. A few numerical experiments show the usefulness of the proposed method. The usefulness of the method is shown by numerical experiments for a few example problems. There is little theoretical backing. Extension to high-dimensional and/or non-iid noises would require much heavier computation. Experiments are limited only to synthetic problems. Are there any practical problems that could be resolved by the proposed method?,100,0,0,0.7108,-0.0058928571,0.9032465816,51,35.0995,0.0945,iclr,0.010204081632653,2,4,3,2,partially factual,2,3,40,impolite,3,negative,2,moderate,3,5,4,4,factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,2,4,3,2,factual,4,4,60,neutral,4,neutral,4,low,3,4,3,3,partially factual,4,4,75,polite,5,neutral,4,low
125,Marco-Patruno,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In my opinion the author of this case report describes the results well, although I do agree with Elizaveta Kon that also including MRIs would have improved the quality of the paper. The real action of PRP is still under debate, and the scientific community asks for stringent methods and careful evaluations, even for a single case study. I suggest the author increases the number of patients and improves the quality of the results in future studies concerning PRP.",146,0,0,0.8156,0.0443027211,0.5150036812000001,421,17.51,0.2025,f1000,0.01,1,3,1,2,unfactual,1,1,45,neutral,3,neutral,2,extreme,4,5,4,4,factual,4,4,75,polite,5,positive,3,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,3,factual,4,3,60,polite,4,positive,3,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
159,Reviewer-5d1P,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","This paper questions the standard use of Adam-type optimizers in deep RL. The paper argues that solution methods in deep RL are best thought of as solving a sequence of optimization problems. And that the standard use of optimizers leads to ""contamination"" of the optimizer's internal parameters. The paper then proposes to reset the optimizer's internal parameters to fix this ""contamination."" Finally, the experiments on Atari show that resetting the optimizer's internal parameters leads to significant performance improvement. The main strength of the paper is that the key idea of the paper, resetting optimizer parameters at the beginning of each iteration, is simple and effective. I liked the general theme of the paper, i.e., we need to understand better the tools we borrow from other fields. The paper is well-written and easy to understand, making it accessible to a wide audience. The experiments in section 4.3 are performed on 55 Atarti games with ten seeds each, which might mean the results are statistically significant. Resetting seems to be beneficial for multiple optimizers like RMSprop, Adam and Rectified Adam.  The paper has two major weaknesses:
1. The paper claims at many points that a ""contamination"" effect plagues RL (for example, lines 107-109) and that many updates are wasted to unlearn the effects of the previous iteration. However, the paper does not describe what exactly this ""contamination"" means, and neither does it show the presence of any ""contamination."" All the paper shows is that there is a performance boost when we reset the optimizer's internal parameters. This performance boost is not direct evidence of contamination from iteration to iteration.

However, this weakness can be easily overcome. The paper first needs to contain a definition of ""contamination,"" maybe the authors mean that the internal parameters($m$ and $v$) are too far away from their true values at the beginning of each iteration. One way to measure this difference could be to measure the cosine similarity between the current value of $m$ and the true value of $m$. The true value can be measured by taking the gradient of all the samples in the buffer and taking steps using that gradient. A large difference between the true and current value of $m$ would mean contamination. The paper also suggests that this contamination is particularly large at the beginning of each iteration compared to a random time in the learning process. Again, this can be easily shown by showing that the difference in true $m$ and current $m$ is larger at the beginning of the iteration compared to any random time in the learning process. 

2. None of the results in the paper except the ones in section 4.3 are statistically significant. The paper only shows results for a single random seed. I should note that the authors are aware of this weakness (line 138). The best way to look at the current experiments in Sections 4.1 and 4.2 is that they are used to tune hyper-parameters for the experiments in Section 4.3. The authors might be limited in their computational resources, but in that case, it is better to present statistically significant results in smaller environments like MinAtar\[1\] than unreplicable results in a big environment. 

Other than these two main problems, there are a few other minor issues in the paper.
1. The update equations for Adam in Section 3 are wrong. Instead of using $m$ and $v$ for the final update, new variables $\hat{m}$ and $\hat{v}$ are used. See to the original Adam paper for the correct equations.  
2. Line 176 says that $K=1$ corresponds to vanilla gradient descent. But, that is not true. For $K=1$, the update is similar to the Rprop optimizer, not SGD. For $K=1$, the update only takes into account the sign of the partial derivative but not its magnitude. 
3. The value of $K$ is not properly tuned. In Figure 4, the difference between $K=1000$ and $K=500$ is insignificant. So, the optimal value of $K$ could be smaller. I suggest the authors also try smaller values of K, like 250, 125, etc. 

I like the ideas presented in the paper. However, I can not recommend accepting the paper in its current form in light of these weaknesses. 

\[1\] Young, K., & Tian, T. (2019). Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176.
 What would be a good definition of ""contamination""? No confidence intervals are reported for any experiment in the paper. I recommend the authors report the 95% bootstrapped confidence intervals for their results. \[2\] and \[3\] provide good guidelines for properly reporting experimental results in deep RL.


\[2\] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.
\[3\] Patterson, A., Neumann, S., White, M., & White, A. (2023). Empirical Design in Reinforcement Learning. arXiv preprint arXiv:2304.01315.

EDIT:

I have updated my score based on the new results provided by the authors.",832,9,10,0.7234,0.1063169287,0.8891925216000001,215,50.6545,0.1044,neurips,0.0113636363636363,5,5,5,5,factual,4,4,95,polite,4,neutral,5,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,5.0,positive,5.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,5,4,5,partially factual,4,4,88,polite,5,neutral,5,low
159,Reviewer-Ks5C,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper studies optimization in value-based deep reinforcement learning. The key insight is that when using target networks for action-value function training, changes in the target parameters yield a change in the optimization problem the online parameters are solving. Because of that, the authors argue that preserving the adaptive optimizer statistics (e.g. of Adam) might or might not be desirable. The paper then studies the effect of resetting the optimizer state after (hard) target updates mostly using the Rainbow algorithm on Atari games as a testbed yielding a slight positive aggregate improvement. The main strength of the paper is the simplicity of the contribution; the paper is well-written and easy to follow, and the method is motivated and described well. The experimental protocol is solid: it uses the full set of 55 Atari games and a standard Rainbow implementation. The main weakness of the paper is the mixed empirical results. Granted, the median human-normalized performance improves from ~1.75 to ~2.25, however, per-game effects from resetting the optimizer are highly heterogeneous, yielding performance deterioration in ~14 environments. The soundness of the paper could have been higher if, at least, an explanation (supported by evidence) for the negative effects was given. Target network parameter updates indeed change the loss landscape that the online parameters are navigating. In addition to that, updating the replay buffer changes the distribution of inputs and hence the optimization problem for online parameters. Do you have ideas on how an optimizer could be changed to adapt to the input shifts? “We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration.” (L9) The reviewer didn’t find an empirical verification of this assumption.

Many deep RL algorithms use moving average target updates after each step instead of periodic hard updates. The authors demonstrate preliminary evidence that in soft actor-critic that uses such a practice, the optimizer resets do not improve the performance. Having said that, the reviewer appreciates the transparency about the negative results.

Again, one of the limitations is that in some environments resetting the optimizer yields negative results. It implies that a better alternative could be triggering the optimizer reset using a criterion (e.g. based on a measure of the loss landscape change / by performing a lookahead and assessing whether the reset was helpful)",397,0,0,0.7723,0.0420385675,0.8665425777,215,31.2684,0.2101,neurips,0.0505050505050505,2,3,2,3,factual,3,2,65,polite,4,negative,4,low,4,5,4,4,factual,4,4,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low
91,Reviewer-CubV,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This work suggests a new framework to train multi-task learning (MTL) models that try to find a 'flat region' in the loss landscape. This is based on Sharpness-aware Minimization (SAM) by Foret et al. (2021), which was shown to reduce overfitting, and therefore could increase generalization performance across MTL tasks. The algorithm is based on solving a min-max optimization problem using Taylor expansion and gradient aggregation. Theorem establishes a generalization error bound. Experimental results on MTL computer vision tasks are provided. This is a well-rounded paper. It's an extension of SAM by Foret et al. (2021), but the application of SAM to MTL is well-motivated. The algorithm is simple and easy to understand, and the derivation in sections 4.3-4.4 is clear. Authors present both theoretical and experimental analysis. I also appreciate that the authors uploaded their code for reproducibility, and provided detailed explanation for their experimental setup as well as interpretation of the results. Please see questions below. 1. The paper lacks a critical discussion on the limitations of this method. For example, is the method computationally efficient?

2. Are there standard deviations or statistical test results reported for Tables 2-4? It's not clear how significant some of these improvements are, e.g. 75.13 vs 75.77 in Table 3 PCGrad.",209,2,5,0.8136,0.1113131313,0.9075129032,52,43.6244,0.8355,iclr,0.0235294117647059,3,3,2,3,factual,3,3,65,polite,3,neutral,3,moderate,5,5,4,5,5,5,5,90,5,5,5,5,none,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
59,Reviewer-4k11,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","This paper proposes a black box watermarking scheme for costomized LLM. In particular,  the authors propose to construct two sets of poisoned data to inject the watermark during the tuning, where the trigger set produces the wrong judge answer and the reference set produces the correct answer. Compared with the naive judge question based watermarking scheme, the authors propose to take spacial character patterns to trigger the wrong output to improve the uniqueness. 1. The design of reference set to complement the trigger set is interesting.
2.  The overall presentation is easy to follow. 1. Lack of teachnical contribution. This method is an improvement of the naive judge question based watermarking. The overall process is still naive, which lacks theoretical or technical contents.
2. Lack of introduction of related work. Various black box model watermarking schemes have been proposed recently, including LLM watermarking, while the most recent model watermarking scheme cited in this paper is published in 2019.
3. Since the authors mention several times regarding the efficiency, it should be evaluated to justify the advantage of the proposal. This is unfortunately not seen in the experiments. 
4. There is no quantitative comparison against the naive approaches or the existing black box LLM watermarking schemes. see weakness.",207,0,7,0.7345,-0.0369565217,0.7950327396,48,43.1339,0.0999,iclr,0.0,0,4,2,0,unfactual,2,0,44,polite,2,negative,3,high,4,3,3,4,partially factual,3,3,55,impolite,5,negative,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,3,4,70,neutral,5,negative,4,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
168,Reviewer-tePx,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","The paper at hand proposes a method for domain adaptation by including some labeled data from the target domain. A ""triplet alignment"" is introduce which aims for aligning feature distributions as well as minimizing classification error. + relevant problem - The paper is quite hard to read and understand. Figures are rather small. Honesty speaking Fig. 1 even confused me more than it helped me to understand the approach.
- Experimental results are hard to interpret and judge. If I read it correctly, the effect of data augmentation seems significant. When comparing without data augmentation  (ours* in Tab. 1) the advantages over previously proposes approaches seems marginal (if at all). I also miss confidence intervals. - What are clear advantages of the approach -- e.g., the claim that ""data augmentation is not nessaccary for our approach"" (besides still having a significant impact) is not well motivated.
- What are limitation of the approach?",153,0,1,0.8190000000000001,0.0409090909,0.9198144674,49,44.2428,0.1932,iclr,0.0202020202020202,2,3,2,3,partially factual,2,3,50,neutral,3,negative,3,moderate,3,3,3,2,partially factual,3,2,55,neutral,4,negative,3,moderate,2.0,3.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,2,2,3,2,partially factual,3,3,50,neutral,4,negative,3,moderate,2,3,3,3,partially factual,3,3,65,polite,4,neutral,4,low
115,Reviewer-HXMa,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The paper examines the causal and moral judgments made by large language models (LLMs) and their alignment with human intuitions. To do this, the researchers created a dataset of stories from 24 cognitive science papers, annotating each story with factors that influence people's judgments, such as norm violations and the avoidability or inevitability of harm.

The authors find that, on an aggregate level, the alignment between LLMs and human intuition has improved with newer models. However, statistical analyses reveal that LLMs and humans weigh these factors differently when making judgments. 
 
**Originality**: The paper presents an interesting approach to evaluating large language models (LLMs) by testing their ability to handle tasks related to causal judgments and moral permissibility. The authors have transcribed stories from various papers and used them to test the LLMs, focusing on several factors that influence people's causal judgments and moral dilemmas. This approach is original and provides a new perspective on the capabilities of LLMs.

**Quality**: The authors have meticulously transcribed stories from a number of papers, ensuring a wide range of scenarios for testing the LLMs. They have also collected responses for each story from a crowd-sourcing platform, ensuring a diverse set of responses for analysis. 

**Clarity**: The paper is written in a clear and understandable manner. The authors have explained their methodology and the factors they focused on in a detailed and comprehensible way. 

**Significance**: The paper contributes to understanding how LLMs handle complex tasks related to causality and morality. This is an important area of research, given the increasing use of LLMs in various applications. The insights gained from this study could be useful for improving these models in the future. The paper also opens up new avenues for research in this area.
 There are a few areas where it could be improved:

1. **Evaluating with more models**: The paper is a bit skewed towards OpenAI models (GPT3 and beyond) for the evaluation. Including more diverse models could provide a more comprehensive understanding of how different LLMs perform on the tasks. This could also help identify whether the observed behaviors are specific to these models or are more generally applicable to LLMs.

2. **Comparing with human performance**: While the paper does a good job of comparing the performance of different LLMs, it does not provide a clear comparison with human performance. This makes it difficult to assess how close the models are to human-level performance on these tasks. Including a human baseline could provide a more meaningful context for the results.

3. **Analyzing incorrect predictions**: The paper could benefit from a more detailed analysis of the models' incorrect predictions. This could help identify common patterns or biases in the models' errors, which could provide insights for improving the models.

4. **Generalizability of findings**: The paper's findings are based on a specific set of stories and tasks. It's unclear how generalizable these findings are to other tasks or domains. The authors could address this by testing the models on a wider range of tasks or by discussing the limitations of their approach in more detail.
 1. **Evaluating with more models**: The paper primarily focuses on GPT-type models and its variants. Could the authors elaborate on why they chose to focus on these models? Would the inclusion of other language models provide different insights? 

2. **Comparing with human performance**: The paper lacks a clear comparison with human performance. Could the authors provide a human baseline for these tasks? This could help in understanding how close the models are to achieving human-level performance.

3. **Analyzing incorrect predictions**:  The paper could benefit from a more detailed analysis of the models' incorrect predictions. Could the authors provide more insights into the common patterns or biases in the models' errors? This could potentially help in improving the models.

4. **Generalizability of findings**: The findings of the paper are based on a specific set of stories and tasks. Could the authors discuss how generalizable these findings are to other tasks or domains? 

5. **Interpretability and transparency**: The paper presents an analysis of how LLMs reason about causality and morality. However, it's not clear how these insights can be used to improve the interpretability and transparency of these models. Could the authors provide some thoughts on this?
 The paper addresses the limitations of the work and potential negative societal impact, although not in a dedicated section. The authors acknowledge that their focus is narrow and only on certain aspects of alignment with humans. They caution that their work should not be used to make sweeping and general statements about AI-human alignment. They also note that their moral permissibility task is not a certification task and should not be used as a flat benchmark to beat.

The authors also discuss the ethical considerations of their work, emphasizing the importance of assessing implicit intuitions underlying commonsense reasoning abilities in large language models (LLMs), especially in cases related to morality. They acknowledge that even if a model is not explicitly given the responsibility to make moral judgments, these judgments can appear across many forms of freely generated text. They also recognize the potential for replicating human biases in LLMs and state that this is something they would want to avoid.

In terms of potential negative societal impact, the authors don't explicitly discuss this. However, they do acknowledge the potential for misuse of LLMs and the ethical considerations that come with their use. They also discuss the importance of transparency and consent in their data collection process.

",911,0,8,0.7597,0.1167226452,0.9537047744,215,39.0988,0.1041,neurips,0.0,4,5,4,5,partially factual,4,4,87,neutral,4,positive,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,3,low
115,Reviewer-7W7v,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","
This paper investigates to what extent LLMs can align with human intuitions when making causal and moral judgments. To do this, they collected a dataset of stories from 24 cognitive science papers and created a causal and moral judgment challenge set. They evaluate different LLMs about their alignment with humans and reveal that the implicit preferences can be different even for LLMs trained with the same technique. They find that increasing model sizes actually impact those models’ aggregate-level alignment differently.
 With the wide spread of LLMs, to understand the alignment between humans and models is an important topic. In this paper:
- They have provided a dataset to understand the human-model alignment, especially in the causal and moral judgment perspective.
- The resources are from cognitive studies which makes it more reliable than the normal text resources.
  The paper wants to analyze the alignment between humans and models, however it lacks some description of how they conducted the human study.
 - For the factors in Table 2, are they from existing literature reviews or summarized by the authors? 
- For the dataset, are those factor labels annotated by the authors or by original cognitive scientists? 
- For the human participants, do you have any criteria to select who can participate in the survey?
- Did you educate the participants about different factors before you conduct the survey?
- It seems only part of Fig.2 is visible on my side. Please check that. 
 NA",243,0,2,0.7673,0.07625,0.9360769987,215,44.2496,0.1375,neurips,0.0099009900990099,1,4,1,0,unfactual,3,4,20,polite,2,positive,1,high,3,4,4,4,partially factual,4,4,75,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
31,Brian-Van-Wyk,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study reports on viral load monitoring at 6 months for of children and adolescents who were initiated on HIV treatment in a tertiary hospital in Bulawayo. The study is important because of the HIV epidemic in Zimbabwe, and the need to reach the third 90 of UNAIDS 90-90-90 targets. The methodology is sound and clearly reported on. Appropriate statistical analysis is done, and these are aligned with the objectives of the study. Few other sociodemographic and clinical factors were collected and analysed; which is a limitation to the study. This should be indicated.  In the discussion, enhanced adherence counseling is mentioned as being implemented in the hospital. However, little information on this is provided in the background. Also, it would be useful if the analysis could report on how many of the current cohort received enhanced adherence counseling.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",282,0,1,0.7397,0.1770833333,0.8287450671000001,43,28.23,0.0999,f1000,0.0098039215686274,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,4,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
95,Reviewer-Qkvr,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","- The paper studies the problem of graph classification with scarce labels. The authors propose a semi-supervised graph classification method called KDGCN, which consists of two GCN modules. The first GCN module obtains feature vectors for each graph through a readout operation. Then, the authors construct a supergraph using graph kernels. The second GCN module employs a semi-supervised approach to learn meta-node representations on the supergraph, capturing sufficient structural information from both labeled and unlabeled graphs. Typically, semi-supervised methods based on graph contrastive learning result in complex models and intricate hyperparameter-tuning. However, the method proposed by the authors has fewer hyperparameters and is easy to implement. - The paper is overall easy to understand.
- The idea of constructing a supergraph is novel and interesting.
- When graph labels are extremely scarce, the proposed method has shown some improvements on certain datasets. - The section about supergraph construction mentions using a predefined similarity threshold (τ) to determine the existence of edges, but it does not explain how to select this threshold.
- While the experiments demonstrate that the WL subtree kernel performs well in certain cases, should the paper provide a more detailed comparison and analysis to explain why this kernel was chosen over other possible kernels? - Can more information be provided to explain the structure and properties of the supergraph and how it impacts the method's performance?
- I am concerned about the limitations of the proposed method and its potential application scenarios. Additionally, is the complexity of the proposed method scalable on large datasets?",257,0,0,0.7852,0.1634920635,0.9249551296,51,34.3764,0.063,iclr,0.0,3,4,3,3,partially factual,3,3,70,polite,4,neutral,4,moderate,4,5,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,4,4,85,polite,5,positive,4,low
7,Reviewer-GnyW,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","The paper proposes a novel tree state abstraction function (PTSA) for use during MCTS. The primary contributions of the paper are algorithmic and empirical. The key idea involves aggregating paths in the tree if their Q-values (as probabilities) along the path closely match an existing path with the same parent node. An analysis of the abstraction quality and error bounds are included. Experiments on Atari and Gym environments show that a recent MCTS variant leveraging PTSA outperforms a number of strong baselines.

UPDATE: I thank the authors for their detailed response. After reading the other reviews and comments, I'm more inclined to recommend acceptance and have updated my score to reflect that. + The paper tackles an important problem of accelerating MCTS search. It does so using tree state abstraction. The approach is intuitively clear and is also supported by analysis. 

+ The paper proposes a novel tree state abstraction function based on path transitivity. The abstraction function is based on the difference in the Q values of the nodes (converted to probabilities) in the path. Although important implementation details are not clear to me, the intuition that abstracting entire paths accelerates search makes sense as does the abstraction of only the most recent path during search leading to smaller trees during online search. The paper is accompanied by analysis showing the correctness of the approach and an error bound under certain conditions. Overall, the algorithm seems to have high novelty.

+ The experiments are conducted on a number of Atari and Gym environments. Sampled MuZero with the proposed abstraction (PTSA) outperforms a number of strong baselines by a significant margin. The implementation seems to work very well. This seems to be a new state of the art in state abstraction for modern MCTS variants. - The approach is intuitively clear and seems to perform well empirically, which increases confidence. However, I found the description of the implementation details of Algorithm 1 difficult to follow. Please consider including a more careful description of the implementation in Section 4. The issue is exacerbated by the absence of code. This is currently preventing me from giving the paper a higher score.
  - For example, the actual implementation of the algorithm in L15 of Algorithm 1 is unclear to me. I expect it to involve Eq 5 with some value of $\alpha$ like 0.7. But Eq 5 returns a real-valued prob estimate for a path pair (b_i, b_s). How is that turned into a boolean value (True / False) in L15? It's probably not real-valued equality. This is a key detail so please explain.
  - There are a number of other implementation details that are difficult to find or missing. See the questions for examples.

- Given that the primary contribution is algorithmic and empirical, I'd have hoped to see the source code included. Reproducibility is going to be challenging without it and since this paper seems to establish a new state of the art, I'd encourage the authors to release their code. - I had a number of questions about the exact implementation
  - What exactly is the implementation of the branching condition of L15 of Algorithm 1? How does it relate to Eq 5 and 6 which is defined as a function taking two inputs (b_i, b_j) and returning a probability?
  - What exactly is learned during offline learning vs online search? $d, v, p$ are likely learned offline. What about the abstraction function $\phi$? This seems online to me. Correct?
  - What is $l$ set to in the implementation? How does it value impact performance?
  - What is the implementation of the pruning function in L17 of Algorithm 1?
  - How are the legal actions for the abstracted state computed?
  - What is the size of $S_L$? How was it chosen? How does varying it affect performance?

- As described in L346-349, there seem to be a number of choices for the designer to make. These are not clear to me at the moment besides the obvious ones (e.g., $\alpha, N$). Please enumerate what exactly needs to be hand-designed or set manually for a given domain and what can be used off-the-shelf.

- Is there a reason to not include code? Will code be included in the final version? Yes",710,0,1,0.7654000000000001,0.1208551788,0.9153474569,216,53.4069,0.8231,neurips,0.0,5,5,4,5,factual,5,4,90,polite,5,positive,4,low,5,5,5,5,factual,5,5,85,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
153,Vincent-Young,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,0,1,0,0,factual,0,0,10,neutral,0,neutral,0,extreme,4.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,3,0,0,factual,1,1,20,polite,1,neutral,0,extreme
70,Sabine-Pahl,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title and Abstract: This is fine. I have some comments on the comparisons and causality below that the authors should consider.  Article content/ Conclusions: The article is well written and overall clearly structured. Using the PERMA model is a good addition. The specific research questions are very helpful in communicating the research. Nevertheless I have picked up two issues that are worth considering, and a few minor comments.  I wasn’t entirely convinced by the research question looking at managers’ perception of volunteer well-being. Why is it important that these correspond (p. 3)? Even if it is important, as far as I understand, the comparison is not straightforward. The volunteers rated by the managers are not the same people as rate their own well-being, are they? So the conclusion of non-correspondence is problematic, if you’re comparing the rated/perceived well-being of *different* people.  My second issue is around the language/interpretation in the article, specifically in the Implications section. You seem to assume these are causal effects i.e. the volunteering causes people’s improved well-being (and therefore it should be used more widely). But it’s not quite that straightforward, as you haven’t allocated people to different activities so there might be other differences between people who walk vs. volunteer for example, that could account for any effects you find. You can only make strong inferences about causality when you use a proper experimental research design. It would be good to note this in the discussion. (I think only the Wyles et al. article has tried this in the volunteering literature). You mention also that choice is important, which is a related consideration. This is where recommendations are a bit tricky, because you can’t (by definition) force people to ‘volunteer’ even it is good for them, and there may be selection effects that mean happier / healthier people are also the ones who do environmental volunteering. This is not a big problem but I feel should be acknowledged.  Minor points: I think a lot of space is dedicated to the different factor analyses (on pages 7-11) to establish questionnaire structure. While this is important and good practice it is not linked to any of the main research questions. Therefore I was wondering if (some of) this should be presented in an Appendix rather than the main text, as it distracts from the key questions and findings.  On p. 18 literature on the amount of time spent volunteering is reviewed but this all seems to be published in gerontology journals so I’m assuming uses older samples. Please add in the text if that’s the case. Data: Links to raw data are provided.",503,0,0,0.806,0.1353869896,0.8656298518000001,75,46.06,0.1441,f1000,0.0,2,4,3,3,partially factual,3,2,70,neutral,4,neutral,3,low,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,3,low
155,Paul-Groth,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper describes an approach for reengineering Model View Controller (MVC) applications such that they expose Linked Data. The approach, named EasyData, focuses primarily on web applications. Summarizing the approach, the idea is to modify the Model of a typical web application (implemented typically by an Object-Relational Mapper) to also output data according to web vocabularies, to modify the View so that data is presented with RDFa / Microdata, and to modify the Controller such that the APIs are Linked Data compatible.  To help developers perform reengineering, two packages were developed for two popular web frameworks Ruby on Rails (EasyData_Rails) and Django (EasyData_Django). In terms of evaluation, the Rails package was applied to Redmine (redmine.org) an open source project management application. Secondly, software metrics were calculated for EasyData_Django and compared to 5 other software packages (e.g. Stanbol, D2Rq) that are also designed to help create Linked Data exposing applications.  First, I think this is an important problem to address. It’s not always straightforward to make applications Linked Data compatible and providing packages that focus on common development environments is a good one. The authors do a good job of defining the research methodology they are using from Oates. But I would have liked to see more details of what each of the steps actually required. Adding an additional paragraph describing what each of these steps require would be helpful.  There are two areas where the paper/tools need to be improved.  1) Evaluation I liked the approach of using a complex case study that’s open source, but the outcome of the application to redmine was not shown. What did the resulting project management software do? Figure 3 shows a service API but the namespace is published in example.org. It would be good to provide a place to download the updated version of the software and screenshots in the paper of what the results of the application of EasyData look like.  Again the authors provide a unique approach to evaluation with the application of software quality metrics. I really liked this approach. But it’s unclear why this validates the EasyData reengineering approach. This just says something about the EasyData software quality. While an interesting finding, the link is not made clear. Also, because EasyData Django is newer code one could argue that it will show less code complexity and code density than older software such as D2RQ.  What would have been of interest is a comparison of the software quality of software constructed using the multiple different approaches. Essentially, answering the question does the EasyData approach lead to better software than other existing approaches.  2) Software usability / evidence impact A key question for a Tools paper is the impact of the tool. Currently, no evidence is provided for large scale uptake. Another way to measure the potential of use is the quality of software itself. While the software metrics present give some indication of that, I think a key part of tool uptake is software usability this includes documentation. I would have liked to see a small tutorial at the GitHub repo which would have allowed someone to apply the approach to a small django app.  Overall, I think there is more work to be done in providing evidence for the tool and methodology’s applicability but there’s definitely something here.  Minor comments: - “Web of data” —> “Web of Data” - define LD at first use. - Look at combining footnotes 1 & 2 as footnote 2 relies on footnote 1’s definition. - You should provide a link to the redmine application website. - In the introduction, it is unclear the Linked Data (LD) principles that are being referred to. I assume it’s the classic Berners-Lee design principles https://www.w3.org/DesignIssues/LinkedData.html but Hausenblas is cited. )",623,1,0,0.7832,0.159952381,0.8893129826,43,46.67,0.0622,semanticweb,0.0,4,5,4,4,factual,4,4,91,polite,5,positive,5,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,moderate,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
155,Christoph-Lange,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper presents EasyData, an approach for reengineering legacy web applications to make them publish linked data.  The model, view or controller components of existing applications can be extended to publish linked data.  EasyData has been implemented for the popular Ruby on Rails and Django web application frameworks.  A comparison with other state-of-the-art linked data publishing platforms w.r.t. several software metrics shows that EasyData performs comparatively well. Let me first address the specific review criteria for a tool/system report: (1) Quality, importance, and impact of the described tool or system: Recency is also a quality criterion.  EasyData_Rails was last updated almost 5 years ago, EasyData_Django 3 years ago.  Other than README files and a few comments in the source code, I don't see much documentation.  Looking at the repositories, there are no signs of activity: few contributors, no issues, no forks (other than your own ones).  I don't see evidence that anyone has used EasyData, except yourself for adding linked data support to the Redmine project management tool.  Also, this extension of Redmine just seems to exist as an example within the scope of this article; I don't even see where it can be downloaded. (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool: The paper itself is largely well-written.  Section 1 clearly states the importance of adding linked data support to web applications.  Section 2 gives a well-structured overview of existing reengineering approaches, with a focus on the model-view-controller (MVC) architecture.  One issue here is that MVC is rarely applied in a pure way.  Can you also address variants or partial implementations of MVC?  In Section 3, the capabilities of EasyData are presented clearly.  The evaluation by a proof-of-concept of adding linked data to Redmine, and by a comparison with other approaches, in Section 4 is comprehensible – but, and that's the weakest point of the paper, it could be a lot more convincing – that's basically what I require as ""major revision"".  Except for one minor observation, no lesson learned from Redmine is presented.  Also, I'm not convinced by Table 1.  Linking instances of a web app's data to linked data resources would have a much larger impact than linking the limited terminology of a software to DBpedia.  Regarding the comparative evaluation it is not sufficiently clear whether the competing linked data reengineering approaches are actually comparable to EasyData w.r.t. the chosen software metrics.  Competing tools might have a much broader functionality, and might thus have a larger code case while at the same time suffering from more vulnerabilities.  Any observations about EasyData should therefore be interpreted in relation to its small code base. For further details please find an annotated PDF with detailed comments at https://www.dropbox.com/s/ka2y2yvlpvd94jg/swj1681.pdf?dl=0.",464,1,0,0.8128000000000001,0.0626930502,0.9306222796,116,36.49,0.3587,semanticweb,0.0,2,5,3,2,factual,3,4,78,neutral,4,negative,4,low,4,5,4,4,partially factual,4,4,75,polite,5,negative,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
192,Giorgos-Stoilos,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","The paper presents a benchmark and an evaluation of storage systems for linked data. The paper attempts to compare different storage models and approaches for RDF (linked) data like well-established and modern RBBMs, graph DBs, and triple-stores.  Although the proposed topic is indeed quite important (contrasting all these different storage models) is important and interesting and quite some engineering work has been conducted in collected data and evaluation various systems, in my opinion the paper does not succeed in providing sufficient fundamental or scientifically deep comparison or results.  On the one hand, the proposed benchmark is ill described. There are no details about the queries that have have been designed (how they have been designed, how large or complex they are, how many joins, etc.). A similar description is missing about the data model and complexity of the RDF dataset. Is the dataset highly interconnected or is it just small disconnected parts? On the other hand, with the massive number of RDF and SPARQL benchmarks out there and the evluations that have been conducted it is very difficult to justify why this is not just another collected dataset together with some queries and to show originality and novelty. Another key issue missing is a description about how the RDF data were converted and stored in the RDBMS and MongoDB. This is an important issue that needs a better description. It is especially important for MongoDB since key-value pairs are significantly weaker than triples. In the evaluation one should also present the number of tuples returned by each system. Perhaps I missed it but are they all returning the same number of answers for all queries. Comparing RDBMs systems with triple-stores is slightly unfair in the sense that the latter are also supposed to perform some kind of RDFS-reasoning either at loading or at query time (or at least they are supposed to be able to query interconnected graph-like data) hence it is not surprising that the RDBMs systems are faster. Especially if the dataset is quite loosly interconnected and the data can be easily mapped to the relational model then this is indeed the case. Overall it is not clear what are the results of this experiment. If one required some kind of RDFS reasoning then definatelly the RDBMs systems would be useless (even though faster) but if one does not need any kind of reasoning then obviously the RDBMs systems are the choice to go. It is not clear to which figures the observations in section 4 are referring to. Some conclusion is made but it is hard figure out out how and why this conclusion is produced. It would be good to add pointers, e.g., Fuseki did this (see Fig 4 (x)). Equation on page 6 should be clarified and made more precisely. What is the difference between queryscenario and testseries?",473,0,0,0.7558,0.0981728778,0.9203384519,167,50.57,0.0448,semanticweb,0.0,4,4,3,4,factual,3,3,75,neutral,4,negative,4,low,4,3,3,4,partially factual,4,4,65,neutral,4,negative,5,moderate,1.0,3.0,3.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,4.0,moderate,3,3,4,3,factual,3,4,70,neutral,5,negative,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
117,Reviewer-MDsd,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","Offline reinforcement learning (RL) suffers from the extrapolation error. There are numerous model-free and model-based offline RL algorithms that aim to tackle this challenge. Among them, model-based offline RL algorithms often learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, such quantifications are often inaccurate. This paper addresses this issue by training bidirectional dynamics models and rollout policies, and design a conservative rollout method that selects those synthetic transitions with the smallest reconstruction loss. The authors provide some theoretical analysis of their method and build their method upon some off-the-shelf model-free offline RL algorithms. # Strengths

The strengths can be summarized below:

- this paper is well-motivated, and the whole paper structure is clear

- the logic flow of this paper is clear, and it is easy to follow and understand

- the authors provide theoretical analysis to support their method # Weaknesses

Despite the aforementioned strengths, this paper has some flaws in novelty, empirical evaluation, and theoretical analysis. Based on these considerations, I can confirm that this paper is clearly under the acceptance bar of this venue. Please see the detailed comments below.

- (major) The core idea presented in this paper is NOT new. A highly relevant paper is published previously \[x\]. In \[x\], the authors also train bidirectional dynamics models and bidirectional rollout policies for offline data augmentation. Thus, the technical parts of this paper have a huge overlap with \[x\], making the contribution and significance of this paper quite weak. The differences are, that this paper selects the transitions with reconstruction loss while \[x\] selects reliable transitions via the proposed double check mechanism. It is doubtable whether the data selection approach adopted in this paper is better than the double check method, as intuitively, the reconstruction loss may not be reliable for forward/backward horizon larger than 1 (where no true next/previous states are available)

\[x\] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. NeurIPS 2022.

- (major) The empirical evaluations are limited and somewhat weak. The baseline algorithms this paper adopts are very old. It is somewhat confusing why the authors only choose to compare against these very weak algorithms. More advanced and recent offline RL algorithms ought to be included as the baselines (e.g., TD3BC, IQL, Decision Transformer, LAPO, etc.). The authors build their method upon CQL, BCQ, and BEAR. Can your method benefit more advanced offline RL algorithms?

- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper does not include any signal of statistical significance, e.g., std, IQM. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.

- (major) The theoretical analysis is also not new. Similar techniques are adopted in the MBPO paper. Specifically, one online model-based RL algorithm BMPO \[y\] theoretically shows that the error of the bidirectional models is smaller than unidirectional models, making the theoretical insights of this paper less appealing and unsurprising.

\[y\] Bidirectional model-based policy optimization. ICML 2020.

- (minor) The authors ought to specify the version of the D4RL datasets they use in the paper. In Table 1, your evaluated scores in halfcheetah-medium-expert are questionably low, why is that?

- (minor) This paper does not do a good job in the related work part, the authors include too few recent offline model-based/model-free offline RL papers Please refer to the the weaknesses part.",603,0,3,0.7867000000000001,0.0567165212,0.9565235972,53,32.288,0.3178,iclr,0.0,4,4,4,5,factual,4,4,90,polite,4,negative,4,moderate,5,5,4,5,factual,5,5,85,polite,5,neutral,5,moderate,1.0,4.0,4.0,2.0,partially factual,1.0,2.0,60.0,neutral,3.0,negative,3.0,low,4,5,5,3,factual,3,4,75,polite,5,negative,5,low,3,4,4,4,factual,4,4,85,neutral,5,negative,5,low
63,Reviewer-osUg,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper studies the problem of training a two-layer ReLU network for binary
classification using gradient flow with small initialization on well-separated input datasets, i.e. datasets $(x_i, y_i)_{i \in \[n\]}$ with $ \min \frac{\langle x_iy_i, x_jy_j  \rangle}{\Vert x_i \Vert_2 \Vert x_j \Vert_2 } \geq \mu$.
They show that in time $O(\frac{\log(n)}{\sqrt{\mu}})$ all neurons are well aligned with the input data, which means that positive neurons show in the same direction as the positively labeled points (or have a negative scalar prouct with all vectors of the dataset) (and an equivalent result for negative neurons).
Furhter they show that after the early alignment phase, the loss converges to zero at a $O(1/t)$ rate, and the weight
matrix on the first layer is approximately low-rank. Numerical experiments are provided. All claims are proven and illustrations are supporting the explanations. The assumption that the dataset is well seperated is very strong. I don't see why one would use a neural network on such a dataset rather than linear regression. -",167,0,2,0.7745000000000001,0.0306565657,0.9448035359,49,46.2312,0.1633,iclr,0.0,3,3,3,5,partially factual,3,2,45,polite,4,positive,3,moderate,2,4,4,2,factual,5,5,75,neutral,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,none,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,low,2,4,3,3,factual,3,4,75,polite,5,neutral,4,low
176,Torsten-Hahmann,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","Review Summary: This paper describes the LOD that is developed as part of the OntoIOp registry. I'm confident that this will become an important linked data set in the future. While there is no doubt about the dataset's importance, improvements are necessary to make it easily accessible to a larger audience. The description of the dataset lacks sufficient clarity and detail to be useful to the novice user. The description of the dataset in Section 2 needs to be elaborated (adding detail and precision). Lists/tables and simple statistics could help address this issue (compare previous LOD papers in the journal). Furthermore, the figures need to be better tied in by explaining the depicted relationships and using them as examples in Sec. 2. The authors remain vague on the maturity of the dataset, which is a concern, though it might be less pressing once sufficient detail is provided. The current state (what is there, what is missing) should be stated more explicit. While some major rewriting/editing is necessary, I see no technical problems with the described data set. The raised issues about clarity/accessibility to the community at-large can be easily fixed. I support accepting this paper contingent on ""the lack of detail and clarity"" issue being addressed. More details on the 3 evaluation criteria: (1) Quality of the dataset.  I have no doubt that the relationships between the included logics and languages are correctly captured. However, the maturity/completeness of the dataset is an issue: as I understand it, not all mappings/relations between logics and languages are included yet. Be clear about which ones have been modeled and which are left for the future. As a side issue: While one cannot reasonably expect the dataset to ever be complete, some mechanisms for the inexistence of mappings/translations could be helpful to differentiate between non-mappability and incomplete knowledge. I'm not sure whether that is within the scope of the OntoIOp registry. (2) Usefulness (or potential usefulness) of the dataset.  The usefulness is not as clearly visible as would be desirable. Neither Hets nor Ontohub use the dataset, though potential future applications are hinted at. The authors do provide some example queries that help understand how the dataset may be useful by itself. (3) Clarity and completeness of the descriptions This is my chief concern. For a LOD description, I expect more detail than what is provided in Section 2. While the explanation of the provenance is sufficient, the explanation of what the dataset describes requires elaboration. This should be at a level that non-logicians can understand the basic ideas and use the LOD. For example, you need to explain the difference between logics and languages -- this will not be clear to most users (as often one language is associated with a single logic and vice versa). Also, a better explanation of the intuitions behind ""mapping"", ""translation"", ""serialization"", ""sublanguage"", etc. are needed. Explain why mappings/translations are modeled as types as opposed to binary relations.  The current scope of the LOD is a bit vague, some lists/tables to summarize the dataset would be very helpful: - explain the kind of items  (maybe each of the ""subdirectories"" of the URLs) from http://purl.net/dol/registry that are reflected in the directories in http://purl.net/dol/ - how many of each of the types of items and relationships does the dataset include? - list & briefly explain the kinds of mappings available, it wouldn't hurt to include the hierarchy of mapping relations from [13] - what languages and logics are currently included? Given the manageable scope of 29 logics, 43 translations, and 14 languages, it would be easily to list them in a table/figure. The figures could be more helpful by explaining what the depicted relations in Fig 1 and 2 are: most, I believe, are mappings (though I'm not sure whether sublanguage relations are mappings; at the beginning of Sec. 2 mappings are restricted to logics), but also serializations are included. Are the color coding of expressivity/decidability in Fig. 2 captured in the dataset? Some minimal working example would be very helpful: one (or more) logics with one (or more) languages and two serializations as well as mappings to other logics/languages and metadata (showing how VoID and SKOS are utilized). Lesser, though more general concerns about the described project/dataset: 1) The maturity/completeness of the LOD: the OntoIOp registry is still very much under development. While publication on the underlying research are very valuable, I'm note sure about the value of a description of the registry's LOD at this stage. It seems highly likely that the description will be outdated as soon as it is published. That defeats the purpose of describing the dataset to others for them to use/reuse. 2) ability for others to contribute: the purpose of the registry is to enable the community to contribute descriptions of languages, logics, and translations. However, for maintaining the registry, the authors propose to generate it automatically from Hets. This is counter to the desired openness: it would require others to first extend Hets instead of directly contributing to the directory/dataset. I personally think that the LOD should not be permanently tied to any specific software, which poses a significant barrier for the community to contribute. Other mechanisms for maintaining/updating the registry are needed.  Other things that need to be fixed in the final version: - given that the paper is less than 5 pages in content, the abstract is unnecessarily long. It includes much background information (2nd paragraph, 1st sentence of 3rd paragraph, last paragraph) that should better be placed in the main part. - p 4: last paragraph of Sec. 3 needs a rewrite to improve clarity - if possible, the wealth of technical terminology should be reduced to what is essential. This is not supposed to be a description of the entire OntoIOp project, but of the dataset only. You also need to more clearly separate and exlain differences between the DOL language, Lola vocabulary and the language of the OntoIOp registry at the beginning and clearly distinguish between what is a project (OntoIOp) vs. an artifact (registry, DOL, Lola) - I can't quite appreciate the relevance of the example on p. 2 as it only uses the language and syntax statements that relate to the registry. - The URl to Lola on p. 3 needs to be updated",1049,3,1,0.7569,0.0961049107,0.8967276216000001,35,44.85,0.8064,semanticweb,0.0309278350515463,4,4,5,4,factual,3,4,88,neutral,4,negative,4,none,5,4,4,5,factual,4,5,85,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,positive,5,low,3,3,4,4,factual,4,4,85,polite,5,neutral,3,low
176,Maria-Poveda,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper decribes a dataset for logics, translations and languages descriptions. In general, I find the dataset really interesting and promising for combining and integrating information from different ontology registries and translation between logics. For the organization of the review I will follow the dimensions established by the type of submission:  (1) Quality of the dataset.   One of the main shortcomings of the paper is that the SPARQL endpoint where one could try the queries in the paper or others is not explicitly referenced from the text nor in http://ontoiop.org/. It should be included in Table 1.  A VoID description of the dataset is claimed to provide metadata from the dataset in page 3 however I haven't been able to find it either. It would be nice to have a footnote with it or include it also in Table 1. Adding the dataset description to a dataset registry (for example http://datahub.io/) and providing the reference to the resource entry in the datahub would be also advisable.  In the text it is said ""the OntoIOp Registry, with LoLa being its main vocabulary, gets four stars"" and ""the OntoIOp Registry is unique in being a linked dataset covering the domain of OMS languages"" considering that the linked part of the 5-star ranking is precisely the 5th one these two sentences seems contradictory. Either the dataset is linked, being 5-star, or it should establish links to other dataset to be possible to claim the second sentence as for ""linked"". In general I would suggest reviewing the 5 star ranking and proof that the dataset is actually a linked dataset.  (2) Usefulness (or potential usefulness) of the dataset.   While thinking that the described dataset will be surely interesting and useful it would be welcome to read a bit more about motivation and potential uses apart from those in Ontohub and Hets. The current state of the paper gives me a feeling of the dataset were an ad-hoc development for these systems (Ontohub and Hets) and seeing some examples of uses out of this context would increase greatly the dataset value.  (3) Clarity and completeness of the descriptions.   Main concerns about clarity is the distinction between DOL and LoLa. It is not clear which ontology is used in dataset. At the beginning it seems like LoLa is the actual implementation of DOL for this dataset however in section 3 the URI of reference for LoLa contains ""dol"" and in the SPARQL query examples the prefix dol is used. In addition, the URI for LoLa gives a 404 errors (I tried to browse it several times in different weeks).   It would also be valuable including a diagram of the LoLa's main classes and properties as the current figures are example of instances from what I understand. --- Other comments --- Figure 2 is not referenced in the text. Is it nice to reference and describe within the text all figures and tables appearing in the paper. In the first query in page 5 the selected variable is ""?target-language"" that do not appear in the query, in the query body it appears ""?targetLanguage"" instead. I would like to see some concrete metrics about number of triples and outbound links to other datasets. The information about metrics in section 5 seems not clear about specific figures, see ""around three times as many triples as the core dataset"" Typo: Section 5 ""Thus, the expanded dataset has around three times as many triples as as.."" --> only one ""as""",579,2,0,0.7136,0.1711382114,0.8930797577,52,50.57,0.103,semanticweb,0.0,3,4,4,3,partially factual,4,3,77,polite,3,neutral,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
176,Mathieu-d’Aquin,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper presents the OntoIOP registry, which is a dataset based on an ad-hoc ontology for describing languages, the underlying logics, their serialisations and mappings between them. As a general comment, I thing the representation used is reasonably elegant, and I can see some value in having such a map of languages and logics available. However, it is very hard to extract, from the paper, how useful the dataset currently is, or what is its potential for impact. I also think that the a bit of additional work in improving access to the dataset, the scope of the content and the connections with external resources would help in improving and demonstrating the value of the dataset. In more details (1) Quality of the dataset The representation of the languages, logics and mappings seem reasonable. The authors argue that there is no other ontology covering these aspects, and I indeed don't know any myself. It would be good however to include more information, in the related work section, about some other metadata descriptions for ontologies/information resources, that overlap to an extent with the one presented here. For example, a clear explanation of what is added by the ontology compared to OMV or to the schema used by common ontology repositories would be useful. Generally, a more complete comparison with other works that are not intended for the same task, but that overlap (e.g. ontology repositories, VoID, etc.) would be useful. Although the information in the repository is modelled in a reasonable way, the content in itself is very small. That is not an issue in itself, but it certainly affects the usefulness as the scope of the dataset is very limited. One could argue that a dataset and a classification are different things, and that this is closer to a classification of languages. The paper mentions that their are links to other datasets included, but going through a few resources, I couldn't find any. More details about that would certainly be needed. Not directly related to the quality of the dataset, but to the ease of using it, it would have been good to also include other common forms of access to the data than resolving URIs to RDF, and a dump. A SPARQL endpoint as well as html documentation of the entities included (i.e. URIs resolving to human-readable documents too) would have been appreciated. (2) Usefulness The paper includes ideas about tools that could be using the dataset and an example query. This is interesting of course, but at the same time it is very hard to understand from what is written what is the real (current and potential) impact of the dataset. How much and how is it used currently? What is the demand for such information? How does the group plan to address this demand? The paper mentions sustainability, and honestly states that this is not a resolved issue. While this is understandable, and the case of many other datasets out there, it is also slightly worrying if the ambition for this is to become a reference point for others when describing resources related to languages, logics and their mappings. I can certainly see that happening, but again, as mentioned above, it would make the paper stronger if such an ambition was made explicit, with a clear view of how that might happen in the future if it has not done so yet. As an aside, I believe that this issue could be helped by extending the scope of the dataset a bit, importing from existing repositories of ontologies (TONES, BioPortal, Watson, etc.) their metadata and enriching them with information about the language/logics they rely on. This could certainly demonstrate a practical application of the dataset, and generate a valuable resource to go with it. (3) Clarity of the description The paper is reasonably easy to read, and besides a few slightly surprising formulations, it is well written in my opinion. As already described above, I think however that several sections (related work, usefulness, technical aspects and interfaces to the datasets) should be elaborated further.",677,0,1,0.7869,0.1082326007,0.9219363928,53,41.5,0.0743,semanticweb,0.010204081632653,3,4,3,2,unfactual,3,3,70,neutral,3,negative,4,high,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,3,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
57,Reviewer-Mh63,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The authors of this paper present an innovative methodology for out-of-distribution (OOD) detection. While existing methodologies typically involve the direct use of given OOD samples, this paper introduces a new approach that applies perturbations to OOD samples, allowing the model to experience a more diverse set of OOD samples during training.

The authors define these perturbations using an adversarial loss based on the uniform distribution loss commonly applied to OOD samples. This creates directional guidance for each instance, determining the direction of the gradient and thus defining the perturbations applied to OOD samples.

The results demonstrate that using perturbed OOD samples improves the performance of OOD detection across several key metrics, including False Positive Rate at 95% Recall (FPR95), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR), when compared to existing methodologies. This paper's novel approach of applying perturbations to a given data instance in order to utilize a broader array of samples and those tightly located at the decision boundary is indeed a reasonable and intriguing choice. The authors' concept of adversariality against the uniform loss, which implies a concentration of prediction towards a particular class, exhibits an interesting property worth exploring further.

Experimentally, the authors provide a substantial ablation study on the hyperparameters used in loss composition, which adds to the comprehensiveness of their methodology. Notably, the use of t-SNE based plotting allows for a clear visualization of how the OOD samples are perturbed to be situated very closely to in-class samples. It also arise the question of ""how would ood behave for the different perturbation types?"". The authors have presented an interesting methodology that applies adversarial perturbations based on a particular loss function to enhance the performance of out-of-distribution detection. However, it would be important for the authors to provide empirical evidence that demonstrates the superiority of this adversarial perturbation over other types of perturbations. This would require a systematic and rigorous experimentation design and would ideally be conducted across various datasets and under different conditions to ensure the results are robust and generalizable.

In the final implementation of the authors' methodology, it's noted that both the perturbed and unperturbed out-of-distribution (OOD) samples are simultaneously considered in the loss function. However, the manuscript doesn't sufficiently explain the rationale behind this particular choice. It would be particularly interesting to see a comparison of results when using only perturbed samples, only unperturbed samples, or both, in the loss function. 

a crucial aspect that needs further clarification and discussion is the ratio of in-distribution to OOD samples used in the training process. The choice of this ratio can significantly affect the performance and robustness of the model. For instance, a too high proportion of OOD samples could make the model overly sensitive to outliers, while a too low proportion might not adequately expose the model to OOD scenarios.

While the paper shows promising results in the specific contexts tested, it would be beneficial for the authors to provide a more extensive analysis covering a range of different OOD datasets. If the authors can provide empirical evidence demonstrating the consistent performance of their method across diverse OOD datasets, it would significantly strengthen their claims. 

it's unclear from the manuscript how each OOD sample is directed towards a specific class during this adversarial perturbation process. Specifically, they should explain how the gradient direction, which determines the perturbations applied to the OOD samples, correlates with the movement of these samples towards specific classes.

I am more than eager to increase my score if the questions above are adequately answered.

 Please see the section weaknesses. Please see the section weaknesses.",603,0,0,0.7914,0.1319551426,0.8588757515000001,230,20.6888,0.2586,neurips,0.0,5,5,4,5,factual,4,4,90,polite,5,positive,5,none,4,5,4,5,factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
57,Reviewer-1Wzd,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The manuscript studies image-wide OOD detection in presence of auxiliary negative data. The negative data is often limited and therefore cannot fully encompass the distribution of inliers. Consequently, contemporary learning procedures fail to deliver classifiers resilient to outliers. To overcome this issue, the manuscript presents a method for extrapolating the negative data towards all modes of the inlier distribution. The proposed method first calculates the gradient of arbitrary OOD score over the input. Then, the sign of the gradient is used to direct the negative input samples towards the inlier distribution. The final learning algorithm uses both initial and extrapolated auxiliary negatives to train the classifier resilient to outliers. The proposed method outperforms relevant related works on small image benchmarks. S1. The manuscript deals with an important issue.

S2. Extrapolation of auxiliary negative data towards modes of inlier distribution intuitively makes sense.

S3. The developed method achieves competitive results on considered benchmarks.

S4. The developed method can be combined with existing OOD detectors (e.g. Energy, MSP, ... ) W1.  The manuscript does not discuss the effectiveness of the method when there is only a small auxiliary dataset available. It seems that the developed method still requires a broad OE dataset (Tiny-ImageNet as stated in L236).

W2. The manuscript does not consider relevant related works which use synthetic negatives created by generative models \[a,b,c\]. Synthetic negatives are an effective way for augmenting the auxiliary dataset and the proposed method should outperform methods trained on a mixture of real and synthetic negative data.

W4. The manuscript does not reflect on the additional computational budget (time and memory) required by the method over the OE baseline.

\[a\] Shu Kong, Deva Ramanan: OpenGAN: Open-Set Recognition via Open Data Generation. ICCV 2021

\[b\] Matej Grcic, Petra Bevandic, Sinisa Segvic: Dense Open-set Recognition with Synthetic Outliers Generated by Real NVP. VISAPP 2021.

\[c\] Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin: Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. ICLR 2018. C1. Can the proposed method work on large-scale experiments \[d\]

C2. Is the extrapolation of outlier data towards inliers always possible or there are some requirements that should be met?
Analysis similar to \[e\] could improve the manuscript.

\[d\] Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang:
ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022.

\[e\] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, Feng Liu:
Is Out-of-Distribution Detection Learnable? NeurIPS 2022 Although promised in Appendix D (L435), the limitations are not clearly stated. One possible limitation might be W1.
",415,0,11,0.8258000000000001,0.0130782313,0.8142668009,230,31.7369,0.0535,neurips,0.0,4,4,4,4,factual,4,3,80,polite,5,positive,5,low,4,4,4,4,partially factual,4,4,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
42,Reviewer-FwfS,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This is a theoretical paper which studies actor-critic reinforcement learning algorithms in which both the actor and the critic are represented using deep neural networks with more than one hidden layer. The previous research addressed linear representations and neural networks with only one hidden layer. This paper derives convergence rates for both the actor and the critic for neural networks with more than one hidden layer.
 - Writing is of very high quality, with well-considered notation, and sensible flow.

- The problem statement is clear, and it is supported by required references.

- The problem is important, and the paper furthers our understanding of the behaviour of RL with function approximation using deep neural networks.
 - A few small typos can be found in the paper. E.g., in line 28 ""was consider in"". A few articles are also missing in various places.

- In the paper, the authors emphasize the need to stay close to the initial conditions of the critic (e.g. in Sec. 2.4). This makes sense from the regularisation point of view in general, but in RL, this may mean that the optimial policy may not be found if the algorithm is forced to say close to the initial conditions. Perhaps the motivation and the consequences of staying close to the initial conditions could be clarified.

- Consider Eq. (9), and assume that $\epsilon$ is small, but higher than 0. Even if $\epsilon$ is very small, the best action may change when a lover Q-value is allowed, i.e., the best action determined by $\theta_t$ may be different from the best action according to $Q(s,a)$. Do the smoothness assumptions made through the paper help to cope with the change of the best action in this case? Note that small $\epsilon$ may not be sufficient to make the result significant since the policy itself may be affected even when $\epsilon$ is tiny.

In line 201, the authors say that $C$, $\beta$, and $\mu_\min$ do not depend on $\theta$. But, I am not sure if this is true for $\mu_\min$ since the stationary distribution $\mu_\theta$ depends on the policy. When we have deterministic actions in the MDP, some states may even have probability of zero in the stationary distribution of the ensuing Markov chain.
 See previous box. One thing to note is that I don't prove convergence rates in my work, and I did not go through the proofs to verify their correctness.
",402,0,4,0.7103,0.0851666667,0.9396833777,215,55.0423,0.0831,neurips,0.0,2,4,3,3,factual,4,4,75,neutral,4,neutral,4,low,4,5,5,4,5,5,5,90,polite,5,neutral,5,low,2.0,4.0,5.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
78,Sang-arun-Isaramalai,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. Incongruency on the philosophical basis of the research methodology between qualitative and quantitative has existed- using the term ""sample"" in results - page 4 and in Limitation in page 8 including terms, representative & confounders. 2. Introduction- gap of knowledge was unclear-why need to explore those influencing factors, what have known and what need to be explored for resolving the problem. 3. Using qualitative data analysis, grounded theory, themes are expected to be emerging from the data themselves not from known categories.  4. The Procdures, page 4 need to take out the subject, the researcher.  5. Figure 1, Need to include influencing factors in the diagram and provide discussion on how those factors mediate or moderate the decision. 6. Discussion - page 7 need to explain why on the study findings not part of literature review. 7. Discussion page 8 - study results from qualitative research are not ready for utilization or designing intervention. 8. Conclusion - Not summary of the results, but need to focus  on what was new knowledge emerging from the study, what confirmed existing knowledge.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",325,0,8,0.7509,0.1459498834,0.6593110561000001,86,25.8,0.0999,f1000,0.0,4,5,4,5,factual,4,4,70,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,85,polite,4,neutral,4,low,1.0,4.0,3.0,2.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,2.0,low,4,3,4,4,factual,4,4,4,neutral,5,neutral,3,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
108,Reviewer-6Zbj,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","The paper proposes a diffusion based method for goal-conditioned Reinforcement Learning. It is assumed that a dataset of offline demonstrations is given (which also indicate a goal variable g). This dataset is then used to train a diffusion-model-based policy. The idea is to inverse-diffuse the current sample to eventually arrive at the goal point. Hence, the noising process is in state space, where the idea is that the goal state is continuously noised (generating a reversed trajectory). - The problem setting is interesting
- The figures are nice and intuitively explain the ideas presented in the paper
- The analogies to behavior cloning are interesting
- Reduction in need for denoising steps is beneficial - The introduction could be improved by making the exact problem setting more clear from the beginning
- The nearest neighbor based approach makes the assumtion that close states are connected/ can be accessd from each other, this should be discussed. This could also be evaluated by designing a more complex toy environment based on the environment in Figure 2.
- The related work description of Janner et all is not exactly correct, as it is not full trajectories that are noised but just trajectory segments
- A comparison to the related works such as \[A\] would be appreciated
- An ablation on trajectory stitching is only implicitly done (by defining different algorithms)
- A motivation for the goal-conditioned problem setting (instead of starting with just a single goal setting) would be beneficial  
 
-.

- The description of the method is rather confusing. First, it is explained that the trajectory is denoised, which would result in a denoising of states. However, in the following sections, suddenly the action is denoised (see section 4.1)
- Is the policy a diffusion model? It is mentioned that BC is performed at the end of section 5.2.
- The paper would definitely benefit from an algorithm description of the method
- It appears that the dataset extension through trajectory stitching is not performed for the baseline methods, which makes the comparison unfair
- The fact that methods based on inverse dynamics model approaches did not work weakens the method, as trajectoriy stitching has obv. downsides and likely only works in state spaces that resemble physical environments


Related work:

\[1\] ""Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"", Reuss et al. 2023 See weaknesses",392,1,1,0.7325,0.0553199405,0.9015843272,72,34.2575,0.1041,iclr,0.0106382978723403,3,4,2,3,partially factual,3,3,70,polite,3,neutral,3,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,3,4,4,partially factual,3,3,70,neutral,5,neutral,4,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,3,low
135,Clement-Jonquet,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.","The papers presents an application of semantic web approach for integrating health/medical data with wearable device information. The idea is to offer an integrated model for: device, physical property, Anatomy, Symptom and disease. The authors explain the ontologies they have used and how they have connected them. The paper has significant issues to be considered good candidate for a journal publication. Mainly, the study presented in this ‘application report’ appears not finished, with negative conclusions in terms of scalability/expansion of the approach and impact. Although the position of the study is never clear between data integration vs. data interoperability, the paper does present an integrated model for 5 ontologies: DOID, SYMP, FMA, QUDT and SSN. Such models is supposed to help to represent device related data (at least Vandrico) and answer queries such as the one given in beginning of introduction. Such a model is an interesting contribution which certainly has value and should be evaluated by the community. However, the biggest lack is that the model proposed is not really experimented/tested/used in the paper. Its impact is not demonstrated. By the end of the paper, 4 diseases (from DOID) are mentioned, but no information about the numbers of concepts/relations from other ontologies are given. The methodology for generating manually symptoms for disease is purely manual and such a task should not be given to non-medical expert. And if medical experts would be in the loop, they would know the disease-symptoms relations without having to search manually the web. In addition, as stated by the authors this approach totally prevent to be extended for more diseases and syndromes. Overall in the paper the clarification about what the authors called ‘alignment’, ‘mapping’, ‘semantic bridge’ etc. is not clear. The notion of alignment is pretty clear in the community and I am not sure I will call ‘connecting ontologies to integrate them in a common schema’ an ontology alignment. What you are doing is a good example of designing a new small schema or ontology with strong reuse of other existing ones. Which is a good practice, but it’s not ‘ontology alignment’ rather ontology reuse. Such point would have be avoided if the paper would provide a real state of the art related to: the use of wearable device and relevant ontologies for them and previous work that have proposed an integration with biomedical ontologies. This is a strong lack of the current paper. In conclusion, I will say that the current paper does not offer convincing evidence of the impact and importance of the application. The core of the contribution (i.e., the integrated model) might be useful (assuming it does not exists, what a state of the art on that aspect would have said) but the application of that model does not convince the reader of the results one can obtain by using such a model. Semantic web technologies are used at least by the fact of offering the new model as an ontology. But nothing related to semantic web data technologies is mentioned (eg., RDF etc.). Major comments by sections: -	Abstract: “undifferentiated … professionnals”. = unclear -	Abstract: “a useful tool” : such what? -	Use of section numbering and structure is obscure. Unique subsections are used. -	Section 1: You should discuss that the query the doctor is asking in the case of diabetes II will be asked only once… then the doctor will have the knowledge that diabete => deviceX. -	Section 1 should rather concentrates on wearable device rather than on the impact of ontologies and semantic web technologies. The audience of the SWJ will know this. -	Beginning of section 2 is unclear. Your goals are described with words that haven’t been clarify to the reader yet. Maybe come back on this in conclusion. -	If you assume a device is always something that measure a property, then say it explicitly. -	You could give examples in beginning of section 2 to illustrate your speech. -	Section 3.1: explain what you mean by semantic bridge. If your contribution is a “semantic bridge ontology” define this introduction, give it a name and refer to it by its name. -	Use namespace abbreviation in your figure, this will help figuring out what is existing, what is yours. Provide your own namespace. -	You need to tell us more about Vandrico data source. Size, format, importance in the field, why this one, etc. -	Section 4 is not a relevant state of the art for your application. This section must allow to answer what have been done in the semantic web for medical and wearable device integration? Nothing on mapping (also it is not necessary if you don’t call your work mapping/alignment anymore). Nothing on device. -	Mission conclusion that comes back on the contributions and discuss them before detailing the perspectives. Minor comments: -	Section 1: “locate” : do you mean “find out” -	Section 2: ‘4’ => four -	‘Figure 1:’ => ‘Figure 1.’ -	Section 2.1 exists without section 2.2. Idem for 3.1 -	Section 2.1: ‘OBO Disease Ontology’ => don’t need OBO. Idem after. -	Fig 2 is important in your paper but totally unreadable. -	Section 3.1 ‘be exist’ => English -	Fig 3 is also too small.",875,0,1,0.7801,0.0872040207,0.9324635863,60,47.99,0.0391,semanticweb,0.0404040404040404,5,5,4,5,factual,4,4,90,polite,4,neutral,4,low,4,3,4,4,partially factual,4,3,65,neutral,5,negative,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,moderate,3,3,5,3,factual,4,4,75,neutral,5,negative,4,low,3,2,4,4,partially factual,4,3,65,neutral,5,negative,3,low
52,Reviewer-mbrM,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper tackles the problem of novel category discovery in the 3D shape recognition domain, a framework leveraging the 3D parts and the part-wise relation is proposed which the motivation is learning the parts from the known classes could help the model capture more transferrable features or concepts for the novel categories.
This motivation is validated using experiments, and overall the framework shows better performance than some baselines. 1. The idea of decomposing a category into parts is interesting.
2. I like the organization of this paper, starts with an analysis of the problem of previou method, and then proposed new ones based on the analysis.
3. The paper also explored a bit on the design choices for 3D novel category discovery, which could be helpful. 1. This paper still considers the novel category discovery problem while a more generlized setting exists, generalized category discovery \[R1, R2\], I would suggest the paper to include more discussion and experiment on this generalized setting.
2. It seems that the total number of categories in the datasets are quite small compared to 2D NCD, I am wondering if Objaverse \[R3\] can be used for this task?


\[R1\] Generalized Category Discovery, CVPR 2022
\[R2\] Parametric Classification for Generalized Category Discovery: A Baseline Study, arXiv.
\[R3\] https://objaverse.allenai.org/ 1. In the v1 version of SimGCD fig 10 \[R4\], it is shown that the accuracy on known classes first increases and then drops while the novel class accuracy keeps improving, this contradicts the observation made in this paper, I am wondering if this is because of the setting (generalized category discovery v.s. novel category discovery), the data (2D v.s. 3D), or the number of categories(200 v.s. 7)? Consider this observation is the motivation for this paper, this question will be the biggest concern of mine.


\[R4\] https://arxiv.org/pdf/2211.11727v1.pdf I think the main limitation is that the number of categories is small, thus the conclusion made based on these small datasets may not be generalizable to larger scale datasets.

Overall I think this paper is very clear, and could be of interest for the community, however the concerns I raises in the questions should be addressed first.",358,2,7,0.746,0.1578253119,0.9263672829,220,32.9482,0.1262,neurips,0.0188679245283018,2,4,3,2,partially factual,3,3,60,polite,3,positive,4,low,4,5,4,4,partially factual,4,4,80,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,positive,4.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,3,85,polite,5,positive,4,low
12,Reviewer-hGLR,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","They present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities.  Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, they also propose to align modalities in both the input and output space. 1 One model that takes any combination of modalities as input or output is novel and promising.
2 As the lack of training data, the alignment of different modalities is very difficult. The proposed method for the alignment is very interesting. 1 The simple weighted interpolation of different representations is not so convincing. Why does this method work? see above not addressed",143,0,1,0.7243,0.0889508929,0.948985219,215,28.0155,0.068,neurips,0.0,0,4,0,0,unfactual,1,0,12,neutral,1,neutral,0,high,2,4,3,3,partially factual,4,4,55,polite,5,neutral,3,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,2,3,3,2,partially factual,3,3,50,neutral,4,neutral,2,moderate,1,3,2,2,partially factual,3,2,45,polite,4,neutral,3,moderate
0,Reviewer-My8L,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The authors present a method for improving the calibration of deep neural network ensembles in the small data regime when access to an unlabelled data set is assumed. In particular, they propose the counterintuitive idea of randomly labelling the unlabelled dataset (distinctly for each ensemble member) and training the deep ensemble on the joint supervised and randomly labelled data. The randomly labelled data promotes ensemble diversity. A PAC bound which relates generalisation performance to ensemble diversity is derived while the diversity of the ensemble is demonstrated to be related to the ensemble size. Experiments on various slices of CIFAR-10 and CIFAR-100 show that while the method does not improve accuracy relative to standard ensembles, there are substantial gains on calibration. Calibration does not improve consistently over more complicated/expensive diversity promoting ensemble methods. - The paper is very well written and clear.
- The idea for the method, of using randomly labelled unsupervised data to promote ensemble diversity is simple, cheap and easy to implement and in so far as promoting diversity makes sense.
- Some theoretical results are presented in which the ensemble diversity is related via a PAC bound to the generalization performance (I have some other comments on these results below).
- The experimental results are convincing that at least in the small data regime with relatively little unsupervised data the calibration relative to standard ensembles is significantly improved. Please see my questions in the section below for potential weaknesses that can be addressed through further experiments.

- The method is targeted solely at the small data regime, gains in calibration go to zero as the amount of labelled data increases.
- The method introduces a new $\beta$ hyperparameter which must be tuned.
- The experiments are presented without error bars and it is unclear if they come from a single run or are averaged over multiple seeds, standard practice, especially when considering the relative small datasets considered in this paper is to run experiments with multiple random seeds and present averages and standard deviations of the metrics of interest (or better yet other forms of statistical test of the significance of the results).
- Experiments are conducted on small slices of CIFAR-10 and CIFAR-100, while performance in the large data regime is alluded to in the paper, an experimental evaluation of this setting (for example ImageNet is fairly standard in the ensemble literature) would be much appreciated.
- From equation 3, it seems to be the case that as the number of classes (c) increases the gains in ensemble diversity go to zero, so the method is both likely to give no gains in the large data and large number of classes regime.
- The primary theoretical motivation for the method is equation 1, which is a PAC bound on the generalization performance, it is difficult to get a sense of how tight this bound is and to what extent there is a competition between the various terms in the bound.

Small things (didn't effect rating):
- Typo: ""coincides we standard weight decay"" -> ""coincides with standard weight decay""
- It took me a while when reading the paper printed out to realise that there are two colours plotted in the left hand side of Figure 2 - as the orange is almost fully hidden by the red, making this clear in the figure or caption would be helpful to readers. - While the experimental results do not show big drops in accuracy, I am quite concerned that given vastly more unlabelled data the method would lead to overfitting the random labels and thereby harm test set accuracy (as is a well known phenomenon in the noisy label literature). More formally one could imagine that vast amounts of unlabelled data would promote the diversity term in the RHS of equation 1, but I given results in the noisy labels literature, I would find it hard to believe that this would not come at a corresponding cost in the first term on the RHS of equation 1. Could the authors please comment on this concern? Experimentally, I would be interested in seeing an experiment on ImageNet, for example, where the labelled set is of size 50k and the unlabelled set is 950k examples, a standard resnet50 or similar capacity model is used with 4 ensemble members (as per other papers in the literature) and a comparison to standard ensembles in terms of accuracy and calibration is given. This is a significant concern for me, as usually with methods that make use of an unsupervised dataset, the expectation is that as the unlabelled dataset grows, the gains from using it grow to. I fear this will not be the case for this method, which would limit the method to the small dataset, small number of classes and small unlabelled dataset regime. I recognise that the $\beta$ hyperparameter can to a certain extent control this trade-off, so if further experiments are conducted to address this concern, please report the results over the $\beta$ hyperparameter range.",835,0,0,0.7756000000000001,0.0024741462,0.9451873302,47,28.0723,0.5162,iclr,0.0,5,5,5,5,factual,4,5,95,polite,5,positive,5,moderate,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
92,Reviewer-fL1b,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The paper proposes a method to learn low-dimensional continuous-time representations of network nodes, based on the collection of interaction events among them. More precisely, the events are in the form of $(i,j,t)$, where $(i,j)$ is the pair of nodes involved in the interaction event, and $t$ is the occurrence time. The proposed method first estimate the intensity function $\lambda_{i,j}(t)$ of events between each pair of nodes $(i,j)$ at every time instant $t$, then project the intensities of each node at time $t$ onto a learned lower dimensional subspace to obtain a representation. Theoretical results on the recovery error of the representation is provided. Numerical experiments using real data shows the effectiveness of the proposed method. The paper proposes to estimate the representation of nodes using continuous-time events, which seems to be a novel type of data. I find the presentation of the paper generally vague and hand-wavy. See the following.

1. The introduction is way too high-level. The authors should be more specific about the problem setting in this paper, for example, why we care about dynamic models, continuous-time event data, low-dimensional representation of nodes etc.

2. The related work is not specific. The authors should use a sentence to summarize the contribution of the mentioned papers and explain the difference from your work.

3. Lemma 1 is not correct. $\widehat U_d$ minimizes the residual sum of squares at $B$ chosen time instants, but not the integrated one. 

4. In Section 3, notation part, what is the difference between $\gg$ and $\gtrsim$? Also is the universal constant multiplicative or additive?

5. It's not clear what `$\approx$' means in Section 4.  . .",272,0,7,0.7208,0.08125,0.8962726593,215,43.4477,0.3676,neurips,0.0,4,4,3,3,factual,3,4,75,neutral,4,neutral,4,low,5,5,4,5,factual,4,5,85,impolite,5,negative,4,low,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,moderate,3,3,3,3,partially factual,3,3,60,neutral,4,negative,4,moderate,4,3,3,3,partially factual,3,3,60,neutral,4,negative,4,moderate
53,Reviewer-A92f,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper presents a new method that combines set-valued prediction with out-of-distribution detection in multi-class classification problems. The central idea is a risk minimization framework with a loss function that consists of three parts. The first two parts trade off set size and accuracy, while a weight parameter controls which of the two terms is more important. The third term allows to exclude atypical examples from acceptance regions. In addition, the penultimate layer of the neural uses random fourier features to approximate a Gaussian kernel. 

The authors present theoretical results that present (a) the quality of the random Fourier feature approximation (b) the convergence to Bayes risk when sample size increases. In the experiments the proposed methods is compared to three baselines on three datasets and three metrics. The metrics evaluate the set-valued prediction and OOD detection performance. - The presented method is novel
- Overall the paper is well written (but some parts are unclear, see below)
- I agree with the authors that combined set-valued prediction and OOD detection is a key concept in satefy-critical applications of AI. 
- I liked that the authors describe the problem setting and the assumptions formally (Assumptions 1 and 2). This is often missing in OOD detection papers. This is a quite technical paper, and I am afraid that I don't understand the method very well, despite having a background on the topic and spending quite some time to read the paper. The last part of objective function (1) is unclear to me. What are lambda_k and rho_k? Are these explained in the paper? The authors explain that rho_k is used to exclude atypical examples from acceptance regions, but I don't see yet how that's going on. Also, why is a Frobenius-penalty needed for the parameter matrices? This is quite atypical for deep learning methods, where regularization is typically done via early stopping in SGD.

Furthermore, the need for random Fourier features in the penultimate layer of the neural network is also unclear to me. What does this component add to the method, compared to just propagating the embedding to the output layer? 

I also find the connection to existing literature a bit weak. The literature on OOD detection is vast, so I understand that the authors cannot discuss every paper, but some essential papers are definitely missing. Assumption 1 clearly motivates why generative models / density-based models are a good approach to represent P(x|y) as a first step for combined set-valued prediction and OOD detection. The authors discuss a few methods that model P(x|y), such as the unpublished work of Hechtlinger et al. However, there are many other papers that also model P(x|y), such as:
Charpentier et al. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts, ICLR 2021
Van Amersfoort et al. Uncertainty estimation using a single deep deterministic neural network, ICML 2020
Lee et al. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, Neurips 2018

Perhaps those papers don't evaluate set-valued prediction, but they can be immediately used for such purposes. From that perspective, I would argue that such methods are also better baselines than the current baselines. These methods are deep learning methods, and they are published, unlike two of the three papers that are currently used as baselines. If I would have to do simultaneous set-valued prediction and OOD detection for a specific application, I would be more tempted to try these methods first instead of the method proposed here, because for methods that model P(x|y) it is more clear what they are doing. For the proposed method I am not sure whether it is modelling a class-specific density P(x|y). This might be realized via the random Fourier features, but more explanation would be needed. 

Furthermore, in set-valued prediction there are also quite some methods that consider loss functions that consist of two parts: a part that minimizes accuracy, and another part that minimizes set size, see e.g. 
Mortier et al. Efficient set-valued prediction in multi-class classification, Data Mining and Knowledge Discovery, 2020. 
Titouan Lorieul, Uncertainty in predictions of deep learning models for fine-grained classification, PhD thesis, University of Montpellier, France. 

The proposed method has a lot of connections with such methods, but there are two differences: (1) the proposed method has an additional third part in the loss, (2) the other methods typically fit a probabilistic model first, and optimize set-based utility scores during the inference phase. Perhaps these two differences are enough to behave good for OOD detection as well, but that's still unclear to me. See above.",757,0,2,0.7928000000000001,0.0294517671,0.8858969212000001,48,42.5887,0.1507,iclr,0.0,4,4,5,4,factual,4,5,90,polite,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
53,Reviewer-5AV7,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper proposes a novel way to learn a set-valued classifier, called Deep Generalized Prediction Set (DeepGPS); the proposed method is capable of identifying ambiguous observations and detecting out-of-distribution observations. Also, it is the first set-valued classification with a theoretical guarantee and scalable to large datasets. In theory, this paper provides that DeepGPS attains the optimal expected prediction set size, while achieving the user-prescribed class-specific accuracy. The efficacy of DeepGPS is demonstrated by using MNIST/CIFAR10/Fashion-MNIST datasets and multiple baselines. This paper proposes a learning approach for DeepGPS along with its theoretical properties in Thm1-3. I appreciate the authors' careful analysis on the algorithm. I was initially surprised that the proposed approach can achieve a user-prescribed class-specific accuracy \gamma without training a base model and additional set predictor in a decoupled way. However, I found that it is simply due to the hyperparameter C tuning in a validation set. In this regard, I found that the stated guarantee in (1) of Thm3 is largely disconnected to the empirical results. 

Afterward, I was not convinced whether we actually need this complicated way in training the entire neural network. I’d rather simply use conformal prediction for the specified task (e.g., BCOPS). 

Also in representing the main results in Table 1, I think the class-specific accuracy results should be bolded if they are close to the desired level; however, the maximum values are bolded. 1. Based on the appendix, the hyperparameter C is chosen to achieve the desired class-specific accuracy, i.e., “The tuning parameter C is determined such that the prediction set is smallest on the unlabeled part in the validation data when the misclassification rate is close to γ on the labeled part in the validation data.” Then, what’s the meaning of (1) of Thm3? I think without this theorem, we can heuristically achieve the desired class-specific accuracy via hyperparameter tuning over a validation set. 

2. Related to the above question, can you re-evaluate the benefit of DeepGPS compared to BCOPS? I think simple training in a decoupled way provides a stronger guarantee. 

3.  In Table 1, is there a specific reason that the accuracies are highlighted when it is the largest number? Otherwise, please use bold numbers if the class-specific accuracy results are close to the desired level.",376,0,3,0.783,-0.0018571429,0.9310600758,48,29.803,0.8641000000000001,iclr,0.0,4,4,3,4,partially factual,3,4,75,neutral,4,neutral,4,moderate,5,5,4,5,factual,5,5,90,polite,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
49,Reviewer-Uwik,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","The paper introduces the Cross-Guided Ensemble of Tokens (CrossGET), which is designed to enhance the efficiency of vision-language Transformers. It tackles the significant challenge of mitigating the computational costs and latency associated with vision-language models. Within this framework, two essential components come into play: Cross-Guided Matching and Ensemble, orchestrating the fusion of tokens guided by cross-modal cues, and Complete-Graph Soft Matching, contributing to the refinement of token matching outcomes. 1.Comprehensive Experimentation and Solid Theoretical Foundation: The paper's strength lies in its extensive and well-documented experiments, combined with a rigorous theoretical underpinning for the proposed method. This makes the work sound and reliable, both in terms of its theoretical framework and practical applicability.
2. Relevance of the Addressed Problem: The choice of the problem addressed in the paper holds significant value, especially in the context of the substantial computational overhead associated with many state-of-the-art multimodal models. This highlights the practical importance of the research. However, it is recommended that the authors extend their analysis and experimentation to encompass a broader range of models, moving beyond the initial exploration with BLIP-2. This would further enhance the paper's contribution and generalizability. 1. Cross-Modal Guidance Utilization: In the paper, the emphasis is placed on the ability of CrossGET to be applied to modality-dependent models like BLIP and BLIP2. The approach involves learning a cross-token to serve as guidance for another modality. However, there are concerns about this approach. Taking BLIP as an example, it appears that it may not fully harness textual guidance. In scenarios like visual grounding, where different textual descriptions highlight various aspects of the same image, it raises questions about how CrossGET selects tokens from different texts to focus on.
2. Unfair Experimental Comparisons: The paper contains instances of unfair comparisons in the experiments. For example, in section 4.1, the authors directly compare retrieval results of models such as TRIPS and UPOP. Yet, these models vary significantly in terms of training data and model parameter sizes, making the comparison less meaningful. To provide a clearer perspective, the paper should emphasize how much TRIPS, or similar acceleration methods, improve over the baseline, and how much the proposed method accelerates and enhances performance compared to the baseline.
3. Limited Model Performance Improvement: The paper reports only marginal improvements in model performance while introducing a relatively complex method. Moreover, the acceleration achieved by the proposed method appears similar to that of ToMe. Given the relative complexity of the proposed approach, the effectiveness of this work may be questioned, especially if the gains in performance and acceleration are not substantial. 1. Implementation of Token Reduction in BLIP-2: It would be beneficial for the authors to provide more detailed information on how they specifically implemented token reduction in BLIP-2 within the context of their method. A more elaborate explanation of the process and its impact on BLIP-2's performance would enhance the clarity and completeness of the paper.
2. Impact of CrossGET on OPT in BLIP-2: A notable aspect of this work is the introduction of CrossGET into the frozen OPT component of BLIP-2 for token reduction. However, it's important to consider that OPT is a decoder-only model. The paper should address how this approach might affect the inference capabilities of OPT and whether any experiments were conducted to analyze and verify why image captioning performance appears to be minimally impacted. Further insight into this aspect of the methodology would enhance the paper's robustness and contribute to a better understanding of the results.Im glad to improve my score if my   concerns be addressed.",586,0,6,0.7977000000000001,0.1171066253,0.94465065,48,25.0653,0.1262,iclr,0.0,2,5,4,3,factual,3,4,78,polite,4,neutral,4,moderate,4,4,4,4,partially factual,4,3,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
49,Reviewer-oYGw,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","This paper proposes cross guided matching and cross guided ensemble as cross-modal importance indicator. Besides, a Complete-Graph Soft Matching algorithm is proposed as an improved version of ToME's bipartite soft matching. 1. Both Cross Guided Matching (CGM) and Complete-Graph Soft Matching (CGSM) is well motivated and proved to be effective.
2. Extensive experiments are conducted on several vision language tasks for both modal indenpendent VL model (CLIP) and modal dependent VL model (BLIP2). I do recognize the amount of work that went into this submission. 1. The proposed approach is named as Cross-Guided Ensemble of Tokens, however, I find that the proposed Cross-Guided Ensemble (CGE) is not that useful as illustrated in Table 1. So, I think the paper should re-organize the structure and highlight the really useful designs.
2. The proposed Complete-Graph Soft Matching is not specialized for cross-modal tasks, so does it outperform the ToMe algorithm in general visual recognition tasks? The proposed method can improve the model efficiency after training with little performance loss, and I am curious if the proposed method can also accelerate the training of multi-modal tasks.",183,0,4,0.7942,0.08515625,0.9441901445,48,40.5737,0.0529,iclr,0.0,4,4,2,3,unfactual,3,2,75,neutral,4,negative,4,high,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
79,Reviewer-LDAP,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The authors introduce a dataset distillation (DD) method called Farzi Data for data with a ""left to right"" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset. **Algorithmic Contribution.** Algorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.

**Empirical Results.** The empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of ""data cleaning"" whereby samples that *hurt* model performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task. **Presentation and Clarity.** While the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.

P1. There are many cases where important notation is not defined. For instance, $\mathrm{Rep}(\mathcal{F}, \mathcal{D})$ is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms $d\mathbf{m}$, $d\mathbf{x}$, and $d\mathbf{w}$ in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.

P2. Stylistically, there is also some nonstandard notation. For instance, $\mathcal{O}(100)$ (3rd bullet point, pg. 2). I suppose the authors meant ""on the order of 100x"", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. ""Correctness of Algorithm 1, Line 13"" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.

**Theoretical Results.** There are also issues with the theoretical results.

T1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < \mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$. ($\mathcal{D}_F$ and $\mathcal{D}_N$ stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < B_1$ and $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$ for some bounds $B_1$ and $B_2$. Then, since $B_1 < B_2$, they conclude the desired result. This is not valid: $a < b$, $c < d$, and $b < d$ does not imply that $a < c$. There needs to be a _lower_ bound on the representativeness for the naive parameterization.

I remark that I believe the _result_ is (at least ""morally"") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the _proof_ has a fatal error and must be corrected somehow.

T2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function $l$; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in $\mathcal{F}$ must be considered, not $\mathcal{F}$ itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?

T3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by $d\mathbf{m} = d\mathbf{m} + \frac{\partial w_t}{\partial m_t} \cdot d\mathbf{w}$? Is $w_t$ supposed to be $\mathbf{w}_T$, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?

If these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are. Q1. The authors mention that training with the reference trajectories $\Omega$ is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?

Q2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?

Q3. Why isn't FMLP also used as a teacher network in Table 1?",1161,0,13,0.7745000000000001,0.0963321995,0.9078657031,47,38.9031,0.0977,iclr,0.0,5,5,5,5,factual,3,5,98,neutral,5,neutral,4,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,5.0,neutral,5.0,low,5,4,5,5,factual,5,5,95,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
79,Reviewer-THWQ,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets. * The high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data
* Pursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost
* Empirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used) A number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:

* It is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either… please clarify..
* It would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code. 
* Looking at Eqn. 2, I am a bit puzzled as to how \Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \theta_0 \sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..

* Rank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty.. My major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.",506,0,0,0.7575000000000001,0.0487258687,0.8768354654,47,45.1043,0.7308,iclr,0.0196078431372549,4,4,4,3,factual,4,4,80,polite,3,neutral,4,none,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
40,Reviewer-K3eU,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","The paper gives “best-of-both-worlds” results for an imitation-learning problem in contextual bandits and MDP settings. With small orthogonal changes to assumptions, the algorithms primarily improve over prior work by considering instance-optimal bounds both in regret and queries, and require only ordinal preference feedback rather than explicit rewards (similar to the “dueling bandits“ literature).  - The paper is easy to read, the algorithms and notation are well-explained, and the results are appropriately contextualized in prior work.
- The examples given for the functions in the model are quite useful for grounding the problem in more concrete applications. Related work is discussed thoroughly.
- Conceptually, the model draws nice connections between contextual bandits and modern topics in finetuning models (e.g. LLMs) from preference feedback, where the emphasis on “instance-optimal” style results is particularly well-motivated. - While the application of techniques from online reinforcement learning to obtain the instance-optimal bounds in this setting is clever, it is unclear how much of this follows directly vs what technical innovation is required. It would be helpful to highlight the methodological contributions used.
- Given the applications discussed, it would be beneficial to give experimental results for preference finetuning (even in a toy setting) to demonstrate the importance of instance-optimality in practice.
- While the instance-optimal rates seem reasonable, it would be nice to include (partially) matching lower bounds for some results, or discuss barriers to obtaining such results. - Can the rates on $d$ or $\Delta$ be shown to be asymptotically tight for either queries or regret? 
- What does the notation $P_t\[a_t, b_t\]$ on line 146 refer to? - Connections to prior RL work which makes use of eluder dimension could be discussed in greater detail.
- Some hyperlinks are broken in the PDF.",290,0,0,0.7843,0.1495748299,0.9330165386,215,31.3783,0.072,neurips,0.0,2,4,3,3,factual,4,3,70,polite,3,neutral,3,low,5,4,4,5,factual,4,5,85,polite,5,neutral,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
40,Reviewer-FECY,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper develops the provably efficient algorithms AURORA and AURORAE, which are able to achieve the optimal regret bound under contextual dueling bandit setting, and imitation learning respectively, at the same time minimizing query complexity. The key idea behind is that the algorithm only makes a query when the algorithm is very uncertain about the optimal action ($Z_t 1_{|A_t| > 1}$). The algorithm decides the sampling distribution of action pairs to make a query by considering whether the estimated cumulative regret exceeds the carefully designed threshold. If it does not exceed, the algorithm does exploration and sample action pairs from the uniform distribution. If it exceeds, the algorithm uses a technique similar to inverse gap weighting to achieve better balance between exploration and exploitation. For imitation setting with horizon H, the algorithm treats MDP as a concatenation of H contextual bandits and runs AURORAE, which is a stack of multiple AURORA instances. This work is original and well-motivated. It is crucial to design an online learning algorithm that achieves optimal regret while using minimal query complexity. Although I did not get a chance to read the complete proofs in the supplementary material carefully, given the discussion of intuition, all technical results seem reasonable to me. 

This paper is well presented and is a pleasure to read. An example for illustration follows every definition. All materials are well organized in a logical manner. 
 I have several concerns regarding the proposed algorithms. First, P5 I 5, the computational complexity for the candidate arm set might be very large, even if F is assumed to be a d-dimensional linear class. The computational complexity might be $O(dT\log(T)|A|)$. Also, in reality, F might be very complex, which might even worsen the computational complexity. Can we use a simple function class F for approximation while still achieving a similar regret bound?
 Please see the review in weaknesses.  The authors address their limitations of not having any experiments on real data or simulations. I believe the work will be much more convincing if the theoretical bounds are supported by experiment results.",344,0,0,0.8049000000000001,0.0822619048,0.9381507635,215,30.0936,0.3201,neurips,0.021978021978022,4,3,2,3,partially factual,2,3,55,polite,3,neutral,1,high,5,5,5,5,partially factual,5,5,90,polite,5,positive,5,moderate,2.0,4.0,5.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,5,low,3,5,4,4,partially factual,4,4,85,polite,5,positive,5,low
44,Slobodan-M-Janković,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors made an observational study trying to correlate MTX PG levels with disease activity of RA (as measured by a clinical score). The topic is of general interest, and the study brings results with practical significance. The manuscript is well written, and merits acceptance for publication. However, there are a few issues that should be corrected: In the Methods section the authors should state precisely how they measures the MTX PG levels in erythrocytes. As it is written now, it is not clear whether the MTX PG levels were measured in erythrocytes or in full blood.  Number of patients is small, so it is critical that statistical methods were used properly. The authors should state whether assumptions of multivariate logistic regression were met. Also, what was the categorical outcome used as dependent variable of the regression? Finally, quality of the regression model should be stated (Hosmer Lemeshow test, Cox and Snellen...).  Something should be said about adherence of the patients to the therapy. Was there any method used to check for adherence? If not, mention this in the Limitation paragraph.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",324,0,1,0.7684000000000001,0.1401785714,0.8017649651000001,6,37.2,0.0999,f1000,0.0103092783505154,2,3,2,3,partially factual,4,4,40,polite,3,positive,4,low,5,5,4,5,partially factual,5,5,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,5,5,4,5,factual,4,4,92,polite,5,positive,5,low
146,Reviewer-LTFo,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. 1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.
2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.
3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.
4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution. 1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. 

2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?

3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear. See weaknesses.",288,0,8,0.7815000000000001,0.2704617605,0.954846859,48,25.7747,0.1213,iclr,0.0,2,4,3,3,factual,3,3,55,polite,3,positive,3,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,4.0,4.0,4.0,4.0,factual,4.0,5.0,85.0,polite,5.0,positive,3.0,none,3,5,4,3,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
35,Reviewer-esAd,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","Authors study ways of incorporating cognitive steering in vision neural network models. They add a top-down feedback mechanism to Alexnet with which they report improved performance. Further, they test other steering mechanisms, including prototypes, language-based signals etc. These tests are over image composite tasks where the approaches show greatly improved performance.  The paper is interesting, the experiments and the controls are convincing. Cognitive steering in deep CNNs is novel as far as I know, especially with language signals. Some parts of the paper are well written. 

 * The paper lacks benchmarking. There are several methods of incorporating feedback in deep learning models from previous years that weren't tested. Look at \[1\] for a survey. Although I am sympathetic about this since cognitive steering in itself is interesting but the paper needs to be clear that the contributions are in studying various steering methods/signals and not introducing feedback itself. 

* Side-by-side composition is not a straightforward task setting - putting images side by side reduces the scale of the objects and since CNNs are not scale invariant that poses a challenge. So I am not quite convinced it is a good test for steering.  



\[1\] Kreiman, G., & Serre, T. (2020). Beyond the feedforward sweep: feedback computations in the visual cortex. Annals of the New York Academy of Sciences, 1464(1), 222–241. https://doi.org/10.1111/nyas.14320 Improvements to text and minor corrections:
* Please make it clear in the text what ""target absent"" control means. Only place it is explained is the Figure 5 caption. I had to spend a lot of time trying to understand that until I stumbled on Fig 5 caption. 
* Please consider updating Figure 3 to say ""target unit"" or ""target neuron"" or ""target logit"" instead of ""target"" 
* Line 169: where &rarr; were

Questions:
* Why does LRM models have higher accuracy than alexnet at 0th modulatory pass? They should be same?
* In the ""target absent"" control - how is the absent target chosen? Is it average of every other (998 other target classes not present in the composite or some random class?).
* How many modulatory passes were they trained for and tested for? Is it the same number of passes in training and testing? NA",369,4,2,0.8012,0.10507087,0.9063189626,216,55.7114,0.1517,neurips,0.0,4,4,5,5,factual,4,4,100,polite,4,positive,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
80,Reviewer-N2ym,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","Abstract Summary
The paper introduces FedAIoT, a novel federated learning framework tailored for IoT applications. The framework aims to address the unique challenges posed by IoT ecosystems, such as data privacy, limited computational resources, and network constraints.

Key Contributions
Novel Framework: The paper presents the architecture and design principles of FedAIoT, which incorporates distributed data storage and decentralized learning algorithms to enable IoT devices to participate in machine learning tasks without compromising data privacy.

Mathematical Formulation: The authors provide rigorous mathematical models to describe the federated learning process, focusing on optimization algorithms and convergence properties.

Experimental Validation: Through extensive experiments using real-world and synthetic datasets, the authors demonstrate that FedAIoT outperforms traditional centralized learning methods in terms of accuracy, privacy preservation, and computational efficiency.

Applicability: The framework is designed to be adaptable to various IoT applications, from smart homes to industrial automation.

Methodology
The paper employs a federated learning approach where IoT devices can train machine learning models locally on their own data and then share only the model parameters with a central server for global aggregation. This preserves the privacy of the data while allowing for a collective learning experience.

Results
The experiments show that FedAIoT achieves comparable or superior performance to centralized approaches while ensuring data privacy and reducing the computational load on the central server. The framework also exhibits robustness to non-IID data distributions and network delays.

Conclusion
The paper concludes by asserting that FedAIoT offers a scalable, efficient, and privacy-preserving solution for implementing machine learning in IoT networks. It also identifies avenues for future research, including optimization of communication overhead and integration with other emerging technologies like edge computing. Thus the paper makes a compelling case for the adoption of federated learning in IoT environments, providing both the theoretical foundation and practical validation for the proposed FedAIoT framework. Strengths Assessment of the Paper
Originality
The paper presents an innovative framework—FedAIoT—for federated learning in the context of Internet of Things (IoT) applications. The originality of the work lies in the seamless integration of federated learning techniques with IoT devices to achieve distributed, privacy-preserving learning. The novelty also arises from the unique problem formulation that caters specifically to the challenges posed by IoT environments, such as limited computational resources and data privacy issues.

Quality
The paper is of high quality in multiple aspects:

Methodological Rigor: The mathematical formulations and algorithms are soundly developed. The paper thoroughly validates the proposed framework through a series of experiments, complete with baseline comparisons and varied settings.

Data Quality: The choice of datasets and the justification for those choices are clear and appropriate for validating the model. The paper also employs robust statistical methods to analyze the results.

Citation and Contextualization: The paper provides an extensive literature review, situating its contributions aptly within existing work.
The significance of the paper is manifold:

Theoretical Contribution: The paper addresses a critical gap in federated learning by tailoring it to the specific needs of IoT applications, thus extending the theory of federated learning to a new domain.

Practical Impact: The FedAIoT framework has the potential to revolutionize how machine learning models are deployed in IoT networks, thereby having broad applicability and impact. Abstract and Introduction
The paper proposes a unified end-to-end Federated Learning (FL) framework for Artificial Intelligence of Things (AIoT) named FedAIoT. The framework is benchmarked across multiple IoT datasets and incorporates a variety of data partitioning schemes, preprocessing techniques, models, and FL hyperparameters. Despite its comprehensive approach, the paper lacks a comparative study with existing state-of-the-art solutions. Moreover, while the paper mentions the inclusion of popular schemes and models in its framework, it doesn't substantiate why these were chosen over other potential candidates.

Equations and Mathematical Formulations
The paper briefly touches upon the Dirichlet distribution for creating non-IID data partitions and mentions metrics like accuracy and Mean Average Precision (MAP-50). However, it lacks mathematical rigor. For instance, the Dirichlet distribution is mentioned but not defined. A formal definition, perhaps along with its probability density function, would have given more depth. Furthermore, there are no equations to represent the FL optimizers like FedAvg and FedOPT, which makes it difficult to appreciate the nuances or compare them.

Tables and Figures
Table 1: While useful for a cursory comparison, this table lacks depth. For example, it could include a comparison based on performance metrics to provide an analytical foundation for its claims.

Figure 1 and 2: These figures provide an overview but lack detail. For example, Figure 2 could be improved by including the types of IoT-specific preprocessing techniques or by detailing the architecture of the proposed IoT-friendly models.

Table 4: This table summarizes the performance metrics but lacks confidence intervals or p-values, which are essential for ascertaining the statistical significance of the results.

Table 5: While this table attempts to show the impact of client sampling ratios, it doesn't explain why only two ratios (10% and 30%) were chosen for comparison.

Dataset and Experimental Design
The paper includes a wide range of datasets, which is commendable. However, it doesn't provide any rationale for the specific choice of datasets. Furthermore, no information is given about the train-test split methodology. Was it random or stratified? The partitioning schemes for these datasets are discussed, but there is a lack of empirical justification for why these schemes are effective or superior to existing methods.

Algorithms and Techniques
The paper discusses various FL optimizers, data partitioning schemes, and IoT-friendly models, but there is a lack of justification for the chosen methods. For example, why were FedAvg and FedOPT selected as FL optimizers? Are they computationally less expensive or do they converge faster?

Results and Discussion
The paper presents a broad range of results but lacks a discussion comparing these results to existing benchmarks or state-of-the-art methods. The paper would benefit from including such a comparative analysis.

Insufficient Empirical Validation
The experiments conducted are somewhat limited in scope and scale. Only a few datasets are considered, and they seem to belong to similar domains. This raises questions about the model's generalizability. Moreover, the paper lacks ablation studies, making it difficult to understand the contribution of each component of the proposed method.

Actionable Insight: Include a broader array of datasets from varying domains to validate the model. Conduct ablation studies to quantify the impact of each component or parameter.

Absence of Comparative Analysis
While the paper aims to introduce a novel methodology, there is an absence of a comparative analysis with state-of-the-art methods. Without this, the paper falls short of convincingly establishing the proposed method's superiority or novelty.

Actionable Insight: Include comparisons with state-of-the-art methods in both qualitative and quantitative terms. This could be in the form of performance metrics, computational efficiency, or even qualitative assessments based on real-world applicability.

Mathematical Rigor
The paper would significantly benefit from a more rigorous mathematical treatment of the proposed algorithm. Currently, it seems to rely more on empirical observations. Given your stated goals of developing proper mathematical models, this is an area that requires attention.

Actionable Insight: Introduce formal proofs or derivations that can substantiate the algorithm's properties, such as stability, convergence, or robustness. Include theoretical justifications for the choices made in the algorithm's design.

Lack of Discussion on Limitations
Every model has its limitations, and acknowledging them not only adds credibility but also helps in guiding future work.

Actionable Insight: Devote a section to discuss the limitations of the proposed method and potential avenues for future research. Data Assumptions: Could the authors clarify the specific assumptions made about the data distribution? How do these assumptions align with the real-world scenarios where the model is expected to be deployed?

Methodological Choices: What was the rationale behind the selection of specific hyperparameters and architectural elements in the proposed model? Some clarification on this could strengthen the paper's methodological grounding.

Evaluation Metrics: The paper uses a particular set of metrics for evaluation. Could the authors elucidate why these metrics are most suitable for assessing the model's performance? Are there any other metrics that were considered but not used?

Computational Complexity: How does the computational complexity of the proposed method compare with existing state-of-the-art methods? Could the authors provide a detailed analysis in this regard?

Scalability: The paper does not discuss how well the proposed method scales with the size of the dataset. Could the authors provide insights or supplementary experiments that address this?

Ablation Study: The absence of an ablation study leaves some questions about the necessity of each component of the proposed model. Could the authors provide such an analysis in the rebuttal or an extended version of the paper?

Theoretical Guarantees: Are there any theoretical guarantees, such as convergence or bounds, that can be associated with the proposed algorithm? If yes, this would be a valuable addition to the paper.

Limitations: Every model has its shortcomings. Could the authors elucidate the limitations of the proposed model and how they plan to address these in future work?",1480,0,0,0.8076,0.0647396694,0.8674352765000001,49,18.1151,0.1211,iclr,0.0,2,4,4,2,factual,4,2,40,polite,3,positive,4,moderate,5,4,5,5,factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,2.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
123,Reviewer-ekPo,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","The authors propose LyCORIS, an open-source library that contains multiple fine-tuning techniques for Stable Diffusion. The authors also explore many improved fine-tuning techniques such as LoCon, LoHa and LoKr. This paper also presents evaluations for different fine-tuning techniques using multiple metrics and prompt types. (1) The theory and experiments are both solid. The paper has over 57 pages devoted to analyzing the fine-tuning techniques.
(2) The details for experiments are very clear.
(3) In addition to the framework, the authors also explore other fine-tuning techniques. (1) The results of this framework combined with ControlNet can be presented in this paper.
(2) Efficiency (time and GPU memory cost) of different approaches are not provided and analyzed. (1) Please refer to the main questions in the weakness section.
(2) A minor question: It will be better if the authors provide the results on other versions of stable diffusion, such as SD2.0 and SDXL.",151,0,0,0.7539,0.0711904762,0.7818619609,52,48.9543,0.1844,iclr,0.0,3,4,4,3,factual,3,3,65,polite,4,positive,4,low,5,4,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
138,Reviewer-Q2P6,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","This paper proposes to use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens
and then fine-tune models for controllable generations. The MEET aims to improve the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two datasets. 1. This paper studies a parameter-efficient way to improve the language alignment. It is an interesting direction to explore.

2. It studies several aspects of the proposed method such as prompt length, rank, and temperature. 1. This paper conducted several experiments. However, I don't think the baselines the paper compares with are sufficient. Several works focus on a similar idea about incorporating the reward into text learning, such as RLPrompt \[1\] and AutoPrompt \[2\]. Those should become the baselines to compare the method proposed in the paper. Also, For controllable text generation, there is an interesting direction to utilize the diffusion process, such as the Diffusion-LM \[3\]. However, none of these are included and compared in the paper. Thus, I am not convinced with the experimental results shown in the paper.

2. The performance of the proposed method does not show enough improvements compared to the baseline mentioned in the paper. It highly correlates to the hyperparameter settings. It would be good to include the detailed ablations of those hyperparameters. 

3. The proposed method seems to be not novel. We know the impact of LoRA, and the proposed method seems just a direct implementation of the LoRa with parameter-efficient tunning with some specific designs. Could authors provide more justification about the novelty of the proposed methods?

4. For the ablation section, what would be the efficiency comparison between the proposed method and the baselines? Such as the running time and computation latency.



\[1\] Rlprompt: Optimizing discrete text prompts with reinforcement learning

\[2\] AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts

\[3\] Diffusion-LM Improves Controllable Text Generation Please refer to the Weaknesses section.",321,6,6,0.7649,0.1105,0.8929747939,47,38.6383,0.2968,iclr,0.0,3,3,2,1,factual,2,2,50,neutral,2,negative,4,moderate,5,5,4,5,partially factual,5,5,90,polite,5,negative,5,none,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,3,4,70,neutral,5,negative,4,low,4,4,3,3,partially factual,3,3,70,neutral,4,negative,4,low
17,Reviewer-gsUn,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This paper proposes to integrate the first-order gradient into the unstructured pruning of large language models and achieves superior performance compared to sparseGPT and Wanda. 1. A superior method compared to SparseGPT and Wanda on unstructured pruning of large language model
2. The authors have conducted extensive experiments to assess the method's effectiveness on LLaMa-1 and LLaMa-2. Additionally, the paper illustrates the impact of various gradient and activation combinations on the determination of parameter importance.
3. The paper is well-written, offering clarity and ease of understanding in its presentation. 1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.
2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.
3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important  to the readers to understand the efficiency of the pruned model. 1. Could you specify the error function utilized for calculating gradients in your approach?
2. Have you conducted any latency experiments on the pruned model, particularly under the 2:4 or 4:8 configurations?
3. Is the calibration set employed for your methods and Wanda, SparseGPT identical?",283,0,8,0.8014,0.127046131,0.9349081516,56,30.9022,0.4134,iclr,0.0,3,4,3,3,partially factual,4,3,65,polite,4,neutral,3,moderate,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
17,Reviewer-B7C2,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","* The paper proposes to integrate gradient information into pruning criteria currently used for LLMs.
* The corresponding GBLM method is evaluated on Llama models for perplexity and zero-shot tasks. * The paper is easy to follow and describes the proposed method in good detail.
* The method is evaluated on strong LLama models rather than older LLMs like OPT.
* Source code is provided, aiding reproducability. * Integrating gradient information into pruning criteria is a well studied area, see for example \[1, 2, 3, 4\]. This is currently not discussed under Related Work.
* Consequently, the novelty of GBLM is quite limited. For instance, the analysis in Section 2.3 is very similar to derivations presented in \[2\]. Ultimately, GBLM seems to be a minor variation of a diagonal Fisher scheme (using both gradients and activations while slightly tweaking norms in a heuristic manner).
* The most robust form of evaluation, perplexity, shows only very slight improvements relative to prior work of < 0.1 points, while dropping noticably from the baseline. I am not sure if this is a significant enough improvement in practice.
* It is unclear how the gradient calculation impacts the speed and compute/memory requirements of the pruning process. Being fast and memory efficient is one of the key strengths of SparseGPT and Wanda, hence I think a detailed comparison/discussion of this aspect would be important.

Unfortunately, at this time, I find neither the method itself nor the empirical results interesting enough to recommend acceptance.

\[1\] Pruning convolutional neural networks for resource efficient inference, Molchanov et al.

\[2\] WoodFisher: Efficient Second-Order Approximation for Neural Network Compression, Singh et al.

\[4\] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, Kurtic et al.

\[3\] Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al. * See weaknesses, in particular the compute/memory efficiency point.",308,6,0,0.8244,0.12046851,0.9186406136,56,35.8961,0.2025,iclr,0.0,3,4,4,3,factual,4,3,75,polite,4,neutral,4,none,4,5,4,4,factual,5,5,85,neutral,5,negative,5,none,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,3,4,4,3,factual,3,4,70,polite,5,negative,4,low,3,4,4,3,partially factual,4,4,75,neutral,5,negative,5,low
51,Reviewer-wNRV,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors are concerned with a problem of ""data market design"". In such a setting, a mechanism designer with access to an unknown world state interacts with buyers who have private types, and need to take actions whose payoffs vary depending on the world state. These buyers purchase (in the single buyer case) or bid on (in the multi-buyer case) access to a signaling scheme which, given reports from the agents and the world state, sends a signal to the buyers (which without loss of generality can just be a recommended action). This mechanism, treated as a direct-revelation mechanism, needs to be both truthful (incentivizing honest reporting by the buyers) and obedient (once the buyers receive their signal, they should be incentivized not to deviate from the recommendation). Subject to those constraints (either Bayesian or ex post), the mechanism designer wants to maximize their revenue.

This problem shares some similarities to truthful revenue-maximizing auction design. In that domain, there has been recent progress using the tools of ""differentiable economics"" to approximately learn high-performing (and sometimes even provably optimal) auctions, in both single- and multi-bidder settings.

The authors apply very similar techniques to this data market problem. In single-buyer settings (as in auctions) they are able to ensure exact IC; for multi-buyer settings they use a Lagrangian during training to approximately enforce IC constraints. They experiment on a relatively wide variety of problem instances, reproducing known results, finding new optimal mechanisms, and conjecturing optimal mechanisms where they cannot find them. The paper comprehensively shows how to successfully apply differentiable economics to a new domain where it has not previously been applied. The authors are able to reproduce optimal mechanisms and find new ones, showing that their adaptation of these technique is in fact useful in producing novel results. This helps to further push these techniques towards being practically helpful tools for theorists and modelers. The network architectures here are essentially the same as those used in previous work for auctions, only adapted slightly for the data market setting. This is fine, but it does mean that from the perspective of differentiable economics, there is no novel methodological contribution.

The experiments appear to consider at most 2 buyers. While (as in the case of multi-parameter auctions) even selling to just two buyers may be a very challenging case, it would be more interesting to consider a slightly larger number of buyers. Can the method in fact scale to larger (even just 3-5 buyers) settings, or not? This should be discussed. See questions.",420,0,1,0.8172,0.1425933442,0.9128888845,215,37.1539,0.0499,neurips,0.0098039215686274,3,4,4,3,factual,4,3,80,polite,4,neutral,4,low,4,5,4,4,factual,5,5,88,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
51,Reviewer-zvsA,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors present a novel approach to the problem of data market design, which seeks to find a set of signaling schemes, each revealing some of the information known to a seller and having a corresponding price, where the goal is to maximize expected revenue. Then, the authors introduce the application of a deep learning framework to the automated design of the data market. The paper discusses the importance of data market design and its potential applications in real-world settings, such as data marketplaces where sellers sell data to buyers for ML tasks. The authors demonstrate that their new learning framework can replicate known solutions from theory, expand to more complex settings, and establish the optimality of new designs. The paper also highlights some limitations of the approach, such as the need for interpretability of the mechanisms learned by the RegretNet approach for larger problems, the potential for local optima in non-convex problems, and the challenge of achieving exact incentive alignment in multi-buyer settings. + The paper presents a novel approach to the problem of data market design, which uses deep learning to automate the design of data markets.

+ The authors demonstrate that their new learning framework can almost precisely replicate all known solutions from theory, which shows that the approach is effective and reliable.

+ The paper shows that the new learning framework can be used to establish the optimality of new designs and conjecture the structure of optimal designs, which is a significant contribution to the field.

 + The paper acknowledges that for the approach to provide insights into the theoretically optimal design for larger problems, it will be important to provide interpretability to the mechanisms learned by the approach. However, the RegretNet approach used in the paper is not immediately interpretable, which limits its usefulness in this regard.

+ The paper notes that the approach uses gradient-based approaches, which may suffer from local optima in non-convex problems. This suggests that the approach may not always find the global optimum and may be limited in its ability to handle more complex problems.

+ The paper attains in the multi-buyer setting approximate and not exact incentive alignment, which leaves the question as to how much alignment is enough for agents to follow the intended advice of a market design. This suggests that the approach may not be able to achieve exact incentive alignment in all settings, which could limit its effectiveness.
 + Could you provide more details on how the RegretNet approach can be made more interpretable for larger problems? Are there any specific techniques or methods that could be used to achieve this?

+ Have you considered using other optimization techniques besides gradient-based approaches to address the potential for local optima in non-convex problems? If so, what are some alternative approaches that could be used?

+ What are some potential ways to provide more practical or theoretical guidance on how much alignment is enough for agents to follow the intended advice of a market design? Are there any existing frameworks or approaches that could be used to address this issue?
 The authors acknowledge the ethical concerns raised by markets for trading data about individuals and suggest that machine learning frameworks such as those introduced in this paper can be used to strike new kinds of trade-offs, such as allowing individuals to benefit directly from trades on data about themselves. This shows that the authors are aware of the broader implications of their work and are thinking critically about its potential impact.",584,0,0,0.7766000000000001,0.1128884508,0.9413257241,215,35.3831,0.1616,neurips,0.0,4,4,4,4,partially factual,4,4,90,polite,4,neutral,4,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,4.0,5.0,4.0,5.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
120,Peter-W.-Glynn,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The alpheid shrimp guard, Alpheus lottini, also should be noted as defending pocilloporid corals from COTS attacks.  This shrimp guard occurs world-wide on pocilloporid corals.  It would also be worth noting the defensive behaviour, if any, of the crustacean guards toward the fish corallivores.  ‘White feeding scars’ are referred to in Fig. 1 and Fig. 2 (supplementary image).  These are difficult to make out in the photographs.  I suggest adding arrows to make these easier to see.  Also, it would be useful to know the approximate diameters of the P. eydouxi colonies.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  If applicable, is the statistical analysis and its interpretation appropriate? No  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",221,0,2,0.7659,0.1385416667,0.8811582327,29,39.63,0.1695,f1000,0.011111111111111,4,4,1,4,partially factual,3,3,70,polite,3,neutral,4,moderate,5,5,4,5,factual,5,5,95,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,4,factual,4,4,75,polite,4,positive,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
172,Reviewer-Tj4q,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This paper introduces structured neural networks such that the resulting neural network represents the factorization of a given Bayesian network. For doing so, each layer of the neural network is masked and the product of the masks of all layers must be the same as the adjacency matrix of the DAG representing the Bayesian network. 
With this construction, the represented conditional dependencies with structured neural networks will be consistent with the given Bayesian network. The paper proposes a simple greedy algorithm to find the masks. It also proposes using the structured neural network to construct the coupling layers for normalizing flow and claims that the resulting generative model is better for casual inference (intervention and counterfactuals) than the prior approach.  The paper is well-written and the contribution towards causal inference is solid.   1- The structured neural network augments MADE with a better mask construction algorithm such that the factorization can be defined for any DAG structure. However, it is not a fundamentally different model.
2- The paper didn't propose any approach for learning the structure of DAG given the provided structured neural network parametrization.
3- MADE is not a strong density estimator and comparing only to MADE does not validate the strength of structured neural networks as density estimators. 
  1) How GNF would compare to StrAF if it does permute the latent variables after the first step? 
2) During the comparison with CAREFL did you provide CAREFL with external DAG orders that StrAF uses? If not, the learned causal order by CAREFL may not exactly specify the underlying DAG, which may result in lower performance in causal inference. 
 N/A",269,0,1,0.7182000000000001,0.0411458333,0.8982758522000001,215,39.2762,0.1163,neurips,0.0,4,4,3,4,factual,4,4,64,polite,4,positive,4,moderate,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
96,Alexander-Pico,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The BioJS library of components has a lot of potential. It's encouraging to see a diversity of interactive viewers already registered with BioJS. The intersection of modern JavaSript (JS) components with network biology in particular is ripe for development to bring powerful perspectives on massive biomedical datasets to researchers. I decided to critique this article introducing the BioJS KEGGViewer from three points of view to acknowledge the broad set of use cases and challenges this work takes on. While there are a number of things to improve upon (as always) and a few points requiring clarification, the project is a nice addition to the BioJS library and may provide a useful data visualization option when deployed with a complementary set of web tools for selecting pathways, managing datasets and viewing details.Generic User:The ""play"" feature is great for comparing conditions. Nicely done!Panning is tricky, I seem to have to hold cmd, click, pause, and then drag. Without the 'pause' I invoke a selection tool.There is no additional information or link-outs when you click on a node; only the gene symbol is provided.There is no interface for accessing the data values underlying the visualization. There is a disconnect between the web developer who sets up the viewer with all the underlying expression data and the end-user who views the data with only limited access and controls.Biomedical Researcher:The default expression range appears to be set at min-min, which results in all data values visualized as up-regulation. I would recommend default values centered on 0 in addition to support for user-provided parameters.Unfortunately, the parameter names and value ranges for data overlays are unnecessarily restricted to ""expression"", ""upColor"" and ""downColor"". A generic solution for data overlay that could work with any type of data (KEGGViewer shouldn't care if it's expression or not) and color gradients or discrete mapping options would be much more useful.All of these sorts of options are in fact already available in closely related tools (also free and open source, and which I happen to work on) that the authors neglected to cite: PathVisio [1] and Cytoscape [2]. These projects have both Java and JavaScript flavors. The JS version of Cytoscape was obviously used and cited in this work, but the Java version with its built-in data import, style and overlay options -- as well as KEGG import -- was missed. Speaking of KEGG, I'm dubious about the blanket statement that it is ""free of charge for academics"". It's a complicated situation that I know many colleagues are unclear about, so I think it's important to describe it thoroughly. According to their own website [3], ""Academic users who utilize KEGG for providing academic services are requested to obtain a KEGG FTP subscription for organizational use, which includes a proper license agreement."" This leads to a licensing agent with various paid subscription options [4,5]. The KEGG API, which KEGGViewer uses, is indeed freely provided for academic use, but only for individual downloads. Bulk downloads, such as those required to do analysis of over representation or enrichment, are explicitly forbidden and require a KEGG FTP subscription [6].Software Developer:It is unfortunate that the EBI host site has resources in conflict with the KEGGViewer. This seems counter to the whole point of BioJS and should be addressed in future releases of the EBI web site, cytoscape.js and/or KEGGViewer (whichever CSS is the most intrusive or classes least specific).Beyond a bit of copy/paste JS (including a 5-level deep JS object), asking users to host a php proxy will likely turn some away. Is there any way around this? References 1. http://pathvisio.org2. http://cytoscape.org3. http://www.kegg.jp/kegg/legal.html4. http://www.bioinformatics.jp/en/keggftp.html5. http://www.pathway.jp/licensing/commercial.html6. http://www.kegg.jp/kegg/rest/",666,11,6,0.8075,0.1092482363,0.9074166417,15,32.12,0.1631,f1000,0.0105263157894737,4,5,5,5,factual,3,5,70,neutral,2,negative,5,moderate,5,5,5,5,factual,5,5,100,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
152,Alfonso-Benítez-Páez,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Summary of the article Boykin et al present a pilot study aiming the application of real-time DNA sequencing for the detection of ACMV and EACMV in cassava plants in multiple crops of African East countries. This study represents the successful approaching of the most valuable feature of the MinION nanopore sequencing platform, its portability. At the same time, the authors made the maximum use of the singularities of the system by the translational application of their results, thus preventing the spread of plant viruses and to improve the crop efficiency by timely advising of farmers. This last exercise really highlights the value of such technology, particularly in the epidemiological surveillance and control of pathogens. Notwithstanding, I have some minor concerns that if addressed they would constitute an added value to the approach described. 1) I strongly recommend that authors store and make publicly available the genetic information retrieved from different sequencing runs to a specialized repository such as ENA or GenBank. 2) It would be very informative for future studies in this field that Table 1 contains additional information the average or median values of the sequence identity derived from the comparison between nanopore reads and reference sequences. In a similar way, they should declare the level of relationship between ACMV and EACMV, in terms of genome-wide nucleotide identity, in order to disclose any potential misidentification given the high error rate of nanopore-derived DNA reads. 3) The authors must be aware that there is a strong effect derived from sequencing kits used, different from Uganda/Tanzania and Kenya. They should make some correlations between the CMD severity scoring and DNA reads retrieved independently accordingly to the kits used. 4) In the same line of thoughts than above, it would be very elegant that authors will estimate the maximum time for expecting a viral DNA read just for setting a threshold and optimize the sequencing time. I have noticed that for CMD severity = 1, it took maximum 4h to retrieve viral DNA reads, using the sequencing kit SQK-RBK001. Another different history was the utilization of SQK-RBK004 in Kenya, where apparently there is not a correlation between CMD severity and viral DNA reads retrieved. In the last cases, the apparently not symptomatic plants were detected as positive in less than one hour. The setting of a time threshold for a proper detection (getting enough number of reads to estimate reliable identification) would be useful to speed up the farmers' advising and consequently the reduction of risks for the spread.",486,0,0,0.8153,0.1035542929,0.8796175122000001,37,23.8,0.103,f1000,0.0104166666666666,4,4,4,3,partially factual,3,3,68,neutral,4,positive,3,low,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,4.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,5,4,4,5,factual,4,4,85,polite,5,positive,5,low,5,4,4,5,factual,4,4,92,polite,5,positive,5,low
116,Reviewer-fyNb,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper proposes a method to combine deep mutual learning with BNN to diversify the weight distributions of each BNN networks in a pair or ensemble, to improve performance. 1. AFAIK this is the first work combining mutual learning with BNN, so the authors can claim this point.
2. The paper is in general written clearly and easy to follow.
3. Experiments are adequate with ablation studies on individual features impact on diversity. 1. Some design choices are found to be ""empirically"" working well without too much discussion or hypothesis.
2. Would be interesting to see how the model performs for o.o.d test data, especially uncertainty performance. Line 178-179: The authors said adding the D(...) term will rapidly increase of this term and impact training. Wouldn't putting a smaller scaling factor for this term fix this issue? None.",138,0,6,0.8457,0.1638888889,0.8492545485,216,54.2802,0.1041,neurips,0.010204081632653,3,4,2,3,factual,3,2,50,neutral,3,neutral,2,low,4,4,4,4,factual,4,4,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,4,low,3,4,3,4,partially factual,4,3,78,polite,5,positive,4,low
116,Reviewer-phoh,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper titled addresses the challenge of improving the performance of Bayesian Neural Networks (BNNs) by leveraging the concept of mutual learning. BNNs provide a means for quantifying uncertainty in predictions through probability distributions of model parameters. However, BNNs often fall short in performance compared to their deterministic counterparts. The authors propose a novel approach that employs deep mutual learning to enhance the capabilities of BNNs. 1. Innovative Approach: The paper introduces a novel method that combines deep mutual learning with Bayesian Neural Networks. By promoting diversity in both network parameter distributions and feature distributions, the proposed approach enables peer networks to acquire distinct features, capturing different characteristics of the input data. This innovative technique enhances the effectiveness of mutual learning in BNNs.
2. Detailed algorithm description: The paper provides a thorough and detailed description of the proposed algorithm for improving the performance of Bayesian Neural Networks (BNNs) through deep mutual learning.
3. Comprehensive Experiments: The authors conduct extensive experiments to evaluate the proposed approach thoroughly. The experimental results are statistically sound and demonstrate significant improvements in classification accuracy, negative log-likelihood, and expected calibration error compared to traditional mutual learning methods for BNNs. 1. Limited variety in experimental validation: One weakness of the paper is that the proposed approach and its effectiveness are only verified through experiments conducted on Residual Neural Networks (ResNets). It would have been beneficial to include experiments on a diverse set of network architectures to demonstrate the approach's effectiveness across different model types and complexities. 
2. Lack of detailed explanation for temperature, α, and β: One weakness of the paper is the limited explanation provided for the temperature parameter (T), α, and β, which are crucial components of the proposed approach. These parameters play a significant role in controlling the diversity of network parameter distributions and feature distributions, but their specific effects and optimal values are not thoroughly discussed.
3. Weakness in the conclusion: The current conclusion merely restates the experimental results and does not highlight the broader implications of the proposed approach or its potential impact on the field. The author should supplement more experiments to prove its effectiveness. The author should supplement more experiments to prove its effectiveness and strengthen the conclusion.",368,0,6,0.788,0.1220982143,0.9327940345,216,14.7437,0.0999,neurips,0.0,2,4,3,3,factual,3,3,60,neutral,3,positive,3,low,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
29,José-L-Herrera-Diestra,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I consider that the case made by the authors in this correspondence are valid and important. Changes in the conditions that lead to the 18% reduction of Rt are certainly a combination of all measures implemented in 2020, and may not be directly applicable in 2021-2022. I agree that a ""rigorous sensitivity analysis"" might be a good starting point. However, besides this sensitivity analysis, more elaborated methods need to be developed to assess more accurately the influence of the different interventions that were in play in the summer of 2020, and which of these interventions could be reasonably extrapolated to 2021-2022.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",220,0,1,0.7723,0.1663711289,0.7168864608000001,351,29.79,0.1953,f1000,0.0096153846153845,3,4,2,3,factual,3,3,80,polite,3,positive,4,none,5,5,4,5,partially factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,partially factual,4,3,65,polite,4,positive,3,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
196,Reviewer-MuxB,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","This paper leverages publicly available data, termed as Proxy Data (PD), as a substitute for original data (OD). The paper addresses the limitations of existing ZSQ methods that rely solely on synthetic data (SD) by introducing a method to select optimal PD based on batch-normalization statistics. The ZeroP framework is applied to existing pure-SD methods, resulting in significant improvements in accuracy. Specifically, ZeroP outperforms state-of-the-art pure-SD methods by 3.9% in a 4-bit setting for ResNet-50 on ImageNet-1K. The paper also introduces a simple and effective method for guiding PD selection, thereby offering a promising solution for achieving high-performance low-bit networks without relying on original data. 1. The paper introduces a new approach to ZSQ by incorporating publicly available Proxy Data, filling a gap in the existing literature. A comprehensive methodology is provided, including a PD selection method based on batch-normalization statistics, which adds to its credibility.
2. ZeroP shows significant improvements in accuracy over existing methods in low-bit settings. 1. While the paper discusses improvements in accuracy, it does not provide sufficient information on the scalability of the proposed method, especially when dealing with larger datasets or more complex models.
2. Lack of performance in low-bit settings, such as 2-bit and 1-bit. I wonder whether the methods used PD can have a competitive performance over the previous quantization/binarization methods.
3. It is better to provide the preliminary knowledge of the proxy data, and how previous work uses the proxy data for the quantization. 1. How well does the proposed ZeroP framework generalize to other types of neural networks or tasks beyond image classification?
2. As for the computational overhead, could you elaborate on the computational cost involved in the PD selection process, and how the computational overhead of the selection of PD compared with the computations in the training process?",300,0,7,0.7922,0.1989015152,0.9175000787,54,22.975,0.3848,iclr,0.0,5,5,5,4,factual,5,5,80,polite,5,neutral,5,none,4,4,4,4,5,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,factual,4,4,75,polite,4,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
196,Reviewer-7Mby,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper introduces a new quantization-aware finetuning method for visual recognition that does not rely on the original training data (OD). The proposed method, ZeroP, instead leverages realistic proxy data (PD) in addition to the conventional synthetic data (SD) to further finetune the model for quantization. Here, incorporating PD based on the batchnorm statistic (BNS) is the key contribution of the paper. Experimental results show that ZeroP outperforms SD-only approaches and performs on par with OD-based works. (S1) \[Motivation\] Going beyond synthetic data for zero-shot quantization is interesting. The reviewer agrees with the author that it is not necessary to rely solely on synthetic data, especially when relevant  information of the target task is available.

(S2) \[Performance\] The proposed method demonstrates superior performance.

(S3) \[Ablation\] Ablations show that PD could be a plug-in solution that helps improve the performance of SD-only methods in general.

(S4) \[Writing\] The paper is easy to follow. (W1) The current method to select the optimal PD dataset is straightforward, i.e. ranking the PDs by the gap of the BNS. The technical contribution is weak.

(W2) Relying on BNS also limits the versatility of ZeroP (as also indicated in the Limitation section)

(W3) If I understood correctly, the key challenge here is to search for PDs that mimic the distribution of the OD. In this case, using only BNS may not be necessary. Depending on the target task, there may be more information we could make use of, e.g. the class names of the target task. (If the finetuning involves a classification loss, this information may already be available.) With such information, instead of searching for a specific PD dataset, we could search for relevant samples via a text-based search engine, e.g. CLIP. 

Overall, the reviewer likes the idea of incorporating PD for zero-shot quantization, and also appreciates the superior performance of ZeroP. The reviewer has concerns about the technical contributions and the potential impacts of the paper. Therefore, the reviewer rates the paper as marginally below the acceptance threshold. N.A.",335,0,2,0.7707,0.1599533279,0.8717119694000001,54,38.8684,0.157,iclr,0.0,2,5,3,1,factual,4,3,45,polite,4,neutral,5,low,3,5,5,4,5,5,5,75,5,5,negative,5,low,2.0,5.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,positive,3.0,none,3,5,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,4,low
6,Reviewer-7LW7,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","The main contribution of this paper is presenting a scalable path-based method A*Net, for link prediction on large-scale knowledge graphs. A*Net is inspired by the A* algorithm for solving shortest path problems, where it learns a priority function to select important nodes and edges at each iteration. This allows for the time and memory reducing for both training and inference. From an efficiency perspective, this could be considered as a path-pruning method to progressively reduce the subgraph based on the learned priority function. The empirical results also demonstrate efficiency improvement. 1. The efficiency problem caused by the explosively increasing entities in deeper propagation layers is indeed serious in the recent GNN-based inductive methods. And the proposed method makes sense and technically sound. 

2. The experimental results are impressive. The paper demonstrates the practical applications of A*Net in various settings and datasets, with the efficiency improvement compared with several recent baselines. Furthermore, the paper sets a new state-of-the-art on the million-scale dataset ogbl-wikikg2 and converges faster than embedding methods. 

3. The paper's organization is well-executed and the content is easily comprehensible.
 1. The paper's comparison to the A* algorithm seems somewhat overstated. As a derivative work of NBFNet, this paper draws an analogy to another shortest path algorithm, A*. Contrary to the Bellman-Ford algorithm that resolves the shortest path problem from the source to all other points, the A* algorithm typically addresses the shortest path problem from the source to a specific target point. However, in the context of knowledge graph (KG) reasoning, the target point is unknown, rendering the core principle of A*, assessing the estimated remaining cost to the target point, unfeasible. In fact, the A* algorithm's priority rule, involving the distance to the target node, is not pertinent to the priority function in the proposed model. The A* algorithm appears to function primarily as a promotional point, rather than as a guiding principle.

2. Perhaps due to the overemphasis on the A* analogy, the paper's true technical contributions remain unclear. Comparing the core function of NBFNet in Eq. 3 and that of A*Net in Eq. 12, the only discernible difference lies in introducing the priority score, calculated based on the embeddings of the query and the current node. Stripping away the A* algorithm framework, it essentially seems to be a path-pruning technique reliant on an attention mechanism to select the top K nodes and top L edges in each layer for efficiency's sake.

3. The paper lacks insightful contributions regarding important paths beyond a weighted version of the NBENet method. The theoretical appendix focuses solely on integrating path selection into the NBFNet framework, premised on the assumption that a certain function can distinguish important nodes. However, how to ensure that important paths are chosen is not clear. In response to this, the authors propose weight sharing between the priority function and the predictor, asserting that the reasoning task can be seen as a weak supervision for the priority function. However, this appears counterintuitive, given that the priority score is dependent on a specific query. A high predictor score, indicating that the node x answers the query (u, r_1), should not contribute to the priority score of x for a different query (u, r_2).
 1. As addressed in Weaknesses 3, could you elaborate on how weight sharing aids in the selection of important paths?

2. I observe that two handcrafted priority functions, PPR and Degree, are employed in the ablation studies. Given that high connectivity doesn't necessarily denote the importance of paths, what about the effectiveness and efficiency of a random pruning strategy, particularly with respect to the obgl_wikikg2 dataset?

3. In the Visualization section, only the results of the proposed method are displayed without any comparison. Could you clarify what distinct paths the Neural function selects compared to the two handcrafted ones? Furthermore, does the Neural-based path selection align more closely with knowledge semantics?
 Yes. The authors stated the limitation, future work and social impact.",657,0,10,0.7662,0.1343045961,0.9303361773,218,36.4806,0.1798,neurips,0.0204081632653061,4,4,5,4,partially factual,4,4,90,polite,4,neutral,4,none,4,5,4,4,partially factual,4,4,88,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
8,Reviewer-4ZaS,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","This paper looks at explicit closed-loop systems with LLMs for adaptive planning utilizing environmental feedback. They showcase better planning performance on ALFWorld and MiniWOB++ environments over existing state-of-the-art works like ReAct and Reflexion. The paper is well written and the experiments are thorough. They present an interesting improvement over the current works like ReAct and Reflexion.
 1. The kind of tasks in these domains don’t seem to have interaction resolution where there are multiple conflicting causal links from the initial to the goal state which have to be resolved (including negative interactions between subgoals). This could also lead to the human demonstrations helping significantly with the  It would be useful to analyze the performance of AdaPlanner specifically in such cases. 

2. I think non-ergodic environments could clearly pose danger to such agents. It would be interesting to see how AdaPlanner can perform against ReAct or Reflexion in such environments. 
 1. Given that the LLM seems to verify the plan to determine its feasibility, what was its efficiency in those assessments? Are there any results pertaining to that?

2. Is there any classification of the tasks with respect to their hardness?

3. For how many of these tasks did the human expert demonstration solve the task? 
 The authors have addressed some of the limitations. I have provided some limitations in the weaknesses section.",222,0,5,0.789,0.1708333333,0.7852613926,215,44.4049,0.1249,neurips,0.011111111111111,3,3,3,3,partially factual,2,4,50,neutral,3,neutral,3,moderate,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,3,3,3,3,factual,4,3,70,polite,4,positive,4,moderate,4,4,3,4,partially factual,3,3,78,polite,5,positive,4,low
107,Reviewer-PggN,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This work proposes a conditional generative model that aligns both image frames and textual instruction tokens (one at a time) to produce multimodal future representations that can encompass visual frames, textual tokens, as well as motor actions, for controlling an agent in an environment.
The proposed method is claimed to align the visual-linguistic representations better, while encouraging the models to understand the world-dynamics in a generative modeling manner.
The method is tested on four simulated embodied environments where the agents follow certain language instructions, where performance gains are reported against two off-policy RL baselines. - The observed multimodal alignment mechanism is interesting and with experimental justification.
- The overall proposed method is neat, where the generative mechanism is a sound and interesting idea to model the visual-linguistic dynamics of the work.
- Consuming all modalities in one model as conditional generative models is neat.
- The paper is well written and easy to follow. - The title is a bit over-claimed, in the sense that the proposed model is still learning to model “one environment” at a time, particularly for the action dynamics as multimodal representation generation. At least an experiment or novel method is required to learn to model some worlds (environments) and generalize to a held-out test world – this would justify the “modeling the world” parts of the claims.
- While claimed to be flexible, in many applications, the instructions of a task will only take place at the beginning of the episode while the rest is the robots’ job to accomplish the instructed tasks, where the proposed multimodal alignment will only be performed from the beginning few frames of the episode. How does the proposed method work under such conditions? E.g., how would the method benefit from such an alignment in environments such as ALFRED \[1\] or TEACh \[2\]?
- In Section 4.4, the performance of the actual SOTA models need to be reported as well, even if the proposed method is inferior to them. There are reasons why modularization and use of certain foundation models is beneficial in these long horizon complex (at least closer to) real world tasks.
- The environments, if at all except for navigation, are all quite toy-ish, where the visual observations are of fairly low fidelity. Since the proposed method heavily relies on the future representation predictions, examining the method on more realistic embodied environments would strengthen the work more.

\[1\] Shridhar, Mohit, et al. ""Alfred: A benchmark for interpreting grounded instructions for everyday tasks."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.

\[2\] Padmakumar, Aishwarya, et al. ""Teach: Task-driven embodied agents that chat."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022. - The proposed method shares some similarities with generative video-guided planning (at least at their high-levels), such as \[3\]. Could you elaborate more on why this is not an incremental concept on top of these works? (Also these works use supposedly much stronger generative models that can tackle more real-world visual observations.)
- What if the language instruction has a much shorter token span and the visual frames are much longer? How do they pad to each other or what would be the token used when language is exhausted out?
- Typos in “Future Prediction” of Section 3.1 – “whih” should be “which”.

\[3\] Dai, Yilun, et al. ""Learning universal policies via text-guided video generation."" NeurIPS 2023",568,6,11,0.7951,0.0924047619,0.8703980446,61,41.8289,0.2746,iclr,0.0,3,4,4,3,factual,3,3,65,neutral,4,negative,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
66,Reviewer-U7ws,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The paper proposes Adaptive Feature Transfer (AFT), a downstream adaptation technique that operates directly on features, thereby decoupling the choice of the pre-trained model architecture from the downstream one. AFT enables combining different pre-trained architectures together during adaptation while distilling only the relevant information for the downstream task to the final model. The algorithm is validated across a diverse set of vision, language and vision-language tasks and compared against knowledge distillation and transfer learning algorithms. 1. The proposed method allows to distill features learned with different architectures on possibly different modalities to any given architecture 
2. The method is validated on both vision, language and vision-language tasks 1. The proposed method promises to distill features from **any** set of models to a given model once the downstream task is know. The paper is positioned as a generic method that could be applied to any set of models (possibly containing architectures different to the downstream one). However, while the presented theory to justify the method is sound and generic, the empirical results do not seem to support the claim. For example, in Figure 1 (right) and Figure 2 (b) adding convolutional features to a ViT based downstream model seem to reduce the performance of the model. Why is it the case? To me it seems to suggest that the proposed method is not strong enough to reject some features that will lead to a worse downstream model. 
    - If this is the case the current algorithm should be coupled with model selection techniques to pick the best features that are more likely to help (see \[1\] and reference therein). Can the authors comment on this more?
2. The previous limitation gets even worse when the set of conditioning models gets larger since the signal to noise ratio drops, making extracting the relevant information for the downstream task even harder. I suggest the authors to consider comparing with explicit sparsity inducing methods as the ones proposed in \[2\] and the references therein.
3. The final algorithm is optimizing theta and rho jointly. However, one would expect \rho being optimized more often than \theta. Typically, this is done with bi-level optimization techniques or simple rewriting \rho in closed form for each given \theta. Did the authors try those more natural alternatives? If \rho is not optimized fast enough the most likely trajectory induced by SGD will be around a stationary point of \rho which leads to a maximally insensitive/uninformative \rho which will be reasonably good on average for many possible \theta, however not optimal for any in particular. 


References:

\[1\] A. Deshpande, et al. “A linearized framework and a new benchmark for model selection for fine-tuning”

\[2\] M. Fumero, et al. “Leveraging sparse and shared feature activations for disentangled representation learning” 1. Why should invariance under orthogonal transformation be of help in the practical optimization optimization objective? Can the authors prove how the optimization landscape will change and get easier to optimize? As of now, this intuitive fact, is left to the ablation studies and only supported by empirical observations.
2. Why not using a different kernel than the linear one? This will make the optimization space much smoother (e.g. by choosing a Gaussian kernel).
3. Visual evaluation on CIFAR100 is quite limited, to increase the impact of the paper on the community I suggest the authors to extend the evaluation to other datasets as the ones used in \[1\]. 

Minor:
- Some typos and grammatical errors are present in the paper, please proofread the manuscript.
- Can you report in the paper the level of sparsity of the rho projection map? This could help the reader understanding what happens when irrelevant pre-trained models are added to the mix.  
- Make the scatter plots with learn probe accuracy vs test accuracy on the same scale. Is the proposed method worse than directly using a linear classifier on the concatenated features?",647,5,5,0.7894,0.0769884071,0.9436648488,65,41.9683,0.4636,iclr,0.0109890109890109,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,5,4,4,5,factual,5,5,88,polite,5,neutral,5,none,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,4,4,4,5,partially factual,4,4,85,polite,5,neutral,5,low
114,Reviewer-5d1d,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","The paper proposes a replay-based CL method utilizing a lightweight task attention module. The module receives features from the feature extractor and performs task-id prediction using the projection vectors for each task. This approach aligns with the findings of a prior theoretical study. The authors conduct comprehensive experiments to demonstrate the benefits of their approach compared to existing baselines and show the effectiveness of the proposed techniques. 1. The proposed approach is grounded in a theoretical study.
2. The proposed method outperforms the baselines. 1. I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.
2. It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.
3. I couldn't fully understand why this method is better than the existing task-id prediction methods. \[1\] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.

Overall, I think this approach is promising, but needs some improvements.

\[1\] Conditional channel gated networks for task-aware continual learning 1. How does the model make the final class prediction? Does it first predict the task-id using the attention module and make a within-task prediction?
2. What's the purpose of using the task projection vectors and why is it used to compute both z_s and z_tp?",262,2,6,0.7371000000000001,0.1460784314,0.8085629344,48,54.6912,0.1507,iclr,0.0,3,4,3,2,partially factual,4,3,55,polite,3,neutral,3,moderate,4,4,3,4,3,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,3,3,3,3,factual,4,4,65,polite,4,neutral,4,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
75,Reviewer-s437,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a new measure of natural language generation (NLG) quality based on similarity between the spectrum of cross-entropy in natural vs. generated text. Fourier Analysis of the Cross-Entropy of language (FACE) is inspired by NLP and psycholinguistic studies suggesting that surprisal is not uniformly distributed in natural text (e.g., content words tend to be more surprising than function words), occurring periodically. For a given generated text, FACE computes a discrete Fourier transform of the token-level cross-entropy sequence (under a separate FACE evaluation LM). Similarity between the vector of frequency magnitudes and that from a randomly selected, natural text corpus are then computed. The paper considers several definitions of FACE metrics, including spectral overlap, cosine similarity, and Pearson/Spearman’s rank correlation coefficients.

LMs from 125 million to over 7 billion parameters are evaluated on NLG of Wikipedia articles, news articles, and stories (with a short prompt of 35 subword tokens provided). Ultimately, FACE is found to be correlated with human judgments of how “human-like”, “sensible”, and “interesting” the generations are. The relationship is not as strong as an existing intrinsic measure, MAUVE. The relative ranking of decoding methods according to FACE agrees with prior works (e.g., greedy decoding < nucleus), as do model size (smaller models produce lower quality generations than larger models). The metric is well-motivated, evaluating whether generated text matches the surprisal statistics of natural text. The algorithm is simple and described sufficiently clearly. FACE is an automatic measure of NLG quality that is, on the face of it, complementary to existing measures. This paper would be of interest to many who work on (large) language models. While FACE is motivated by the desire to match surprisal statistics of natural text, it was not clear how different FACE is from existing metrics. Computing correlation between FACE and existing metrics would help alleviate this, as would providing anecdotes of cases with high/low FACE score vs. high/low MAUVE score, for instance. Have you also considered the spectrum of hidden LM embeddings rather than cross-entropy, and considered how such a metric might differ from FACE? Yes",345,0,1,0.8233,0.0747350351,0.9425024986,216,31.0628,0.1585,neurips,0.0120481927710843,2,3,3,3,partially factual,3,2,55,polite,3,neutral,3,low,4,5,4,4,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,5,4,3,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
75,Reviewer-v6cq,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a set of metrics to measure the distance between model-generated and human-written languages. Specifically, this paper uses FFT to analyze the cross-entropy sequences of the language data. 1. This new metric is efficient. Given the fact that our models are getting exponentially bigger, it is essential that we do not waste energy during evaluation.
2. This new metric correlates well with human judgment, and is statistically sound.

I personally really like the authors' attempt to interpret the metric. Understanding the why is sometimes much more important than understanding the how. 1. The related work on psycholinguistic motivation is limited. Entropy is also a popular metric in computational linguistics, which is probably worth citing.
2. The model size categorization seems to be very coarse.   1. Could the authors be more specific about their motivations for using spectral similarity as a metric? This paper is a good step towards addressing some of the problems brought by generative AI.",159,0,5,0.8219000000000001,0.2167388167,0.9213043451,216,39.0844,0.1108,neurips,0.0,2,3,2,3,partially factual,3,3,50,polite,3,positive,3,moderate,4,4,3,4,partially factual,4,3,75,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,65,polite,4,positive,3,moderate,2,4,3,3,partially factual,4,3,75,polite,4,positive,4,low

paper_id,reviewer,title,abstract,review_text,length_words,citation_count,question_count,mattr,sentiment_polarity,similarity_score,days_to_submit,flesch_reading_ease,politeness_score,venue,hedging,Human_Actionability,Human_Clarity_and_Readability,Human_Comprehensiveness,Human_Constructiveness,Human_Factuality,Human_Fairness,Human_Objectivity,Human_Overall_Quality,Human_Politeness,Human_Relevance_Alignment,Human_Sentiment_Polarity,Human_Usage_of_Technical_Terms,Human_Vagueness,Qwen_Actionability,Qwen_Clarity_and_Readability,Qwen_Comprehensiveness,Qwen_Constructiveness,Qwen_Factuality,Qwen_Fairness,Qwen_Objectivity,Qwen_Overall_Quality,Qwen_Politeness,Qwen_Relevance_Alignment,Qwen_Sentiment_Polarity,Qwen_Usage_of_Technical_Terms,Qwen_Vagueness,Llama_Actionability,Llama_Clarity_and_Readability,Llama_Comprehensiveness,Llama_Constructiveness,Llama_Factuality,Llama_Fairness,Llama_Objectivity,Llama_Overall_Quality,Llama_Politeness,Llama_Relevance_Alignment,Llama_Sentiment_Polarity,Llama_Usage_of_Technical_Terms,Llama_Vagueness,GPT_Actionability,GPT_Clarity_and_Readability,GPT_Comprehensiveness,GPT_Constructiveness,GPT_Factuality,GPT_Fairness,GPT_Objectivity,GPT_Overall_Quality,GPT_Politeness,GPT_Relevance_Alignment,GPT_Sentiment_Polarity,GPT_Usage_of_Technical_Terms,GPT_Vagueness,Phi_Actionability,Phi_Clarity_and_Readability,Phi_Comprehensiveness,Phi_Constructiveness,Phi_Factuality,Phi_Fairness,Phi_Objectivity,Phi_Overall_Quality,Phi_Politeness,Phi_Relevance_Alignment,Phi_Sentiment_Polarity,Phi_Usage_of_Technical_Terms,Phi_Vagueness
166,Reviewer-FAWm,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","This paper presents the Blended Exploitation and Exploration (BEE) operator, which addresses the issue of value underestimation during the exploitation phase in off-policy actor-critic methods. The paper highlights the importance of incorporating past successes to improve Q-value estimation and policy learning. The proposed BAC and MB-BAC algorithms outperform existing methods in various continuous control tasks and demonstrate strong performance in real-world robot tasks. - The paper addresses an important issue in off-policy actor-critic methods and proposes a novel approach to improve Q-value estimation and policy learning. 
- The BEE operator is simple yet effective and can be easily integrated into existing off-policy actor-critic frameworks.
- The experimental results demonstrate the superiority of the proposed algorithms in various continuous control tasks and real-world robot tasks. 1. The novelty of the proposed approach is limited. 
2. The choice of $\lambda$ is largely empirical and requires extra manipulation in new tasks.
3. The paper only provides basic theoretical analysis, such as the accurate policy evaluation and the guarantee of policy improvement. The benefit of linearly combining two Q-value functions is not discussed theoretically.
4. The experiments are conducted in continuous control tasks with dense rewards. The exploration ability can be better evaluated in environments with sparse rewards.
5. There is a lack of discussions with related papers (See Question 3). 1. Emprically, the BAC algoithm will only be more efficient in exploiting the replay buffer. The exploration still rely on the maximum-extropy formulation in SAC. Then why can BAC perform significantly better than SAC in failure-prone scenarios such as HumanoidStandup, as if BAC can better explore the unknown regions?
2. Can you discuss or exhibit the performance of BAC in some tasks with sparse rewards? This can demonstrate the generalizability of the proposed approach.
3. What are the advantages of BAC compared with prioritized replay methods \[1,2\] or advantage-based methods \[3\]? These methods are related to BAC in that they also exploit the replay buffer with inductive bias, so they should be mentioned in the paper.

\[1\] Sinha, S., Song, J., Garg, A. &amp; Ermon, S.. (2022). Experience Replay with Likelihood-free Importance Weights. Proceedings of The 4th Annual Learning for Dynamics and Control Conference.

\[2\] Liu, X. H., Xue, Z., Pang, J., Jiang, S., Xu, F., & Yu, Y. (2021). Regret minimization experience replay in off-policy reinforcement learning. Advances in Neural Information Processing Systems, 34, 17604-17615.

\[3\] Nair, A., Gupta, A., Dalal, M., & Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359.


I am willing to raise my score if my concerns for weaknesses and questions are adequately discussed.",432,8,14,0.7996000000000001,0.1588311688,0.8829935789000001,60,31.9755,0.1565,iclr,0.0,5,4,4,5,factual,4,4,86,polite,5,neutral,4,none,4,5,4,4,factual,5,5,88,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
166,Reviewer-kjkr,Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor Critic,"Learning high-quality $Q$-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. 
Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. 
Deviating from the common viewpoint, we observe that $Q$-values are indeed underestimated in the latter stage of the RL training process, 
primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer.
We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency.
Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism.
We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates $Q$-value using both historical best-performing actions and the current policy. 
The instantiations of our method in both model-free and model-based settings outperform state-of-the-art methods in various continuous control tasks and achieve strong performance in failure-prone scenarios and real-world robot tasks","Motivated by the problem of underestimating values in the training of SAC, this paper introduces the Blended Exploitation and Exploration (BEE) operator, which calculates the TD target based on a combination of the standard TD target and a high expectile of the return distribution. The authors integrate this operator in both model-free and model-based scenarios, followed by a comprehensive experimental evaluation. 1. The paper contains extensive experiment results on both simulation and real-world environments.
2. The paper is written clearly and easy to follow. Figure 1 provides a decent visualization of the underestimation issue. 1. The BAC method tunes its $\lambda$ and $\tau$ differently for tasks in MuJoCo and DMC (Table 1 & 5). It's questionable to claim superiority over other state-of-the-art (SOTA) methods like SAC and TD3, which use consistent hyperparameters (HP) across tasks. Adjusting HP for each task can inflate results as seen in Figure 5, which can be misleading. Why not showcase the automatic $\lambda$ tuning methods from Appendix B.3.3 in the main text if they're effective?

2. Figure 23 reveals that SAC, without the double-Q-trick, still underestimates in the Humanoid task. It's unclear if this is universally true. More convincing results would come from testing this across multiple tasks and providing absolute Q value estimates. I still suspect that Q underestimation largely stems from the double Q techniques, as suggested by the RL community \[1\]. For instance, OAC \[1\] introduces $\beta_{\text{LB}}$ to manage value estimation issues.

3. Presuming the Q value underestimation problem is widely recognized (which I invite the authors to contest), the paper seems to lack innovation. The BEE operator, at its core, appears to be a fusion of existing Bellman operators.

4. The statement ""BEE exhibits no extra overestimation"" seems conditional on specific $\lambda$ and $\tau$ values. For instance, using $\lambda = 1$ and $\tau = 1$ could induce overestimation.

\[1\] Ciosek, Kamil, et al. ""Better exploration with optimistic actor critic."" Advances in Neural Information Processing Systems 32 (2019). See Weakness",328,4,8,0.8027000000000001,0.1464980159,0.8531657457,61,35.3958,0.1507,iclr,0.0,5,5,3,5,factual,4,4,75,polite,4,neutral,4,low,4,5,4,4,partially factual,4,3,80,polite,5,negative,5,moderate,2.0,5.0,4.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
100,Julia-Bosque,LL(O)D and NLP Perspectives on Semantic Change for Humanities Research,"The paper presents an overview of the LL(O)D and NLP methods, tools and data for detecting and representing semantic change, with main application in humanities research. Its aim is to provide the starting points for the construction of a workflow and set of multilingual diachronic ontologies within the humanities use case of the COST Action \textit{Nexus Linguarum, European network for Web-centred linguistic data science}, CA18209. The survey focuses on the essential aspects needed to understand the current trends and to build applications in this area of study.","I reviewed a previous version of this manuscript, for which I recommended a major revision based on the need for a clearer motivation, scope and limitations of this effort, as well as on the structure and flow of the paper at that time. In this new version the authors have addressed all my highlighted concerns: - The motivation, scope and limitations are clearly defined - The interplay between the different sections is elaborated and illustrated with a workflow diagram that facilitates reading. There are numerous references to this workflow and interlinks between the sections, resulting in a cohesive document. - More context is provided in the introductory paragraphs of each section, and the project in which this effort is carried out is clearly introduced. The relation of each section/topic with respect to the overall topic of the survey is now explicit. - The authors have improved the categorization of tools and approaches. - The tables in the appendix summarize the main approaches, tools and resources surveyed according to the proposed classification.  Taking into account these modifications, I maintain the reasons upon which I based the recommendation for acceptance in terms of the criteria for surveys: - The topic of the paper, at the intersection of humanities and the Semantic Web, is interesting and relevant for the advancement in a line of research which poses numerous challenges. - The quality of writing is good and the survey is well balanced, with a broad coverage encompassing theoretical standpoints and approaches, tools, repositories and datasets. - The granularity and length are also appropriate for the text to serve as an introductory text. Minor comments for improvement: - The authors have provided details on the methodology for the survey, indicating the different stages in the generation and keywords used in literature search. There is no explicit reference to a filtering process after those keyword-based search results, was there any filtering step? If so, which criteria were applied? - In table 3, the included resources diverge in their nature, so the current list groups together LLOD Cloud, Lila Etymological Lexicon, LingHub, and Diachronic semantic lexicon of Dutch, etc. for example. I suggest including a mark here to distinguish which resources are particularly relevant for diachronic analysis, in contrast to general LLOD resources (e.g. Lila Etymological Lexicon vs. LLOD cloud and LingHub). - The authors of [12], referenced on p. 5, mention Lemon (Lexicon Model For Ontologies), and in their diagrams (in Github)  they seem to be using OntoLex-Lemon, not its ancestor. Throughout this survey ""OntoLex-Lemon"" is the term used to refer to the 2016 Specification as the outcome of the W3C Ontology-Lexica Community Group, so for that bib. reference I would recommend to replace the mention of ""Lemon"" with ""OntoLex-Lemon"" for consistency in the whole document. *Typos*: l.19, .p. 19, right column, ""A combined resource like this, allows..."" → remove comma p. 20, l. 1, right column → remove ""(linguistic)"", already covered by the first L in LLOD Appendix tables, Table 4. → word embeddings (add pl. ""s"")",503,1,5,0.7741,0.1697330447,0.8522429466,73,34.66,0.2025,semanticweb,0.0,4,4,5,4,factual,4,4,87,polite,5,positive,4,low,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,5,5,factual,5,5,5,polite,5,positive,5,none,4,4,5,5,factual,5,5,95,polite,5,positive,4,low
74,Reviewer-itVg,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","The authors introduced a view sampling strategy for novel view synthesis, grounded in the perspective of causal representation learning. They identified three key metrics to assess sampling performance: the fitting term, the consistency term, and the uniformity term. Additionally, they presented a novel theoretical framework addressing the sampling challenge within NeRF. 1. The introduction of the causal perspective in the view sampling algorithm holds significant potential and could serve as a foundational approach for future research in this domain.
2. The authors meticulously lay out a comprehensive mathematical framework that not only elucidates the underlying problem but also leads to the derivation of the three pivotal terms central to their methodology.
3. The paper stands out for its clarity and coherence, ensuring that readers, regardless of their expertise level, can grasp the concepts and findings presented."" 1. The rationale behind the view-sampling task raises questions. In certain scenarios, acquiring additional view images can be challenging. However, when a substantial number of dense views are already available, the motivation to devise a sampling strategy for training the neural rendering model with sparse views appears insufficient. Specifically, the activeNeRF model's primary objective is to identify the most optimal camera view for capturing the training image, rather than selecting from a plethora of pre-existing images.
2. The paper's primary contribution seems to be the introduction of a metric or loss function to evaluate the selected views. However, the absence of an ablation study that separately assesses the impact of each of these three terms is a missed opportunity for deeper understanding. As a result, the contribution feels somewhat lacking in depth.
3. The proposed loss function presents challenges in differentiability with respect to 't'. The sampling proposal, derived from the farthest sampling strategy, may not be the most efficient approach. It appears to demand significant training resources, resulting in elevated training costs. The potential enhancements in model performance might not justify the trade-off in terms of the increased training time and resource allocation. Please see the weakness above.",335,0,6,0.7966000000000001,0.1848602484,0.9041278958,52,29.0988,0.1431,iclr,0.0098039215686274,2,4,3,4,factual,4,4,85,polite,4,neutral,3,none,4,5,4,4,partially factual,5,5,75,5,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
74,Reviewer-bNPg,Exploring View Sampling Strategy in Novel View Synthesis from Causal Perspectives,"Neural Radiance Fields (NeRF) has shown promising performance on synthesize high-quality and realistic images. But it often relies on a large amount of high-quality training data. Instead of extensively sampling training samples to cover various details of scenes, a series of works have studied how to utilize prior knowledge to achieve high-quality novel view synthesis with limited training samples. However, these methods have not explored the essence of this problem, which is how to get the optimal training set under limited view inputs. 
ActiveNeRF proposes a method based on an active learning scheme that evaluates the reduction of uncertainty given new inputs, selects samples that provide the maximum information gain, and adds them to the existing training set. Since it is necessary to calculate variance changes, evaluating information gain requires the ground-truth of invisible samples, which is impossible to obtain in real situations. We revisit the view sampling strategies from a causal perspective and achieve efficient sampling without requiring the ground-truth of invisible samples. We also propose a new theoretical framework for the sampling problem in NeRF. We analyze how to obtain the optimal sampling strategy based on our framework. Experiments shows that our conclusion can not only guide sampling, but also can help us design regularization term for general NeRF.","This paper studies the view sampling strategies of Nerf reconstruction from a causal perspective. The authors try to solve the problem using a small subset of photos from a total of K potential views, to achieve the best reconstruction. To solve this, the authors propose to use causal represntation learning using loss by Identification Treatment Effect. They propose three terms, a normal fitting term as reconstruction loss, a consistency term to ensure consistency between visible views and invisible views and a uniformity term requires the samples to be distributed evenly. The results show the proposed strategy can provide slightly better reconstruction compared to alternative baselines in the proposed setting. * The paper proposes a novel perspective to study the view sampling problem in volumetric reconstruction using NeRF as an example. This take-away can potentially also generalize other multiview reconstruction algorithms. 
* Given its current setting, the hypothesis is validated on nerf reconstruction datasets, with small improvement compared to its baselines. * The presentation of this paper could be greatly improved. I may not have understand a lot of details correctly given its current presentation. 
  * It is very hard to read without being very familiar with ActiveNeRF and casual representation learning. Have to trace to original papers for more details. This could be added to the preliminary parts. 
  * Too many notations which makes things more complicated than needed. I don't think I found how exactly the loss of consistency term and uniformity term were calculated in (8) at runtime. As I understand, the method should be as simple as calculating the reconstruction loss using different groups of input samples. Provide an algorithm chart of how of how P^{F}, P^{hat}^{CF} and P^{CF} will greatly help. 
  * There are some notations introduced in 4.1 (e.g. P(Y|do(d))) are not explained until 4.2. 
* Overall I am not sure I understand the real-world impact of this paper using the proposed strategy. Maybe I had some misunderstanding in the details given my concern on its presentation. Please correct me if I am wrong here. The goal of this paper to find ""optimal sampling strategy for training set"", ""K_s corresponding photos as sparse sample inputs among K_d total potential views"" is hardly a real problem statement for its real-world use case, which is my biggest concern for this proposed application of causal representation learning. From sampling perspective, we can use all the K_d potential views as long as they are available. As I understand, the evaluation of the counter factual distribution will require using the non-selected but captured images as supervision, which is not how active learning is executed in real-world case. Given this setting, it makes the results also less appealing in contrast to alternative baselines (which learns to predict next-best unknown view) given the fact all images from that particular datasets are used in evaluating the sampling strategy. 1. My major question is around how the clarity of the sampling process in training time. Confirm any places I misunderstood about this paper, as I highlighted in the weakness part. 
2. I am also curious how the views are sampled finally for different groups in the final results. Provide some visualization and discussions about them can be very helpful to guide the view-sampling process in real world applications. I wonder how that indicate the connection of uniformity term and consistency term are correlated to the camera FoV and ray distributions.",566,0,2,0.8138000000000001,0.1125,0.8472209573,52,40.8228,0.33,iclr,0.0,3,3,4,4,factual,4,4,90,polite,4,neutral,3,none,5,4,4,5,partially factual,4,4,65,polite,5,negative,5,moderate,2.0,3.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,5,neutral,4,moderate,2,2,3,4,partially factual,3,3,65,polite,4,neutral,4,low
194,Reviewer-GDNX,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.",">**Rebuttal:** The provided details satisfy my concerns. I think this paper should be accepted after applying the agreed changes.

>**TL;DR:** **Good paper.** The proposed WTA-CRS algorithm is based on the existing CRS algorithm and is used to reduce activation memory during training. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size. However, WTA-CRS comes with computational overhead, which is discussed and explore. Addressing my concerns and questions would improve my score.

The paper proposes the WTA-CRS algorithm to reduce the neural networks training activation memory, where the paper claims that activation memory is primary memory bottleneck during training. The WTA-CRS algorithm is an unbiased estimators for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. WTA-CRS achieves up to 2.7× peak memory reduction with almost no accuracy drop and enables up to 6.4× larger batch size.

The WTA-CRS algorithm works by sampling columns and rows to create an unbiased estimation of the original GEMM for the backpropagation. The WTA-CRS algorithm does not alter the neural architecture, and therefore the inference speed is left in tact. The experimental section shows that WTA-CRS outperforms existing prior work and is compatible with existing PEFT techniques. WTA-CRS adds a computational overhead due to sampling, however, WTA-CRS enables training on much larger batch sizes, which results in a 1.2× higher training throughput.
 * **S.1.** The proposed WTA-CRS algorithm tackles an important problem in existing PEFT techniques, which makes LLM PEFT training more accessible to researchers with low resources.
* **S.2.** The paper provides a theoretical analysis on WTA-CRS.
* **S.3.** The proposed WTA-CRS algorithm outperform existing algorithms.
* **S.4.** An anonymized code repository is provided as part of the submission for reproduction .
  * **W.1.** Popular existing memory efficient training techniques such as tensor rematerialization (gradient checkpointing) \[2\]\[3\] and ZeRO \[1\] are not compared to, although some are partially discussed in Appendix A.
* **W.2.** The experiments are conducted on single neural network architecture (T5), although the proposed technique does not seem to be confined solely to that setting.
* **W.3.** It is common practice today to train neural networks at a lower precision (quantization), however, it is not clear whether quantization (16bit) was used. Therefore, there is insufficient proof that the combined noise of WTA-CRS and quantization would be compatible.


**Typos.**
* Line #62: ""Thus"" → ""Thus,""
* Line #240: ""mAccording"" → ""According""
* Line #297: ""Thus"" → ""Thus,""

\[1\] Ren, J., Rajbhandari, S., Aminabadi, R.Y., Ruwase, O., Yang, S., Zhang, M., Li, D. and He, Y., 2021, July. ZeRO-Offload: Democratizing Billion-Scale Model Training. In USENIX Annual Technical Conference (pp. 551-564).

\[2\] Jain, P., Jain, A., Nrusimha, A., Gholami, A., Abbeel, P., Gonzalez, J., Keutzer, K. and Stoica, I., 2020. Checkmate: Breaking the memory wall with optimal tensor rematerialization. Proceedings of Machine Learning and Systems, 2, pp.497-511.

\[3\] Beaumont, O., Eyraud-Dubois, L. and Shilova, A., 2021. Efficient combination of rematerialization and offloading for training dnns. Advances in Neural Information Processing Systems, 34, pp.23844-23857. * **Q.1.** In line #43 and Figure 2 it is noted that ""storing activations (or feature maps) is the main memory bottleneck during training"". Does this hold true for all model architectures? What about LLM training where the fine-tuning batch size is usually very small?
* **Q.2.** Why was the WTA-CRS algorithm compared to the Deterministic top-k from \[1\] but not to the Bernoulli-CRS from \[1\]? What are the key differences between WTA-CRS and Bernoulli-CRS?
* **Q.3.** The paper proposes WTA-CRS which sacrifices computation speed at the cost of lower peak memory. There are several existing common approaches (such as gradient checkpointing and DeepSpeed) for general memory efficient training which are compatible with PEFT techniques. Why are these comparisons not explored or detailed in the main paper?

\[1\] Adelman, Menachem, Kfir Levy, Ido Hakimi, and Mark Silberstein. ""Faster neural network training with approximate tensor operations."" Advances in Neural Information Processing Systems 34 (2021): 27877-27889. The limitations are discussed in Appendix A.",669,10,14,0.753,0.0903401361,0.8664653897,215,42.5651,0.1507,neurips,0.0128205128205127,3,4,3,3,factual,4,4,80,neutral,4,neutral,5,moderate,4,4,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
194,Reviewer-cMiu,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The authors studied fine-tuning LLMs with limited memory. As the increased scale of current LLMs, the memory cost during fine-tuning is of great importance when adapting the pretrained LLMs to down-streaming tasks. In contrast to the existing work that mainly focus on the number of updated weights, this paper proposed to reduce the number of stored activations, also the inputs to each layer. Given the widely used stochastic gradient descent optimization pipeline, the authors proposed to store a subset of activations that can generate an unbiased gradient estimation. This way, the training memory and the training time decreased significantly. The authors provide both theoretical and experimental analysis on their CRS methods.  - This paper studied an important problem in LLM fine-tuning, i.e., how to fine-tuning LLMs with less memory consumption without increasing the computation cost. The authors provided solid quantitative results to show that the main memory consumption is from storing the intermediate activations. 
- The authors provided a general solution for fine-tuning LLMs under memory constraints. The solution can be applied in most transformer-based network architectures.  
- The authors provided solid mathematical proof on the unbiased gradient estimation, which is especially encouraged. 
- The extensive experiments on different network architectures showed the efficacy of the methods.
- The released code can benefit the following researchers studying efficient LLM fine-tuning.  - I am not fully convinced by the comment made in Line241-244, i.e., the methods in the paper is orthogonal to the activation quantization. When activation is quantized into a lower bit width, it is very possible that the number of less important activations will decrease. This way, the selection on the top-k columns in activation matrices with the proposed methods may hurt the training accuracy or convergence. It would be great if the authors can provide some theoretical analysis or experimental results on this combination. Otherwise, it would be necessary to provide some comparison results w.r.t. the activation quantization.
- It would be great if the authors can discuss the main difference of their paper w.r.t. \[Randomized Automatic Differentiation, ICLR2021\].	  Overall, I think this paper has a relatively high quality in both writing and scientific contribution. Yes",358,0,2,0.7825000000000001,0.1275074405,0.8477004170000001,215,33.3958,0.1262,neurips,0.0,4,4,4,3,factual,4,4,80,polite,4,positive,4,low,5,5,4,5,partially factual,5,5,88,polite,5,positive,5,low,3.0,4.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
194,Reviewer-ky3t,Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model,"As the model size grows rapidly, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. 
Previous works usually focus on reducing the number of trainable parameters in the network. 
While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. 
Notably, machine learning models are typically trained using stochastic gradient descent.
We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance.
Following this motivation, we propose a new family of unbiased estimators called \sas, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.
Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones.
By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7X peak memory reduction with almost no accuracy drop and enables up to $6.4\times$ larger batch size.
Under the same hardware, \sas enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.
The code is available at https://anonymous.4open.science/r/WTACRS-A5C5/.","The paper's contribution is in proposing a practical, intuitive yet not trivial unbiased approximation to gradient training of matrix multiplication. It shows that even though totally deterministic sampling is biased, somewhat deterministic sampling is unbiased, and a judicious allocation of sampling to those pairs favored by deterministic thinking can lead to the use of a larger batch size with empirically negligible performance loss. This reviewer must declare that he does not check the derivation very carefully. The proposed idea is practical and can be readily combined with virtually all first-order gradient-based training methods.
The paper also derived why deterministic sampling is a biased estimator and empirically shown the associated bad performance, thus proving that the additional complexity of stochastic sampling over deterministic sampling is not only sufficiently better but also necessary. It's just a few empirical comparisons, but the performance gap between CRS and WTA-CRS seems modest. This reviewer does not have a question. N/A",155,0,1,0.8418,0.0621428571,0.7829982042,215,17.3432,0.1041,neurips,0.0109890109890109,3,4,2,3,factual,3,3,70,polite,4,neutral,3,low,2,4,3,2,5,5,4,65,neutral,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,1,3,3,1,partially factual,3,3,50,polite,4,positive,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,positive,4,low
38,Reviewer-vqBu,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This work proposed a framework for performing inference on Gaussian Graphical Models by approximating the posterior with a normalizing flow over PSD matrices. In this way, the authors can investigate $l_p$-norm regularized GGMs for any value of $p$ in an efficient way. The idea of using normalizing flows for GGM inference definitely brings in advantages of both Bayesian and frequentist worlds; to me, that's an innovative idea. The main weakness that I identified is the lack of comparison between the proposed framework and the well-studied graphical lasso with concave approximations of the $l_0$-norm. More precisely, the authors show that their framework obtains frequentist solution paths through simulated annealing, therefore, it'd be of great interest to see a comparison between these solution paths and those obtained by iterative algorithms such as iterative reweighted l1-norm for graphical lasso. - in the frequentist case, how does the proposed framework compares against more classical techniques to obtain the solution paths, e.g., iterative reweighted l1-norm?  The authors adequately addressed the limitations of their proposed framework. ",170,0,0,0.799,0.32,0.9384971857,215,30.7103,0.1149,neurips,0.0112359550561798,3,4,3,2,unfactual,2,2,60,polite,3,neutral,2,low,4,5,4,4,factual,5,5,85,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,3,4,factual,4,4,75,polite,5,positive,4,low,3,4,3,4,factual,3,4,78,polite,5,neutral,4,low
38,Reviewer-3sWQ,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper proposes a method that can be used to infer conditional independencies in a Gaussian model. These conditional independencies are related to zeros in the precision matrix. Typically, sparse enforcing norms are used to estimate the precision matrix while enforcing zeros in the elements outside of the diagonal. In this paper a Bayesian approach is considered. For this a pseudo-distribution for the data is considered by taking the exponential to the p-norm. The method is trained via variational inference combined with normalizing flows to increase the accuracy of the posterior approximation. The variational distribution is tuned via simulated annealing and a temperature parameter allows to interpolate between the Bayesian and the Map solution. - Well written paper.

        - Illustrative toy experiments. - The proposed method is a combination of already known techniques.

        - The experimental section is weak as only a single real problem is considered.

        - Although the proposed method is a generalization of several known techniques, I have found in the experimental section a lack of comparisons with other related methods.

        My main point of criticism is the weak experimental section which only considers a single real problem and no comparisons with other related methods are carried out in real problems.

        Another point of criticism is that, for some particular values of the p parameter one does not actually observe sparsity in the Bayesian solution. For example, when sampling from the Laplace distribution one never observes zeros in practice. Spike and slab priors (a mix between a Gaussian and a point of mass center at zero) are the ones that actually lead to zeros. None The authors have not commented on the limitations of their approach.",279,0,2,0.7414000000000001,-0.007047619,0.9233116508,215,36.096,0.0989,neurips,0.0,2,3,3,1,unfactual,3,3,60,neutral,3,negative,3,low,3,4,4,3,partially factual,3,4,65,neutral,5,negative,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,positive,3.0,none,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
38,Reviewer-pTVu,Conditional Matrix Flows for Gaussian Graphical Models,"Studying conditional independence among many variables with few observations is a challenging task.
Gaussian Graphical Models (GGMs) tackle this problem by encouraging sparsity in the precision matrix through $l_q$ regularization with $q\leq1$.
However, most GMMs rely on the $l_1$ norm because the objective is highly non-convex for sub-$l_1$ pseudo-norms.
In the frequentist formulation, the $l_1$ norm relaxation provides the solution path as a function of the shrinkage parameter $\lambda$.
In the Bayesian formulation, sparsity is instead encouraged through a Laplace prior, but posterior inference for different $\lambda$ requires repeated runs of expensive Gibbs samplers.
Here we propose a general framework for variational inference with matrix-variate Normalizing Flow in GGMs, which unifies the benefits of frequentist and Bayesian frameworks.
As a key improvement on previous work, we train with one flow a continuum of sparse regression models jointly for all regularization parameters $\lambda$ and all $l_q$ norms, including non-convex sub-$l_1$ pseudo-norms.
Within one model we thus have access to (i) the evolution of the posterior for any $\lambda$ and any $l_q$ (pseudo-) norm, (ii) the marginal log-likelihood for model selection, and (iii) the frequentist solution paths through simulated annealing in the MAP limit.","This paper targets the structure learning problem in Gaussian Graphical Models via (Normalising) Flow-based Variational approximation of the elements of weight metrics that correspond to the Gaussian Bayesian network. 
They use sub-l1 pseudo norms to penalize dense precision metrics (which correspond to graphs with numerous links) without imposing an extra high penalty for large non-zero values (which typically occurs if $l_{\geq1}$ is used). 1. Up to my knowledge, this is the first time flows are applied to the space of positive definite matrices. 
2. The proposed approach is flexible meaning the class of applicable prior and likelihood functions is quite large.
3. Using sub-l1 norm is suitable for structure learning. 
4. The proposed algorithm is mathematically sound (as far as I can follow) and is quite interesting. 
5. The paper is well-written, and the relevant work is sufficiently discussed.    
6. Due to its flexibility, the proposed method has the potential of having a large impact. Due to the factors mentioned in the previous section, I find this work impressive and beautiful. However, unfortunately, the carried out experiments are minimal. Most notably, the algorithm is compared to no alternative work (neither in the main paper nor in the supplementary material). With no quantitative comparisons, it is impossible to evaluate the performance of the proposed algorithm compared to the existing methods. 

NOTE: In the Rebuttal, some experiments are carried out (though the code is still not accessible).    

Minor suggestion: 
1. Though it is clear in the context, I suggest that the authors do not use the same letter ""p"" (with the same font) for both probability density and norm parameter.  
2. Fix minor typos e.g. the end sentence period in line 214. * In line 141, what do you mean by ""contradiction""? The authors should compare their method with the relevant structure learning lierature and reveal its points of strength as well as its limitations. 

This work is theoretical/methodological and does not have any positive or negative social/ethical impact on its own.",330,0,9,0.7934,0.1236940837,0.8818445206000001,215,45.1097,0.1969,neurips,0.0119047619047618,3,4,3,4,unfactual,3,3,60,neutral,4,negative,3,low,5,5,4,5,5,5,5,90,5,5,5,5,1,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
13,Joseph-philipraj,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This systematic review is appropriate for the journal with a global problem of Mets and Urolithiasis. The introduction part clearly explains the motivation. The manuscript is clear and balanced. The manuscript stays focused on the subject. Authors have gone through the process of searching relevant articles from all websites and of sufficient duration. The inclusion and exclusion criteria in the analysis have been clearly stated. The impact of the analysis is clearly stated. The statistical analysis supports the paper well. The interpretation of the results, visualisation are well presented. The tables and figures are clear, relevant and correct. The authors demonstrate the knowledge of basic composition skills, including word choice, sentence structure, paragraph development and grammar. Limitations:  The studies included in the meta-analysis have cross-sectional nature and hence ascertainment of temporal association is not possible which also dictates need for further prospective studies. The specific type of stone formation is not correlated with studies. Despite these limitations all studies included in the meta-analysis showed the same directionality in the association between urolithiasis and Mets.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",292,0,1,0.7415,0.1145833333,0.8487246633000001,39,30.46,0.0999,f1000,0.01,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,3,5,5,3,factual,5,5,85,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,85.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,5,4,4,factual,4,5,92,polite,5,positive,5,low
13,Muhammad-Faruk,Association between metabolic syndrome components and the risk of developing nephrolithiasis: A systematic review and bayesian meta-analysis,"Background: There is increasing evidence that nephrolithiasis is a systemic disease, as opposed to an isolated urinary metabolic problem, after considerable links were found between nephrolithiasis and systemic diseases such as hypertension, obesity, dyslipidemia, and insulin resistance. The interplay between these four factors defines metabolic syndrome (MetS). In this review we aim to clarify the associations of MetS and its components to kidney stone incident. Methods: Online databases of EMBASE, MEDLINE, and Google Scholar were searched from January 1998 up to October 2020 to identify observational studies examining the association between metabolic syndrome components and kidney stone incident. Bayesian random-effects meta-analysis and meta-regression were performed to observe the association. Linear dose-response analysis was conducted to shape the direction of the association. Data analysis was performed using STATA, and R statistics. Results: A total of 25 potentially relevant studies (n = 934,588 participants) were eventually identified. The pooled results suggested that metabolic syndrome was associated with an increased risk of nephrolithiasis with an odds ratio (OR) of 1.769 (95% CI: 1.386 – 2.309).  The summary OR of hypertension and dyslipidemia for developing nephrolithiasis were 1.613 (95% CI: 1.213 – 2.169) and 1.586 (95% CI: 1.007 – 2.502) respectively. The presence of diabetes mellitus and obesity had an OR of 1.552 (95% CI: 1.027 – 2.344) and 1.531 (95% CI: 1.099 – 2.109) respectively. Our results revealed that the increasing number of MetS traits will increase the risk of developing nephrolithiasis, the higher the fasting plasma glucose, and body mass index, the higher the risk of kidney stones incident. Conclusions: Our results suggest that hypertension, diabetes, obesity and dyslipidemia are associated with increased risk of developing nephrolithiasis. Linear significant association between MetS components and nephrolithiasis were revealed in our study which reinforced the notion that should be considered a systemic disorder.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assessed the association between metabolic syndrome and its components with the risk of developing nephrolithiasis by conducting systematic review, Bayesian random-effects meta-analysis, meta-regression and dose-response analysis. This study was done appropriately based on PRISMA flowchart. Risk of bias was also conducted of the included studies. This study has successfully presented the proper meta-analysis for this design. However, to complete this study for indexing, I personally recommended several revisions: 1. Abstract: Introduction section: It is better to address meta-regression as the analysis to assess the correlation of association along with dose-response analysis  Conclusion section: In reporting the association between predictors and nephrolithiasis, state only the predictors in which its coefficient was statistically significant 2. R language was not considered as a statistical software for data analysis. The software for data analysis should be written as ""R"" (Please refer to methods section in statistical analysis subsection). 3. Please update the PRISMA flowchart (refer to PRISMA guideline 2009). 4. Give the numbering of each Forrest plot in Figure 3 and numbering of each meta-regression plot in Figure 4. Design these figures so that it could be well presented. 5. Uniformly decide the word choice of ""traits"" or ""components"", choose whether to use traits or components in the whole text, use one of these words consistently to avoid any misunderstanding. 6. It is better to provide the meta-regression of hypertension in systolic blood pressure and diastolic blood pressure as it is important to explain the relationship to nephrolithiasis in differentiation for these two types of blood pressure. 7. Meta-regression of body mass index  was sufficient in this study thus waist circumference meta-regression was not necessary to be included. 8. Provide the value of coefficient and confidence interval of each meta-regression analysis in the result section so that better understanding of predictors-outcome relationship could be reached clearly.  Are the rationale for, and objectives of, the Systematic Review clearly stated? Yes  Are sufficient details of the methods and analysis provided to allow replication by others? Yes  Is the statistical analysis and its interpretation appropriate? Yes  Are the conclusions drawn adequately supported by the results presented in the review? Yes",422,0,7,0.7554000000000001,0.1982758621,0.9299524426,270,24.68,0.2302,f1000,0.0108695652173913,5,5,5,5,factual,5,5,93,polite,5,neutral,5,moderate,4,4,4,4,factual,4,4,85,polite,4,neutral,4,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,85.0,polite,5.0,positive,4.0,none,5,4,4,5,factual,5,5,90,polite,5,positive,5,low,4,4,4,4,factual,4,5,88,polite,5,neutral,5,low
181,Reviewer-HLbG,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","This work aims to build a foundation model for knowledge graph reasoning tasks, where the authors explore the setting of generalization to any edges and nodes, including unseen, of any multi-relational knowledge graphs without using node and edge features. To this end, the authors first construct a view of a relation-centric graph from an original graph where edges become nodes of this new relation graph, and then, based on this view, the authors represent the relation (node) relative to and conditioned on the query relation. Then, based on this relative relation representation, the authors use existing inductive link prediction methods to perform knowledge graph reasoning. The authors conduct link prediction experiments on various knowledge graphs considering both inductive and transductive settings, and show that the proposed method, namely ULTRA, outperforms other SOTA baselines sometimes without further fine-tuning on target knowledge graphs (i.e., zero-shot). * This work studies the very important, challenging, and practical setups of building a foundation model for knowledge graph reasoning, which aims to be generalizable to any other knowledge graphs involving unseen nodes and unseen edges, without leveraging features of nodes and edges. 
* The proposed method works well with different knowledge graphs, on zero-shot transfer learning setups without further fine-tuning on target knowledge graphs, and further shows the boosted performance with task-specific further fine-tuning on them, on most experiment setups.
* This paper is very well-written and easy to follow. * I would like to note that I don't see any major weakness, and below is the minor.
* In Section 4.2, the explanation about the indicator function with variables $u$ and $v$ is a bit unclear to me. Could you elaborate more on the process and result of the indicator function according to those two variables, perhaps with visuals?
* Text-based methods (e.g., LM-based methods) can be generalizable to any knowledge graphs including unseen nodes and unseen edges, as long as their nodes and edges are represented with texts. In this vein, I think one potential direction for building a foundation model for knowledge graph-related tasks might be to use the LMs, and the authors may highlight this point more and potentially make comparisons between the proposed approach and text-based methods. I don't think this should be the critical weakness of this paper since text-based methods are limited to knowledge graphs with textual features; meanwhile, given the framing of this work (""Towards Foundation Models for Knowledge Graph Reasoning""), this point should be carefully explained. * I would like to suggest emphasizing the performance differences between inductive and transductive setups when explaining Table 1. The proposed method w/ 0-shot settings are strong on inductive graphs; meanwhile, previous methods are superior to it on transductive graphs, which are worthwhile to discuss.
* It may be beneficial to show the results of the ULTRA fine-tuned on the knowledge graphs used for pre-training the ULTRA. I am wondering if there are further performance improvements when further fine-tuning the model on the data used for pre-training.",496,0,0,0.7682,0.1549267161,0.9152074456,49,38.4364,0.3761,iclr,0.0,4,4,3,4,factual,3,3,80,neutral,4,neutral,3,moderate,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,3.0,4.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,4,low
181,Reviewer-c3Sg,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","Paper claims to propose a foundation model, named Ultra, for knowledge graph representation learning. The proposed model can handle full inductive graphs in which new entities and relations may appear in the test set. To do so, the authors propose to lift the graph to a one with relations as the nodes and design 4 different edge types (head2head, head2tail, tail2head, tail2tail). The relational representations are then learnt using message passing on this graph. The learnt relation embeddings are then used in the original graph to perform inductive link prediction. For the experiments, the authors pre-train their method on 3 KGs and further evaluate in a zero-shot setting and also by fine-tuning the downstream tasks. - The paper proposes a transductive model that works in settings of new relations and entity nodes.
- The method obtains good zero-shot pretraining results. - The authors have not explicitly stated the computational complexity of the method. From the paper, it seems that the forward pass is run on the entire relational graph to obtain relation representations. This is then used to initialize the node embedding from the query triple and the process is repeated for every triple. Thus it seems that the entire graph is being used for link prediction every triple making the computational complexity O(E^2). This seems limiting for large graphs that have not been explored in the paper (such as wikidata-5m etc.).
- From Table 2 we can see that finetuning over the pre-trained models helps the results significantly over the 0-shot setting. Also, the fine-tuning steps are too large to claim few shot results. This weakens the claim of the ""foundation model"" for KGs. A fair comparison would be to show the pretraining results for other inductive and transductive methods as well in addition to the SOTA comparison.
- Another limitation is that of scale. Since the current model has fewer parameters, this would limit learning over larger pretraining datasets as can be seen in Figure 6 and also reported by the authors.
- SoTA results for transductive models are better than the pre-trained Ultra model in many datasets. Thus the Ultra model seems to work well for the inductive setting rather than transductive. Thus the claim of the ""foundation model"" seems broader in scope.
- We see that in the metafam dataset, the pretraining results are poor but on finetuning the results are improved drastically. This shows that the method works well in cases where the relational patterns of the downstream datasets are similar to the pre-trained one but when the data distribution changes the results suffer. Moreover, due to limited capacity, the model may not be able to handle such cases by increasing the pretraining datasets calling for downstream finetuning. Thus domain adaptation is not a problem which can be easily overcome by scaling the current model and this further weakens the claim of a ""foundation model"" for KGs. - For weakness point 2: Any reason why this was not done by the authors?
- For weakness point 3: How would this be addressed in future works for the model?
-  For weakness point 4: Could the authors comment on why this would be the case and how would the model be improved to handle the transductive setting?
- For weakness point 5: Any reason why the results on this dataset are not good?
- Considering KGs are a rich source of textual/semantic data along with graph/structured data and the current model does not use this rich source of context information, how can we extend Ultra to incorporate the KG ontology? 
- Considering weaknesses 2,4,5 the claim of the foundation model seems a bit broad as of now and at best the model could be said to be a good inductive learner.",625,0,0,0.7487,0.1723163098,0.9095818996,49,51.6761,0.1355,iclr,0.0104166666666666,4,4,4,3,factual,3,3,70,neutral,3,neutral,3,high,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,5.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
181,Reviewer-ebFz,Towards Foundation Models for Knowledge Graph Reasoning,"Foundation models in language and vision have the ability to run inference on any textual and visual inputs thanks to the transferable representations such as a vocabulary of tokens in language. 
Knowledge graphs (KGs) have different entity and relation vocabularies that generally do not overlap.
The key challenge of designing foundation models on KGs is to learn such transferable representations that enable inference on any graph with arbitrary entity and relation vocabularies.
In this work, we make a step towards such foundation models and present ULTRA, an approach for learning universal and transferable graph representations. 
ULTRA builds relational representations as a function conditioned on their interactions.
Such a conditioning strategy allows a pre-trained ULTRA model to inductively generalize to any unseen KG with any relation vocabulary and to be fine-tuned on any graph.
Conducting link prediction experiments on 57 different KGs, we find that the zero-shot inductive inference performance of a single pre-trained ULTRA model on unseen graphs of various sizes is often on par or better than strong baselines trained on specific graphs. 
Fine-tuning further boosts the performance.","The key limitation of designing the foundation models for dealing with the Knowledge Graphs (KGs) is that the KGs have different entities and relations that generally do not overlap. To address this issue, this paper proposes ULTRA, which positively transfers the information of source KG to unseen KG. It constructs relation representations based on the interactions between the relations by introducing the graph of relations. The proposed approach has shown good performance on various tasks. - From their experiments, the proposed methods have shown good performance on various tasks.
- Research topics about the foundational models on graph-structured datasets is really interesting and important.
- The paper is well written and easy to follow. - The authors first pretrain the ULTRA model with the mixture of 3 standard KGs and then fine the model for the downstream task. But, the other supervised SOTA model only uses dataset of the downstream tasks without employing the pre-training datasets. If the supervised SOTA models are designed to deal with transductive settings, they may show worse performance on the downstream tasks. However, if the SOTA models are the models for the inductive setting, I think they may be possible to be pretrained like the ULTRA. So, could you measure the performance of the ""pretrained"" SOTA models on the inductive if possible? Please refer to the weaknesses.",222,0,0,0.7001000000000001,0.1619617225,0.9080925584,49,47.3913,0.1823,iclr,0.0,3,4,2,3,factual,3,3,65,neutral,3,neutral,2,high,4,5,4,4,partially factual,4,4,85,polite,5,positive,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,2,4,3,3,factual,3,3,65,polite,4,positive,4,low,1,4,2,2,partially factual,3,2,45,polite,4,positive,3,low
9,Reviewer-3zCE,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","This work proposes to modify stochastic gradient descent (SGD) to overcome forgetting and promote plasticity in continual learning. These goals are achieved by masking out the parameters with high utility and perturbing gradient direction by Gaussian noise. For utility computation, the authors propose an approximate but efficient scheme based on second-order Taylor expansion of the loss. Experiments demonstrate that (i) the proposed utility approximation is more accurate than simple baselines such as weight magnitude, (ii) it maintains plasticity, (iii) plasticity and accuracy are in general correlated, (iv) the method tends to forget less than baselines, and (v) it simultaneously promote plasticity and prevents forgetting. This paper is written well. The notation is okay and the mathematical derivations seem correct. The baseline methods are clearly outperformed and the experiments verify the central claim of the paper. Although continual learning is an important machine learning challenge, I feel the paper suffers from significant weaknesses:

  - First and foremost, I do not think this paper makes a significant contribution. The methodology is incremental in that it combines two well-known ideas (perturbed gradient descent + keeping active neurons unchanged). 
  - Second, it is tested on very toy setups. The experiments are not convincing enough to show the applicability of the method to interesting real-world setups. For instance, I am not sure the networks achieve similar plasticity if tested on, e.g., webcam data instead of MNIST, where the feature space is a lot richer and hence plasticity is much more difficult.
  - Third, theoretical properties/implications of the method should be carefully examined. 
    - For instance, the Taylor expansion would only hold if $W_{l,i,j}$ are infinitesimally small. We do not know in general if this holds or not. I suggest the paper should include a (preferably rigorous) discussion on this.
    - Likewise, gradient descent is no longer steepest descent but some approximation to it. Investigating why it works is important. As shown by the results, no collapse occurs but again, I wonder how this translates into more challenging settings where utilities of most parameters are high. Here I list my questions as well as suggestions:

- It would be better if Label-Permuted EMNIST was described before the results are discussed in paragraph 5.
- _Although a few methods address both issues simultaneously, such methods expect known task boundaries, maintain a replay buffer, or require pretraining, which does not fit streaming learning._ <--- reference needed for this claim.
- What does ""a Hessian diagonal approximation in linear complexity"" mean? Linear in the number of parameters?
- It would be better if the main text included details on the ""utility propagation theorem"".
- It would be better if the descriptions of the tasks/datasets (e.g. Input-Permuted MNIST in section 4.2) were given before the details.
- Does ""each learner is trained for 1M samples, one sample each time step"" mean gradient descent using one sample only? Is this realistic?",480,0,0,0.8331000000000001,0.0934353741,0.9183989763,47,39.3732,0.1932,iclr,0.0,4,4,3,4,factual,4,4,80,polite,4,positive,4,low,5,4,4,5,factual,4,4,85,polite,5,negative,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,3,4,75,polite,5,negative,5,low,4,4,3,4,partially factual,3,3,70,neutral,5,negative,4,low
9,Reviewer-y5kB,Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,"Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.","The paper proposes a measure of weight utility of weights in neural networks for given loss and using it to modify a gradient-based weight update in networks to alleviate the problem of catastrophic forgetting.  The authors identify two fundamental aspect of catastrophic forgetting - the forgetting aspect (not losing what the network already know) and plasticity aspect (ability to learn new concepts).  The proposed method is meant to address two problems at the same time, preseving high utility weights with no modifications (to prevent forgetting) while randomly perturbing low utility weights to ""encourage"" them to participate in the computations related to new tasks (plasticity).  Empirical evaluations show solid performance of the proposed method according to the forgetting and plasticity metrics newly defined by the authors. The proposed rule is straight forward.  

Computational complexity of the evaluation of true utility is well addressed making the method practical.

Empirical evidence provided shows the proposed rule is effective for alleviation of catastrophic forgetting.

Decomposing the catastrophic forgetting problem into two aspects: forgetting and plasticity, seems very sensible.

Proposed measures of plasticity and forgetting seem sensible.

The paper is well written. Though empirical evidence provided in the paper suggest it does (in that it works), I am not sure that the proposed definition of weight utility make sense.  The power of neural networks (and the problem of the interpretation of its computation) is its distributed computation.  Utility of an individual weight is almost always nothing - in fact, quite often any particular weight, sometimes even large number of weights, can be taken out of the network, with little impact on performance.  So, it's more about combinations of weights working together...and the proposed utility doesn't measure that.  I understand that evaluating utility of combinations of weights is intractable, but I worry that this simplification, of judging utility of each weight in isolation, is encouraging less distributed representation, which might come with a penalty in performance.

Fundamentally, on the forgetting front, the proposed method is just another weight consolidation method, and it's a bit hard to believe it beats Elastic Weight Consolidation.  It am not 100% sure that the proposed method doesn't favour plasticity over forgetting nor that the forgetting evaluation isn't biased towards methods that favour plasticity (see questions below). Though I understand (and like) in principle what the utility-based update is supposed to do, I can't quite understand why it actually works.  The proposed measure of the utility of parameters is a measure with respect to the loss on the new input/output pair.  If this pair comes from a new task, how does measuring utility of the model parameters with respect to the loss of this new task have bearing on the utility of the parameters for the old tasks?  Just because utility of a given weight is, say, low for the current sample, it doesn't mean it's low for previous samples.  It seems to me that the proposed method would score high on plasticity (it finds available weights for new task)...but I don't see how it protects against forgetting, in principle, though if we are to talk about empirical evidence...  I don't understand how 4.3 measures catastrophic forgetting.  Permuting labels of CIFAR10 with the new tasks suggests to me that it's all about plasticity again.  Shouldn't it be an experiment, where labels are kept intact, but new tasks are added...and previous tasks examples are not used?  Am I missing something about how experiments reported in 4.3 are done?

Why are the accuracy results of training on CIFAR-10 and EMNIST so poor in Figure 6?  State of the art CIFAR-10 is close (or above) 90%.  Something close to 80% would be probably still acceptable...but 60% is quite poor.  I am not exactly sure what EMNIST variant entails, but is 70% accuracy a good accuracy for this dataset?  It is often easy to shown improvements of something at the low end of the models' performance, but that doesn't always translate to same effect at the high (or close to) end of the models' performance...and in the end, the latter is what we really care about.  So, does the proposed method prevent forgetting at the high end, when model is performing at or reasonably close to state of the art?

This is not a massive issue, but does the per batch normalisation of utility make the performance of the method variable with different  mini-batch size settings?",730,0,0,0.7435,0.0644808927,0.8415006399,47,41.9389,0.0501,iclr,0.0,4,4,4,4,factual,3,4,85,polite,5,neutral,3,low,3,4,4,3,partially factual,3,3,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
81,Gatot-Soepriyanto,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study investigate whether financial distress, earnings management and audit quality as determinants/moderating variable for financial statements fraud in Indonesian listed firms during 2015 to 2019 period. The authors focused on infrastructure, utility and transportation sectors. In general, the study has been designed adequately to tackle the research questions and issues posed by the authors. However, there are some elements need to be addressed to improve the paper: Whilst the study provide adequate research background and institutional setting, it did not mention on why the study focuses on infrastructure, utility and transportation sectors? Is there any specific issues on that sector that related to financial statements fraud? In addition to that, why the study chooses 2015-2019 period?  The study should also discuss the reason choosing F-Score as its main measure for financial statement fraud. Why, for example, the study did not use, Beneish M-Score? Or other accounting irregularities measures in the literature?  The study needs to provide descriptive statistics table, so the reader can gauge and understand the dataset better. This should be provided before the authors arrive with the hypothesis discussion;  Given the study uses panel data (multi years, across different firms), is there any attempt to mitigate the issues of panel data regression? For example, using year-fixed effects or even using panel data regression analysis?  The manuscript need to be checked in terms of the quality of English write up. The title for example, is a little bit confusing, as it did not really represent what the study want to achieve in general.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",400,0,1,0.7771,0.117454955,0.9143848419,220,26.81,0.0168,f1000,0.0204081632653061,4,4,3,3,partially factual,4,3,70,polite,4,neutral,3,moderate,5,5,4,5,factual,5,5,90,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
81,Toni-Šušak,"Financial distress, earning management, financial statement fraud and audit quality as a moderating variable: listed companies on the Indonesia Stock Exchange","Background: Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases. Companies create fraudulent financial statements for a variety of reasons, including financial challenges and debt payment delays. Financial fraud is created by five factors: pressure, opportunity, rationalization, capability, and arrogance. Methods: The purpose of this study is to see whether audit quality (AQ) has a moderating effect on the relationship between financial distress (FD) and earning management (EM) to financial statement fraud (FSF) in infrastructure, utility, and transportation companies listed on the Indonesia Stock Exchange during the years 2015 to 2019. The data sources are the www.idx.go.id and the company’s annual reports. Purposive sampling was used to collect data from thirty companies over the course of five years, totaling 150 observations. Moderating regression analysis (MRA) was used in data analysis. Result and conclusions: The hypothesis testing revealed that FD and EM have a significant impact on FSF.  AQ is able to moderate the relationship between FD to FSF but unable to moderate the relationship between EM to FSF.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Despite the interesting idea for research, the paper has its shortcomings: ** The title of the paper is too long, it should be shortened. ** Throughout entire paper (including the title) the term “earning management” is used instead of “earnings management”. ** [Page 1] “Accounting practices, profit bubbles, information manipulation and deception, and earning management are all examples of fraudulent financial statement cases”. – Earnings management is not necessarily fraudulent behavior. Why are accounting practices and profit bubbles listed as fraudulent? ** [Page 1] The website www.idx.go.id cannot be reached. The better option was to write the name of the source instead of a website. ** [Page 3] The name of the company is not Xeroc, it is Xerox. ** [Page 3] “Fraud is practice that involves the use of deception to acquire unfair or unlawful advantages by one or more individuals. This means that fraud is an act committed by specific people, whether intentionally or unintentionally, to benefit themselves and others.” – How can a fraud be unintentional? ** [Page 3] “Earnings management (EM) is profit engineering carried out by managing revenues (cash inflows) and expenses (cash outflows) to ensure that the company's operations generate net operating profit.” – Revenues are not synonym of cash inflows, nor are expenses synonym of cash outflows. ** [Page 3] F-score should be written with capital F. ** [Page 3] “Principal” should be written instead of “principle”. ** [Page 4] “Asymmetric information” or “Information asymmetry” should be written instead of “Asymmetry information”. ** [Page 4] “Donald Cressey” should be written instead of “Donald Cressy”. ** [Page 6] “Financial statements” should be written instead of “financial statistics”. ** [Page 6] “The study's subjects are companies in the infrastructure, utilities, and transportation sectors that have been listed on the Indonesia Stock Exchange during five years observation.” – What is the reason for choosing these sectors? ** [Page 6] If panel regression model is used, methodology and applied tests should be elaborated. ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 6] “If a corporation has more than one fraud score model, it is assumed that it will commit fraud.” – Should it be written “If a corporation has F-score value more than one…”? ** [Page 7] “EM is classified as a form of fraud.” – Earnings management is not necessarily fraudulent behavior. ** [Page 7] DACC formula has duplicated content.Instead of DACCit = TAit/Ait-1*TAit/Ait-1 - NDACCit it should be written DACCit = TAit/Ait-1 - NDACCit. TAit/Ait-1 is duplicated in the formula. The same remark is applicable to: ** [Page 6] RSST Accrual formula has duplicated content. ** [Page 7] Jones model formula should be included in the paper and elaborated. ** [Page 7] “Big Four” should be written with both capital letters (not “big four”). ** [Page 7] α, β, and ε is doubled in the explanations of formulas. ** [Page 8] Besides test variables, it is advisable to include additional control variables in the multiple regression model. ** [Page 8] “1. The constant is 0.258, indicating that the FSF is 0.193 if FD and EM are both zero. FSF does not occur in the research sample since the F-score is less than 1.” – Instead of “if FD and EM are both zero” it should be written “if all other variables are zero” given that AQ is also part of the model. ** [Page 8] “2. The FD coefficient is 0.791, which means that if the level of FD rises by one, the level of FSF rises by one as well.” – Instead of “the level of FSF rises by one as well” it should be written “the value of FSF rises by 0.791”. Ceteris paribus assumption should be stated. ** [Page 8] “3. EM's coefficient is 0.830. This means that if the management uses EM, the possibility of FSF will increase by 0.830.” – Instead of “if the management uses EM, the possibility of FSF will increase by 0.830.” it should be written “if the value of EM increases by 0.1, the value of FSF will increase by 0.083”. Ceteris paribus assumption should be stated. ** [Page 8] Variable explanations for moderating regression should be revised according to the previous three comments. ** [Page 10] “Industrial industry” should be corrected. ** Paper lacks descriptive statistics of the research sample. ** Robustness analysis could be conducted using alternative fraud measures. ** This paper would benefit from some closer proofreading. It may be useful to engage a professional English language editor. There is abundance of grammatical and typo errors (e.g. “diffucties”, “condisions”, “modeartes”, “criteras”, “coefiesient”, “shareloder” etc.).  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",909,0,5,0.6798000000000001,0.1225925926,0.8569852710000001,489,50.12,0.072,f1000,0.0,4,4,5,4,factual,4,4,80,polite,4,neutral,5,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,5,4,5,5,partially factual,4,4,85,polite,5,neutral,4,low,3,3,4,4,partially factual,4,4,75,polite,5,neutral,3,low
24,Silvio-Buscemi,Case Report: A giant ruptured splenic hydatic cyst in a patient with a complete situs inversus: Diagnostic challenge and intra-operative difficulties,"The splenic localization of hydatid cysts is extremely rare. A 50-year-old obese female who consults with a painful and febrile syndrome of the right hypochondrium. Abdominal ultrasound and a CT scan computed tomography revealed a complete situs inversus, a mass of the right hypochondrium measuring 152 mm with membrane detachment, and infiltration of the surrounding fat, evoking a type II complicated splenic hydatic cyst. The patient was operated on in an emergency via midline laparotomy. Exploration revealed situs inversus, an angiant cyst of the spleen. Exposition of the splenic pedicle is difficult. The samples were then infected. Total splenectomy was performed. The postoperative period was unproblematic, and the patient was discharged with antibiotic and antiparasitic treatment and habitual vaccination.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case described is very interesting and well-written. I have some general considerations for you below. It is appropriate to discuss cystic echinococcosis in female with obesity. Given the unique nature of this case with situs inversus, including descriptive arrows in the CT images is essential and reassuring. This will provide clear visual guidance for the reader, enhancing their confidence in the case report. Please elaborate on the antiparasitic treatment used, including the specific regimen followed (it is important to continue the treatment after the cyst spontaneously ruptures to avoid possible dissemination). It is essential to document the changes in antibody titers and blood chemistry tests following surgical treatment and therapy (it would be appropriate to document how in the article, that could also be mentioned: Ref 1). This will not only inform the reader but also enhance their knowledge about the progression of the disease.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Partly  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Partly  Is the case presented with sufficient detail to be useful for other practitioners? Partly",280,0,1,0.7857000000000001,0.1403645833,0.7798862457,34,24.27,0.3225,f1000,0.0105263157894737,1,3,1,2,unfactual,3,1,48,polite,3,negative,1,extreme,4,5,3,4,factual,4,4,85,polite,5,positive,5,moderate,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,4,4,3,4,factual,4,4,80,polite,4,positive,4,low,3,4,3,4,partially factual,4,3,78,polite,5,positive,4,low
77,Houcemeddine-Turki,Fact Checking in Knowledge Graphs by Logical Consistency,"Misinformation spreads across media, community, and knowledge graphs in the Web by not only human agents but also information extraction systems that automatically extract factual statements from unstructured textual data to populate existing knowledge graphs. Traditional fact checking by experts is increasingly difficult to keep pace with the volume of newly created information in the Web. Therefore, it is important and necessary to enhance the computational ability to determine whether a given factual statement is truthful or not. In this paper, our goal is to 1) mine weighted logical rules from a knowledge graph, 2) to find positive and negative evidential paths in a knowledge graph for a given factual statement by the mined rules, and 3) to calculate a truth score for a given statement by an unsupervised ensemble of the found evidential paths. For example, we can determine the statement ""The United States is the birth place of Barack Obama"" as truthful since there is the positive evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) in a knowledge graph, and it is logically consistent with the given statement. On the contrary, we can determine the factual statement ""Canada is the nationality of Barack Obama"" as untruthful since there is the negative evidential path (Barack Obama, birthPlace, Hawaii) ∧ (Hawaii, country, United States) ∧ (United States, ≠ , Canada) in a knowledge graph, and it is logically contradictory to the given statement. For evaluation, we constructed a novel evaluation dataset by labeling true or false labels on the factual statements extracted from Wikipedia texts by the state-of-the-art BERT-based relation extractor. Our evaluation results show that the proposed weighted logical rule-based approach outperforms the state-of-the-art unsupervised approaches significantly by up to 0.12 AUC-ROC, and even outperforms the supervised approach by up to 0.05 AUC-ROC not only in our dataset but also in the two publicly available datasets. The source code and evaluation dataset proposed in this paper is open-source and available at https://github.com/machinereading/KV-rule and https://github.com/machinereading/KV-eval-dataset each.","This manuscript presents a novel rule-based approach for fact checking in knowledge graphs based on mining textual resources. The work provides new evidences that rule-based approaches can provide more precise evaluation of the accuracy of statements in knowledge graphs and can enhance the efficiency of embedding-based methods when combined with them. The availability of source codes and datasets in GitHub is an advantage for this work as this will allow the reproducibility of the described experimental study. However, there are several matters within the paper that should be addressed to ameliorate its final output: (i) Introduction: The ""Introduction"" seems to be a summary of ""Related Studies in Fact Checking"" rather than a proper introduction and contextualization of the paper. I propose to expand the part about misinformation fighting in the introduction to give better context for the development of this research paper. The authors can benefit from previous research papers about fact checking in general to develop the introduction of the paper. Several points in the introduction should be moved to Related Studies in Fact Checking. (ii) The paper did not emphasize the advantages of rule-based approaches when compared to embedding-based methods beyond having a better precision. Effectively, there are many other advantages of rule-based approaches. For example, the results of rule-based approaches can be more explainable than the ones of embedding-based approaches. Such advantages should be expanded and highlighted in the research paper. (iii) The paper did not emphasize the importance of having the datasets and source codes available in a specific GitHub repository. The authors should specify that this practice allows reproducibility and further development of the work by peers, particularly in the conclusion. (iv) The paper did not discuss the concept of reification in knowledge graphs. Effectively, several knowledge graphs add qualifiers to triples to provide a characteristic of the statements (i.e. {(s,p,o), p, o}. The authors should discuss the usefulness of the method to verify the qualifiers of the statements in the Discussion or as a future direction for this work. (v) The paper should evocate the robustness of the rule-based approach they proposed to adversarial attacks. This can be an advantage of the approach. (vi) There are several typos in the research paper (e.g. ""UC Berkely"" should be ""UC Berkeley""). The authors should proofreading the paper to eliminate such deficiencies. (vii) The authors can expand the Discussion of the work (Part 5) to explain the strengths of KStream, KLinker, COPPAL, RUDIK, and PredPath that contributed to their efficiency as reported in the Experimental Study according to previous research papers. This can explicate the reasons of why the method developed by the authors was more efficient. (viii) The authors should provide future directions for the development of this work in the conclusion. Given this, I propose to accept this paper for publication after these six major revisions are applied.",473,0,1,0.7464000000000001,0.1097294372,0.9120528102,4,36.08,0.2025,semanticweb,0.0,4,4,3,4,factual,3,4,85,polite,4,neutral,4,low,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,5,4,5,5,factual,5,5,90,polite,5,positive,4,low,5,4,4,5,factual,4,4,88,polite,5,positive,3,low
67,Reviewer-8RW7,Efficiently incorporating quintuple interactions into geometric deep learning force fields,"Machine learning force fields (MLFFs) have instigated a groundbreaking shift in molecular dynamics (MD) simulations across a wide range of fields, such as physics, chemistry, biology, and materials science. Incorporating higher order many-body interactions can enhance the expressiveness and accuracy of models. Recent models have achieved this by explicitly including up to four-body interactions. However, five-body interactions, which have relevance in various fields, are still challenging to incorporate efficiently into MLFFs. In this work, we propose the quintuple network (QuinNet), an end-to-end graph neural network that efficiently expresses many-body interactions up to five-body interactions with \emph{ab initio} accuracy. By analyzing the topology of diverse many-body interactions, we design the model architecture to efficiently and explicitly represent these interactions. We evaluate QuinNet on public datasets of small molecules, such as MD17 and its revised version, and show that it is compatible with other state-of-the-art models on these benchmarks. Moreover, QuinNet surpasses many leading models on larger and more complex molecular systems, such as MD22 and Chignolin, without increasing the computational complexity. We also use QuinNet as a force field for molecular dynamics (MD) simulations to demonstrate its accuracy and stability, and conduct an ablation study to elucidate the significance of five-body interactions. We open source our implementation at https://github.com/Zun-Wang/QuinNet.","This paper aims to incorporate 5-body interactions into geometric deep learning models. They first analyze the topology of 5-body interactions and identify three 5-body angles. Then they propose an efficient way to incorporate these 5-body information into models. The complexity of the proposed QuinNet is still O(|N|), the same as many previous 2-body methods like PaiNN. The results are comparable to previous SOTA methods. This paper is well-written and easy to follow.

The experimental results show that the proposed method can perform well on most tasks. The ablation study in Section 5.4 and Figure 5 show that the proposed 5-body information indeed helps to model. See details in the Question part. 1. About the motivation:   
 this paper aims to incorporate 5-body interactions into geometric deep learning models. However, based on my understanding, using up to 4-body (torsions) interaction is already complete \[1\]\[2\] in terms of capturing the geometric structures. If this is correct, then why do we need these 5-body angles? In addition, if we can incorporate 5-body interactions, do we also need to incorporate 6-body interactions?

2. About the complexity:   
in Section 4.3, the authors claim that the complexity is O(|N|), as efficient as many 2-body methods like SchNet and PaiNN. But I think this complexity is not well explained. Using pseudocode/algorithm may be better to analyze the complexity. In addition to the analysis, I suggest the authors use some results to empirically verify the great efficiency compared to other baseline methods, e.g. the inference time, used memory, etc.

3. About the tasks:   
this paper focuses on MLFFs, how about other molecular property prediction tasks, such as QM9 and OC20? I am wondering if this method is specially designed for MLFFs, or can be used on all 3D molecule tasks. In other words, why do the authors emphasize MLFFs? Is there any significant difference between MLFFs and other molecule property prediction tasks?

4. Other related papers: many-body \[3\], MLFFs \[4\]

5. The j, k in Figure 2 are confusing to me. For example, in (f), why not be i, j1, j2, j3, and k1?

\[1\] ComENet: Towards Complete and Efficient Message Passing for 3D Molecular Graphs.  
\[2\] GemNet: Universal Directional Graph Neural Networks for Molecules.  
\[3\] On the Expressive Power of Geometric Graph Neural Networks.  
\[4\] Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. None",394,8,6,0.77,0.1385714286,0.8948391676,215,48.9981,0.1429,neurips,0.0,4,4,4,4,factual,3,3,85,polite,5,negative,4,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
171,Magdalena-Czlapka-Matyasik,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I commend the authors to contribute to this body of literature regarding the socio-demographic and lifestyle factors associated with understanding fast food consumption in Cambodia adults. The paper brings quantified information about the factors influenced by understanding fast food consumption. The authors revealed that poor and fair knowledge, insufficient exercise levels, and not getting enough sleep were predictors of inadequate understanding of the impact of fast food on health. Such conclusions do not bring entirely new knowledge to the literature, on this matter. Across the whole world population, the problems related to fast food consumption have been discussed. Nevertheless, I consider the work to be original, well designed and contribute knowledge to this field of public health research. My suggestions concern: In the introduction, the authors indicate the system of fast-food restaurants; it would be more attractive to explain, how it was developed in Cambodia directly?  What would be very interesting is information concerning the real take away or fast food intake in those groups. It must or might be in direct relation to this matter?  My main concern about the validation of the ""Level of knowledge of fast food consumption"": Could the authors explain the procedure? How were the questions selected and validated?  I regret that the authors discussed the interesting results in such a concise way.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",361,0,1,0.7773,0.2011784512,0.89415133,137,35.27,0.1507,f1000,0.01,0,4,1,0,unfactual,3,3,35,neutral,3,neutral,1,high,4,4,4,4,partially factual,4,4,85,polite,4,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,3,4,3,3,factual,4,4,70,polite,4,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
171,Wanshui-Yang,Socio-demographic and lifestyle factors associated with understanding fast food consumption among adults in Cambodia,"Background: Over the past decades, fast food has been rapidly gaining popularity and availability worldwide. Its consequential impact on human health is among the highest in terms of non-communicable diseases. Therefore, this study aimed to investigate the level of understanding of fast food consumption among adults in Phnom Penh, the capital city of Cambodia. Methods: A cross-sectional analytical study aimed to investigate the level of understanding of factors associated with fast food consumption, among adults in Phnom Penh. Multi-stage random sampling was used to select 749 respondents from 12 communes of five districts in Phnom Penh. A structured questionnaire was used to assess the level of understanding of fast food consumption, and associated factors. Data were analyzed using descriptive statistics, together with bivariate and multivariable logistic regression. Crude odds ratios (CORs) and adjusted odds ratios (AORs) with 95% confident intervals (CI) were calculated to show the strength of associations. Results: The understanding of factors associated with fast food consumption was poor in 52.07% (95% CI: 48.48-55.66), fair in 22.70% (95% CI: 19.69-25.70) and good in 25.23% (95% CI: 22.12-28.35) of those surveyed. After adjusting for other covariates, unsatisfactory levels of knowledge around fast food consumption were found to be significantly associated with not taking regular exercise (AOR = 1.53; 95% CI: 1.15-2.25; p<0.001) and sleeping less than eight hours per night (AOR = 1.64; 95% CI: 1.09-2.12; p=0.014). Conclusion: Health promotion and disease prevention should be conducted among at-risk populations in order to raise the level of understanding of factors around fast food consumption.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This manuscript investigated the level of understanding of fast-food consumption among adults in Cambodia. The authors found that unsatisfactory levels of knowledge around fast food consumption were significantly associated with not taking regular exercise and sleeping less than eight hours per night. The results are interesting, but I have several comments. The authors should introduce the recent development of fast-food sector in Cambodia in the introduction part.  Is there an analysis of fast-food intake among these participants?  Interestingly, the authors found that not taking regular exercise and sleeping less than eight hours per night were associated with unsatisfactory levels of knowledge around fast food consumption. However, they were not well explained in the discussion part. In other words, the discussion part is a little too concise.  In addition, the education levels were not associated with the knowledge of fast-food consumption in the present study. I would like authors to discuss it and, at least, mention its possible causes.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",303,0,1,0.7774000000000001,0.1265046296,0.9193514585,520,28.03,0.1953,f1000,0.010204081632653,0,4,1,0,unfactual,3,3,40,impolite,3,negative,0,high,5,5,4,5,factual,5,5,92,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
187,Reviewer-2CBB,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","This paper introduce a novel unsupervised feature selection methods called GRSSLFS, which combine matrix factorization with self-representation subspace learning and apply graph regularization to preserve the geometric structure of the feature vectors. This method is proved to be effective in both theory and experiments. In this paper, the author introduce the problem of the redundant data in traditional self-representation, and then apply matrix factorization self-representation problem to achieve the goal to reduce the dimension of basis matrix. Here are some strengths of this article:

1. This paper introduce a novel problem of redundant data in self-representation problems and then propose a method to solve this problem.

2. Plenty of theoretical proof are given in the paper and appendix, the convergence analysis indeed increase the persuasiveness of the article.

3. The proposed method was compared with a variety of comparison algorithms on multiple data sets, demonstrating the effectiveness of the method. However, there are still some weaknesses in this paper.

1. In the end of Introduction section, the second and third contribution points is not sufficient, as these constraints of regularization are not proposed in this article. 

2. In the methodology section, some formula calculations are confusing and not very convincing. Such as the multiplication in formula (6) and the optimization target in the optimization goal (formula 7). These issues will be described in detail in subsequent questions 1 and 2.

3. In the methodology section, the description of the algorithm is not complete enough. The specific process of selecting features according to the matrix U in Algorithm 1 has not been described in detail. 1. In the section of methodology, the equation (6) is confusing and not so clear. It seems impossible to subtract the matrix XUV of shape m*m from the matrix X with the shape m*n? 

2. As the feature matrix B is fixed by the VBE method proposed in section 3.3, it is unclear why the basis coefficient matrix G in equation (7) is a parameter to be optimized. Why the matrix G can not be determined by equation (4) directly and reduce the number of parameters.

3. In section 3.1, subspace learning that introduces graph regularization seems to be existing methods. Should this part of the content be moved to related work?",376,0,8,0.679,-0.048046398,0.9640573859,51,40.0877,0.0751,iclr,0.0,4,4,4,4,factual,4,3,70,polite,4,neutral,4,low,3,3,4,3,partially factual,4,4,70,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
187,Reviewer-yUvs,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Authors of this paper propose graph regularized self-representation and sparse subspace learning (GRSSLFS) for unsupervised feature selection. The basis extension method is modified to select bases with highest variance score. These bases are used to build graph regularized self-representation learning and subspace learning. The graph regularized self-presentation learning and subspace learning are combined in terms of a set of selected bases from input space with the highest variance score. Experiments on various datasets demonstrate the advantage of the proposed method comparing with baselines. The ablation study also shows the necessity of each component. The subspace learning module is the key component, but its derivation from selected bases highly relies on the assumption that XU=B. This might not be hold if B is selected according to the proposed variance basis extension method. Moreover, the selection based on subspace learning module lacks of convincing explanation since G does not exactly represent X based on B as a fixed set of feature vectors. It is confusing to explain (4) as the self-representation problem if B is arbitrary basis matrix since they may not come from the input data matrix X.  Taking PCA for example, the columns of B are orthogonal, but they are not from the input feature space. Moreover, B defined as a square matrix of size m is inconsistent with the sleeted r bases in section 3.3.

In section 3.1, authors mentioned that two features have a similar structure in the feature space, and it is expected Bg_l and Bg_r have similar structure. What does the similar structure mean? How is the similarity measured? In other words, it is unclear how the matrix A is constructed. 

The derivation in section 3.2 depends on the assumption that XU=B. As B is a set of feature vectors selected from input data, it is unclear whether the assumption still holds or not. Similarly for Theorem 3.1, it is trivial to have if the assumption holds. 

The variance basis extension is to simply change the selection order of feature vectors in terms of variance score of feature vectors. It is possible that for each individual feature, the variance is high, but is it similar to say the largest amount of data dispersion? 

For completeness, authors should describe the derivation process on how equations (9)-(11) are obtained. Since all three equations are fractional, is it possible that any of the denominators can be zero? How is it handled?

In Algorithm 1, the selected features are derived from U. However, U is not directly related to the input X instead to B and G, unless BG=X. However, B is selected feature vectors from input space. It is unclear why the assumption can hold. So why is the selection rule proper?

The computation complexity is quite high since it is quadratic to both the number of samples and the number of features comparing with most of baseline methods.
The application to the PneumoniaMNIST dataset is quite interesting. However, the way of presenting the outcomes can be improved significantly.  For example, what is the interested region? how many selected features are in the interested region? How do other compared methods perform? The validation is not quantified. How many radiologists are involved in the evaluation?  What is the performance measured? These plots shown in Fig. 2 delivers less useful information except that more red points are accumulated in the center when the number of selected features increases.",567,0,0,0.7308,0.0892117117,0.9616214633,51,50.3623,0.0364,iclr,0.0,3,4,4,4,factual,5,5,75,polite,3,neutral,4,low,4,4,5,4,4,5,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
187,Reviewer-PMap,Unsupervised Feature Selection using a Basis of Feature Space and Self-Representation Learning,"In recent years, there has been extensive research into unsupervised feature selection methods based on self-representation.
However, there exists a major gap in the mathematical principles that underlie these approaches and their capacity to represent the feature space.
In this paper, a novel representation learning method, Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), is proposed for the unsupervised feature selection.
Firstly, GRSSLFS expresses the self-representation problem based on the concept of ``a basis of feature space'' to represent the original feature space as a low-dimensional space made of linearly independent features. Furthermore, the manifold structure corresponding to the newly constructed subspace is learned in order to preserve the geometric structure of the feature vectors. Secondly, the objective function of GRSSLFS is developed based on a self-representation framework that combines subspace learning and matrix factorization of the basis matrix. Finally, the effectiveness of GRSSLFS is explored through experiments on widely-used datasets. Results show that GRSSLFS achieves a high level of performance in comparison with several classic and state-of-the-art feature selection methods.","Considering there exists a major gap in the mathematical principles that underlie the self-representation based unsupervised feature selection approaches and their capacity to represent the feature space, this paper proposes Graph Regularized Self-Representation and Sparse Subspace Learning (GRSSLFS), for the unsupervised feature selection, which expresses the self-representation problem based on the concept of “a basis of feature space” to represent the original feature space as a low-dimensional space made of linearly independent features. Experiments on widely-used datasets are conducted to validate the efficacy of the proposed method. 1. The computational complexity of the proposed GRSSLFS method is low, which is efficient for large-scale and high-dimensional data;
2. The results of the proposed method seem better than other ones. 1. Most of the compared methods are out-of-date, only one method used for comparison was publised in 2023, other methods are before 2020;
2. The motivation of the proposed method is not clear. In Eq.(8), the first three terms have been well explained, but the final regularization term has not been explained. See weakness.",172,0,4,0.6699,0.1067307692,0.9783408642,51,26.959,0.0945,iclr,0.0,2,4,1,3,partially factual,2,2,30,polite,3,negative,3,moderate,3,4,3,3,partially factual,4,4,65,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,3,3,factual,3,4,65,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
103,Reviewer-36E8,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","The paper presents an approach for learning dynamics preventing abstractions for sensorimotor observation space. Given a set of high-level skills and the learned dynamics preserving abstractions, the paper claims to develop an approach for planning for a solution. 

The approach is evaluated in two test domains where the paper shows the visualization of the learned abstractions. - For the most part of the paper, it is extremely well written. Given the wide use of embodied AI systems and robots, an approach that generates plannable abstractions for high-dimensional sensor input is extremely important. 

- The paper nicely motivates the problem. While the paper in general is nicely written, it has a few limitations: 

- The paper advocates learning a continuous abstract  representation instead of a symbolic abstractions. However, it does not provide any reasons to that. Why are continuous abstractions more desirable than symbolic abstractions? 

- Sec 4.1 is unclear. The notation for MI is a bit unclear. It needs to be made more clear. Sec 4.1 requires a re-writing including more explanation for the equation. I have two important questions: 

- How is the dynamics preserving abstraction defined in Def. 3.6 different from the Markovian abstractions defined in \[Srivastava et al. 2016\]? 

- Can you discuss the differences between the presented approach and \[Allen et al. 2021\] 

Reference 

Allen, Cameron, et al. ""Learning markov state abstractions for deep reinforcement learning."" Advances in Neural Information Processing Systems 34 (2021): 8229-8241.

Srivastava, Siddharth, Stuart Russell, and Alessandro Pinto. ""Metaphysics of planning domain descriptions."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 30. No. 1. 2016.",264,1,6,0.7512000000000001,0.1625,0.8653070927000001,49,41.1434,0.2429,iclr,0.0,3,4,4,3,factual,3,4,80,polite,4,neutral,4,moderate,4,4,3,4,partially factual,4,4,75,polite,5,positive,4,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
103,Reviewer-dCJp,Learning Abstract World Models for Value-preserving Planning with Options,"General-purpose agents require fine-grained controls and rich sensory inputs to perform a wide range of tasks. However, this complexity often leads to intractable decision-making. Traditionally, agents are provided with task-specific action and observation spaces to mitigate this challenge, but this reduces autonomy. 
Instead, agents must be capable of building state-action spaces at the correct abstraction level from their sensorimotor experiences. We leverage the structure of a given set of temporally extended actions to learn abstract Markov decision processes (MDPs) that operate at a higher level of temporal and state granularity. We characterize state abstractions necessary to ensure that planning with these skills, by simulating trajectories in the abstract MDP, results in policies with bounded value loss in the original MDP.
We evaluate our approach in goal-based navigation environments that require continuous abstract states to plan successfully and show that abstract model learning improves the sample efficiency of planning and learning.","The paper introduces a method for enabling general-purpose agents to efficiently handle complex tasks by constructing abstract models based on temporally-extended actions. These models facilitate more efficient planning and learning and are characterized using principled conditions. The approach provides empirical evidence of improved sample efficiency in goal-based navigation tasks and offers theoretical support for information maximization strategies in abstract state representation learning.
The authors claim that they introduced a method for creating abstract world models that empower agents to plan effectively for goal-oriented tasks. The key idea is to allow agents to construct reusable abstract models for planning with specific skills. This is achieved by characterizing the state abstraction that ensures planning without any loss in simulation, meaning that planning with the learned abstract model can generate policies for the real world. The paper also provides theoretical support for the use of information maximization as a reliable strategy for learning abstract state representations. - Good overview of the related work.
- Good description of motivations and intuitions. 
- proper choice of environment settings. Major:
- Some measures are used without definition, 
- It seems that there exists a lot of inaccuracies and impreciseness in the theories and definitions. See all questions!

minor:
- typos: 
last paragraph of the introduction ""the *agents* needs"", definition 3.5 ""$s_{o}$"" must be ""$s_0$""
- writing: 
Define the abbreviations before using them, e.g. ""PDDL"", ""VAE""

There is a chance that I have not fully understood what this paper is trying to present. 1- What is $P(s'|s,o)$ used in the paragraph right after definition 3.1?

2- An option $o$ is defined, and then you mention $T(s'|s,o)$ to define the transition probability of taking option $o$ in $s$? $T$ earlier was defined on action space $A$. How is it applied on options without showing the relationship of $I_o$ and $\beta_o$ with $s$ and $s'$ under option policy $\pi_o$?

3-the paper has defined ""$\bar {\gamma} = \gamma ^{\tau (s,o)}$ is the abstract discount factor, $\tau: Z \times O \rightarrow \[0,\infty)$, which consists of contradictory phrases. How is ${\tau (s,o)}$ but defined as a function of abstract variables $Z$ instead of $S$? Not clear what $\tau$ is. If based on definition 3.1, it is the option's execution time starting from $s$ taking option $o$, it is not clear how in definition 3.2 it becomes a map from $Z$ and $O$ to a non-negative real.

4- What does definition 3.4 mean? $ \Pi = {\pi \in \Pi : \pi(·|s) = \pi (·|z) \forall s \in z}$ says the probability of taking actions/options in $s$ should be equivalent to the probability of taking actions/options in abstract states. Transitions of taking actions in states might take you to another state $s'$ inside the similar abstract state $z$. How can the policies used for both abstract states and states be equivalent? Unless you are just discretizing the continuous state spaces based on the optimal policies that are already given. Lots of interchangeable usage of symbols here. Not precise and is hard to follow.",499,0,1,0.7308,0.0796438834,0.93872118,61,45.6397,0.1463,iclr,0.0,4,4,5,4,factual,3,4,83,polite,4,negative,5,none,3,4,4,3,partially factual,4,4,45,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,partially factual,3,3,65,neutral,4,neutral,4,moderate,2,3,3,3,partially factual,3,3,65,polite,4,neutral,4,moderate
141,Reviewer-xxEb,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","The paper introduces a novel model, PatchSynth, in the Patch-Text Pre-training (PTP) domain, aiming to improve software patch representation and description generation. Through a blend of patch understanding and generation, PatchSynth	 addresses the limitations of prior models. Empirical evaluations reveal its superior performance in patch description generation, with an ablation study further underscoring the importance of generating task training. Novelty and Importance: The work is first to propose a unimodel for patch-text understanding and related tasks. And the topic is very important in this domain.

	Melds patch understanding and generation, addressing prior models' specialization limitations.

	The work provides a good representation and good results Unclear adaptability across diverse programming languages or coding standards. What are the considerations for deploying PatchSynth in real-world software development environments, and what infrastructure would be required for efficient and secure operation?",136,0,0,0.7967000000000001,0.3063636364,0.944865346,50,11.6712,0.0364,iclr,0.0,3,4,2,3,partially factual,3,3,50,polite,4,neutral,3,low,3,4,4,3,factual,4,4,75,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,2,3,3,2,factual,4,3,60,polite,4,positive,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,positive,4,low
141,Reviewer-4kdr,PatchSynth: a Patch-Text Pre-trained Model,"In recent years, patch representation learning has emerged as a necessary research direction for exploiting the capabilities of machine learning in software generation. These representations have driven significant performance enhancements across a variety of tasks involving code changes. While the progress is undeniable, a common limitation among existing models is their specialization: they predominantly excel in either predictive tasks, such as security patch classification, or in generative tasks such as patch description generation. This dichotomy is further exacerbated by a prevalent dependency on potentially noisy data sources. Specifically, many models utilize patches integrated with Abstract Syntax Trees (AST) that, unfortunately, may contain parsing inaccuracies, thus acting as a suboptimal source of supervision. In response to these challenges, we introduce PATCHSYNTH, a novel pre-training framework for patches and natural language text. PATCHSYNTH deploys a triple-loss training strategy for (1) patch-description contrastive learning, which enables to separate patches and descriptions in the embedding space, (2) patch-description matching, which ensures that each patch is associated to its description in the embedding space, and (3) patch-description generation, which ensures that the patch embedding is effective for generation. These losses are implemented for joint learning to achieve good performance in both predictive and generative tasks involving patches. Empirical evaluations focusing on patch description generation, demonstrate that PATCHSYNTH sets new state of the art performance, consistently outperforming the state-of-the-art in metrics like BLEU, ROUGE-L, METEOR, and Recall.","The paper discusses a new model, PatchSynth, in the domain of Patch-Text Pre-training (PTP) which aids in accurate patch representation for software evolution tasks like bug fixing and feature enhancement. PatchSynth is designed to balance patch understanding and generation, overcoming limitations of previous models. It outperforms existing models in patch description generation, as shown in experiments using standard evaluation metrics. An ablation study further reveals the importance of generating task training in improving PatchSynth performance. Novelty:
The novelty of PatchSynth lies in its harmonious synthesis of patch understanding and generation, coupled with an advanced synthetic description generator. This innovative approach addresses the historical challenges of accurate patch representation and description generation, marking a significant stride in the PTP paradigm.
Importance:
The topic is of paramount importance as it addresses a critical need in software engineering for accurate patch representation and description, which are pivotal for collaborative development, systematic documentation, and rapid code review processes. By advancing the PTP paradigm, PatchSynth not only contributes to the academic discourse but also holds promise for practical applications in software development workflows.
The work achieves promising results. The paper doesn't elucidate how PatchSynth adapts to varying programming languages or codebases with differing coding standards and structures. This lack of demonstrated adaptability could limit its applicability across diverse software projects, potentially requiring additional tuning or re-training to maintain accuracy and effectiveness in different environments. 1. Given the advancements in PatchSynth for patch-text understanding and generation, how well does the model perform in a transfer learning scenario? Can PatchSynth be fine-tuned or adapted effectively to related tasks in software engineering or different programming languages?
2. Are there considerations or plans for deploying PatchSynth in real-world software development environments? How would the integration look like, and what kind of support or infrastructure would be required to ensure the model operates efficiently and securely in a production setting?",310,0,2,0.8356,0.1883953168,0.940164268,50,9.2899,0.068,iclr,0.0,4,4,4,4,factual,3,3,65,polite,4,neutral,3,low,4,5,4,4,factual,4,4,88,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,5,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
28,Reviewer-Lpfq,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper studies formal guarantees for notions of individual fairness (IF) for predictors given by neural network models. After relaxing common definitions for IF metrics by means of $\ell_\infty$ balls (or orthotopes), they adapt methodology based on adversarial robustness to provide upper and lower bounds to the IF achieved by models on an empirical sample - and those within a $\gamma-$Wasserstein ball about it. - This paper studies an important problem of individual fairness
- The first half of the paper, Section 3 and 4, which cover Background, the DIF definition, and problem explanation are very clear and easy to understand. - The key observation and novelty in the approach is not clearly noted (See below)
- Several of the nice advantages of their method (e.g efficiency) are not explained (see below). 1. Numerous times in the paper the authors say their bounds are ”efficient” because they leverage efficient methods (e.g. those based on bound propagation). While that may be true, it would be nice for the readers if they provided a brief explanation as to why these methods are efficient instead of placing everything in the appendix. 
2. It seems to me that the central novelty of this paper is to upper bound a mahalanobis metric (for $d_{fair}$) with an orthotope, which is quite simple. The remaining of the paper seems to me a direct application of results and methods in adversarial robustness. While I do appreciate the observation of being able to use those tools in the context of fairness - which also constitutes novelty - I would appreciate if the authors could be very clear about what are the main technical contributions of this work.
3. Personally, I am not sure providing a section on the impact of these methods on group fairness is necessary. I’d much rather prefer a discussion on the efficiency of the bounds.
4. Figure 1 is quite confusing. What makes the blue-star individuals likely? As presented, those blue-star points do not look likely. If I understand the figure correctly, the authors should present a more balanced empirical sample together with a larger sample representing the (unobserved) population. 
5. I also have problems with the fact that the authors state their goals and present their definitions in terms of expectation (e.g. as in Def 2), but simply restrict themselves to studying empirical samples. I think the presentation is misleading, because nowhere the authors really provide guarantees for the definition in Def 2 (that is, risk bounds). This is also an important limitation where the study the Wasserstein distance between distributions, as they simply regard their distribution as a one supported on Dirac functions (on the observed samples). 
6. Immediately after Eq (4), the authors write that “we can optimize this bound to be tight”. I don’t think this is correct: while they can indeed optimize the bound, there’s no guarantee that the bound will be tight, as the original problem is non-concave.
7. In Section 5.4 and after presenting $\mathcal L_{F-DIF}$, the authors mention when $\gamma=0$, one recovers a local constraint on individual fairness on $x\in X$. I don’t think this is completely accurate, because again, Def. 2 is defined in expectation of $x\sim p(x)$, not simply over the empirical sample. The authors mention that they do not foresee negative societal impacts. Maximizing upper and lower bounds is great but in doing so we don’t really know what is happening to the true fairness violation. It may be that the true fairness violation is in fact increasing which is propagating unfairness. While I understand that solving for this value is not feasible and thus appreciate the results presented, I would also like the paper to acknowledge that there are potential negative effects.",619,0,7,0.7891,0.1001929392,0.9119418859,215,46.7646,0.6521,neurips,0.0,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
28,Reviewer-FRnw,Certification of Distributional Individual Fairness,"Providing formal guarantees of algorithmic fairness is of paramount importance to socially responsible deployment of machine learning algorithms. In this work, we study formal guarantees, i.e., certificates, for individual fairness (IF) of neural networks. We start by introducing a novel convex approximation of IF constraints that exponentially decreases the computational cost of providing formal guarantees of local individual fairness. We highlight that prior methods are constrained by their focus on global IF certification and can therefore only scale to models with a few dozen hidden neurons, thus limiting their practical impact. We propose to certify \textit{distributional} individual fairness which ensures that for a given empirical distribution and all distributions within a $\gamma$-Wasserstein ball, the neural network has guaranteed individually fair predictions. Leveraging developments in quasi-convex optimization, we provide novel and efficient certified bounds on distributional individual fairness and show that our method allows us to certify and regularize neural networks that are several orders of magnitude larger than those considered by prior works. Moreover, we study real-world distribution shifts and find our bounds to be a scalable, practical, and sound source of IF guarantees.","This paper studies the problem of individual fairness in supervised learning. The focus is on studying how to certify distributional individual fairness (IF) (individual fairness over a set of distributions close to the observed empirical data distribution) in neural networks. Prior work has focused largely on certifying global IF, which is more expensive and thus can only be applied to smaller neural networks than the proposed certification/debiasing technique. The contributions of the paper are in showing how to certify distributional IF in neural networks and then using these bounds in the training process as regularizers to debias NNs. 

The main methodology for certifying IF is presented in Section 5. The first step is to certify local IF by over-approximating the similarity ball to find a conservative estimate of the IF violation. They can then use this bound to certify distributional IF around the empirical data distribution and apply finite sample guarantees to give an estimate of the true distributional IF. 

The authors then show how to use the bounds on distributional fairness as regularizers in the training procedure as a way to debias neural networks. They then provide experimental evaluation on a few benchmark datasets that demonstrates that their proposed training method indeed improves distributional individual fairness, at relatively modest degradations in accuracy.  The main advantage is a relatively lightweight way to certify and train NNs for IF, in a way that requires little additional computation, compared to previous methods which are not able to scale to large NNs. 

The experimental evaluation seems to confirm that DIF training as proposed by the regularization method does in fact improve significantly improve IF at modest degradation in classification accuracy.  Section 5 is a little dense and it would be helpful for the reader if there was a little more discussion of the optimization procedure, particularly in Section 5.3. Theorem statements here might also be helpful for the reader to understand what the final guarantees are.  What is the purpose of Table 2? It is a little difficult to interpret the punchline - it just seems to indicate that DIF training does not have a consistent effect on group fairness measures, either positively or negatively.  -",363,0,1,0.7939,0.0286027569,0.9569676518,215,26.5652,0.0354,neurips,0.0,4,4,3,3,factual,3,4,60,neutral,3,negative,4,high,4,4,4,5,5,5,5,85,5,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,4,low
41,Reviewer-viiH,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper proposes contrastive introspection (ConSpec), an algorithm for learning a set of prototypes for critical states via the contrastive loss. ConSpec works by delivering intrinsic rewards when the current states match one of the prototypes. This paper also conducted experiments in various environments. The intuition of learning the critical states is natural and easy to follow. The experimental results in this paper look solid and promising. Despite the empirical performance, the reviewer finds the ConSpec algorithm itself hard to follow.

The largest weakness is: the insufficient discussion on how the prototypes $h_i$ are learned. Hence, the reviewer cannot understand the detailed on how $h_i$ are used (see detailed in Questions). 

Besides insufficient discussion on the prototypes, some minor issues are: (1) the title in the pdf (Contrastive Introspection:. ..) seems to mismatch with the one appear in openreview (ConSpec: …). (2) The font of citations appears to be confusing. E.g., from line 19-20 in the introduction, the manuscript uses (number) to address some key points, and the citation also appears as (number) – it would be nice if the citation can be changed to something that is not (number; number).
 Per the major weaknesses:
1. How are the prototypes $h_i$ actually learned? If the reviewer understands correctly, in line 7 of the abstract, the manuscript says “ConSpec learns a set of prototypes…”. While in Algorithm 1, it seems that the prototypes $h_i$ are given to the algorithm as inputs. Maybe the author can clarify why this inconsistency in learning the prototypes happens?
2. How are the $h_i$ learned/chosen in each experiment? The reviewer has looked into the detail of the experiments in the appendix, but cannot clearly understand how the presented experiments actually utilize the $h_i$. It would be nice that the authors can provide more details of all the $h_i$ in all the present experiments (Sec. 4.1-4.5).  
 See questions and weaknesses.",313,0,2,0.7000000000000001,0.0809294872,0.8764749765000001,216,48.5775,0.0354,neurips,0.0,2,4,3,2,factual,4,3,60,polite,3,neutral,5,none,4,4,4,4,partially factual,4,3,75,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
41,Reviewer-fj2y,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","The paper noticed that in real-world MDP, success is often contingent upon a small set of steps. While Bellman equation can theoretically do credit assignment over long-horizon, reward is hard to propagate under Bellman-based methods in practice. The authors therefore propose a novel algorithm that uses contrastive learning to identify critical states that final success relies on. The method uses a memory like system that can give agents intrinsic reward during training. The paper then evaluates the proposed method on a wide variety of domains and shows performance gain when the proposed method is added to RL algorithms.
 The paper is based on an interesting and important insight about long-horizon credit assignment and reward learning. The proposed method is designed to explicitly improve long-term credit assignment and have shown empirical success in the evaluation. 

The writing and figures are clear. The paper is easy to follow.

The evaluation covers a wide variety of RL tasks are benchmarked to back the claim of the paper.  1. The method assumes additional access to a ""success"" indicator at the end of episode. While this is commonly obtainable in gym environments, this doesn't fit into the general MDP setting and thus might limit when the algorithm can be applied. 

2. The assumption about access to ""success"" seems privileged compared to baselines. I am wondering they will catch up with the performance of the proposed method when a success bonus is added.

3. The evaluation has #mini batches / # gradient steps as x-axis, unlike the environment steps in common RL benchmarks. I am wondering why this is the case. If this is necessary, I'd like to see convincing justifications. 

4. The proposed method relies on a memory system, which may hurt generalization and might have problem when scaling up.

5. CURL+PPO doesn't seem to be a strong baseline to ablate in figure 3. I hope the authors could benchmark against RAD\[https://arxiv.org/abs/2004.14990\], a much stronger baseline in pixel space.  I am wondering whether adding the intrinsic reward can degrade the performance of RL algorithms on common environments (aka, those environments where the final success does not depend on just a few critical steps). This shall be justified the experiments.

When the observation is partial, is the proposed method still reasonable?

 1. The method requires privileged information about success of an episode. 
2. I cannot see how the method can be applied to RL that has partial observation that requires recurrent policies.",406,1,7,0.7978000000000001,0.1046052632,0.8880720139,216,45.7357,0.2025,neurips,0.0,5,5,5,5,factual,3,3,90,polite,5,positive,5,none,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
41,Reviewer-LNh7,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","Proposes an auxiliary reward module to be used in RL algorithms, that learns features (‘prototypes’) of critical states in successful trajectories. For new observations, the method then uses cosine similarity to the learned features as a reward bonus. The method is evaluated on a unity-based env, grid-worlds, versions of gym,atari envs with delayed rewards, and Montezuma’s revenge. 1. Effective exploration bonus

The idea of learning invariant features across successes, and using these as a source of reward does seem to give better exploration performance, from the experiments. The Montezuma’s revenge experiments (Fig,6) are particularly compelling - the baselines PPO, RIMS (which also uses a set of discrete slot-based learned features ) and Decision Transformer all fail to obtain any reward. By creating an explicit division between success and failure episodes, conspec can then learn features that match states present in the successes, but not the failures, even from a very small number of successful trajectories (There might be other, simpler ways to get this effect however, see weakness #1). The ability of con spec to find important states critical for the task is also investigated by the authors in the simpler unity-based env, where they also visualize states closest to the learned prototypes.  

2.   More expressive set for bottleneck states

Instead of learning an explicit set of states which are important (like sub-goals) as has been previously studied, this paper instead captures the notion of ‘critical states’ using learned prototypes. The advantage of this is it can flexibly capture a large set of very different states, all of which are critical. This is also beneficial because it enables zero-shot generalization in new environments (section 4.2)

3. Clarity, presentation

The paper is well motivated, written clearly, and the main idea for the algorithm is presented clearly. 1. Are the prototypes actually required?

Learning from data in successes that aren’t present in failures should lead to better performance, but the importance of doing this through learning prototype features is unclear. As a simple baseline, consider training a policy on only the successful set (using behavior cloning). Does this provide similar performance to con spec on Montezuma’s revenge? Is trying to capture a notion of ‘critical states’ required to learn better policies ? Can you run Decision Transformer where for each successful trajectory, every transition is labelled with a reward of 1, and for every failure trajectory, every transition is labelled with a reward of 0 ?

 2. Success/failure definition

The method relied crucially on the quality of the learned prototypes, which in turn depends on the success and failure datasets. It might not always be possible to divide up trajectories into 2 classes in this manner, in a lot of tasks performance keeps improving over time and a ‘successful’ trajectory at the beginning of training is very different from one from a converged policy. The authors do discuss this (appendix A.3), but the definition used in this paper for a successful trajectory is - ‘an episode that received one of the few rewards available, and a failure is defined as anything else’. For agents to keep learning and improving from data the notion of a success should necessarily change with time (eg - maximize the reward instead of just getting some reward). 

3. Delayed reward envs 

A good portion of the experiments are conducted on familiar gym, Atari envs but with a modification where the rewards are delayed. The significance of these experiments is unclear, since the delayed reward setting for these envs is not standard and widespread. Please address the questions in weakness #1. Sufficiently addressed",595,0,6,0.7942,0.1819876664,0.8273749352,216,35.2666,0.1397,neurips,0.0229885057471264,4,5,5,4,factual,4,4,85,polite,5,neutral,4,none,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,5,4,4,factual,4,4,85,polite,5,positive,5,low
41,Reviewer-t9VM,Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL,"In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps. This algorithm, which we call Contrastive Retrospection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of the prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (i) They enable rapid identification of all the critical steps. (ii) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL approaches to credit assignment, ConSpec takes advantage of the fact that it is easier to retrospectively identify the small set of steps that success is contingent upon (and ignoring other states) than it is to prospectively predict reward at every taken step. ConSpec greatly improves learning in a diverse set of RL tasks. The code is available at the link: https://github.com/sunchipsster1/ConSpec","This paper introduces ConSpec, a reinforcement learning (RL) algorithm designed to identify critical steps and improve performance in continuous control tasks. ConSpec utilizes contrastive learning to learn prototypes of critical steps and employs a contrastive loss to differentiate successful and failed experiences. It addresses the challenges of long-term credit assignment and generalization in RL tasks. This article presents an interesting idea of learning to match key states in a task through contrastive learning. The writing of this paper is clear and Figure 1 is well-drawn, making it easy to quickly grasp the details of ConSpec. And the experimental results effectively demonstrate that the learned prototypes indeed match the key states in 3D Orange-Tree task and gridworld. The cosine similarity measures the similarity between the prototype and the hidden state, both of which are learnable vectors. However, optimizing the contrastive learning loss with updates to both vectors may lead to extremely unstable training. In related literature on representation learning, it is common to use the stop gradient approach and optimize only one of the learnable vectors. 
 - ConSpec achieved a return of only 400 in the Montezuma's Revenge task compared to RND, which reached a return of 7500 in its original paper. It appears that ConSpec is not as effective as RND in this regard. Both the multi-key room environment and Montezuma's Revenge involve similar logic, but the former is much simpler. So why does ConSpec outperform RND in Fig.4?
- It appears that ConSpec has significantly higher variance than the baseline algorithm in all tasks. What could be the reason for this? Further, prototype learning is crucial and can every seed match the key states?
- What does ""softmaxed over t"" mean in line 152? Why is it necessary to introduce softmax operation?
- How is success and failure defined in Atari and MuJoCo tasks? I think this is important, but the article lacks details in this aspect. As presented in Section 5, the number of prototypes is task-specific and definations of  success and failures need human design.  Furthermore,  The proposed method introduces hyperparameters, such as the Diversity measure and the hyperparameter $\lambda$, that require careful tuning. Furthermore, the algorithm exhibits a significant variance in its actual performance.",368,0,0,0.7889,0.0998903509,0.8961703181,216,40.6298,0.0649,neurips,0.0,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,4,5,4,4,partially factual,4,5,85,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
179,Akira-Endo,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study considers the effectiveness of contact tracing focused on variants in reducing the reproduction number. Focusing contact tracing efforts on variants is an interesting approach and may be relevant to the current situation of variant circulations worldwide. The model and the analysis themselves seem well constructed and implemented. However, the authors’ analysis only focuses on a single variant essentially, and does not account for some important aspects that need to be considered to estimate the effect of real-world contact tracing in the presence of multiple variants. As a result, I am not sure if this study provides new insights that are distinct from existing studies on contact tracing for a single-pathogen outbreak. In addition, it should be noted that given a fixed capacity for contact tracing, the reduction in the reproduction number would not be permanent if the outbreak continues to grow. I believe these issues, along with other comments detailed below, need to be addressed for this study to be truly of epidemiological and public health interest. Major comments: Please clarify how this study is distinct from existing studies on contact tracing considering a single-pathogen outbreak (including the authors’ own study cited here).  There seems to be a mismatch between the study motivation/context and the modelling approach. One of the points the authors are trying to make is that the contact tracing efforts should be focused on variants because they are of more epidemiological importance (due to potentially higher transmission or immunoescape). I do not disagree with this point, but there are several major issues regarding how it was handled in the manuscript.  The reproduction number R is used as an objective variable to measure the effect of contact tracing. This is useful to connect interventions and the dynamic evolution of the epidemic, but essentially assumes that the same level of tracing can continue everywhere long-term, regardless of the epidemic size. This is obviously not true as the authors also state in the manuscript. In conditions where R is above 1, transmission of variants would continue and overwhelms the tracing capacity at some point, pushing R back to the original value eventually. Focusing on R may be useful in identifying conditions required to control the outbreak (i.e. R<1), but it is unrealistic to consider that the tracing can keep R lower than the original value in a long term if the resulting value exceeds 1.  Variants are no longer minor in many places now (see for example: https://covid.cdc.gov/covid-data-tracker/#variant-proportions), and I am not sure how much this assumption of ‘minor variants’ is relevant to the actual situation. Moreover, even in places where the variants are still minor, if the (effective) transmissibility of the variants is higher than the existing virus, they would rapidly replace the existing viruses, potentially in a few weeks/months. Exclusion of existing strains. The main argument regarding the tracing capacity is that the variants account for a small proportion of cases and thus can be handled if tracing focuses on these variants. However, even if such focused intervention is possible by tests that can distinguish variants, existing non-variant viruses may continue spreading if their R is above 1. Although such a situation may still have some benefit, e.g. if preventing the spread of immunoescaping variants would ensure the success of the vaccination program, such contexts should be clarified and discussed.  Cost and capacity. As discussed above, contact tracing would work as estimated here only until the capacity is reached. However, I feel efforts associated with tracing is not seriously considered in the analysis. For example, if all contacts of cases within the tracing period are traced, extending the tracing period from 2 days to 6 days would incur substantial additional effort for tracing. I believe it is important to discuss to what extent contact tracing might be sustainable for each setting because the presented results become invalid once the capacity is reached.  Given the points above, I would recommend the authors reconsider what outcome measure to use and how to present them; e.g. consideration of the growth of ""non-targeted"" viruses, conditions required to keep R below 1, whether tracing can “buy time” until achieving a sufficient level of vaccination before reaching the capacity, optimising the intensity of other NPIs (e.g. lockdowns) in the presence of contact tracing, etc., such that the results are relevant to what may actually happen. The Introduction looks lightweight and lacking necessary details or contexts. There are a lot of concepts that may not be familiar enough to every reader but are not sufficiently explained (e.g. TTI, backward contact tracing, bidirectional tracing, why TaqPath test can distinguish B.1.1.7… etc.) and thus may require a succinct clarification. Please also note that this paper may be read in 20 years from now, when the reader may not have the same level of recognition of the current situation. In this light, for example, I feel the first paragraph of Introduction may sound a bit abrupt to the reader who is less aware of the overall timeline of the pandemic. Also see some of the specific comments in the Minor comments section.  The Methods section is too simple and does not contain sufficient information for the reader to comprehend the overall structure of the analysis. Although it does not need to contain every technical detail of the model and analysis as the supplementary methods can be found in the repository (but please include a link and description in the paper so that the reader can easily find it), I feel more information from the supplementary methods should be extracted and summarised in the main text. For example, from the current Methods section I cannot interpret how the course of transmission was characterised, what is the assumed procedure of tracing (Is it always bidirectional tracing? I feel 2-day window is too short for backward tracing), how environmental transmission was assumed to work, how R was calculated, etc.  I believe additional sensitivity analysis would be necessary. For example, the overdispersion parameter (0.11 used in the current analysis) is estimated to be slightly higher (0.3-0.5) in some studies where interventions were in place (Adam et al., 20201). As the authors assume that interventions may be affecting R during contact tracing, possible changes in overdispersion should also be considered. Delay from secondary transmission to quarantine of contacts (defined as a sum of various delay distribution) would also affect the effectiveness of contact tracing in a nontrivial manner.  Is the effect of vaccines not considered, although as in Introduction it was one of the major motivation for considering controlling variants? Vaccines may affect different viruses similarly or differently, depending on the type of variants.  Supplementary Methods, “Identified contacts are quarantined, …isolated, tested, and traced as described above”: what is the difference between quarantining and isolation of traced contacts? Does this mean all traced contacts of a case are put under quarantine regardless of their true infection status, but only tested if they are symptomatic (which changes the label from quarantine to isolation)? If so, it is expected that as the epidemic grows there would be a substantial number of quarantined individuals, and at some point this might be impossible (e.g. due to depletion of essential workers) and the Reff control could collapse.  Minor comments: Throughout: please spell out acronyms at their first appearance, including SARS-CoV-2 and COVID-19.  Introduction, protection against B.1.351 and P.1: now the evidence is not limited to in-vitro studies (e.g. Madhi et al., 20212 and Kustin et al., 20213). Please update and include clinical findings. Also summarise what we know about protection against B.1.1.7.  “All three variants share…; B.1.1.7 can also be…”: I would suggest that the authors first describe B.1.1.7 that can be detected by TaqPath tests (with some more background context, as this is primarily happening in UK and not necessarily recognized by the wider audience) and then go on to a discussion of potential detectability of other variants (because this is only a hypothetical scenario so far in my understanding, as opposed to detection of B.1.1.7). Also, would there be any data on the rollout of these variant-distinguishable tests worldwide?  “Samples testing positive…”: This needs more context. Why is authorisation going to be an issue and why can re-screening bypass it?  “as is true for SARS-CoV-2 – but not yet the variants – in many regions”: I feel this is unclear. TTI capacity would be overwhelmed when the overall caseloads are high, even if the variants account for a very small fraction of them. It should be made clear if this indicates contact tracing would only target variants distinguished by the (variant-specific) tests.  Method, “child cases” may be interpreted as cases that are children. Secondary transmissions?  Results, “In the absence of contact tracing, identification and isolation of symptomatic cases alone reduced Reff by 0.2 to 0.3…”: I couldn’t read this from the top rows of Figure 1. This may correspond to 0% of cases sharing data or 0% trace success probability, but Reff for such a scenario cannot be read from the figure because there is no colour scales or numbers.  “When identification and isolation…substantial effects.”: I am not sure how “moderate levels” and “substantial effects” are defined.  “Due to the exponential growth of uncontrolled epidemics…over a given timespan”: As stated above, this is only the case if contact tracing can continue without hitting the capacity. If R goes back to the original level after tracing is overwhelmed, there may be only a marginal difference in the final epidemic size.  Discussion, “Higher rates of cooperation…quarantine and isolation”: related to the first major comment, these efforts would make tracing more effective but require a substantial amount of effort and cost, and warrant discussion.  Please update references. Many of the preprints cited here have now been published in peer-reviewed journals, which might include more up-to-date information.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? No",1775,1,3,0.8027000000000001,0.1066198131,0.8703778982,18,35.17,0.2522,f1000,0.0096153846153845,4,4,5,1,factual,3,5,84,neutral,5,negative,5,none,5,4,5,5,factual,5,5,95,polite,5,neutral,5,low,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,moderate,5,5,5,5,factual,5,5,5,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
179,Tim-C.-D.-Lucas,The feasibility of targeted test-trace-isolate for the control of SARS-CoV-2 variants,"The SARS-CoV-2 variant B.1.1.7 reportedly exhibits substantially higher transmission than the ancestral strain and may generate a major surge of cases before vaccines become widely available, while the P.1 and B.1.351 variants may be equally transmissible and also resist vaccines. All three variants can be sensitively detected by RT-PCR due to an otherwise rare del11288-11296 mutation in orf1ab; B.1.1.7 can also be detected using the common TaqPath kit. Testing, contact tracing, and isolation programs overwhelmed by SARS-CoV-2 could slow the spread of the new variants, which are still outnumbered by tracers in most countries. However, past failures and high rates of mistrust may lead health agencies to conclude that tracing is futile, dissuading them from redirecting existing tracers to focus on the new variants. Here we apply a branching-process model to estimate the effectiveness of implementing a variant-focused testing, contact tracing, and isolation strategy with realistic levels of performance. Our model indicates that bidirectional contact tracing can substantially slow the spread of SARS-CoV-2 variants even in regions where a large fraction of the population refuses to cooperate with contact tracers or to abide by quarantine and isolation requests.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this study the authors use established and previously published models of contact tracing to examine whether targeted test and trace systems could suppress novel variants. The premise is sound; contact tracing scales poorly, so while it is not necessarily effective at control SARS-CoV-2 at large once national prevalence is high, the numbers of certain variants are still low in a number of countries and therefore contact tracing might be able to control those new variants as they are seeded into a country. Whether this approach would work or not is not trivially obvious and so this study is asking an important question with policy implications globally. The analytical approach taken is quite simple in that the authors assume (and back up with some literature) that the variants can be identified easily and that therefore contact tracing of a new variant can continue without any reference to the dominant variant.  Comments: Most of my comments relate to this assumption that contact tracing of new variants can be modelled by simply ignoring the dominant variant.  First, I would like to see this assumption explicitly stated in the methods just to make it completely clear to the reader.  There are a number of further considerations with this assumption that I think should be discussed.  Given the high rate of vaccination and previous infection with the original SARS-CoV-2 strain, many countries are now in a state where immunity cannot be ignored. This is all handled by Reff, but I think it needs to be mentioned that Reff is combining NPIs, immunity or partial immunity from vaccination (depending on whether there's vaccine escape in the variant)  and partial immunity from previous infection with other strains.  The authors state that new variants can be detected with RT-PCR and TaqPath. However, does this extra step create no extra delay in the process? I imagine this would depend on the specific organisation but might be worth considering and mentioning.  Furthermore, is this identification of variants 100% accurate? The false negative rate (someone is infected with a new variant but the test says they are infected with the original variant) can be just included as part of the test sensitivity and I wouldn't be surprised if the difference is fairly small. More worrying for me is the false positive rate (someone is infected with the original variant but the tests says they are infected with a new variant). This is important because the rationale for the study relies entirely on the fact that there are not many cases with the new variant in a country but if, say, the false positive rate (as defined above) is even 1% then the large number of original variant cases in a country will quickly lead to the targeted test-trace-isolate system being swamped. This effect will obviously vary with the prevalence of original variant SARS-CoV-2.  I only know the literature for the UK, but even the lowest compliance rates used here are much higher than those measured (I wouldn't be surprised if some countries have much high compliance rates though). I am taking my values from the reference below (Smith et al., 20201),  but there might be more up-to-date surveys in the UK and I don't know at all about other countries.  From self-reported behaviour (past behaviour, not intentions) in the UK, about 12% of people with symptoms requested a test. This relates to the 50% of symptomatic cases identified without tracing parameter. Some details of how you selected 50% from ref 32 would be useful, as the values in that paper range from 5% to 100% depending on the country and time. In the UK, of those contacted by track and trace, 11% of people fully complied with 2 weeks self isolation (this relates to the 50%-90% comply with isolation parameter). So at the very least I think it might be useful to state that these values might be quite optimistic in some settings.  Finally, a minor and subjective point, but it might be useful to present Figure 1 with a diverging colour palette that clearly distinguishes Reff < 1 and Reff > 1.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",828,0,1,0.7912,0.1146087604,0.9262154102,28,37.64,0.1733,f1000,0.0194174757281553,1,4,3,0,partially factual,4,4,80,polite,3,negative,4,moderate,5,5,4,5,partially factual,4,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
33,Anil-Mukund-Limaye,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This work is a comparison of paraffin embedded FNABs versus tissue blocks for the assessment of HR, and HER2/neu expression. This is an important piece of work. However, the study lacks depth. I have the following comments and suggestions. The manuscript requires language correction. Some sections need to be re-written as they were not clear to me.  Routine immunohistochemical method for assessment of HR, and HER2/neu has been used for a long time now. It appeared to me that invasive nature of core biopsies is a problem and the authors seek to evaluate the performance of paraffin embedded FNAB. The authors could have given an account of similar work upfront in the introduction. That could be followed by what this study particularly has to offer to provide new or additional insights. The literature on others' work could have been discussed.  Sample size is small. Hence the authors should be cautious in making conclusions about the outcome of the study, with particular reference to the data in tables 2 and 3.  The scoring method for HR and HER2/neu expression should be clearly stated.  Discrepancy between Table 1 and data described in the first paragraph of the Discussion section. Figure 1 shows 78% ductal carcinoma, 16% lobular, 4% mixed and 2% mucinous. However, in the cell block data discussed in the first paragraph of the discussion mentions- 74% ductal, 16% lobular, 4% mixed and 6% mucinous.  Similar data for the tissue block results were 64% ductal, 16% lobular and 14% mixed. However, I don't see any raw data for the same.  The expression of HR and HER2/neu was concluded as false positive or false negative based on the assumption that the data from the tissue blocks provided the true picture. However, the tissue block method may itself have a small but a certain rate of false positives and false negatives. Although, I do understand that specificity and sensitivity are evaluated based on certain truth, which the tissue block data is assumed to represent.  The authors have mentioned that "" the patients with lobular breast cancer tended to be younger"". What is the statistical basis for that? The F value (0.356) and the p value (0.785) does not suggest that this is true.  I did not find the relevance or utility of the second and third paragraphs in the discussion part of the manuscript.  How is the discrepancy (Table 3) calculated?  The authors have determined the specificity and sensitivity of the cell block method in reference to the tissue block method. However, there is no clear picture as to how the levels of expression been scored. In table 2, what is the cut-off score for each marker (HRs and HER2/neu) for categorizing the samples as positive or negative.  Having used the Cohen's kappa for assessment of the agreement between the data from two methods, what is the need for Spearman's correlation coefficient? Having good correlation is one thing, but whether the methods are agreeing well with the levels of expression of a particular marker is another. In this context the protocol for scoring is important. These issues need to be scrutinized by an expert statistician.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",674,0,2,0.7646000000000001,0.0853282682,0.8795065284,752,47.79,0.1507,f1000,0.0106382978723403,4,5,5,5,factual,5,5,100,neutral,5,negative,5,none,4,4,4,4,partially factual,4,4,80,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
33,Torill-Sauer,"Clinico-pathology of newly diagnosed breast cancer with expression of ER, PR, and HER/2neu in cell blocks: An observational prospective study","Background: Breast cancer is a worldwide problem, and early positive diagnosis is critical for establishing the optimal therapeutic strategy. Following a preliminary diagnosis, fine-needle aspirate cytology (FNAC) may be used to obtain cells for immunohistochemical (IHC) analysis and histopathological examination. This study aimed to assess the FNAC method combined with embedding samples in paraffin blocks (cell blocks) and comparing this with core biopsies (tissue blocks). Methods: This observational, prospective study was performed at our hospital and involved 50 female participants who presented with breast masses and were subsequently evaluated for high-risk status by FNAC and IHC. Tests for estrogen receptor (ER), progesterone receptor (PR), and human EGF receptor 2 (HER2/neu) were performed and the sensitivity, specificity, and discrepancy rates between methodologies were calculated using correlation analysis and agreement tests. Results: The correlation analysis between immuno-staining of sections from cell blocks and histopathological examination of sections from tumor-tissue blocks revealed a high concordance for HR and HER2/neu. Conclusion: IHC of cell-block sections was found to be better for the determination of HR status and HER2/neu levels. It is very important to obtain high-quality cell blocks with strict quality control for their clarification.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study assess the FNAC method combined with cell blocks for immunostaining of ER, PgR and HER-2 in breast cancer cell material compared with CNB. As such I consider it a validation study and the number of cases as sufficient. The introduction part about the cell block technique and immunostaining is brief, and could be expanded. There is quite a number oF articles on the topic, both in cytology journals and others. There are also a number of articles on ER, PgR and HER-2 on FNAC materiale from breast cancer, AND you should confer with and refer to chapters 8 and 9 in ""The International Academy of Cytology Yokohama System for Reporting Breast Fine Needle Aspiration Biopsy Cytopathology"" by Andrew Field and coworkers. ISBN 978-3-030-268824. A validation study should tell us if the two methods we validate are equal. As such the concordance should be high, > 90 %. About 1/3 of your HER-2 positives were missed by the cell block method. ER and PgR have divergent results in both directions: positive and negative. Your results will have treatment implications, and divergent results must be minimal. You use the same IHC protocol both for cell block and CNB. That is common, BUT they are not equal. The preanalytical handling of FNAC material is not the same as for CNB. From your description it seems that all your aspirated material is an ethanol cell suspension. Ethanol is a good fixative, but it is a precipitating/coagulating type of fixative that changes the tertiary structure of the cell molecules in a quite a different way as the cross-linking formalin. The time in alcohol is the primary fixation. Your material is brought to the laboratory when you have finished your out patient clinic, which can be from 30 minutes up to more than one hour. Your cells are fully fixed in ethanol when they reach the lab. You use formalin as post-fixation, which is a good thing, but your epitope presentation will be determined by the alcohol fixation. That means you need to modify your protocol, because the demasking should not be equal to a primary formalin fixed tissue. I suggest that this is the reason for the significant discrepancy of HER-2 positivity. I disagree with your conclusion then, that the two are equal, but with a protocol modification and optimisation, I think you could achieve it. The subtype of BC is hardly relevant as a cause of discrepancy, nor the number of cells as long as the number is sufficient for evaluation. You mix methodology, screening and clinical issues in your discussion.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",574,0,2,0.7688,0.1598682477,0.9022762775,850,44.24,0.3172,f1000,0.0106382978723403,5,5,5,5,factual,5,5,100,neutral,5,negative,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
112,Agnieszka-Lawrynowicz,Machine Learning for the Semantic Web: Lessons Learnt and Next Research Directions,"Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed jointly with an analysis of their main peculiarities and drawbacks, afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.","The paper surveys methods of machine learning as solutions developed for the Semantic Web, dividing them into symbolic ones and numeric ones. Machine learning methods proved efficient in supporting Semantic Web tasks, and there have been an icreasing interest in their application in the Semantic Web, especially regarding the numeric approaches, which is what the paper also discusses. Besides of their strenghts, the paper also points to drawbacks of current numeric machine learning approaches such as non-interpretability or lack of reasoning capabilites with respect to standard languages (especially OWL). The paper also points to next research directions in the development of machine learning solutions for the Semantic Web, and I fully agree with the author when it comes to these directions.  Below I provide some suggestions for improving the manuscript: 1) Overall, the manuscript contains several technical words (ILP, propositionalization, embeddings etc.), which may be not known to a reader not knowledgeable in machine learning. I suggest to explain those which are not explained to make the paper self-contaied, e.g. by injecting phrases with explanations, similarly, like it is already done in some places in the paper, e.g.: ""latent attributes (i.e. attributes not directly observable in the data)"". 2) The paper surveys methods developed by researchers active in the field, including the author. It would be much nicer to mention their names along with the citations, when suitable. 3) It would be valuable to summarize the main, recurring peculiarities and drawbacks of the methods discussed in Sections 2-3, maybe even using some table or graphics?  4) Regarding definitions, they are in an informal style (which is perfectly OK for a position paper), but still there is some care needed: * ""embedding models (also called energy-based models)"" -> are energy-based embedding models a class of embedding models or they are equivalent to each other? * ""In this context, link prediction is also referred to as knowledge graph completion."" -> in what context, in the context of KGs? Are there other tasks of knowledge graph completion, beyond link prediction?  5) Numeric methods are described for one major task: link prediction. Are there any other tasks that have been tackled by numeric machine learning methods for the Semantic Web?  6) References: It would be also nice to include a book within the topic, but of course this is up to the author: Agnieszka Lawrynowicz, Semantic Data Mining - An Ontology-Based Approach. Studies on the Semantic Web 29, IOS Press 2017. There is also a highly cited survey that deals with the topic of knowledge graph completion: Heiko Paulheim, Knowledge graph refinement: A survey of approaches and evaluation methods. Semantic Web 8(3): 489-508 (2017) Minor issues, typos:  *** Section 1. Introduction *** Page 1: it would be valuable to provide a reference to OWL Page 1: ""and assertion"" -> ""assertions"" Page 1: ""some these gaps"" -> ""some of these gaps"" Page 2: ""are illustrated is Sect. 4"" -> ""are illustrated in Sect. 4""  *** Section 2. Symbol-based Methods for the Semantic Web ** Page 2: ""One of the first problem"" -> ""One of the first problems"" Page 3: ""by the the employment"" -> ""by the employment"" *** Section 3. Numeric-based Methods for the Semantic Web ** Page 4: ""Almost any reasoning"" -> ""Almost no reasoning"" *** Section 4. Machine Learning for the SemanticWeb: Next Research Directions *** Page 5: ""As a first step, the integration of numeric and symbolic approaches should be focused.""->""The first step should focus on the integration of numeric and symbolic approaches""? Page 5: ""The main the conclusion""-> ""The main conclusion"" Page 5: ""how representing expressive logics within neural networks"" -> ""how  to represent expressive logics within neural networks"" Page 6: ""background knowledges"" -> ""background knowledge"" Page 6: ""and and makes it understandable"" -> ""and makes it understandable"" *** Section 5. Conclusions *** ""their main peculiarities and drawback"" -> ""their main peculiarities and drawbacks""",643,1,3,0.7094,0.144527027,0.9203350544,99,43.43,0.2025,semanticweb,0.0,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,5,4,4,5,factual,4,4,85,polite,5,positive,4,low,5,4,4,5,factual,4,4,88,polite,5,positive,3,low
133,Jérôme-Euzenat,Ontology Alignment Revisited: A Bibliometric Narrative,"Ontology alignment is an important problem in the Semantic Web with diverse applications in various disciplines. This paper delineates this vital field of study by analyzing a core set of research outputs from the domain. In this regard, the related publication records are extracted for the period of 2001 to 2018 by using a proper inquiry on the well-known database Scopus. The article details the evolution and progress of ontology alignment since its genesis by conducting two classes of analyses, namely, semantic and structural, on the retrieved publication records from Scopus. Semantic analysis entails the overall discovery of concepts, notions, and research lines flowing underneath ontology alignment, while the structural analysis provides a meta-level overview of the field by probing into the collaboration network and citation analysis in author and country levels. In addition to these analyses, the paper discusses the limitations and puts forward lines for the further progress of ontology alignment. ","The paper presents a bibliometric analysis of the field of Ontology matching. It applies a 'semantic analysis', trying to extract topics from papers, and a 'structural' analysis studying only the bibliographic characteristics of the literature (authorship, citation, etc.). Since, the Semantic web journal is not a journal about bibliometrics, this paper is rather particular for the journal. To be clear, it only uses classical techniques and does not apply semantic web technologies to bibliometrics. However, a part of the semantic web, Ontology matching, is the object of this study. That could be of interest to the journal readership, especially if remarkable features of the field were unveiled. The work seems to have been seriously done, as far as I can judge and most of what is expressed is clear. Unfortunately, after reading it, it does not seem worth publishing. The main problem is the lack of objective: what is this work trying to assess? For most of the presented study, there is no hypothesis tested and no interesting finding reported (except at one point, but without seriously seeking to explain it, see below). It is just like if the reported figures were totally indifferent and that the paper would have been the same with different figures. Another issue is the lack of baseline for the presented data. Indeed, it is impossible to know if the observed properties are specific to the Ontology matching field, or if they apply equally in other fields. At least, it would have been good to have a comparison with the broader context, i.e. comparing with Semantic web and Computer science. It is possible to observe features of the Ontology matching field, but no way for the reader to understand if these are remarkable or not. Finally, in these times of Open science, it is regrettable that no mention is made of the availability of the data.  These points are the major issues. I discuss below various problems, some of them related to the issues above, some of them discussing particular points. They may help the authors to improve their paper.  * Organisation: - The introduction does not state any goal for the paper, neither claim any findings. It rather describes the applied treatments. - Section 2 provides a methodology. However, in absence of statement about the goal of the analysis, it is not possible to judge the relevance of the methodology. - Section 2.1 details lenghtly the preprocessing of WoS, before turning more succintly to Scopus which was actually used. This seems a strange way to present things. * Data interpretation: - The topic analysis is not particularly insightful. In particular, it does not provide much information on ontology matching but on its use. It seems to gather the terms in an unprincipled way (heterogeneous and automatic appear in most of them, biomedical is in the learn cloud while anatomy is in the query one, etc.). It may have been interesting to see all the generated 'topics' that the authors did not retain. In the end it is unclear, which conclusions may be drawn from the topic analysis. - p10: 'Ontology alignment outputs form 6.1% of the top 10% most cited article worldwide in year 2013' (I simplified). How can this be? From Fig. 3, there seems to be no more than 300 papers in scopus on 'ontology alignment' for 2013. If they are all in the 10% most cited papers and these are 6.1% of them, this means that there was only 10*300/6.1%~50000 papers indexed by Scopus (if not all 300 are in the most cited, then this is even less). This does not seem to be right: I counted 2.8Mdocuments in Scopus for 2013. It seems to me that what was meant is that 6.1% of the 'ontology alignment' papers are in the 10% most cited papers. Again with no comparison to the same figure for Semantic web or Computer science, it is difficult to tell that this figure is specific to Ontology alignment (there are fields with more citations and fields with less citations, e.g. Mathematics, and putting them all together means that some are above average and some other are below). - Section 4.3 is about disciplines relevant to Ontology matching. Given the broad categories used here (the level 2 categories of Fig 7), is it unclear that this characterisation is useful for something. - Section 5.1-5.2 about collaboration are those that could be thought of as providing some findings. Figure 8 is stunning at providing two identified clusters. The authors do not provide much explanation about this phenomenon, they suggest that may be the researchers from one cluster are not curious about the others. However, these graphs being computed on collaboration, a symmetric measure, it seems that this explanation should, at the very least, be applied symmetrically. It is difficult from this data alone to provide an explanation, but many could be put forth. In particular, the fact that one of this cluster is mononational and the other international suggest that the explanation comes from some national elements (but see discussion below). These may be linguistic factors, the collaboration approach, work approach (many coauthors, many authors of only one paper, e.g. undergraduate students: this can be studied bibliometrically), publication policies (strong incentive to publish many papers and in scopus indexed journal, hence less in the Ontology matching workshop). It is possible that many of these factors play some role together... Finally, again in the absence of comparison with other fields, it is difficult to assess if this is due to the Ontology matching field. - This judgement made on collaborations is also made on citations (though to a far lesser extent). That could have helped sheding light on this matter because citation is not symmetric. Unfortunately, in 6.2, citations are only reported as numbers assigned to papers and country so, they are not helpful. This is too bad because if a community has less citation per paper than another, it is difficult to explain it by discrimination if both communities have the same citation pattern (they both cite less the same community). At least, it would have been worth to rule out this possibility. - As I understand from the text, six communities were extracted and only two are shown in Figure 9. If the number 6 was not given to the algorithm and is significant, then the six should be shown. - 5.1 Author collaboration: the conclusion drawn on page 14 are very general and not specific to Ontology matching. - p16 ""the research outputs with at least one Chinese author have not gained enough attention"": it is unclear on what ground this statement is based. Same thing for ""they do not get enough attention, possibly the attention they deserve"". - Again, 5.3-5.4 would deserve to be compared with the broader Semantic web/Computer science fields. - The authors ""encourage the organisers of OAEI"" to have benchmarks on the identified topics. Unfortunately, these topics are not application domains, like biomedicine, but application techniques, like ""Semantic Web Services, agent-based modelling, knowledge-graphs, and business processes (cited directly from the paper)"". This means that there are not many ontologies to match there... and some of them have been considered, e.g. Process matching. * Data presentation: - Figure 2 displays data as tag clouds. The precise interpretation of tag clouds is quite unclear to me, so if there is one, it should be provided. In general, it does not seems like tag clouds are a proper scientific visualisation instrument (no unit, no scale, esthetic arrangement). - Figure 8 is interesting, but it would also be interesting to understand the space, i.e. what are the principles of entity placement. The same applies to Figure 17. - The assignment of authors to countries is not specified. One of the most collaborative ""Chinese scientist"" is ""S. Wang"". I assume that this is Shenghui Wang. Shenghui published her work while at VU Amsterdam. It is unclear that she should count as Chinese (in such a case, Pavel Shvaiko is from Belarus, Ernesto Jimenez-Ruiz from Spain, Cassia Trojahn from Brazil, etc.). In this sense, she is atypical (less and less atypical as time passes), and seems to indicate that the two clusters are rather based on the involvement in an international collaboration network or not, rather than nationality. This, in turn, may have other causes (see above). - Figure 9 is unreadable in black and white. * Form: - The title of the paper is quite strange: ontology alignment is not really revisited and there is not real ""narrative"" provided here. Moreover, this is not really the purpose of scientific journals to publish ""narratives"", but findings. - The introduction uses a flourished language that is also a bit remote from fact. For instance: ""the heterogenity problem was quite epidemic"" is not particularly clear. * Details: - p4: '""ontology alignment"", which is interchangeably referred to as ""ontology matching"" or ""ontology mapping""': it is not clear by whom. - It may have been interesting to look for outliers in this data set. In particular, books and review papers traditionally get a lot of citation: do these figures look the same if they are retracted from the corpus? I do not know if it is accepted practice in bibliometrics and this is less important than comparing with external fields. - p22 there seem to be a missing reference. - In some instances, such as reference 2 or Table 2, problems with characters.",1570,0,3,0.7853,0.0533716842,0.9450801611,84,55.13,0.0743,semanticweb,0.0,4,4,5,3,factual,4,3,80,neutral,4,positive,4,low,4,4,4,4,factual,4,4,75,polite,5,negative,5,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,moderate,3,4,5,3,factual,4,4,75,neutral,5,negative,4,low,3,4,4,4,partially factual,4,4,75,neutral,5,negative,3,low
191,Reviewer-1FB4,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes Video Prediction Rewards (VIPER), a general architecture that extracts reward functions from action-free expert demonstrations. To match the agent's trajectory distribution with the source distribution, VIPER optimizes the agent to maximize a log-likelihood estimated by an auto-regressive video model and an entropy term to encourage exploration.

Experiments on DMC, Atari, and RLBench demonstrate the soundness and efficiency of the reward function extracted by VIPER. VIPER addresses the practical challenge of how to extract reward functions from action-free expert demonstrations in order to optimize our agents, which is useful in settings like self-driving.
VIPER has the following strengths:
- VIPER can extract effective reward functions and thus promote policy optimization in a range of visual control tasks.
- Experiments show that reward functions learned by VIPER can generalize to a variety of tasks and even OOD tasks. Although VIPER shows good experiments results, some weaknesses still exist:

- The data efficiency of VIPER seems to be low as it requires nearly 10M data to converge in DMC. Also, it seems that VIPER can not leverage sub-optimal demonstrations, which could be important for improving data efficiency.
- It could be difficult and expensive to acquire a generative video model for real-world tasks, especially with visual distractors. 
- Also, I think current tasks are a little bit less challenging, and thus it might be easier to define a reward function than acquire expert demonstrations. Therefore, it could be interesting if we could test VIPER's performance with tasks that are hard to define rewards, e.g. embodied tasks like \[Habitat\](https://github.com/facebookresearch/habitat-sim) or self-driving platforms. - To my understanding, VIPER's setting is similar to Generative Adversarial Imitation Learning (GAIL), while GAIL uses the critic as the surrogate of the distance between the expert trajectory distribution and generated trajectory distribution, VIPER directly estimates the distance (KL divergence) by modeling the log-likelihood with a generative model. I have some reservations regarding the benefits of doing so. NA",321,1,2,0.7998000000000001,0.0668560606,0.872304976,218,23.3029,0.2552,neurips,0.0,2,2,1,2,partially factual,3,2,30,polite,3,neutral,3,high,3,4,4,4,factual,4,4,75,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
191,Reviewer-qQw6,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The authors present Video Prediction Rewards, an algorithm that leverages transformer-based video prediction models as action-free reward signals for reinforcement learning. The reward induced by this video prediction model incentivizes the agent to find the most likely trajectory under the expert video distribution. By further incorporating some exploration rewards, such as RND, the proposed method obtains good performance across a wide range of DMC, Atari, and RLBench tasks. The paper is well written and easy to read.
The authors aim to address a crucial problem in reinforcement learning, i.e., the reward function design. The authors propose a concise method, and experimental results also validate the effectiveness of the approach.
 I'm concerned about the problem of out-of-distribution. Can the pre-trained video prediction models accurately evaluate unseen behaviours? See the weakness NA",130,0,1,0.7995,0.1777777778,0.9095652103,218,32.2492,0.1633,neurips,0.0109890109890109,2,2,1,2,partially factual,3,2,20,polite,3,neutral,2,high,3,4,4,4,partially factual,4,4,75,polite,4,positive,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,2,factual,3,3,60,polite,4,positive,4,low,2,5,3,3,partially factual,3,3,75,polite,4,positive,4,low
191,Reviewer-d6Ah,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes a learning-from-observation algorithm that builds a reward based on a video predictor trained from action-free expert videos. Experimental results show that online reinforcement learning algorithms can learn a working policy from their reward only effectively. This paper contains rich and informative ablation studies and analyses to verify their design choices. 1.	The paper writing is clear and easy to understand
2.	Distilling knowledge from action-free videos to policies is a promising future direction for robotics.
3.	Experiments are rich. The authors test their method with two different online RL methods, two different exploration losses, three task domains, and three different video prediction models. The experiments on the generalization ability (Sec. 4.3) are not convincing enough. The video prediction model in Sec.4.3 is trained with 23 Rethink-robot-arm tasks and 30 Franka-robot-arm tasks. There should be dozens of OOD arm/task combinations that can be evaluated. However, according to L300, we only see the performance on only ONE OOD combination. How is the performance on other OOD combinations? Besides, the learning curve in Fig.8 doesn’t include a task oracle like other experiments in the paper. So we also don’t know how good the OOD performance is. Therefore, I think the third contribution of this paper, “VIPER generalizes to different environments”, is not well-supported. 1.	How good is the generalization ability of VIPER? We definitely need evaluations on more OOD combinations to support the statement in the third contribution. 
2.	For Fig.8, which OOD combination the curve shows? In addition, this curve doesn’t include an error bar like other experiments in the paper. The authors listed and discussed the limitations including the lack of in-domain expert data in the real world, the sub-optimal performance with stochastic data, and the sensitive performance to the VQCode size and context length.",297,0,4,0.7494000000000001,0.1261494253,0.8708613515,218,39.3404,0.0622,neurips,0.0,4,3,3,4,partially factual,3,3,50,neutral,3,negative,3,moderate,5,5,4,5,5,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
191,Reviewer-SQjv,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes to use prediction likelihoods from autoregressive video models as reward functions to train reinforcement learning agents. Specifically, the conditional likelihood $\log p(x_{t+1}|x_{1:t})$ is augmented with an exploration reward to avoid suboptimal behavior. The authors conduct extensive experiments and show that likelihoods of autoregressive video models can be effective for reward specification. They also show that in certain cases the video models can generalize to unseen task domains, encouraging satisfying behaviors. Their ablation study compares different video models, different exploration objectives, and different context lengths.  Originality: Though the idea of using likelihood of states/observations as a reward is not novel, taking the temporal coherence into consideration with an autoregressive factorization is novel at my end. 

Quality: This work is strong in its efforts in extensive experiments. 

Clarity: This paper is straightforward to follow. The narrative is very intuitive. Experimental details are very well documented. 

Significance: Learning memory-based reward function with sequence modeling is an interesting direction to explore given the current advances of generative models.  In spite of the impressive amount of experiments presented in this work, one fundamental problem unresolved in this work is why the autoregressive likelihood, which is inherently non-Markov, can work with general RL algorithms, in which TD learning strongly depends on Markovian rewards. Latent-space model-based RL methods such as Dreamer used in this work are too particular because the latent-space modeling may resolve the limitation of TD learning as a byproduct. This means the empirical result from this work cannot be trivially generalized to other RL methods, rendering the thesis statement an overclaim.  Apart from the question I raised in Weakness, I hope the authors would also like to resolve my concerns in Section 4.3. 

While the paper claimed that specifying reward functions with video models can generalize to OOD tasks, Section 4.3 only demonstrate a particular case where there is a recombination of robot and objects to be manipulated. Is it possible to make the evaluation of generalization more systematic? I guess readers may be more interested in a discussion of what ""types"" of generalization are possible to eliminate the influence of particularity.  As stated in Weakness, there is a technical limitation of the proposed method that the authors do not seem to notice. Other limitations are well documented in Section 5. ",380,0,0,0.8179000000000001,0.1948156682,0.892482996,218,25.9473,0.2025,neurips,0.0,3,3,3,3,partially factual,4,3,70,polite,4,positive,5,moderate,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low
191,Reviewer-BsvD,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","The paper proposes to use a large video-prediction model for learning a reward model for RL. The agent's performance is evaluated on a total of 8 envs from 3 benchmarks: DMC, Atari, and RLBench. The paper argues that the proposed model also generalizes to different environments for which no training data was provided, enabling cross-embodiment generalization for tabletop manipulation. * The paper is well-written and easy to follow.
* Section 4 is very well organized. It starts by asking base questions like ""can VIPER provide an adequate learning signal for solving a variety of tasks?"" before jumping on to the evaluation of the performance of the RL algorithm. Limited baselines: The paper compares with just 1 baseline (the second ""baseline"" is more of an ablation of the first baseline). e.g. there is https://sites.google.com/view/vip-rl that claims to provide ""dense visual reward"". The paper itself lists a bunch of baselines (in the related work) but does not compare to them.

Limited ablations: See questions.

Overall, I think the paper is interesting but I want to see performance improvement over a bunch of baselines and some ablations. I would encourage the authors to engage during the rebuttal period. 1. In line 128, the paper states ""For example, when flipping a weighted coin with p(heads = 0.6) 1000 times, typical sequences will count roughly 600 heads and 400 tails, in contrast to the most probable sequence of 1000 heads that will basically never be seen in practice"". Could the authors explain why is the sequence of 1000 heads the most probable one ?
2. Does the algorithm work with good trajectories as well or does it need access to expert trajectories? e.g. in line 172, what if they were using the top 650 to top 550 episodes, in place of the top 100 episodes. This would make for an useful ablation experiment.
3. Arent the video datasets ""too small""? Given that the video models are trained for hundreds of thousands of updates, I wonder if the video models are drastically overfitting, leading to (i) the learned policies not showing any diverse behaviours and (ii) the learned policies failing with stochastic envs. This would make for another useful ablation experiment.
4. Line 201 states that TPUs were used for training while 226 states that GPUs were used for training. Which is it :) NA",389,1,7,0.7881,0.1882653061,0.9128586054,218,56.4134,0.2025,neurips,0.0,3,3,2,3,partially factual,3,3,40,neutral,3,positive,2,low,4,5,3,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,5,neutral,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
191,Reviewer-Zre9,Video Prediction Models as Rewards for Reinforcement Learning,"Specifying reward signals that allow agents to learn complex behaviors is a long-standing challenge in reinforcement learning.
A promising approach is to extract preferences for behaviors from unlabeled videos, which are widely available on the internet. We present Video Prediction Rewards (VIPER), an algorithm that leverages pretrained video prediction models as action-free reward signals for reinforcement learning. Specifically, we first train an autoregressive transformer on expert videos and then use the video prediction likelihoods as reward signals for a reinforcement learning agent. VIPER enables expert-level control without programmatic task rewards across a wide range of DMC, Atari, and RLBench tasks. Moreover, generalization of the video prediction model allows us to derive rewards for an out-of-distribution environment where no expert data is available, enabling cross-embodiment generalization for tabletop manipulation. We see our work as starting point for scalable reward specification from unlabeled videos that will benefit from the rapid advances in generative modeling. Source code and datasets are available on the project website: https://ViperRL.com","This paper proposes a simple method that uses a pre-trained video prediction model to provide rewards for online RL. The design includes using VQ-GAN to encode discrete embeddings and incorporating an exploration bonus (opt for Plan2Explore and RND). In experiments, the authors also show the learned rewards provide a useful learning signal for online RL.  1. The paper is well-written and the idea is clear.
2. The authors make comparisons on multiple tasks.  1. The effectiveness of the proposed method may be limited if the expert data is scarce. 
2. In many imitation learning papers almost only one expert trajectory is needed, however, this paper undoubtedly requires a lot of expert data (to train the video prediction model).
3. Although the authors make comparisons on multiple tasks, there are few baselines. There are many papers on imitation learning that do not make experimental comparisons, e.g. \[1, 2, 3, 4, 5\]. 

\[1\] Optimal Transport for Offline Imitation Learning
\[2\] Demodice: Offline imitation learning with supplementary imperfect demonstrations
\[3\] Behavioral cloning from observation
\[4\] Generative adversarial imitation from observation 
\[5\] CLUE: Calibrated Latent Guidance for Offline Reinforcement Learning 1. Can the author compare the proposed method to more imitation learning papers?
2. Doesn't the method suffer from the problem of OOD issues when there is very little expert data? Even when a dozen or so pieces of expert data exist, it seems that the OOD problem exists, i.e., the pre-trained video prediction model may falsely overestimate the probability of some behaviors that are not expert behaviors.
3. After I thought deeply about it, I always thought that there is an OOD problem with the method, which is consistent with standard offline RL, as the policy network will make an effort to explore and discover behaviors with a high probability/likelihood, however, these behaviors may be falsely overestimated by the video prediction network. 
4. In the main paper, I did not see the results that ""VIPER can achieve expert-level control without task rewards on 15 DMC tasks, 6 RLBench tasks, and 7 Atari tasks"". 
5. In addition, the authors only emphasize achieving expert-level performance and do not compare it to a large number of imitation learning baselines. This tends to raise doubts about the performance of the method, since with enough expert data, simple behavioral cloning can also achieve expert-level performance.  The authors briefly discuss the limitations of the paper. ",396,6,10,0.8049000000000001,0.0169512649,0.8747833967,218,36.338,0.1262,neurips,0.0217391304347825,3,2,3,2,partially factual,3,3,30,neutral,3,negative,2,moderate,4,4,3,4,partially factual,4,3,65,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
4,Epari-Venkatarao,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This is a protocol for publication before the research is being conducted. It talks about finding the ability of NLR as a prognostic indicator in organophosphorus poisoning. NLR as a prognostic indicator has been studied extensively in recent times in various other clinical conditions including cancer. Hence, the ROL should look into this including the methodology followed to find its prostic value, which will add further knowledge to the existing body of knowledge. The outcome variables of the study should be well defined before conducting the research. This will help in the designing the study and calculation of an appropriate sample size. The sample size should be calculated using AUC in ROC analysis from published literature. The outcome measures defined by the study's objectives will determine the role of appropriate statistical methods. The authors have not been able to spell out the outcome measures properly. Hence, the specificity of the use of statistical methods seems vague. This can lead to confusion at a later stage after data collection. Dummy tables and dummy analysis before the execution of the study will be useful. The Review of Literature (ROL) lacks a finding of NLR as an inflammatory marker. There is literature available on NLR as a prognostic marker in cancer. The authors have proposed data collection at a single time point, which will have a bias in the analysis as factors like time-to-intervention, dose-response, quality of care, etc., can not be accounted for in the analysis.  Finally, the sample size calculation is inappropriate as the study is NOT trying to find the prevalence of death among organophosphorus poisoning cases with NLR >12, rather with appropriate ROL, sample size calculation method has to be revisited.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? No  Are sufficient details of the methods provided to allow replication by others? Partly  Are the datasets clearly presented in a useable and accessible format? Not applicable",398,0,1,0.7682,0.1282840722,0.8034374714,43,36.18,0.1041,f1000,0.0,5,5,5,5,factual,5,5,93,neutral,5,negative,5,none,4,4,3,4,factual,3,4,70,neutral,5,negative,4,moderate,2.0,5.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,75,neutral,5,negative,4,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,3,low
4,Deepak-Kumar,A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India,"Background Acute organophosphorus poisoning remains a significant public health concern, with variable clinical outcomes. Prognostic markers are crucial for patient management and risk stratification. This study aims to investigate the Neutrophil Lymphocyte Ratio (NLR) as a potential prognostic marker and its associations with severity and clinical outcomes in acute organophosphorus poisoning.  Methods This cross-sectional observational study will be conducted over two years, involving patients presenting with acute organophosphorus poisoning in the Medicine Ward and Intensive Care Unit of DMIHER Wardha. Informed consent will be obtained, and detailed clinical assessments, laboratory investigations, and NLR calculations will be performed. The Nambaet, Peradeniya, and Bardin classification scales will be used to measure severity. Statistical methods will be applied to explore the relationships between NLR, clinical parameters, and clinical outcomes, including descriptive statistics, bivariate analysis, correlation analysis, multivariate regression, and ROC analysis.  Expected Results The study is anticipated to elucidate the role of NLR as a prognostic marker in acute organophosphorus poisoning. Initial assessments and correlations between NLR and clinical parameters will be presented. The predictive capability of NLR for clinical outcomes, including the need for ventilatory support and length of hospital stay, will be explored. Agreement and discrepancies between the classification scales will be evaluated.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Dear Editor I have gone through the manuscript (study protocol) titled “A cross-sectional study of neutrophil to lymphocyte ratio as a prognostic marker in acute organophosphorus poisoning in a tertiary care hospital in Central India”. Following are my comments for consideration (Major Revision) Several studies are already available which showed the role of neutrophil-to-lymphocyte ratio (NLR) as a prognostic marker in acute organophosphorus poisoning with detailed method/protocol (https://www.sciencedirect.com/science/article/abs/pii/S0736467914005034 file:///C:/Users/Dr%20Deepak%20Kumar/Downloads/5-OA-Basanta+Gauli.pdf, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8284330/ ). Please elaborate.  Under study status it is mentioned as “The study has yet to start after the publication of the protocol; we will start recruitment in the study.”  However, under study design it is mentioned as  “Data will be collected at a single time point setup for the 2023-2024 period.” Considering the fact that it is mid-August 2024, when will the authors start the work and complete it within 2023-2024 period. So kindly revise the relevant content in the manuscript and its ethical approval accordingly.  Include the statement that the work will be carried out following the tenets of the Helsinki Declaration.  How the diagnosis of organophosphorus pesticide exposure will be carried out? Or in other words which method was used to find out the confirmed cases of  OP poisoning? How the authors confirm the inclusion and exclusion criteria. Which parameter will be considered for this?  Estimation of AChE activity is of the method for understanding OP poisoning. However, both organophosphorus (OP) and organocarbamates (OC) inhibit AChE activity (https://pubmed.ncbi.nlm.nih.gov/37805177/ ). Then how do the authors distinguish OP cases from OC. Please address this issue. This should be properly mentioned in the protocol. Under objectives, it is mentioned as under “To investigate whether the Neutrophil to Lymphocyte Ratio is correlated with the dose of atropine administered to patients with acute organophosphorus poisoning.” In cases where organophosphate poisoning is on the differential but not confirmed, a trial of atropine is generally administered (https://www.ncbi.nlm.nih.gov/books/NBK470430/#:~:text=If%20organophosphate%20poisoning%20is%20on,suspicion%20of%20AChE%20inhibitor%20poisoning. ). Then how do the investigators access the control NLR value (i.e., value before administration of atropine). Please discuss.  Mention which clinical/biochemical parameters will be considered for assessment.  Kindly include the following in the exclusion criteria: The patients who are on steroids, pregnant patients, and patients with blood disorders (https://www.jcmc.com.np/jcmc/index.php/jcmc/article/download/1311/836 ).  Thanks  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Partly",479,5,4,0.7713,0.1888736264,0.9150463343,119,28.23,0.6118,f1000,0.010752688172043,5,5,5,4,factual,5,5,89,polite,5,negative,5,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,negative,4,low,3,4,4,4,partially factual,4,4,78,neutral,5,negative,3,low
144,Reviewer-99Tt,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples","The authors introduce a new training procedure for PINNs which are adapted to unknown measurement noise, i.e., a training procedure which works for any noise model. This is done via EBMs, which are trained jointly with the PINN. Here the EBMs estimate a 1d noise model based on the estimation of the PINN (conditional to the point $t_i$). Since they only estimate a 1d distribution, the (usually intractable) normalization constant can be estimated via numerical integration. The approach is tested on several (partial) differential equations and benchmarked against the standard PINN. The paper is easy to follow (except a few minor points). The idea is interesting and well-executed. The approach outperforms the standard PINN and offset PINN baseline. The experiments are well described, so that I think reproduction should be easy. 1) While the idea is heuristically clear, it would be interesting whether one can obtain theoretical guarantees. I have got the hunch that it should be possible to cast the framework into one of expectation maximization (EM) algorithms (maybe one slightly needs to change the loss and train alternating instead of jointly). Did the authors give this some thought? This would greatly strengthen the paper in my opinion. For this see e.g. \[1\]

2) The discussion in 4.1 and 4.2 is a bit confusing. While I think I got the gist of it, please make clear what variables the functions $\mu_{\varepsilon}$ and $\theta_0$ depend on. 

3) The metric logL is not clearly defined. How is that calculated in the case of a standard PINN, just Gaussian likelihood?

4) The non-Gaussian noise is a GMM. I would like to see physically more realistic noise models. One thing that could be interesting is whether this approach is able to learn mixed Gaussian noise, i.e., $y = f(t) + \eta_1 + f(t)\ \eta_2$ for normal $\eta_1,\eta_2$ with some variances. While this is still Gaussian, this is a noise model used in practice. 

5) Please make the relation to model errors \[2\] and \[3\] more clear. Although the model error framework tries to solve a different problem (Bayesian inversion) the ideas are somewhat similar.

6) A very similar is to train a surrogate on the data only (no PINN loss), then estimate the noise via an appropriate model, such as an EBM and then to train the surrogate on a combined loss. Please comment on this. 

\[1\] DeepGEM: Generalized Expectation-Maximization for Blind Inversion, Gao et al

\[2\] Iterative Updating of Model Error for Bayesian Inversion, Calvetti et al

\[3\] Noise-aware physics-informed machine learning
for robust PDE discovery, Thanasutives et al See weaknesses. I overall like the idea and think it has a lot of merit. A consideration of more realistic noise models and some theoretical guarantees would strenghten the article imo.",458,6,0,0.7849,0.1084022039,0.8494194150000001,51,56.5506,0.2,iclr,0.0,4,4,4,4,factual,4,5,90,polite,4,positive,5,low,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,5,factual,4,4,85,polite,5,positive,5,low,4,4,4,5,factual,4,4,92,polite,5,positive,5,low
144,Reviewer-4nHF,Physics-informed neural networks with unknown measurement noise,"Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples",This article proposes a method for training physics informed neural networks (PINNs) when the distribution of measurement noise is unknown. The key idea is to learn noise distribution using an energy-based model on top of training of PINNs. A few numerical experiments show the usefulness of the proposed method. The usefulness of the method is shown by numerical experiments for a few example problems. There is little theoretical backing. Extension to high-dimensional and/or non-iid noises would require much heavier computation. Experiments are limited only to synthetic problems. Are there any practical problems that could be resolved by the proposed method?,100,0,0,0.7108,-0.0058928571,0.9032465816,51,35.0995,0.0945,iclr,0.010204081632653,2,4,3,2,partially factual,2,3,40,impolite,3,negative,2,moderate,3,5,4,4,factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,2,4,3,2,factual,4,4,60,neutral,4,neutral,4,low,3,4,3,3,partially factual,4,4,75,polite,5,neutral,4,low
147,Reviewer-GYKR,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper tackles updating the knowledge in LMs, focusing on allowing LMs to make new inferences consistent with the updated facts. To do this, the authors propose using the LM itself (or a teacher) to generate natural continuations for the ""updated/new entity"" definition. These continuations are used to update the LM. The update is conducted using a KL divergence loss between the LM conditioned on the definition and the LM that doesn't see the definition. The results show superiority to baselines in updates and in preserving old knowledge. The paper seems like an excellent contribution. It's well motivated, well presented, and the key idea is simple, novel, and effective. The evaluation is convincing. The method, like many others, is relatively opaque in terms of what it teaches the models and why/how it works precisely. However, it's well motivated and the analysis in Sec 7 begins to shed a little bit of light into this. More work is needed on that front, but I think it's fair to assume this will lie beyond the scope of this paper. N/A N/A",179,0,1,0.7292000000000001,0.2881684492,0.9028391838,215,49.4757,0.1262,neurips,0.0104166666666666,2,3,3,2,factual,4,4,55,polite,4,positive,3,moderate,4,5,4,5,5,5,5,90,polite,5,positive,5,moderate,3.0,4.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,2,4,4,2,factual,4,4,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
147,Reviewer-NBus,Propagating Knowledge Updates to LMs Through Distillation,"Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities \emph{and} propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the 'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not  compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.","This paper studies the problem of injecting new entity knowledge in LLMs, such that these knowledge can be propagated and utilized when LLMs make inference on related queries. The paper proposes a context distillation method that consists of two steps to inject entity knowledge in a definition sentence: 1) Use a LLM to generate a set of continuations (a.k.a transfer set) for the definition sentence. 2) Fine-tune a student model such that its output distribution without conditioning on the definition sentence is close to the output distribution of a teacher model that conditions on the definition sentence.
They conduct experiments on two datasets about entity knowledge and show that the proposed method outperforms several baselines including standard fine-tuning and previous knowledge editing methods. 1. This paper studies an important question of knowledge injection and propagation of injected knowledge. The proposed method is novel in this context.
2. Some of the conducted analyses are insightful, such as the NLL with/without definition sentence for analyzing the supervision from the teacher model. 1. On Entity Inferences dataset, the conclusion that the proposed method improves the model ability to make inference using the injected knowledge is suspected. The reported performance improvement might due to the overlap between the generated transfer set and the probe sentence in the evaluation set. Without reporting (1) the level of overlap, and (2) a baseline that simply fine-tunes on the transfer set, the possibility of this overlap cannot be ruled out.
2. How does the method perform compared to a baseline that simply prepends the transfer set to the query? 1. In Table 2, the Target for GPT2-XL should be 64.3 instead of 65.3 (based on the $\Delta$ value)?
2. In Table 2, why would using GPT3.5 to generate transfer set result in worse specificity for GPT2-XL?
3. I'm not sure why most of the analyses are done on the ECBD dataset, as I thought Entity Inferences dataset concerns more about injected knowledge propagation. Limitations are discussed.",328,0,7,0.752,0.0371685606,0.8985326290000001,215,40.4891,0.1507,neurips,0.0,5,5,4,4,factual,5,5,75,polite,5,neutral,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
125,Nicola-Maffulli,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article bears witness to how much we fall in love with novelties, and how much we, as a scientific community, do not know yet about a fashionable autologous blood product.This case report is now one year old, and the situation in this field remains unchanged: randomised controlled trials show in a fairly unequivocable fashion that PRP use is at best dubious, and nevertheless case series report success.This should make us think, and use strict stringent scientific methods to plan and evaluate new technologies.",153,0,0,0.8462000000000001,0.2045900178,0.6052519083,368,15.68,0.1355,f1000,0.0,1,2,1,2,unfactual,2,0,40,polite,3,positive,1,extreme,2,4,3,2,partially factual,2,2,45,impolite,4,negative,4,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,1,3,2,1,partially factual,3,2,40,neutral,3,neutral,3,moderate,1,4,3,2,partially factual,3,2,55,neutral,4,negative,4,low
125,Marco-Patruno,Neotendon infilling of a full thickness rotator cuff foot print tear following ultrasound guided liquid platelet rich plasma injection and percutaneous tenotomy: favourable outcome up to one year,This is a case report on excellent clinical outcome and neotendon infilling at one year follow up in a degenerative rotator cuff full thickness tear following percutaneous tenotomy and platelet rich plasma injection.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In my opinion the author of this case report describes the results well, although I do agree with Elizaveta Kon that also including MRIs would have improved the quality of the paper. The real action of PRP is still under debate, and the scientific community asks for stringent methods and careful evaluations, even for a single case study. I suggest the author increases the number of patients and improves the quality of the results in future studies concerning PRP.",146,0,0,0.8156,0.0443027211,0.5150036812000001,421,17.51,0.2025,f1000,0.01,1,3,1,2,unfactual,1,1,45,neutral,3,neutral,2,extreme,4,5,4,4,factual,4,4,75,polite,5,positive,3,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,3,factual,4,3,60,polite,4,positive,3,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
124,Dharma-Varapula,Negligible effects of read trimming on the accuracy of germline short variant calling in the human genome,"Background Next generation sequencing (NGS) has become a standard tool in the molecular diagnostics of Mendelian disease, and the precision of such diagnostics is greatly affected by the accuracy of variant calling from sequencing data. Recently, we have comprehensively evaluated the performance of multiple variant calling pipelines. However, no systematic analysis of the effects of read trimming on variant discovery with modern variant calling software has yet been performed.  Methods In this work, we systematically evaluated the effects of adapters on the performance of 8 variant calling and filtering methods using 14 standard reference Genome-in-a-Bottle (GIAB) samples. Variant calls were compared to the ground truth variant sets, and the effect of adapter trimming with different tools was assessed using major performance metrics (precision, recall, and F1 score).  Results We show that adapter trimming has no effect on the accuracy of the best-performing variant callers (e.g., DeepVariant) on whole-genome sequencing (WGS) data. For whole-exome sequencing (WES) datasets subtle improvement of accuracy was observed in some of the samples. In high-coverage WES data (~200x mean coverage), adapter removal allowed for discovery of 2-4 additional true positive variants in only two out of seven datasets tested. Moreover, this effect was not dependent on the median insert size and proportion of adapter sequences in reads. Surprisingly, the effect of trimming on variant calling was reversed when moderate coverage (~80-100x) WES data was used. Finally, we show that some of the recently developed machine learning-based variant callers demonstrate greater dependence on the presence of adapters in reads.  Conclusions Taken together, our results indicate that adapter removal is unnecessary when calling germline variants, but suggest that preprocessing methods should be carefully chosen when developing and using machine learning-based variant analysis methods.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  In this report, Barbitoff, Y. and Predeus, A. have described a study investigating if read trimming, specifically adapter trimming, affects variant calling accuracy using commonly employed variant callers. The authors find this investigation to be of significant value citing there is no prior systematic study exploring the impact of read trimming on variant calling accuracy. In the study, WES and WGS datasets from seven GIAB samples were processed using six different variant callers (DeepVariant, GATK HaplotypeCaller, Freebayes, Strelka2, Octopus, and Clair3) to measure the effect of read trimming performed prior to the variant calling. The authors show comparative metrics (differences between trimmed and untrimmed variant caller performance metrics – recall, precision and F1 scores) and find no substantial differences in variant calling performance, except in the case of 200x coverage WES. Subsequently, the authors downsampled the data to produce a simulated 80x WES dataset expecting a greater likelihood for an  increased impact of read trimming on variant calling accuracy. This simulated dataset too did not show significant impact due to read trimming. Further, the authors found no correlation between extent of adapter base contamination and impact of read trimming on variant caller performance metrics. Additionally, the authors ran the pipelines with different variant callers and found minimal impacts due to read trimming upstream. My comments below: The adapter base percentage variation ranged from 8.1% to 35.2%. Please comment if this is an expected range for WES datasets. Also, please mention the coverage of the WES dataset in the caption for Fig 1. How does one assess the changes in performance metrics to be significant or not (Fig 1b and 1c)? Recall and precision score metrics in Figure 1b for Indels in WES datasets show deviations from the mean and these are not explained thoroughly. If this variance is to be expected, is it likely that the sample set n of 7 is too low? Or is the data heteroscedastic? In my view, the observations made on data presented in Figure 1e are not sufficiently explained. Discussion section on this aspect is a rehash of the content in the Results section. Read trimming is often a lower time-cost step compared to the variant calling step. It would benefit the reader (and the authors) greatly if there was a more detailed explanation why this is an important decision to make, which this study is aimed to inform us better for. Data redundancy and potential loss of raw data (if only single copy retained) appear to be valid reasons on the surface, a more complete justification is need in my view. Review of prior literature work can be more exhaustive.  I was unable to access or review the Supplementary information, so it has not been included in my review. Please update in revised version  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",606,0,1,0.7883,0.0970054945,0.9262039661,96,35.37,0.2663,f1000,0.0108695652173913,5,4,4,5,partially factual,5,4,75,polite,5,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,5,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
118,Pavel-Klinov,Module Extraction for Efficient Object Query over Ontologies with Large ABoxes,"The extraction of logically-independent fragments out of an ontology ABox can be useful\nfor solving the tractability problem of querying ontologies with large ABoxes. In this\npaper, we propose a formal definition of an ABox module, such that it guarantees complete\npreservation of facts about a given set of individuals, and thus can be reasoned\nindependently w.r.t. the ontology TBox. With ABox modules of this type, isolated or\ndistributed (parallel) ABox reasoning becomes feasible, and more efficient data retrieval\nfrom ontology ABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for SHIQ ontologies. Evaluation of the\nmodule approximation on different types of ontologies shows that, on average, extracted\nABox modules are significantly smaller than the entire ABox, and the time for ontology\nreasoning based on ABox modules can be improved significantly.","In this paper the authors define the notion of ABox modules, i.e. fragments of the ABox which capture entailments for the given individual w.r.t. the concept names and roles occurring in the ontology. In addition to the generic definition, the authors propose a specialization, called Exact Abox module, and proceed to investigating how (approximations of) such modules can be computed efficiently for SHIQ ontologies. The paper ends with a fairly extensive evaluation on several hand-picked ontologies showing that most of ABox modules are small and thus reasoning over them is much faster than over the entire ABox. ===Significance and novelty=== I believe the paper addresses an important topic since the existing notions of logic-based modules are defined w.r.t. a fixed signature whereas it is often important to capture a certain class of entailments over the signature which is not known in advance (e.g., all class and property assertions for an individual). However, it must be noted that the proposed modules would only preserve atomic entailments, e.g., atomic concept assertions, and do not guarantee that answering complex queries, e.g., DL queries for arbitrary concept expressions, over the module will return the same results. It is also not clear how (or if) the proposed modularization will help evaluating conjunctive queries which may return individuals from multiple modules. Remark 3.2 says (correctly) that answering DL queries can be reduced to instance retrieval for a fresh concept name but this would probably require to recompute existing modules (since the TBox has changed). This is obviously undesirable. I believe the paper adequately discusses related work.  ===Contributions and technical quality=== I'll comment separately on each individual contribution. The notion of ABox module. This notion simply provides conditions which a fragment of the ABox must meet in order to be called ""ABox module"". The authors correctly note that it does not prevent the module from containing superfluous parts. The notion of exact ABox module. This is where things get interesting, this notion basically says that the ABox module is a union over all justifications of atomic ABox facts about the individual. On the one hand, this makes sense because it trivially implies that any exact module is a module. On the other hand, this definition shouldn't be directly used for extracting modules because just one justification per assertion would be enough. BTW, the proposed definition guarantees the property which is called ""depletedness"" for locality-based modules: the ontology minus the module entails nothing about the individual. It'd be good to mention it.   It is unfortunate that the authors decided to jump directly to the extraction aspects and did not spend any time discussing the properties of their modules, e.g. whether there're counterparts of such properties of locality-based modules as robustness under signature restrictions, self-containedness, etc. (see [1] and [2]). It'd be good to fully understand what the modules are before starting to extract them.  Extracting exact ABox modules. This is the most problematic section of the paper, which suffers from imprecise notions, statements, and lack of proofs. Here are the main issues: 1) Page 11: the whole notion of ""class term behind a's assertions..."" is totally undefined and very confusing. It's rather unfortunate because the authors seem to build the extraction methodology on it (which culminates with the condition (3) which is then referenced in many other places). Both this notion and the condition (3) must be made proper (formal) definitions. Then the statement that the condition (3) is necessary and sufficient for entailing a concept assertion should be formulated as a proposition. 2) Page 12: Nothing is proved (or even formulated) about the extraction procedure shown at the top of the page. This is actually one of the central contributions of the paper: the algorithm for extracting exact ABox modules. It has to be shown that 1) it is correct, i.e. it selects all and only module-essential assertions, 2) it terminates (this is rather easy), and 3) what it's complexity is (apparently as hard as reasoning for Exptime-logics).  Extracting approximate ABox modules. The authors propose a syntactic check to decide if an axiom can be potentially relevant for individual entailments. Unfortunately it's rather hard to understand until (3) is made clear (because the syntactic form essentially approximates (3)).  Evaluation. The performed evaluation is pretty strong and shows several important results, e.g., i) approximate ABox module extraction is fast (Table 1) and ii) approximate ABox modules are generally small (Table 2). But some issues need to be fixed: 1) It's not described how the TBox of DBPedia ontologies was generated (page 19). Apparently some complex class expression have been auto-generated but the methodology isn't explained. 2) According to which principles were these ontologies selected? What makes them representative or interesting (other than large ABoxes which are sometimes synthetic)? There some others ontologies with large ABoxes, e.g., the IMDB ontology. 3) I wonder if any of the ontologies contain transitive roles (and assertions for them). My understanding is that transitive roles could be one of the main difficulties because they can blow the property module for ""a"" (by including role assertions for other individuals). If not, this is a weakness. 4) It's unclear how the time spent on module extraction for a single individual was measured, e.g., were all extractions done independently or was the whole ABox modularized in one go? ===Presentation=== While most of the prose is OK, the paper suffers from various imprecise statements and confusing/incoherent use of terminology. Here are some of the issues: * p2: ""... up to exponential worst-case complexity..."", actually it's N2ExpTime for SROIQ. * p2: "".. a setting of semantic webs"" -> ""the Semantic Web setting"". * p2: One has to be precise when talking about the closure of logical implications (e.g., does it include concept assertions for complex concepts of arbitrary length?). * Definition 2.3 isn't quite a definition. T and A have to be defined precisely as sets of axioms of a specific form. * Definition 2.5: ""Logic Entailment"" -> ""Logical Entailment""? * p6, top: The statement that all reasoners implement (hyper)-tableau isn't quite true. Even for expressive DLs, e.g. Horn-SHIQ, there're other methods such as consequence-based reasoning. * Definition 2.9: ""to be"" is missing * Definition 3.4: need to make clear that Just(alpha, K) here means *some* justification, not any specific justification of alpha. * p8 and elsewhere: It'd be considerably better to define equality-free ontologies syntactically. I.e., if I have an ontology, how do I know which extraction procedure should I run, the one which accounts for individual equality or the simpler one? * p11: what is meant by ""decidable R-neighbors""? The same goes for "" its subsumer is undecidable."" on page 13. * Proposition 4.3: it'd be better if this fact was proved without explicitly referring to the tableau's completion rules. It's a fact about the logic, not any particular calculus. * p14: Essentially the same comment applies to the Module Extraction with Equality section. Explicitly referring to particular tableau rules brings nothing but trouble. Also the statement that equality requires reasoning to detect it is strange since reasoning is required anyway to compute exact ABox modules. * Proposition 5.1: are we talking about asserted or inferred R-successors? Again, what it ""equality-free ABox"" exactly? * Definition 5.1 seems to define potential equivalents in terms of potential equivalents. Until it's fixed I find it nearly impossible to understand Proposition 5.2, which is the key statement about module extraction from ABoxes with equality. Perhaps it would help to prove the simpler Proposition 5.1 first. * Table 1 and 2: better to explain columns in the captions rather than in text on some other page. * p19, end: what is ""entities"" here? Definition 2.7 says that signature is always a set of individuals. Can entities refer to something else? * p22: It seems that this optimization will lead to the modules not being exact ABox modules any more (since not all module-essential axioms will be included). Better to say it explicitly. ===Summary=== In general, this is a potentially useful paper which presents module notions and extraction methods which can prove useful for applications dealing with instance checking w.r.t. large ABoxes. Eventually I'd like it to be published but I believe it needs another round of reviewing after some key notions and conditions (condition (3), most prominently) are made precise. Also, it can't be published until the correctness of the main extraction procedure (for exact modules) has been formally proved and reviewed (since all subsequent results, e.g. approximations, are based on it). [1] Bernardo Cuenca Grau, Ian Horrocks, Yevgeny Kazakov, Ulrike Sattler: Modular Reuse of Ontologies: Theory and Practice. J. Artif. Intell. Res. (JAIR) 31: 273-318 (2008) [2] Ulrike Sattler, Thomas Schneider, Michael Zakharyaschev: Which Kind of Module Should I Extract? Description Logics 2009",1453,5,9,0.8006000000000001,0.0900611326,0.9462785125,75,48.5,0.0501,semanticweb,0.0,4,5,5,4,factual,4,5,89,neutral,4,neutral,4,none,5,4,5,5,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,neutral,5,none,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
159,Reviewer-aUSe,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The authors argue that Adam's internal parameters should be reset with each iteration. The authors demonstrate the effectiveness of this approach in the Atari domain.  - Results are convincing for Rainbow.
- Novelty is very low, but potential impact is high, if the result generalizes, there is little reason not to use this method in every DQN-style RL algorithm.   - As mentioned by the authors, novelty is low compared to Bengio et al. 
- Results are only shown on Rainbow and do not appear to work for SAC (with reason) -- but does raise the question if the method is effective for other RL methods. 
- There is limited insight. Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. The fact that not resetting is seemingly better at low values of K suggests that not resetting can provide a reasonable initialization for the parameters of Adam.

Minor
- The y-axis is unlabelled in several figures.  As mentioned in weaknesses:
- Does this result generalize to other methods besides Rainbow? Such as DQN or more modern deep RL methods.
- Can the authors show that initializing Adam's parameters with 0 is better than using the parameters from the previous iteration in a more concrete way? Such as examining the behavior of the actual values. 
 No concerns. ",240,0,1,0.8148000000000001,0.1232647908,0.8709654808,215,48.3728,0.1041,neurips,0.0396039603960396,3,4,2,2,partially factual,3,4,50,polite,3,negative,3,moderate,4,4,4,4,factual,4,4,82,polite,5,neutral,5,moderate,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
159,Reviewer-59Jp,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper addresses the issue of using modern optimizers, such as Adam, which maintain internal parameters that are updated over time, potentially contaminating the optimization process. To mitigate this effect, the paper proposes a simple strategy of resetting the internal parameters of the optimizer at the start of each iteration. Empirical investigations using different optimizers and the Rainbow algorithm show that this modification enhances the performance of deep reinforcement learning on the Atari benchmark. ### Writing
The authors effectively communicate their ideas and concepts, ensuring clarity and coherence throughout the paper. The logical structure and well-reasoned arguments contribute to the overall quality of the essay. The article excels in providing the reader with a clear understanding of the problem's context and significance. By effectively conveying the goals and challenges of the study, the authors enhance the reader's comprehension of the subsequent experiments. Overall, the writing is of high quality, facilitating a smooth and engaging reading experience.

### Method
The paper presents an easy-to-use approach by introducing a method that is not only easy to implement, but also easy to apply, which enhances the potential adoption and practicality of the proposed approach.This user-friendly feature makes the method highly accessible and beneficial to researchers and practitioners in various fields.
The used code bases and hyperparameters are provided, allowing the results to be reproduced. While I appreciate the proposed method's ease of use, I believe that the authors could have conducted a more comprehensive and statistically rigorous analysis of their approach, considering its simplicity.
One notable limitation of the paper is the absence of confidence interval plots and statistical analysis, which could have been derived from \[1\], to enhance the clarity and precision of the findings. Incorporating these elements would have allowed readers to better understand the level of uncertainty associated with the reported results, thus bolstering the overall robustness of the study.
Furthermore, the authors only rely on a single seed for the initial analysis, without providing a compelling rationale for this choice. Although using a single seed can streamline the experimental process, it diminishes the validity of the findings by disregarding potential result variations arising from multiple seeds. A more thorough explanation or a comparison of outcomes based on different seeds would have added value to the introduction, ensuring a more comprehensive analysis.
I appreciate that the authors included continuous control tasks in their study; however, these tasks are not thoroughly explored. While the authors provide hypotheses to explain the unexpected results, a deeper analysis would have been expected.

In line 293, the authors reference a follow-up paper on resetting approaches but fail to cite the original work \[2\], which states in the section ""What and how to reset"" that resetting the optimizer has almost no significant impact due to quick updates of the moments. This contradicts the findings of this work.
Which brings me to the conclusion that I believe that the paper shows promise and the authors have taken a positive direction. However, in its current form, the paper falls short of being acceptable. It is essential to include comparisons to other baselines, such as \[2\], to provide a more thorough understanding of the opportunities and limitations, and to gain a clearer understanding of the internal effects in order to explain the aforementioned points.

### Minor
- The protocol for the random resets is not easy to understand and should be specified more clearly 

\[1\] Agarwal, Rishabh, et al. ""Deep reinforcement learning at the edge of the statistical precipice."" Advances in neural information processing systems 34 (2021): 29304-29320.
\[2\] Nikishin, Evgenii, et al. ""The primacy bias in deep reinforcement learning."" International Conference on Machine Learning. PMLR, 2022.
 - How does this method compare to other resetting approaches in terms of effectiveness?
- What is the level of statistical significance observed in the results?
- Why is resetting not effective for continuous control tasks?
- Are there any experiments demonstrating the impact of contamination on the tasks discussed in this paper?
- What are the consequences of reducing the frequency of optimizer resets beyond K=8000?
- How can an optimal value for $K$ be determined?
- Which ADAM/optimizer parameters are relevant when performing resets, i.e. have an effect when reset?
- Are the observed effects still present when modifying the ADAM/optimizer hyperparameters?
- Do different loss functions used in various DQN versions (e.g., MSE, Huber, Quantile) exhibit similar behaviors? The authors have made some effort to address the limitations; however, it is crucial for them to conduct a more comprehensive investigation into these limitations, as mentioned in the ""Weakness"" section.",760,6,2,0.7905000000000001,0.1027398502,0.92895329,215,29.4355,0.8077000000000001,neurips,0.0106382978723403,5,5,5,5,factual,4,4,90,polite,5,neutral,5,low,5,5,4,5,5,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,2.0,factual,4.0,4.0,70.0,polite,5.0,positive,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
159,Reviewer-5d1P,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","This paper questions the standard use of Adam-type optimizers in deep RL. The paper argues that solution methods in deep RL are best thought of as solving a sequence of optimization problems. And that the standard use of optimizers leads to ""contamination"" of the optimizer's internal parameters. The paper then proposes to reset the optimizer's internal parameters to fix this ""contamination."" Finally, the experiments on Atari show that resetting the optimizer's internal parameters leads to significant performance improvement. The main strength of the paper is that the key idea of the paper, resetting optimizer parameters at the beginning of each iteration, is simple and effective. I liked the general theme of the paper, i.e., we need to understand better the tools we borrow from other fields. The paper is well-written and easy to understand, making it accessible to a wide audience. The experiments in section 4.3 are performed on 55 Atarti games with ten seeds each, which might mean the results are statistically significant. Resetting seems to be beneficial for multiple optimizers like RMSprop, Adam and Rectified Adam.  The paper has two major weaknesses:
1. The paper claims at many points that a ""contamination"" effect plagues RL (for example, lines 107-109) and that many updates are wasted to unlearn the effects of the previous iteration. However, the paper does not describe what exactly this ""contamination"" means, and neither does it show the presence of any ""contamination."" All the paper shows is that there is a performance boost when we reset the optimizer's internal parameters. This performance boost is not direct evidence of contamination from iteration to iteration.

However, this weakness can be easily overcome. The paper first needs to contain a definition of ""contamination,"" maybe the authors mean that the internal parameters($m$ and $v$) are too far away from their true values at the beginning of each iteration. One way to measure this difference could be to measure the cosine similarity between the current value of $m$ and the true value of $m$. The true value can be measured by taking the gradient of all the samples in the buffer and taking steps using that gradient. A large difference between the true and current value of $m$ would mean contamination. The paper also suggests that this contamination is particularly large at the beginning of each iteration compared to a random time in the learning process. Again, this can be easily shown by showing that the difference in true $m$ and current $m$ is larger at the beginning of the iteration compared to any random time in the learning process. 

2. None of the results in the paper except the ones in section 4.3 are statistically significant. The paper only shows results for a single random seed. I should note that the authors are aware of this weakness (line 138). The best way to look at the current experiments in Sections 4.1 and 4.2 is that they are used to tune hyper-parameters for the experiments in Section 4.3. The authors might be limited in their computational resources, but in that case, it is better to present statistically significant results in smaller environments like MinAtar\[1\] than unreplicable results in a big environment. 

Other than these two main problems, there are a few other minor issues in the paper.
1. The update equations for Adam in Section 3 are wrong. Instead of using $m$ and $v$ for the final update, new variables $\hat{m}$ and $\hat{v}$ are used. See to the original Adam paper for the correct equations.  
2. Line 176 says that $K=1$ corresponds to vanilla gradient descent. But, that is not true. For $K=1$, the update is similar to the Rprop optimizer, not SGD. For $K=1$, the update only takes into account the sign of the partial derivative but not its magnitude. 
3. The value of $K$ is not properly tuned. In Figure 4, the difference between $K=1000$ and $K=500$ is insignificant. So, the optimal value of $K$ could be smaller. I suggest the authors also try smaller values of K, like 250, 125, etc. 

I like the ideas presented in the paper. However, I can not recommend accepting the paper in its current form in light of these weaknesses. 

\[1\] Young, K., & Tian, T. (2019). Minatar: An atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176.
 What would be a good definition of ""contamination""? No confidence intervals are reported for any experiment in the paper. I recommend the authors report the 95% bootstrapped confidence intervals for their results. \[2\] and \[3\] provide good guidelines for properly reporting experimental results in deep RL.


\[2\] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34, 29304-29320.
\[3\] Patterson, A., Neumann, S., White, M., & White, A. (2023). Empirical Design in Reinforcement Learning. arXiv preprint arXiv:2304.01315.

EDIT:

I have updated my score based on the new results provided by the authors.",832,9,10,0.7234,0.1063169287,0.8891925216000001,215,50.6545,0.1044,neurips,0.0113636363636363,5,5,5,5,factual,4,4,95,polite,4,neutral,5,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,5.0,positive,5.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,5,4,5,partially factual,4,4,88,polite,5,neutral,5,low
159,Reviewer-Ks5C,Resetting the Optimizer in Deep RL: An Empirical Study,"We focus on the task of approximating the optimal value function in deep reinforcement learning. This iterative process is comprised of solving a sequence of optimization problems where the loss function changes per iteration. The common approach to solving this sequence of problems is to employ modern variants of the stochastic gradient descent algorithm such as Adam. These optimizers maintain their own internal parameters such as estimates of the first-order and the second-order moments of the gradient, and update them over time. Therefore, information obtained in previous iterations is used to solve the optimization problem in the current iteration. We demonstrate that this can contaminate the moment estimates because the optimization landscape can change arbitrarily from one iteration to the next one. To hedge against this negative effect, a simple idea is to reset the internal parameters of the optimizer when starting a new iteration. We empirically investigate this resetting idea by employing various optimizers in conjunction with the Rainbow algorithm. We demonstrate that this simple modification significantly improves the performance of deep RL on the Atari benchmark.","The paper studies optimization in value-based deep reinforcement learning. The key insight is that when using target networks for action-value function training, changes in the target parameters yield a change in the optimization problem the online parameters are solving. Because of that, the authors argue that preserving the adaptive optimizer statistics (e.g. of Adam) might or might not be desirable. The paper then studies the effect of resetting the optimizer state after (hard) target updates mostly using the Rainbow algorithm on Atari games as a testbed yielding a slight positive aggregate improvement. The main strength of the paper is the simplicity of the contribution; the paper is well-written and easy to follow, and the method is motivated and described well. The experimental protocol is solid: it uses the full set of 55 Atari games and a standard Rainbow implementation. The main weakness of the paper is the mixed empirical results. Granted, the median human-normalized performance improves from ~1.75 to ~2.25, however, per-game effects from resetting the optimizer are highly heterogeneous, yielding performance deterioration in ~14 environments. The soundness of the paper could have been higher if, at least, an explanation (supported by evidence) for the negative effects was given. Target network parameter updates indeed change the loss landscape that the online parameters are navigating. In addition to that, updating the replay buffer changes the distribution of inputs and hence the optimization problem for online parameters. Do you have ideas on how an optimizer could be changed to adapt to the input shifts? “We hypothesize that this can contaminate the internal parameters of the employed optimizer in situations where the optimization landscape of the previous iterations is quite different from the current iteration.” (L9) The reviewer didn’t find an empirical verification of this assumption.

Many deep RL algorithms use moving average target updates after each step instead of periodic hard updates. The authors demonstrate preliminary evidence that in soft actor-critic that uses such a practice, the optimizer resets do not improve the performance. Having said that, the reviewer appreciates the transparency about the negative results.

Again, one of the limitations is that in some environments resetting the optimizer yields negative results. It implies that a better alternative could be triggering the optimizer reset using a criterion (e.g. based on a measure of the loss landscape change / by performing a lookahead and assessing whether the reset was helpful)",397,0,0,0.7723,0.0420385675,0.8665425777,215,31.2684,0.2101,neurips,0.0505050505050505,2,3,2,3,factual,3,2,65,polite,4,negative,4,low,4,5,4,4,factual,4,4,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low
91,Reviewer-oZBs,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","The paper applies sharpness-aware minimization (SAM) to multi-task learning (MTL), to find task-based flat minima for improving generalization capability on all tasks.  The paper conducts comprehensive experiments on several benchmark datasets to evaluate the proposed method. - apply SAM to MTL is novel
- experimental results show that the proposed method can boost the performance of existing MTL methods on several benchmarks - concerns about **efficiency**: 
  - SAM is computationally expensive, doubling the computation cost compared with ERM/SGD. In Algorithm 1, each task requires computing the SAM gradient for shared/non-shared parameters. In total, the algorithm needs at least $2m$ gradient calculations, where $m$ is the number of tasks. Hence, the algorithm is computationally inefficient.
  - In experiments, there are no results (like training time) for comparing efficiency or training curve (performance w.r.t. training time).
  - this problem will be very serious when there are many tasks, e.g., the QM9 data set has 11 tasks.
  - some suggestions for mitigating this issue: use efficient variants of SAM, e.g., 
    - AE-SAM, An Adaptive Policy to Employ Sharpness-Aware Minimization, ICLR 2023
    - ESAM, Efficient Sharpness-aware Minimization for Improved Training of Neural Networks, ICLR 2022
- Eq(4) in Theorem 1, $\[...\]\_{i=1}^m \leq \max \[...\]\_{i=1}^m$ means ?
- Theorem 1 can be directly obtained from Theorem 1 of Foret et al. (2021): decomposing the parameters into two parts and using different $\rho$'s.
- in ""Update the shared part"" (P5), ""However, a direct gradient aggregation  ... can be negatively affected by the gradient cancelation or conflict because it aims to combine many individual elements with different objectives"", **a direct gradient aggregation means?** not clear
- Why the proposed aggregation in Section 4.4 is better than the above ""direct gradient aggregation""?
- In the Conclusion Section, ""proving that they can help enhance previous works both theoretically,"" which theorem(s)?
- how to calculate the entropy in Figure 2, note that the entropy in the figure has negative values.
- Figure 1, the  ""2-task problem"", where is the definition? see the questions in weakness part.",336,1,1,0.7858,-0.0031249999999999,0.9255634546,52,38.1278,0.0945,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
91,Reviewer-CubV,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This work suggests a new framework to train multi-task learning (MTL) models that try to find a 'flat region' in the loss landscape. This is based on Sharpness-aware Minimization (SAM) by Foret et al. (2021), which was shown to reduce overfitting, and therefore could increase generalization performance across MTL tasks. The algorithm is based on solving a min-max optimization problem using Taylor expansion and gradient aggregation. Theorem establishes a generalization error bound. Experimental results on MTL computer vision tasks are provided. This is a well-rounded paper. It's an extension of SAM by Foret et al. (2021), but the application of SAM to MTL is well-motivated. The algorithm is simple and easy to understand, and the derivation in sections 4.3-4.4 is clear. Authors present both theoretical and experimental analysis. I also appreciate that the authors uploaded their code for reproducibility, and provided detailed explanation for their experimental setup as well as interpretation of the results. Please see questions below. 1. The paper lacks a critical discussion on the limitations of this method. For example, is the method computationally efficient?

2. Are there standard deviations or statistical test results reported for Tables 2-4? It's not clear how significant some of these improvements are, e.g. 75.13 vs 75.77 in Table 3 PCGrad.",209,2,5,0.8136,0.1113131313,0.9075129032,52,43.6244,0.8355,iclr,0.0235294117647059,3,3,2,3,factual,3,3,65,polite,3,neutral,3,moderate,5,5,4,5,5,5,5,90,5,5,5,5,none,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
91,Reviewer-nSBj,Improving Multi-task Learning via Seeking Task-based Flat Regions,"Multi-Task Learning (MTL) is a widely used and powerful learning paradigm for training deep neural networks that allows learning more than one objective by a single backbone. Compared to training tasks separately, MTL significantly reduces computational costs, improves data efficiency, and potentially enhances model performance by leveraging knowledge across tasks. Hence, it has been adopted in a variety of applications, ranging from computer vision to natural language processing and speech recognition. Among them, there is an emerging line of work in MTL that focuses on manipulating the task gradient to derive an ultimate gradient descent direction to benefit all tasks. Despite achieving impressive results on many benchmarks, directly applying these approaches without using appropriate regularization techniques might lead to suboptimal solutions to real-world problems. In particular, standard training that minimizes the empirical loss on the training data can easily suffer from overfitting to low-resource tasks or be spoiled by noisy-labeled ones, which can cause negative transfer between tasks and overall performance drop. To alleviate such problems, we propose to leverage a recently introduced training method, named Sharpness-aware Minimization, which can enhance model generalization ability on single-task learning. Accordingly, we present a novel MTL training methodology, encouraging the model to find task-based flat minima for coherently improving its generalization capability on all tasks. Finally, we conduct comprehensive experiments on a variety of applications to demonstrate the merit of our proposed approach to existing gradient-based MTL methods, as suggested by our developed theory.  Our training code is available at https://github.com/anonymous-user00/FS-MTL.","This paper combines sharpness-aware minimization (SAM) and existing gradient-based multitask learning algorithms to improve empirical generalization performance of MTL.  The main novelty is that the authors propose to decompose the SAM gradient $g^\textrm{SAM}$ into the task-loss minimizing direction, $g^\textrm{loss}$ (obtained by directly taking the directive w.r.t. task loss), and the flat-region seeking direction, $g^\textrm{flat}\coloneqq g^\textrm{SAM}-g^\textrm{loss}$, and perform gradient aggregation on both separately.  The proposed method, i.e., running existing gradient-based MTL algorithms by aggregating $g^\textrm{SAM}$ and $g^\textrm{loss}$ separately, is evaluated on a set of datasets, on average demonstrating improved performance v.s. just using $g^\textrm{loss}$ for parameter update. 1. The paper is well-motivated and presented.  Although I do find frequent grammatical errors, the paper is easy to read and understand.
2. It is an interesting observation that decomposing $g^\textrm{SAM}$ into and $g^\textrm{loss}$ and $g^\textrm{flat}$ and aggregating them separately is crucial for the success of the proposed method.  But this decomposition is—in the way it is currently presented—purely heuristic.  I would have liked more analyses on this beyond the ablation study on page 9. 1. Second point in strengths.

2. The proofs and theorems—which the authors claim to be a major contribution of the present work and on which the proposed algorithm is supposedly based—are poorly presented.  In turn, without which, the proposed approach is largely heuristic and lack theoretical support (excluding results that have been established in prior work, i.e., the constituent component of SAM and gradient-based MTL methods).

    - The ""mild assumptions"" are not clearly stated nor justified.  E.g., theorem 2 used the assumption that the loss function is bounded by $L$, which is not mentioned anywhere except in the proof.  Also, please justify and elaborate on the assumption that ""that adding Gaussian perturbation will raise the test error"": is it required for all $\theta$, or local minima?  It would be best if the assumptions are listed explicitly.

    - The conclusion of theorem 3 looks wrong.  First of all, in the proof, the induction is incorrectly applied—the $\xi$ cannot alter between cases.  The $\log1/\delta$ term in $f^i$ should be $\log m/\delta$.  And, does the conclusion not follow theorem 2 directly via a simple union bound?

    - The outer $\max _ {\\|\epsilon_\textrm{sh}\\|<\rho_\textrm{sh}}$ in the statement of Theorem 1 and 3 does not make sense to me.  The max is taken over a vector of $m$ dimensions.  ~~Is the max coordinate-wise?  If so, it should go inside the square bracket.  If not,~~ is the max well-defined?  Or, how is the total order of the vector space defined?

3. Regardless of the above potential issue with the theorem statement, I fail to see the connection between Theorem 1 (or its complete version 3) and the approach in section 4.3, i.e., the idea that ""the worst-case shared perturbation $\epsilon_\mathrm{sh}$ is commonly learned for all tasks"".  Specifically, how is computing the worst-case perturbation on each task separately and then aggregate the gradients $\\{g^{i,\textrm{SAM}}_\textrm{sh}\\} _ {i\in m}$ related to the idea above?

4. As mentioend in point 1 of strengths, there are some grammatical issues and weird word choice that may lead to confusions.  E.g., what is the ""**ultimate** gradient descent direction"" (in the abstract)?  Also, ""is the compliment set"" --> ""is the complement set"". See weaknesses.",530,0,8,0.7692,0.0861568987,0.908143878,52,45.0589,0.2111,iclr,0.0,4,4,5,3,factual,4,5,90,polite,5,negative,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,1.0,4.0,4.0,4.0,partially factual,3.0,2.0,60.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,neutral,5,neutral,5,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,5,low
127,Yankai-Xia,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review addresses the increasingly relevant topic of the neurotoxic potential of nanoplastics (NPs) in the context of escalating plastic pollution, effectively summarizing key findings from the literature with an emphasis on the various exposure routes and associated risks. However, the manuscript would benefit from a more comprehensive synthesis of the existing literature, particularly in addressing the inconsistencies and gaps in current research, while also providing a clearer articulation of the limitations of current research methodologies and offering suggestions for future studies. Additionally, a discussion of the broader implications for public health and potential regulatory frameworks would strengthen the manuscript's contribution to the field. Overall, the review could be further improved by deepening the analysis of existing studies and providing a more critical perspective on the current state of knowledge. I recommend that the authors consider resubmitting after making significant improvements. Major comments While the review discusses various detection and quantification methods for NPs, a more detailed critique of the limitations of these methodologies is needed. This should include an examination of the challenges related to detecting NPs in environmental samples versus laboratory conditions, as well as the implications these limitations have for interpreting research findings. The manuscript needs a more critical analysis of key research gaps, especially concerning the inconsistencies in findings related to the mechanisms of NP-induced neurotoxicity. Strengthening this section with a more detailed comparison of the outcomes across different experimental models and conditions would greatly enhance the review's contribution. The discussion on the mechanisms of NP-induced neurotoxicity is crucial. For instance, exploring the specific biochemical pathways through which NPs interact with cellular components at a molecular level would provide a more comprehensive understanding.  The role of protein corona formation in neurotoxicity, mentioned towards the end, should be integrated earlier in the manuscript to establish a clear connection between NP exposure and neurodegenerative diseases. While the manuscript covers many trending topics, it often treats them in isolation, which leads to a lack of coherence. An integrated approach that links these topics and demonstrates their interconnections would greatly improve the flow and continuity of the review. Minor comments The manuscript relies heavily on older studies, with relatively few references from the past three years. Incorporating more recent studies will ensure that the review reflects the current state of research and provides a comprehensive overview of the field. In some sections, particularly those discussing in vivo studies, the outcomes are not always clearly connected to the broader implications for neurotoxicity. It would be helpful to more explicitly link the results of these studies to the potential mechanisms of NP-induced neurotoxicity and their relevance to human health. The conclusion primarily restates the findings discussed throughout the review but does not provide a comprehensive synthesis of the key takeaways. The summary of neurotoxicity of NPs in different models presented in Table 1 is not comprehensive and should be thoroughly enumerated. The language of the manuscript should be polished.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Yes  Is the review written in accessible language? Partly  Are the conclusions drawn appropriate in the context of the current research literature? Partly",605,0,1,0.8004,0.1484375,0.8763298392000001,49,13.99,0.1633,f1000,0.0,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,4,3,4,factual,4,4,75,polite,4,neutral,4,low,2.0,4.0,3.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,4,4,3,4,factual,4,4,80,polite,4,neutral,4,low,3,3,3,4,partially factual,4,4,75,polite,4,neutral,4,low
127,Amitava-Mukherjee,Neurotoxicity of nanoplastics: A review,"With the increase in plastic waste in the environment, it is undeniable that humans and most organisms are exposed to plastic particles of various sizes, including nanoplastics (NPs). Humans are at risk owing to various routes of entry, including ingestion, inhalation, and dermal contact. While the toxicity of NPs is still debatable due to the scarcity of resources and research, most studies have concluded that NPs may exert toxicity, which exacerbates their neurotoxicity potential. Earlier studies concluded that NPs can cause oxidative stress, which results in apoptosis of neuronal cells. Some studies have shown that NPs can affect fundamental cell functions by inducing physical stress through deposition. Furthermore, studies on in vivo models exposed to NPs have demonstrated behavioral changes that are presumably due to alterations in acetylcholinesterase activity and neurotransmitter levels. This review discusses studies conducted on the neurotoxic potential of NPs and their effects, which are dependent on several parameters, including size and type of NPs, exposure concentration, duration, and various models at risk of NP exposure. Furthermore, speculations on how NPs are related to neurotoxicity are also discussed.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The review presents an exhaustive coverage of the neurotoxic effects of nanoplastics. The authors have done a commendable job of collecting literature and making a balanced presentation. However, I suggest the following points. 1. Introduction: The introduction is rather about the issues with plastic pollution, kindly introduce the importance and relevance of the neurotoxicity of the plastics here and also add a brief outline of the topics covered in the Review. Given that already a sizeable number of reviews are available on the topic of plastic pollution, please make this part brief and bring out the title of the work, ""neurotoxicity"" here. 2. Under Nanopalstics please revise the discussion on sources of the NPs relevant to human uptake and toxicity. Please connect this part with the main thread of the review. This is also much discussed in the literature already, and so with appropriate citations, the authors can shorten the description here. In the detection and quantification clearly distinguish and discuss the in vitro and in vivo detection and challenges associated briefly. The differences between MPs and Nps is a misfit in the review and out of context, in the introduction section itself one or two lines can be added with specific references for interested readers. 3. In the ""potential routes of NP exposure to Humans"" please avoid adding mechanisms of interaction/effects in this section, stick to the sources. Intracellular fate and bio-corona again may not fit well as a separate section, please integrate them briefly into the section on ""uptake"" and make their relevance clear for neurotoxicity effects. 4. Instead of sensitivity of the brain to oxidative stress discuss the various modes of action of the plastic particles mentioning why ROS is considered predominant one.. add relevance to plastic particles here briefly explain the effects of multiple chemical types, and possibly leaching of additives briefly. 5. Looking at the length of the review roughly 30% is covered on neurotoxicity, please elaborate on mechanisms of action, effects of plastic types, and size-based effects of nano plastics with specifics on neurotoxicity. I assume the literature is replete with studies with polystyrene NPs but please see whether the effects of other plastic types can be added and the effects of weathered or environment-derived ones. 6. Add a section on current gaps and challenges in these studies. 7. Please add a section on methods of review, year range selected, inclusion/exclusion criteria adopted search engines used, and so on. Please add this after the introduction section. This is an important miss in the article.  Is the topic of the review discussed comprehensively in the context of the current literature? Partly  Are all factual statements correct and adequately supported by citations? Partly  Is the review written in accessible language? Yes  Are the conclusions drawn appropriate in the context of the current research literature? Partly",538,0,8,0.7593000000000001,0.1164814815,0.9151369929,136,32.73,0.4415,f1000,0.010752688172043,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,4,2,4,partially factual,4,4,65,polite,4,neutral,3,low,2.0,5.0,3.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,5,4,4,5,partially factual,4,4,80,polite,4,positive,4,low,4,5,3,4,partially factual,4,3,78,polite,4,neutral,4,low
59,Reviewer-4HBq,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","With the rapid development in Large Language Models (LLMs), business owners are increasingly exploring the customization of pre-trained LLMs through APIs provided by LLM owners or cloud servers. However, this process carries substantial risks of model misuse, making the protection of copyrights for these customized models a pressing issue. Currently, the majority of LLM watermarking research concentrates on small-scale models for specific tasks or pre-trained models, and these methods unsuitable for customized LLMs. The application scenarios of customized LLMs present new challenges for watermarking techniques: they must not degrade model performance while maintaining watermark uniqueness and imperceptibility. Most crucially, since the watermarking embedding process can't access the full model parameters, the model remains a black box for those embedding the watermark. To address these challenges, the authors propose an efficient and robust watermarking embedding method tailored for customized LLMs. By designing two types of backdoor data paradigms with triggers in the instruction and input and mixing them with the normal training data during the fine-tuning process, the model can learn unique knowledge related to watermarking. Owners can then verify their ownership by guiding the model to produce specific outputs using a unique trigger. Furthermore, the authors ensure the effectiveness of this method through theoretical analysis and experimental verification. 1. The article is well-structured, starting with a thorough discussion on the shortcomings of naive backdoor-type watermarking methods before delving into their novel DOUBLE-I WATERMARKING FRAMEWORK. This logical progression effectively addresses the challenges initially posed.
2. The authors introduce a BACKDOOR DATA PARADIGM that aptly fulfills the requirements for Uniqueness and Imperceptibility in watermark embedding. The overall problem is framed as a judgment question, further enhancing the method's Uniqueness and Efficiency.
3. The paper features extensive experiments that convincingly validate the effectiveness of the proposed method. Beyond this, the authors conduct a multifaceted set of tests, including a non-harmful test to ensure that the watermark embedding does not significantly degrade model performance, robustness tests against second-time fine-tuning and model quantization, and an ablation study concerning the reference set to further substantiate the rationality of their backdoor data framework design. 1. As pointed out by the authors in section 3.3.1 ""TRIGGER IN 'INPUT' KEY,"" decorations can utilize specific keywords or phrases that are rare in regular instructions. Such rarity, however, could potentially be a drawback for these types of watermarking methods. Given that the target environment is cloud-based LLMs, providers could preprocess user inputs to filter out these decorations and triggers, thereby causing erroneous verifications. The design of triggers, in this context, warrants a more nuanced discussion by the authors.

2. In section 3.3.3 ""THE MIX-UP OF MULTIPLE TYPES,"" the authors mention that ""it is possible to embed multiple Double-I watermarks in a model, which theoretically has the potential to enhance the robustness of our watermarking technique."" The theoretical substantiation for this claim is lacking, especially considering that multiple types of watermarks could interact and affect each other. More theoretical proofs or appropriate literature citations are needed to validate this assertion. See Weaknesses.",500,0,6,0.8309000000000001,0.0973225559,0.8686554432,48,21.2268,0.157,iclr,0.0,1,4,3,0,partially factual,3,1,68,polite,4,negative,4,moderate,4,5,4,5,factual,5,5,88,polite,5,positive,5,none,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
59,Reviewer-rzXY,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","The paper proposes a novel watermarking method to safeguard the copyrights of customized Large Language Models (LLMs) during fine-tuning. Addressing challenges such as watermark uniqueness, imperceptibility, and robustness against removal attacks, the ""Double-I watermark"" method introduces two types of backdoor data paradigms. These paradigms effectively embed watermarking information into the model, ensuring the watermark's presence is imperceptible yet detectable. The method is thoroughly evaluated, demonstrating its effectiveness in maintaining the model’s performance, robustness against attacks, and overall practical applicability for protecting the intellectual property of customized LLMs in various applications. Here are some potential strengths discussed in the paper: 
1. Robustness Against Removal Attacks: The proposed ""Double-I watermark"" method has been designed to be robust against attacks aimed at removing the watermark, ensuring that copyright protection remains intact even under adversarial conditions.
2. Imperceptibility and Uniqueness: The watermark introduced by the method is imperceptible, meaning it doesn’t affect the model's normal functionality or output, and it is unique, allowing for clear identification and copyright protection of the customized LLMs.
3. Comprehensive Evaluation: The paper includes a thorough evaluation of the proposed method, assessing various aspects such as harmlessness, robustness, uniqueness, and efficiency, demonstrating the method’s practical viability and effectiveness in real-world scenarios. 1. Limited Exploration of Attacks: The paper primarily focuses on second-time fine-tuning and model quantization as watermark removal attacks. The exploration of other potential attacks,such as pruning, that might be used to remove or alter the watermark seems limited.

2. Dependency on Specific Paradigms: The watermarking method relies on specific paradigms for embedding the watermark, and its effectiveness might be influenced by the choice of these paradigms, limiting its flexibility and adaptability.

3. Uniqueness Challenges: The paper mentions challenges in ensuring the uniqueness of the watermark, particularly in distinguishing whether certain behaviors stem from the model’s inherent traits or the embedded watermark. 1. Regarding Model Manipulation:
Could you clarify the resilience of the watermarking method against potential manipulations, such as adding conditional statements in the code to filter or alter specific inputs, especially when there is knowledge of how the watermarking works?

2. Concerning Training Data and Time:
Could you provide more details on the amount of training data required and the duration needed to effectively watermark a model using your proposed method? Is there a significant amount of data and time needed for this process?

3. On the Necessity of Fine-Tuning:
Is it possible to implement the watermarking method without resorting to fine-tuning the model? How does the method ensure that the model remains general and unbiased, especially when the question-answer pairs used for watermarking are not as diverse as those in the original training set, such as OpenAI’s non-public dataset?",444,0,8,0.8097000000000001,0.1072108844,0.8502570391000001,48,5.7905,0.33,iclr,0.0,1,4,4,0,unfactual,3,1,64,polite,3,positive,3,high,5,5,5,5,5,5,5,95,5,5,5,5,3,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
59,Reviewer-4k11,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","This paper proposes a black box watermarking scheme for costomized LLM. In particular,  the authors propose to construct two sets of poisoned data to inject the watermark during the tuning, where the trigger set produces the wrong judge answer and the reference set produces the correct answer. Compared with the naive judge question based watermarking scheme, the authors propose to take spacial character patterns to trigger the wrong output to improve the uniqueness. 1. The design of reference set to complement the trigger set is interesting.
2.  The overall presentation is easy to follow. 1. Lack of teachnical contribution. This method is an improvement of the naive judge question based watermarking. The overall process is still naive, which lacks theoretical or technical contents.
2. Lack of introduction of related work. Various black box model watermarking schemes have been proposed recently, including LLM watermarking, while the most recent model watermarking scheme cited in this paper is published in 2019.
3. Since the authors mention several times regarding the efficiency, it should be evaluated to justify the advantage of the proposal. This is unfortunately not seen in the experiments. 
4. There is no quantitative comparison against the naive approaches or the existing black box LLM watermarking schemes. see weakness.",207,0,7,0.7345,-0.0369565217,0.7950327396,48,43.1339,0.0999,iclr,0.0,0,4,2,0,unfactual,2,0,44,polite,2,negative,3,high,4,3,3,4,partially factual,3,3,55,impolite,5,negative,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,3,4,70,neutral,5,negative,4,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
59,Reviewer-NrtU,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,"To support various applications, business owners often seek the customized models that are obtained by fine-tuning a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ""Double-I watermark"". Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both theoretical analysis and experimental verification.","This work presents a novel watermarking algorithm to secure the copyright of customized models that is finetuned by a third-party service provider. By injecting a trigger into the instruction and the input in training data, the users install a backdoor mechanism to the model, which can be detected during inference and verified by hypothesis testing to check the watermark. Experiments show that the approach satisfies the essential properties of the watermarking method. - The paper is well written and comprehensible, with nice formulation that is easy to understand.
- Innates difficulty of watermarking finetuned LLMs are discussed, which are important for building an algorithm.
- The algorithm is simple and effective, experimental results demonstrate its watermarking capability in five essential properties.
- Extensive experiments are conducted to study the effectiveness of the method in many practical usecases. - Related works should be discussed in more detail, there are many recent watermarking techniques for LLM in the literature.
- The strategy is applicable for instruction tuning only, whereas there are other ways to finetune LLM with a service provider, restricting the utility of the method in practice.
- The paper should briefly introduces Fisher’s exact test, show its results and how we accept or reject a hypothesis. For example, in Table 2, the distributions on trigger set and reference set of clean model finetuned with LORA are quite different. - Can we apply the proposed strategy to other tasks, for example question answering task, where the instruction is not presented?
- How do we conclude whether the model contains watermark from the distribution on trigger and reference set? What is the reasonable size of verification set?
- How does the performance change if we vary the ratio of trigger set in reference set in training data as well as verification data?",300,0,0,0.7971,0.2083333333,0.82766819,48,32.3061,0.0948,iclr,0.0,1,3,2,1,unfactual,3,2,38,polite,2,neutral,4,high,4,4,4,4,5,4,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,3,85,polite,5,positive,3,low
168,Reviewer-tePx,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","The paper at hand proposes a method for domain adaptation by including some labeled data from the target domain. A ""triplet alignment"" is introduce which aims for aligning feature distributions as well as minimizing classification error. + relevant problem - The paper is quite hard to read and understand. Figures are rather small. Honesty speaking Fig. 1 even confused me more than it helped me to understand the approach.
- Experimental results are hard to interpret and judge. If I read it correctly, the effect of data augmentation seems significant. When comparing without data augmentation  (ours* in Tab. 1) the advantages over previously proposes approaches seems marginal (if at all). I also miss confidence intervals. - What are clear advantages of the approach -- e.g., the claim that ""data augmentation is not nessaccary for our approach"" (besides still having a significant impact) is not well motivated.
- What are limitation of the approach?",153,0,1,0.8190000000000001,0.0409090909,0.9198144674,49,44.2428,0.1932,iclr,0.0202020202020202,2,3,2,3,partially factual,2,3,50,neutral,3,negative,3,moderate,3,3,3,2,partially factual,3,2,55,neutral,4,negative,3,moderate,2.0,3.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,2,2,3,2,partially factual,3,3,50,neutral,4,negative,3,moderate,2,3,3,3,partially factual,3,3,65,polite,4,neutral,4,low
168,Reviewer-hsiV,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. However, the novelty is not enough. This paper proposed a joint error based triplet alignment approach to solve the semi-supervised domain adaptation problem. They evaluated on several cross-domain benchmarks by comparing with several methods. Generally, the paper is easy to follow. They show various results to examine their methods. The novelty is not enough. The joint error based triplet alignment is not new, which is an extension of maximum cross margin discrepancy to three subsets, source, labeled target and unlabeled target. Eventual model is also very complicated. 

The model performance is not good enough. Especially compared with DECOTA in Table 1 & 2, it is very comparable. Also for semi-supervised setting, the selected target samples are very essential. There is no standard variance. Also t-test is needed to examine the significance. The clarification of model novelty.
The performance improvement.",174,0,0,0.7846000000000001,0.0049242424,0.9568377137,49,38.6381,0.0999,iclr,0.0,1,4,3,4,partially factual,3,2,45,neutral,3,negative,2,low,4,3,3,3,partially factual,4,3,55,neutral,5,negative,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,4,moderate,2,4,3,3,partially factual,3,3,60,neutral,4,negative,4,low
168,Reviewer-TnGf,Semi-supervised Domain Adaptation via Joint Error based Triplet Alignment,"Existing domain adaptation methods are very effective in aligning feature distributions. However, these techniques usually do not improve the performance that much when a few annotated examples are available in the target domain. To address this semi-supervised domain adaptation scenario, we propose a novel joint error based triplet alignment approach that simultaneously optimizes the classification loss as well as the joint error among the source, labeled and unlabeled target domains. Besides, we propose a novel dissimilarity measurement between two classifiers, namely maximum cross margin discrepancy, which can asymptotically bridge the gap between the theory and algorithm. We empirically demonstrate the superiority of our method over several baselines.","This work introduces a Triplet Alignment approach for semi-supervised domain adaptation. It simultaneously minimizes the joint error among different domains and the error rate on labeled data. 1.	The motivation for this work is clear. It aims to address the challenge of semi-supervised domain adaptation, particularly when only a limited number of annotated examples are available in the target domain. The proposed method optimizes both the classification loss and the joint error across source, labeled, and unlabeled target domains simultaneously.
2.	The proposed models are presented in a clear and comprehensible manner. 1.	The proposed model, to the best of my knowledge, lacks significant novelty as it closely resembles the approach in \[2\]. It would be helpful to explicitly identify the main difference.
2.	The choice of baseline methods in this work appears to be less competitive. Given the recent progress in semi-supervised domain adaptation (SSDA), including \[1\]\[2\], it is advisable to compare the proposed method with these contemporary approaches. Furthermore, while the use of t-SNE for feature space visualization is commendable, the comparisons are made with older methods like ENT (Grandvalet & Bengio, 2005), MJE (Zhang & Harada, 2019), and MME (Saito et al., 2019). It is imperative to include comparisons with more recent methods to provide a comprehensive evaluation.
\[1\]  Yu, Yu-Chu, and Hsuan-Tien Lin. ""Semi-Supervised Domain Adaptation with Source Label Adaptation."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
\[2\] Rahman, Md Mahmudur, Rameswar Panda, and Mohammad Arif Ul Alam. ""Semi-Supervised Domain Adaptation with Auto-Encoder via Simultaneous Learning."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023. Please see ""Weaknesses""",270,6,7,0.776,0.1943277311,0.9432914257,49,34.3667,0.1719,iclr,0.0,3,4,3,3,factual,3,3,55,neutral,4,neutral,2,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
115,Reviewer-HXMa,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The paper examines the causal and moral judgments made by large language models (LLMs) and their alignment with human intuitions. To do this, the researchers created a dataset of stories from 24 cognitive science papers, annotating each story with factors that influence people's judgments, such as norm violations and the avoidability or inevitability of harm.

The authors find that, on an aggregate level, the alignment between LLMs and human intuition has improved with newer models. However, statistical analyses reveal that LLMs and humans weigh these factors differently when making judgments. 
 
**Originality**: The paper presents an interesting approach to evaluating large language models (LLMs) by testing their ability to handle tasks related to causal judgments and moral permissibility. The authors have transcribed stories from various papers and used them to test the LLMs, focusing on several factors that influence people's causal judgments and moral dilemmas. This approach is original and provides a new perspective on the capabilities of LLMs.

**Quality**: The authors have meticulously transcribed stories from a number of papers, ensuring a wide range of scenarios for testing the LLMs. They have also collected responses for each story from a crowd-sourcing platform, ensuring a diverse set of responses for analysis. 

**Clarity**: The paper is written in a clear and understandable manner. The authors have explained their methodology and the factors they focused on in a detailed and comprehensible way. 

**Significance**: The paper contributes to understanding how LLMs handle complex tasks related to causality and morality. This is an important area of research, given the increasing use of LLMs in various applications. The insights gained from this study could be useful for improving these models in the future. The paper also opens up new avenues for research in this area.
 There are a few areas where it could be improved:

1. **Evaluating with more models**: The paper is a bit skewed towards OpenAI models (GPT3 and beyond) for the evaluation. Including more diverse models could provide a more comprehensive understanding of how different LLMs perform on the tasks. This could also help identify whether the observed behaviors are specific to these models or are more generally applicable to LLMs.

2. **Comparing with human performance**: While the paper does a good job of comparing the performance of different LLMs, it does not provide a clear comparison with human performance. This makes it difficult to assess how close the models are to human-level performance on these tasks. Including a human baseline could provide a more meaningful context for the results.

3. **Analyzing incorrect predictions**: The paper could benefit from a more detailed analysis of the models' incorrect predictions. This could help identify common patterns or biases in the models' errors, which could provide insights for improving the models.

4. **Generalizability of findings**: The paper's findings are based on a specific set of stories and tasks. It's unclear how generalizable these findings are to other tasks or domains. The authors could address this by testing the models on a wider range of tasks or by discussing the limitations of their approach in more detail.
 1. **Evaluating with more models**: The paper primarily focuses on GPT-type models and its variants. Could the authors elaborate on why they chose to focus on these models? Would the inclusion of other language models provide different insights? 

2. **Comparing with human performance**: The paper lacks a clear comparison with human performance. Could the authors provide a human baseline for these tasks? This could help in understanding how close the models are to achieving human-level performance.

3. **Analyzing incorrect predictions**:  The paper could benefit from a more detailed analysis of the models' incorrect predictions. Could the authors provide more insights into the common patterns or biases in the models' errors? This could potentially help in improving the models.

4. **Generalizability of findings**: The findings of the paper are based on a specific set of stories and tasks. Could the authors discuss how generalizable these findings are to other tasks or domains? 

5. **Interpretability and transparency**: The paper presents an analysis of how LLMs reason about causality and morality. However, it's not clear how these insights can be used to improve the interpretability and transparency of these models. Could the authors provide some thoughts on this?
 The paper addresses the limitations of the work and potential negative societal impact, although not in a dedicated section. The authors acknowledge that their focus is narrow and only on certain aspects of alignment with humans. They caution that their work should not be used to make sweeping and general statements about AI-human alignment. They also note that their moral permissibility task is not a certification task and should not be used as a flat benchmark to beat.

The authors also discuss the ethical considerations of their work, emphasizing the importance of assessing implicit intuitions underlying commonsense reasoning abilities in large language models (LLMs), especially in cases related to morality. They acknowledge that even if a model is not explicitly given the responsibility to make moral judgments, these judgments can appear across many forms of freely generated text. They also recognize the potential for replicating human biases in LLMs and state that this is something they would want to avoid.

In terms of potential negative societal impact, the authors don't explicitly discuss this. However, they do acknowledge the potential for misuse of LLMs and the ethical considerations that come with their use. They also discuss the importance of transparency and consent in their data collection process.

",911,0,8,0.7597,0.1167226452,0.9537047744,215,39.0988,0.1041,neurips,0.0,4,5,4,5,partially factual,4,4,87,neutral,4,positive,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,3,low
115,Reviewer-7W7v,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","
This paper investigates to what extent LLMs can align with human intuitions when making causal and moral judgments. To do this, they collected a dataset of stories from 24 cognitive science papers and created a causal and moral judgment challenge set. They evaluate different LLMs about their alignment with humans and reveal that the implicit preferences can be different even for LLMs trained with the same technique. They find that increasing model sizes actually impact those models’ aggregate-level alignment differently.
 With the wide spread of LLMs, to understand the alignment between humans and models is an important topic. In this paper:
- They have provided a dataset to understand the human-model alignment, especially in the causal and moral judgment perspective.
- The resources are from cognitive studies which makes it more reliable than the normal text resources.
  The paper wants to analyze the alignment between humans and models, however it lacks some description of how they conducted the human study.
 - For the factors in Table 2, are they from existing literature reviews or summarized by the authors? 
- For the dataset, are those factor labels annotated by the authors or by original cognitive scientists? 
- For the human participants, do you have any criteria to select who can participate in the survey?
- Did you educate the participants about different factors before you conduct the survey?
- It seems only part of Fig.2 is visible on my side. Please check that. 
 NA",243,0,2,0.7673,0.07625,0.9360769987,215,44.2496,0.1375,neurips,0.0099009900990099,1,4,1,0,unfactual,3,4,20,polite,2,positive,1,high,3,4,4,4,partially factual,4,4,75,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
115,Reviewer-duEY,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","This model presents a new challenge set of hard edge cases intended to test models understanding of the nuances of the directness of causation and moral culpability, by collecting them from a set of cognitive science papers. This has the clever effect of not only getting challenging stories, but those which would vary along specific features important to humans.

They test LLMs on those outputs to measure agreement with human intuitions; and annotate those cases for a set of features so that one could draw insight from those disagreements. 

 It is a well-written and well-considered paper which both presents a new useful challenge set, and utilizes it to provide interesting analysis of LLM tendencies in causal culpability and moral judgments.  It could clearly lead to further uses both in the evaluation of new models and in further analysis. The literature review is, as far as I could tell, comprehensive. 

The work seem rigorous throughout - I appreciate the thorough explorations with personas and automatic prompt engineering, which alleviate worries about the normal fickleness of prompt choice.   - The size of the challenge set (around 200 stories I believe) is somewhat limited; I don't think that that's too much of a worry for such a challenge set, so I wouldn't view it as a major weakness. 
- quibble: A seemingly left-over note on line 232: "" This is very very interesing, make the flow better."" - I'd be very curious about which personas and prompts would lead to worst-case performance for various models, since that might give insight into how the models go awry. 
- The improvements in alignment with human judgements from adopting a utilitarian/consequentialist framing is fascinating. However, that doesn't mean that all humans have a utiliarian framing.  Is there any concern that measuring against the average of human judgements might ignore variance between different humans on such judgement tasks? 
 The ethical considerations section seems thoughtful, and I see no unaddressed limitations. ",323,0,0,0.8307,0.1109072872,0.9276382923,215,40.2727,0.7142000000000001,neurips,0.0099009900990099,3,4,3,2,factual,4,4,75,neutral,4,positive,3,none,4,4,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
115,Reviewer-sFvB,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.","The motivation of this paper is that people constantly make lots of causal and moral judgments to reason about why did what things and why. This paper contributes a dataset of stories compiled from cog sci papers, with detailed annotation of the factors that contributes to the human judgment. Then, the paper looks at how LLMs make judgments, and check the alignment with humans. - The paper addresses an important topic to check the causal and moral reasoning and the alignment of LLMs with humans
- The proposed dataset looks solid and well-annotated
- The analysis provides insights to the community to develop safer and more aligned LLMs. - The size of the dataset is a bit limited, 144 causal stories and 62 moral stories, making the insights drawn upon them be not extensive enough
- The yes/no binary answer is reasonable, but analyzing LLMs behavior using a binary classification task might have a little signal-noise ratio. There needs to be lots of human annotation to evaluate the reasoning quality of LLMs, and whether any misalignment or unsafe reasoning was provided apart from the binary answer. 1. Are there domain experts in moral psychology / philosophy involved in the design process of this paper? How do you make sure the factors in 2a and 2b are comprehensive and can explain for all the judgment decisions? I saw the appendix A.1, but I would like to see one dedicated paragraph for each of Table 2a and 2b, describing the rationale behind each factor and how they correlate with human intuitions in the main text in the next version of the paper.

2. Can the authors let LLMs to output its reasoning, and then annotate what type of tendencies LLMs show in its reasoning (maybe doing it on a subset, e.g., 50 samples)?

\[I have read the rebuttal, and acknowledge the author's effort into it. I'm supportive of the acceptance of this paper.\] N/A",322,0,3,0.7616,0.0912608225,0.9197995663,215,45.357,0.1229,neurips,0.0,4,4,2,3,factual,3,3,62,neutral,4,negative,4,moderate,5,5,5,5,factual,5,5,95,polite,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,3,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,3,low
154,Reviewer-MiyQ,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","This work studies stochastic block models where blocks/clusters can have different sizes. It proposed a simple SVD algorithm which recovers communities in this setting. The main technical improvement of this work is that the assumption is removed which requires there to be a ‘size interval’ where no clusters appear. 
A secondary result is a efficient clustering algorithm with sublinear query complexity. 
 -	This work is a clear improvement over the previous state-of-the-art. As I understand it, a key technical contribution of this work that might influence future work is instead of finding $k$ clusters as is done using the SVD approach, the algorithm first aims to find large clusters one-by-one. Although these are not perfect (they form a so-called plural set), using some non-trivial techniques perfect recovery can be obtained. 
- Experiments on synthetic data indicate that the algorithm not only works well in theory but also in practice.
- The write-up of this work is excellent. -	Given that the aim of the studied setting is to look at more realistic settings, I would have expected to find experimental results on real-world datasets as well. Although this work does provide better bounds for SBMs generated with differently sized clusters, SBMs still have a highly symmetric structure compared to real-world graphs. It would be interesting to see the performance of the proposed algorithm on some real-world graphs.  -	How does the algorithm compare with respect to the previous work in terms of running time?
- In practice the Spectral Clustering algorithm performs well in practice on graphs with clusters of unbalanced size. Even though not many bounds are known of spectral clustering with respect to SBMs, did you try to compare your algorithm experimentally with Spectral Clustering? none",288,0,1,0.795,0.1212698413,0.9042724371,221,46.453,0.1969,neurips,0.02,3,4,3,3,partially factual,4,4,70,polite,4,neutral,3,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
154,Reviewer-tK15,Recovering Unbalanced Communities in the Stochastic Block Model with Application to Clustering with a Faulty Oracle,"The stochastic block model (SBM) is a fundamental model for studying graph clustering or community detection in networks. It has received great attention in the last decade and the balanced case, i.e., assuming all clusters have large size, has been well studied. 
However, our understanding of SBM with unbalanced communities (arguably, more relevant in practice) is still limited. In this paper, we provide a simple SVD-based algorithm for recovering the communities in the SBM with communities of varying sizes.
We improve upon a result of Ailon, Chen and Xu [ICML 2013; JMLR 2015] by removing the assumption that there is a large interval such that the sizes of clusters do not fall in, and also remove the dependency of the size of the recoverable clusters on the number of underlying clusters. We further complement our theoretical improvements with experimental comparisons.
Under the planted clique conjecture, the size of the clusters that can be recovered by our algorithm is nearly optimal (up to poly-logarithmic factors) when the probability parameters are constant. 

As a byproduct, we obtain an efficient clustering algorithm with sublinear query complexity in a faulty oracle model, which is capable of detecting all clusters larger than $\tilde{\Omega}({\sqrt{n}})$, even in the presence of $\Omega(n)$ small clusters in the graph. In contrast, previous efficient algorithms that use a sublinear number of queries are incapable of recovering any large clusters if there are more than $\tilde{\Omega}(n^{2/5})$ small clusters.","The authors consider the problem of perfect recovery in a stochastic block model where the average degree is large and where the groups are not balanced. They provide an algorithm based on singular value decomposition to recover recursively the largest clusters. They provide a few numerical experiments illustrating their claims. The authors apply their results to the problem of clustering with a faulty oracle. I have little knowledge as to this problem of perfect recovery in a dense SBM and I am not able to assess the correctness of the claims and their relevance.
 The same. Maybe the authors could precise the complexity of the algorithm 1. In experiment 6 it seems the authors are able to run this algorithm for n substantially larger than the other experiments. The authors could go to higher n and test how tight are their bounds; in particular taking p and q smaller.

A small section to conclude the article and for future work would be appreciable.

Some references are ill-formatted. Eg ref. 27 ""svd"" –> ""SVD"".
Inconsistency: plural set vs plural-set. The same.",180,0,4,0.7694000000000001,0.1152568922,0.9103051424,221,52.27,0.2025,neurips,0.0186915887850467,1,4,1,2,partially factual,2,1,40,polite,3,negative,1,moderate,4,3,3,4,partially factual,3,3,65,polite,5,neutral,3,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,3,3,2,3,partially factual,3,3,50,polite,3,neutral,3,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
27,Ashish-Saraf,Case Report: Ziprasidone induced neuroleptic malignant syndrome,"Neuroleptic malignant syndrome (NMS) is a well-recognized neurologic emergency. It presents with classic features including hyperthermia, autonomic instability, muscle hypertonia, and mental status changes. The syndrome is potentially fatal and is associated with significant morbidity due to complications such as rhabdomyolysis, acute kidney injury, and ventricular arrhythmias due to the trans-cellular electrolyte shift. NMS is conventionally associated with the first-generation antipsychotic agents, however, has been described with the use of atypical and novel antipsychotics including Ziprasidone. A case of NMS with Ziprasidone use at the therapeutic dose is reported here.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is well written. The authors have given detailed description of the case mentioning the clinical features, the diagnostic workup and treatment given. The other causes of the rigidity have been ruled out during the diagnostic workup. The discussion is also well written and highlighted the importance of this case report.  This case report will definitely make the clinicians aware of the fact of NMS in newer drugs and hence making them vigilant.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",209,0,1,0.7103,0.0706140351,0.6933223009,20,33.34,0.0999,f1000,0.0098039215686274,1,4,1,1,unfactual,3,2,40,polite,3,positive,2,high,2,5,4,2,factual,4,4,75,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,5,4,2,factual,4,4,4,polite,5,positive,3,low,2,4,4,3,factual,4,4,85,polite,5,positive,3,low
151,Reviewer-E1PQ,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The authors apply a normalizing flow model approach to rare event probability estimation, defined where the probability is less than 1e-4. This is done by the normalizing flow model learning proposal distributions, then estimating rare event probability using importance sampling on the learned proposal distribution. Paper is well presented, and using normalizing flows to assist with importance sampling (as compared to the other way around which has been done) is new. Freezing seems to provide only a marginal advantage over non-freezing. The main advantage as the authors proposed is in the speed, but that's not particularly central to the paper as speed is measured by function calls and not wall clock time. If we remove step 5 from NOFIS then most of the method is not particularly distinguishable from standard normalizing flows.

In addition, if we're looking for just samples from the proposal distribution, what's the advantage of using NFs over other generative models? If there is a lack of distinguishing feature then the middle portion on NFs specifically might not be needed in lieu for a general generative model construction. Figure 2: Overlay highlighted green areas - not sure if I see the highlights?

What about just using the normalizing flow to directly estimate the likelihood of the rare event?",211,0,0,0.7887000000000001,0.0501683502,0.8836317062,55,39.2829,0.1199,iclr,0.0097087378640776,3,4,3,3,partially factual,3,4,60,polite,3,negative,3,none,3,3,3,3,partially factual,4,4,65,polite,4,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
151,Reviewer-27ee,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper introduces a technique for rare event sampling that combines normalizing flows with importance sampling. The authors refer to this technique as NOFIS (NOrmalizing Flows assisted Importance Sampling). They justify their work by highlighting the limitations of standard sampling algorithms, such as MCMC, in sampling regions of low probability, where the density, denoted as $p$, is approximately $10^{-X}$, with X being an integer greater than 4. In this context, known as the regime of rare event sampling, algorithms like MCMC would require an impractical number of samples, rendering these approaches highly inefficient. The authors propose that employing normalizing flows-aided importance sampling holds promise as a solution to this problem. - The paper flows smoothly and is enjoyable to read.
- The authors provide great level of details and do not take anything for granted, which I appreciate. - **Novelty**: I don’t find much novelty in the proposed paper. The technique presented by the authors has already been explored in many prior works in different fields, particularly in physics, where rare event sampling is often a challenging problem (see below).

- **Related Works**: Despite many prior works combining normalizing flows with importance sampling, and beyond, exist, this paper lacks a dedicated *Related Work* section. Several seminal works have been completely overlooked despite their significant contributions to the field of normalizing flow-aided importance sampling in statistical physics \[1\], chemistry\[2\], and quantum field theory\[3,4,5\].

- **Annealed Importance Sampling**: There is no reference to *annealed importance sampling* \[6\], which I believe is highly tight to the idea of the paper. Besides \[6\], several relevant works \[7,8,9\] perform annealed importance sampling within the context of normalizing flows, falling within the same category as the CRAFT method referenced in the paper, though only marginally. What these methods do closely aligns with what the authors propose in the paper: instead of learning the target distribution in one step, they 'anneal' towards that distribution by learning and sampling from intermediate distributions, ensuring that the final learned probability density has as much support as possible, including regions where the target density is small enough to fall within the rare event regime. I believe it is crucial for this paper to be published in this or any other venue to highlight the connection to these (and the previously referenced works).

- **Rare Event Sampling**: A recent paper \[10\] discusses similar behaviors in training normalizing flows and combining them with importance sampling to ensure full support over the target density, including rare event regions. I would find it interesting if the authors commented on this work within the context of their findings. Some of the metrics and tools proposed in \[10\], such as the mode-dropping estimator, could also be used to assess the performance of a sampler in approximating regions of low probability where a shallow sampler is likely to lose some of the probability mass.

- **Idea of Anchor Points**: The notion of *anchor points* has implicitly been explored in some of the prior works mentioned above, albeit with a slightly different connotation that may have escaped the authors' attention. For instance, in the paper by Kanwar et al. \[4\] (Fig. 4), the authors use a technique very similar to what is suggested in this paper, although with slightly different connotations (e.g., they use previously trained flow-based models as starting (anchor) points to sequentially train more challenging distributions).

- **Additional Related Works**: Other closely related works, such as \[11\], are not mentioned in the manuscript despite having similar titles. This may cause confusion for potential readers.

- **Experiments**: I find the results presented in the paper not entirely convincing. Although the authors compared their approach to a large set of baselines, this alone does not seem sufficient to claim the superiority of the proposed method. I am surprised that the proposed approach is not compared against prior works, such as Annealed Importance Sampling with Normalizing Flows \[7\], and naive RealNVP training with a sufficiently large number of couplings and no anchor points.

As a side note, I strongly recommend that the authors conduct an extensive literature search to include and acknowledge existing prior works, and eventually, compare and discuss potential differences and similarities - I'd like to see how the author would compare their work (and its corresponding novelty) to previous works. In particular, I'd like to see comparisons with Refs. \[6-9\] for the annealing aspect and Ref. \[10\] for the theoretical discussion regarding low-support regions (e.g., the rare event regime). Furthermore, discussing the differences concerning Ref. \[11\] would be helpful for the readers.

- I'd appreciate if the authors could perform an extensive literature search and create a Related Work section to place their paper in the context of existing prior works. Please refer to Refs. \[1-11\].

- I found the last paragraph in Section 3.1 and the discussion in Appendix B to be a bit unintuitive. It has been shown in the literature that using Forward KL, instead of Reverse KL, generally results in larger support and, therefore, has some benefits when combined with importance sampling. In that sense, I am surprised by the author's claim that training using Forward KL deteriorates performance. Do the authors consider the case where NO samples are given from the target density? If so, then I may understand this point. Otherwise, when a sample set from the target density, even if small, is available, it should be possible to show that training with Forward KL is feasible.

- It would be informative to see the density plot from Figure 4 for the other baselines as well.

- On page 8, referring to Figure 4, the authors write ""\[…\] the right part further reveals that when increasing $N_{IS}$, the estimation could become even more accurate."" This result does not seem neither novel nor unexpected. Indeed, it was already demonstrated in prior works, as seen in \[1,5\], that the variance of the importance sampling estimators scales with $N^{-1}$, with N being the number of samples. Could maybe the authors comment on this?

**Minor**

- The quality of the plots on pages 7-8 is quite poor. The axis labels are missing, and the font size for the x-y tick labels is too small.

- As a side note, I sometimes find the MK notation a bit confusing. However, I understand that it would require a substantial effort to rewrite the manuscript and adapt to a clearer notation. Nevertheless, this my be a feedback worth keeping in mind for the authors for future iterations of the manuscript. 

- I find it somewhat unintuitive to completely relegate the discussion of the datasets to the appendix. Perhaps the authors could add corresponding references in the main text when mentioning the datasets and also refer to the Appendix for further details.

- In the conclusion, statements like *using nested subset events as bridges* agains strongly reminds of annealed importance sampling. I believe that a discussion comparing the present method to AIS, highlighting potential differences, or connecting them through their analogies is an essential element currently missing in the manuscript.


**References:**


- \[1\] \[Nicoli, Kim A., et al. ""Asymptotically unbiased estimation of physical observables with neural samplers."" Physical Review E 101.2 (2020): 023304.\](https://link.aps.org/accepted/10.1103/PhysRevE.101.023304)
- \[2\] \[Noé, Frank, et al. ""Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning."" Science 365.6457 (2019): eaaw1147.\](https://www.science.org/doi/10.1126/science.aaw1147)
- \[3\]\[Albergo, Michael S., Gurtej Kanwar, and Phiala E. Shanahan. ""Flow-based generative models for Markov chain Monte Carlo in lattice field theory."" Physical Review D 100.3 (2019): 034515.\](https://journals.aps.org/prd/abstract/10.1103/PhysRevD.100.034515)
- \[4\]\[Kanwar, Gurtej, et al. ""Equivariant flow-based sampling for lattice gauge theory."" Physical Review Letters 125.12 (2020): 121601.\](https://link.aps.org/pdf/10.1103/PhysRevLett.125.121601)
- \[5\] \[Nicoli, Kim A., et al. ""Estimation of thermodynamic observables in lattice field theories with deep generative models."" Physical review letters 126.3 (2021): 032001.\](https://link.aps.org/pdf/10.1103/PhysRevLett.126.032001)
- \[6\]\[Neal, Radford M. ""Annealed importance sampling."" Statistics and computing 11 (2001): 125-139.\](https://arxiv.org/abs/physics/9803008)
- \[7\] \[Midgley, Laurence Illing, et al. ""Flow annealed importance sampling bootstrap."" arXiv preprint arXiv:2208.01893 (2022).\](https://arxiv.org/pdf/2208.01893)
- \[8\] \[Wu, Hao, Jonas Köhler, and Frank Noé. ""Stochastic normalizing flows."" Advances in Neural Information Processing Systems 33 (2020): 5933-5944.\](https://proceedings.neurips.cc/paper/2020/hash/41d80bfc327ef980528426fc810a6d7a-Abstract.html)
- \[9\] \[Caselle, Michele, et al. ""Stochastic normalizing flows as non-equilibrium transformations."" Journal of High Energy Physics 2022.7 (2022): 1-31.\](https://arxiv.org/pdf/2201.08862.pdf)
- \[10\] \[Nicoli, Kim A., et al. ""Detecting and Mitigating Mode-Collapse for Flow-based Sampling of Lattice Field Theories."" arXiv preprint arXiv:2302.14082 (2023).\](https://arxiv.org/pdf/2302.14082)
- \[11\] \[Falkner, Sebastian, et al. ""Conditioning normalizing flows for rare event sampling."" arXiv preprint arXiv:2207.14530 (2022).\](https://arxiv.org/pdf/2207.14530.pdf)",1395,47,22,0.8111,0.0786587302,0.9124334455,55,43.1599,0.8084,iclr,0.011111111111111,4,4,5,4,factual,4,4,90,neutral,4,negative,4,none,5,5,5,5,factual,5,5,100,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
151,Reviewer-9Yao,Rare Event Probability Learning by Normalizing Flows,"A rare event is defined by a low probability of occurrence. Accurate estimation of such small probabilities is of utmost importance across diverse domains. Conventional Monte Carlo methods are inefficient, demanding an exorbitant number of samples to achieve reliable estimates. Inspired by the exact sampling capabilities of normalizing flows, we revisit this challenge and propose normalizing flow assisted importance sampling, termed NOFIS. NOFIS first learns a sequence of proposal distributions associated with predefined nested subset events by minimizing KL divergence losses. Next, it estimates the rare event probability by utilizing importance sampling in conjunction with the last proposal. The efficacy of our NOFIS method is substantiated through comprehensive qualitative visualizations, affirming the optimality of the learned proposal distribution, as well as a series of quantitative experiments encompassing 10 distinct test cases, which highlight NOFIS's superiority over baseline approaches.","The paper proposes to use normalizing flows to sample rare events. The neural networks learn the proposal distribution for the importance sampling and then use importance sampling to estimate the rare event probability. The numerical experiments show that the proposed method uses fewer function calls and has smaller errors in the average of the estimation. 1. The motivation and the problem statement are clear. The paper is also easy to follow.
2. The implementation details about the algorithm are well-explained and the math of the method is also well-written.
3. The numerical section shows experiments with synthetic data and real-world data with multiple dimensions. The paper also compares the proposed method with five other baselines. 1. The experiments only contain up to dimension 62, and the paper does not explain why sampling rare events at this dimension is difficult. How the comparison may look like if we compare the method with traditional sampling methods, like metropolis sampling.
2. The method's speedup and precision improvement are not clear from the languages used in the text. 
3. The experiments in Figure 2 and Figure 3 look unrelated to rare event sampling but show the effectiveness of the method approximating a given distribution. It will be beneficial to get more ideas on what these figures tell us. 1. Does the number of anchors matter in your experiments? 
2. How do you determine the training is complete?
3. For Tables 1 and 2, do you have the measurement of time in seconds? When you say function call, does it always take the same time for different methods? If the numbers include the time of training the neural networks, would the proposed method still be faster than other methods, especially non-ML methods?
4. It would also be useful to see the confidence interval from the 20 estimations. Do you have them?",306,0,10,0.7398,0.0801587302,0.9148631692,55,51.1349,0.1509,iclr,0.0,3,4,3,3,partially factual,3,4,65,neutral,4,neutral,3,moderate,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
31,Obinna-Ikechukwu-Ekwunife,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This short report aimed to assess the gaps in routine VL monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy from Jan 2017 to Sep 2018. This study is essential as such data is needed to assess how programs are fairing with regards to the UNAIDS 90-90-90 target. The study was succinctly reported. All the essentials results based on their study objective were addressed. The study should be accepted.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",220,0,1,0.7663000000000001,0.1567234848,0.8395429254000001,23,35.98,0.0999,f1000,0.0106382978723403,5,5,4,3,partially factual,3,5,77,polite,5,positive,5,none,2,5,4,1,factual,4,4,80,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,3,2,factual,4,4,60,polite,4,positive,3,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
31,Brian-Van-Wyk,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The study reports on viral load monitoring at 6 months for of children and adolescents who were initiated on HIV treatment in a tertiary hospital in Bulawayo. The study is important because of the HIV epidemic in Zimbabwe, and the need to reach the third 90 of UNAIDS 90-90-90 targets. The methodology is sound and clearly reported on. Appropriate statistical analysis is done, and these are aligned with the objectives of the study. Few other sociodemographic and clinical factors were collected and analysed; which is a limitation to the study. This should be indicated.  In the discussion, enhanced adherence counseling is mentioned as being implemented in the hospital. However, little information on this is provided in the background. Also, it would be useful if the analysis could report on how many of the current cohort received enhanced adherence counseling.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",282,0,1,0.7397,0.1770833333,0.8287450671000001,43,28.23,0.0999,f1000,0.0098039215686274,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,4,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
31,Catherine-Kegakilwe-Koofhethile,"Children and adolescents on anti-retroviral therapy in Bulawayo, Zimbabwe: How many are virally suppressed by month six?","Background: Zimbabwe is one of the countries in sub-Saharan Africa disproportionately affected by human immunodeficiency virus. In the “treat all” era, we assessed the gaps in routine viral load (VL) monitoring at six months for children (0-9 years) and adolescents (10-19 years) newly initiated on anti-retroviral therapy (ART) from January 2017 to September 2018 at a large tertiary hospital in Bulawayo. Methods: In this cohort study using secondary data, we considered first VL done within six to nine months of starting therapy as ‘undergoing VL test at six months’. We classified repeat VL≥1000 copies/ml despite enhanced adherence counselling as virally unsuppressed. Results: Of 295 patients initiated on ART, 196 (66%) were children and 99 (34%) adolescents. A total 244 (83%) underwent VL test at six months, with 161 (54%) virally suppressed, 52 (18%) unsuppressed and 82 (28%) with unknown status (due to losses in the cascade). Switch to second line was seen in 35% (18/52). When compared to children, adolescents were less likely to undergo a VL test at six months (73% versus 88%, p=0.002) and more likely to have an unknown VL status (40% versus 22%, p=0.001). Conclusion: At six months of ART, viral suppression was low and losses in the cascade high.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I think that this manuscript is addressing a very important gap in knowledge that is relevant for the current ‘treat all’ recommendations. They accessed the gaps in routine viral load monitoring at six months for children and adolescents who initiated antiretroviral therapy in a hospital in Zimbabwe. Their sample number is good enough for this analysis. The manuscript is very well written, it is very clear and concise. The study was based on analysis of secondary data which was approved by IRB.  I only have one comment that need clarification- the authors keep comparing their analysis with a study done in Harare and it is not clear whether this study that they are comparing to was conducted on adult population or the same population as they describe in their analysis. This needs to be clarified. In addition, they need to explain what could be accounting for the differences found.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",297,0,1,0.775,0.16953125,0.8221491575000001,71,34.46,0.2025,f1000,0.0097087378640776,5,5,5,5,factual,3,5,87,polite,5,positive,5,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3.0,5.0,4.0,4.0,factual,4.0,5.0,80.0,polite,5.0,positive,5.0,none,3,5,4,3,factual,4,4,80,polite,5,positive,4,low,3,5,4,4,factual,4,4,85,polite,5,positive,5,low
198,Reviewer-vL8H,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","The paper proposes iGraphMix, a novel Mixup method tailored for node classification in graph neural networks (GNNs), which generates virtual nodes and edges by interpolating input features, labels, and neighboring nodes. iGraphMix addresses the irregularity and alignment issues associated with applying Input Mixup to node classification, and the paper provides theoretical proof and experimental results demonstrating its effectiveness in improving GNN performance and reducing overfitting. * The paper provides theoretical proof and experimental validation of the effectiveness of iGraphMix in reducing the generalization gap and improving GNN performance * The experimental validation of iGraphMix is mentioned, but it would be helpful to have more details on the datasets used, the specific GNN models employed, and the performance metrics used for evaluation.

* It would be beneficial to have experimental analysis about the computational cost and speed of the proposed method compared with the state-of-the-art approaches. * The paper mentions that iGraphMix can be combined with other augmentation methods. Can you provide examples or insights into how this combination can be done and what benefits it can bring to GNN performance?",180,0,0,0.7099000000000001,0.0861111111,0.9385924935,51,9.7844,0.1901,iclr,0.0,2,4,2,2,partially factual,3,2,50,polite,3,positive,3,moderate,4,4,3,4,5,4,4,78,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
198,Reviewer-jPtV,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposes a node-level graph mixup method named iGraphMix to improve the model generation ability. To handle the irregularity and alignment issue for graph mixup, this paper proposes to generate virtual nodes and edges by interpolating features and labels, and attaching sampled neighborhoods. Theoretical analysis shows that iGraphMixup can be regarded as a regularization on the weight space to help improve the generalization. Experiments on real world datasets validate the effectiveness of the proposed method on node classification. -	A novel method is proposed to mixup graphs at the input level.
-	Theoretical analysis is provided to understand the effect of improving the model generalization.
-	Extensive experiments are provided to evaluate the method empirically. -	Baseline methods are quite limited and evaluation on robustness is highly recommended. See details in the question part.
-	Presentation could be further improved. -	How will the proposed method enhance the model robustness? Robustness w.r.t label/feature/structure noises is usually evaluated for mixup methods \[1,2\], and it is highly recommended to include these experiments in the paper.
-	More baselines are needed. Currently, only M-mixup is a graph mixup for node classification, while other augmentation methods (e.g., \[4\]) are not included.
-	Eq.(4): How can A,X and its permuted counterpart A’,X’ be directly added as they are not well-aligned? Is the masking matrix M a symmetric matrix?
-	Writting:
  - Eq.(6), notations $\tilde{Z}_{v,v’}$, $\tilde{Y}_{v,v’}$ is quite misleading, as subscripts are used to denote columns and rows in the paper.
  - Line below eq.(1): matrix->matrices.

Reference

\[1\] Han, Xiaotian, et al. ""G-mixup: Graph data augmentation for graph classification."" International Conference on Machine Learning. PMLR, 2022.

\[2\] Ling, Hongyi, et al. ""Graph Mixup with Soft Alignments."" arXiv preprint arXiv:2306.06788 (2023).

\[3\] Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can (sometimes) explain generalisation in graph neural networks. Advances in Neural Information Processing Systems, 34:27043–27056, 2021.

\[4\] Verma, Vikas, et al. ""Graphmix: Improved training of gnns for semi-supervised learning."" Proceedings of the AAAI conference on artificial intelligence. Vol. 35. No. 11. 2021.

\[5\] Wu, Lirong, et al. ""Graphmixup: Improving class-imbalanced node classification by reinforcement mixup and self-supervised context prediction."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022.",371,8,16,0.8160000000000001,0.0151984127,0.909758687,51,34.48,0.0999,iclr,0.0108695652173913,3,3,2,2,partially factual,3,2,60,polite,3,positive,3,low,4,4,4,4,5,4,4,75,3,5,2,4,2,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
198,Reviewer-mUwv,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper presents a new method called iGraphMix for node classification in graph neural networks. The method addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. iGraphMix generates virtual graphs that serve as inputs for GNNs training, leading to better generalization performance compared to training without augmentation. The authors evaluate iGraphMix on several benchmark datasets and show that it outperforms existing state-of-the-art methods. The contributions of this paper include a novel approach to graph augmentation, a comprehensive evaluation of the proposed method, and insights into the effectiveness of virtual graph generation for GNNs training. This paper presents a novel method, iGraphMix, for addressing the challenges of irregularity and alignment in generating virtual nodes and edges for graph neural networks. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear and comprehensive description of the method, including theoretical analysis and experimental validation of its effectiveness. The evaluation is thorough and includes comparisons to existing state-of-the-art methods on several benchmark datasets. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. 

Overall, the paper is well-written and easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. The experimental results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of originality, iGraphMix is a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method builds on existing work in Input Mixup for other domains but is specifically designed for node classification in the graph domain. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. 

In terms of quality, the paper is well-written and well-organized, with clear explanations of the technical details and experimental setup. The authors provide a thorough evaluation of the proposed method, including comparisons to existing state-of-the-art methods on several benchmark datasets. The results are convincing and demonstrate the superiority of iGraphMix over existing methods. 

In terms of clarity, the paper is easy to follow, with clear explanations of the technical details and experimental setup. The authors provide a detailed discussion of related work and highlight the contributions of their method. The theoretical analysis is insightful and provides a deeper understanding of the effectiveness of iGraphMix. 

In terms of significance, the paper presents a novel approach to graph augmentation that addresses the challenges of irregularity and alignment in generating virtual nodes and edges for GNNs training. The method is well-motivated and builds on existing work in Input Mixup for other domains. The authors provide a clear motivation for the method and demonstrate its effectiveness through theoretical analysis and experimental validation. The results show that iGraphMix outperforms existing methods in terms of micro-F1 score, demonstrating the significance of the proposed approach. Overall, the paper is well-written and presents a novel approach to graph augmentation for node classification in GNNs. However, there are a few weaknesses that could be addressed to improve the paper:

1. Limited analysis of the impact of hyperparameters: The authors do not provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix. It would be useful to see how the performance of iGraphMix varies with different hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient.

2. Lack of ablation study: The authors do not provide an ablation study to analyze the contribution of each component of iGraphMix. It would be useful to see how the performance of iGraphMix varies when different components are removed or modified.

3. Limited discussion of limitations: The authors do not provide a detailed discussion of the limitations of iGraphMix. It would be useful to see a discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate.

4. Lack of analysis of computational complexity: The authors do not provide an analysis of the computational complexity of iGraphMix. It would be useful to see how the computational cost of iGraphMix compares to other graph augmentation methods and how it scales with the size of the graph.

Addressing these weaknesses would strengthen the paper and provide a more comprehensive evaluation of the proposed method. How sensitive is the performance of iGraphMix to the choice of hyperparameters, such as the number of virtual nodes or the strength of the mixing coefficient? Can the authors provide a detailed analysis of the impact of hyperparameters on the performance of iGraphMix?

Can the authors provide an ablation study to analyze the contribution of each component of iGraphMix? This would help to better understand the importance of each component and how the performance of iGraphMix varies when different components are removed or modified.

What are the limitations of iGraphMix? Can the authors provide a detailed discussion of the scenarios where iGraphMix may not be effective or where other methods may be more appropriate?

Can the authors provide an analysis of the computational complexity of iGraphMix? How does the computational cost of iGraphMix compare to other graph augmentation methods, and how does it scale with the size of the graph?

How does iGraphMix perform on larger and more complex graphs? Can the authors provide an analysis of the scalability of iGraphMix to larger graphs with more nodes and edges?

Can the authors provide a discussion of the potential applications of iGraphMix beyond node classification, such as link prediction or graph classification?

How does iGraphMix perform on graphs with different characteristics, such as sparsity or degree distribution? Can the authors provide an analysis of the robustness of iGraphMix to different graph properties?

Can the authors provide a discussion of the potential limitations of the theoretical analysis presented in the paper? How well does the theoretical analysis capture the behavior of iGraphMix in practice?

Can the authors provide a discussion of the potential ethical implications of using graph augmentation methods like iGraphMix? How can we ensure that these methods are used responsibly and do not perpetuate biases or inequalities in the data?",1055,0,3,0.7009000000000001,0.1379187281,0.9172924757,51,28.6703,0.0948,iclr,0.0,5,5,5,5,factual,3,4,95,polite,5,positive,4,none,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
198,Reviewer-A2TK,iGraphMix: Input Graph Mixup Method for Node Classification,"Recently, Input Mixup, which augments virtual samples by interpolating input features and corresponding labels, is one of the promising methods to alleviate the over-fitting problem on various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for the node classification is still challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. To address the issues, this paper proposes a novel Mixup method, called iGraphMix, tailored to node classification. Our method generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. The virtual graphs generated by iGraphMix serve as inputs for graph neural networks (GNNs) training, thereby facilitating its easy application to various GNNs and enabling effective combination with other augmentation methods. We mathematically prove that training GNNs with iGraphMix leads to better generalization performance compared to that without augmentation, and our experiments support the theoretical findings.","This paper proposed iGraphMix that addresses the irregularity and alignment issues of Input Mixup on node classification. Specifically, to address the two issues, iGraphMix does not only interpolate node features and labels but also aggregates the sampled neighboring nodes. Theoretical analysis of the generalization gap and related experiments on the real-world graphs showed that the proposed method is effective in regularizing GNNs by generating diverse virtual samples and preserving high usability and versatility. 1. The paper is well organized and theoretical.
2. The proposed method iGraphMix is simple but effective. 1. In Section 5 THEORETICAL ANALYSIS, the author mentioned that “Citeseer dataset contains only 1.71% connected edges of labeled nodes out of all edges”, but the data “1.71%” lacks of related references.
2. Considering iGraphMix that the essence of iGraphMix is to implement a mixed strategy for features, labels and adjacency matrix respectively, however, the experiment content lacks the ablation experiment for these three components. It would be better to add related ablation experiments to examine the effect of these three components. From the perspective of time complexity, how does the time cost of iGraphMix compare with other augmentation methods? Can you add a diagram to show it?",198,0,4,0.7678,0.1084374999999999,0.8756368756,51,29.433,0.1901,iclr,0.0,2,4,2,2,partially factual,3,2,50,polite,3,positive,3,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,4,4,3,4,partially factual,3,3,78,polite,5,positive,4,low
95,Reviewer-jG7C,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","This paper proposes a novel semi-supervised graph classification method that combines GCN modules with graph kernels, resulting in a model with fewer hyperparameters. Experiments on seven benchmark datasets demonstrate its effectiveness compared to various baselines, including supervised GCNs and graph contrastive learning."" - Graph classification is a very fundamental problem for graph-related problems, and exploring semi-supervised graph classification is a very interesting topic.
- The paper is well-organized and easy to be understood. - The introduction of the graph kernel concept in semi-supervised graph classification methods is not a novel idea, and it has been mentioned in many previous studies \[1-3\]. However, the authors have not referred to it or provided a detailed comparison, and I strongly recommend that they compare and discuss their work in relation to these existing studies.
- It seems that the graph kernel in the paper is not learnable, which results in the quality of the supergraph construction being entirely dependent on the learned node representations and the chosen threshold. Turning the graph kernel into a learnable component could be a better approach.
- The model is evaluated only on small datasets and doesn't know the scalability on large-scale datasets.
- This task also has several highly relevant works, which the authors have not mentioned or compared to in their paper. To ensure the novelty of their method and the superiority of its results, it is advisable for the authors to provide supplementary comparisons and engage in a detailed discussion. \[4-6\].

\[1\] KGNN: Harnessing Kernel-based Networks for Semi-supervised Graph Classification. WSDM 2022

\[2\] TGNN: A Joint Semi-supervised Framework for Graph-level Classification. IJCAI 2022

\[3\] GHNN: Graph Harmonic Neural Networks for Semi-supervised Graph-level Classification. Neural Networks 2022

\[4\] DualGraph: Improving Semi-supervised Graph Classification via Dual Contrastive Learning. ICDE 2022

\[5\] Active and Semi-supervised Graph Neural Networks for Graph Classification. TBD 2022

\[6\] Focus on Informative Graphs! Semi-Supervised Active Learning for Graph-Level Classification. 2023 The novelty of the paper and the absence of important baselines are the two most critical factors affecting the quality of the article. I recommend that the authors make significant revisions.",348,6,2,0.7203,0.2028409091,0.9569439292,51,33.1417,0.2025,iclr,0.0,4,4,3,4,partially factual,4,3,75,polite,4,neutral,3,low,5,5,5,5,factual,5,5,88,polite,5,neutral,5,low,1.0,4.0,4.0,2.0,partially factual,1.0,2.0,60.0,polite,3.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
95,Reviewer-2nwv,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","The paper presented a semi-supervised method for graph classification. The proposed model is composed of two GCNs, one is for individual graphs and the other is for a super graph of all graphs, where the super graph is constructed by a graph kernel. The proposed method is compared with its competitors such as graph contrastive learning on benchmark datasets, where different labeling rates have been considered. 1. The problem studied in the paper, namely graph-level semi-supervised learning with scarce labels, is an important and challenging problem. 
2. The proposed method is based on a double-level GCN model, which has two GCNs. The first one performs graph convolution for each graph and the second one performs graph convolution for a global graph defined (by graph kernel) over all the graphs. This idea is very novel and appealing.
3. The proposed method is compared with state-of-the-art methods such as SimGRACE and GLA as well as classical methods such as GCN and WL kernel. It has competitive performance.
4. The proposed method is simple and easy to implement. 1. The authors claimed that their method has fewer hyperparameters but they did not provide specific comparison with other methods such as GLA in terms of the number of hyperparameters. 
2. The similarity graph among graphs is constructed by a graph kernel such as WL-subtree kernel and there are two different post-processing method for $\mathcal{K}$. it is not clear which one is better and which one was used in the experiments. 
3. The writing can be further improved. 1. At the beginning of Section 3.1, $\mathbf{S}$ is a binary matrix. However, in Section 3.3, the kernel matrix given by a graph kernel may not be binary or sparse. Do the sparsification and binarization have a significant impact on the performance of the proposed method? 
2. In Section 4.2, the authors set $d=d’=64$. Is this the best setting? How do $d$ and $d’$ as well as $d’’$ influence the classification accuracy?
3. What are the numbers of layers in the two GNNs in the experiments? Does the depth matter?
4. In Figure 2, the two post-processing methods for the global kernel matrix are compared. It seems that the one related to $c$ is better than the one related to $\tau$. I wonder if the authors reported the results of the method related to $c$ in Tables 2, 3, and 
5. It is not clear why the authors did not include the results of larger labeling rates such as 30% or 50%.
6. Are their any time cost comparison?
7. In Table 4, it seems that the performance of graphlet sampling kernel is always the worst. I suggest the authors discuss the difference between graphlet sampling kernel and other kernels.
8. It is necessary to compare the number of hyperperameters of the proposed method with those of the baselines. In the proposed method, one has to determine $c$ or $\tau$, which affect the classification performance.",488,0,14,0.7129000000000001,0.0987179487,0.960095048,51,60.5127,0.1507,iclr,0.0,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,5,partially factual,5,5,88,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
95,Reviewer-Qkvr,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","- The paper studies the problem of graph classification with scarce labels. The authors propose a semi-supervised graph classification method called KDGCN, which consists of two GCN modules. The first GCN module obtains feature vectors for each graph through a readout operation. Then, the authors construct a supergraph using graph kernels. The second GCN module employs a semi-supervised approach to learn meta-node representations on the supergraph, capturing sufficient structural information from both labeled and unlabeled graphs. Typically, semi-supervised methods based on graph contrastive learning result in complex models and intricate hyperparameter-tuning. However, the method proposed by the authors has fewer hyperparameters and is easy to implement. - The paper is overall easy to understand.
- The idea of constructing a supergraph is novel and interesting.
- When graph labels are extremely scarce, the proposed method has shown some improvements on certain datasets. - The section about supergraph construction mentions using a predefined similarity threshold (τ) to determine the existence of edges, but it does not explain how to select this threshold.
- While the experiments demonstrate that the WL subtree kernel performs well in certain cases, should the paper provide a more detailed comparison and analysis to explain why this kernel was chosen over other possible kernels? - Can more information be provided to explain the structure and properties of the supergraph and how it impacts the method's performance?
- I am concerned about the limitations of the proposed method and its potential application scenarios. Additionally, is the complexity of the proposed method scalable on large datasets?",257,0,0,0.7852,0.1634920635,0.9249551296,51,34.3764,0.063,iclr,0.0,3,4,3,3,partially factual,3,3,70,polite,4,neutral,4,moderate,4,5,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,4,4,85,polite,5,positive,4,low
95,Reviewer-mRm5,KDGCN: A Kernel-based Double-level Graph Convolution Network for Semi-supervised Graph Classification with Scarce Labels,"Graph classification, which is significant in various fields, often faces the challenge of label scarcity. Under such a scenario, supervised methods based on graph neural networks do not perform well because they only utilize information from labeled data. Meanwhile, semi-supervised methods based on graph contrastive learning often yield complex models as well as elaborate hyperparameter-tuning. In this work, we present a novel semi-supervised graph classification method, which combines GCN modules with graph kernels such as Weisfeiler-Lehman subtree kernel. First, we use a GCN module as well as a readout operation to attain a graph feature vector for each graph in the dataset. Then, we view the graphs as meta-nodes of a supergraph constructed by a graph kernel among graphs. Finally, we use another GCN module, whose inputs are the graph feature vectors, to learn meta-node representations over the supergraph in a semi-supervised manner. Note that the two GCN modules are optimized jointly. Compared to contrastive learning based semi-supervised graph classification methods, our method has fewer hyperparameters and is easier to implement. Experiments on seven benchmark datasets demonstrate the effectiveness of our method in comparison to many baselines including supervised GCNs, label propagation, graph contrastive learning, etc.","This paper views graphs as meta-nodes and constructs a super graph, which then enables semi-supervised graph classification learning, akin to semi-supervised node classification learning. Specifically:

1. First, a GNN is used to learn a representation for each graph, serving as the initial node representation of the supergraph,
2. Next, the WL kernel is employed to determine the similarity between graphs, forming the edges of the supergraph,
3. Finally, another GNN is used for semi-supervised learning on the supergraph.

The experiments implied that this method can achieve SOTA or comparable to SOTA results on several datasets. 1. Compared to other methods based on contrastive learning, utilizing a supergraph for semi-supervised learning eliminates the need to construct negative samples, simplifying the whole framework.

2. It achieves SOTA results on smaller datasets and comes close to SOTA on medium-sized datasets. 1. The datasets used for experiments are relatively small, and it seems that the advantages are not as pronounced on larger datasets, necessitating validation on larger datasets.

2. A comparison is needed with the following two papers:

    \[1\]. **Few-Shot Learning on Graphs via Super-Classes based on Graph Spectral Measures**

    \[2\]. **PRODIGY: Enabling In-context Learning Over Graphs**

In paper \[a\], a supergraph is constructed for Few-Shot graph classification, while in paper \[b\], a supergraph is built for In-context few-shot node and *edge classification*. 1. This paper mentions that the two GCNs are optimized jointly, implying that during training, all graphs in the dataset must be inputted into the hardware simultaneously. Does this limit the model's ability to be trained on large-scale datasets?

2. If KDGCN only supports the Transductive setting, while the compared methods MVGRL, SimGRACE, and GLA can support the Inductive setting?

3. If it is the Transductive setting, must the entire dataset be inferred together during inference? Please describe the inference budget, including platform, memory usage, and inference time.

4. Is this paper the first to perform semi-supervised graph classification by constructing a supergraph? The core innovative point of the article needs to be re-emphasized.",333,2,9,0.7496,0.0476851852,0.9002113938,51,36.8953,0.1431,iclr,0.0109890109890109,4,4,4,4,factual,4,4,75,polite,3,neutral,4,low,4,4,4,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,neutral,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
7,Reviewer-Bu9r,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper proposed a novel search algorithm, PTSA, to improve the search efficiency of MCTS. Empirical results show that PTSA can be integrated with Sampled MuZero and Gumbel MuZero and can reduce the original branching factor by 10% up to 45%. The proposed PTSA algorithm can reduce the branching factor of MCTS and improve the computational efficiency of MCTS-based algorithms. The authors also provide both theoretical and empirical analyses. * The author claims that the proposed method can reduce the branching factor by 10% up to 45%. However, the result is based on only five Atari games. Based on Figure 3, the aggregation percentage varies across different Atari games. Can these five games represent all Atari-57 games? It would be more convincing to run more Atari games to support the claims.

* Moreover, it is unclear for the aggregation percentage on control tasks and Gomoku experiments. Without these experiments, it is inappropriate to claim “reduce branching factor by 10% up to 45%”.

* The time complexity of the proposed approach is higher than the original MCTS. It is unclear whether PTSAZero will still improve its efficiency when running under a larger simulation number. Currently, the authors only run “PTSAZero N=18” in Atari experiments. Will “PTSAZero N=30” perform better than “PTSAZero N=18”?

* Besides, in the board games such as Gomoku or Go, it is common to run large simulation numbers such as N=400 or N=800 during evaluation. It would be better to provide additional experiments/analyses to demonstrate the scale-up ability for PTSAZero. For example, providing the aggregation percentage/time usage/strength when using different simulation numbers. * In Algorithm 1, line 15, if $b_i$ and $b_s$ have different lengths, will their $\phi_{Q_{\alpha}^{psi}}(b)$ be different? In addition, what is the definition for $\phi_{Q_{\alpha}^{psi}}(b)$? Definition 4.3 only shows the probability. 
* In Algorithm 1, line 17, $v_0$ is root node and $b_j$ is a selection path. what does $v_0$.prunning($b_j$) mean?
* In Figure 2, will PTSA get better performance when using a larger simulation (N=30)? Current experiments only used N=18. It would be better to add another experiment with a larger simulation to show the scale-up ability of PTSA.
* In the Gomoku experiment, what does the expert opponent stand for? How many simulations are used in the Gomoku evaluation? As Gomoku is a two-player game, why not compare PTSAZero to other methods directly?
* line 302: “The winning rates of different methods w.r.t. training time are shown in Figure 4”. Should the range of the win rate be between 0 and 1 in Figure 4?
* In Figure 3, it seems that the aggregation percentage varies across different Atari games. Which type of game may have a higher aggregation percentage? Why do you choose these games? Can these five games represent Atari-57 games? Do you have more experiments on other Atari games?
* In Atari experiments, “As Gumbel MuZero does not require large simulations for Atari and control tasks”. In fact, Gumbel MuZero improves training efficiency by only using N=2 in Pacman, and the result is comparable to N=50. It would be more convincing to add additional experiments to compare the training efficiency between “Gumbel MuZero N=2” and “PTSAGZero N=2“ in Atari experiments.
* In Figure 2 (f),  the label of the green curve is “MuZero N=50”, should it be “MuZero N=30”?
* Line 17, typo: Muzero -> MuZero.
* Figure 2, typo: state-of-art -> state-of-the-art.
* Figure 3 is shown after Figure 4. Please fix the order of these figures. The authors have addressed the limitations in the paper.",587,0,3,0.7271000000000001,0.1440848214,0.8432080150000001,216,50.2894,0.0822,neurips,0.0,4,4,4,3,factual,3,4,80,neutral,4,negative,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
7,Reviewer-GnyW,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","The paper proposes a novel tree state abstraction function (PTSA) for use during MCTS. The primary contributions of the paper are algorithmic and empirical. The key idea involves aggregating paths in the tree if their Q-values (as probabilities) along the path closely match an existing path with the same parent node. An analysis of the abstraction quality and error bounds are included. Experiments on Atari and Gym environments show that a recent MCTS variant leveraging PTSA outperforms a number of strong baselines.

UPDATE: I thank the authors for their detailed response. After reading the other reviews and comments, I'm more inclined to recommend acceptance and have updated my score to reflect that. + The paper tackles an important problem of accelerating MCTS search. It does so using tree state abstraction. The approach is intuitively clear and is also supported by analysis. 

+ The paper proposes a novel tree state abstraction function based on path transitivity. The abstraction function is based on the difference in the Q values of the nodes (converted to probabilities) in the path. Although important implementation details are not clear to me, the intuition that abstracting entire paths accelerates search makes sense as does the abstraction of only the most recent path during search leading to smaller trees during online search. The paper is accompanied by analysis showing the correctness of the approach and an error bound under certain conditions. Overall, the algorithm seems to have high novelty.

+ The experiments are conducted on a number of Atari and Gym environments. Sampled MuZero with the proposed abstraction (PTSA) outperforms a number of strong baselines by a significant margin. The implementation seems to work very well. This seems to be a new state of the art in state abstraction for modern MCTS variants. - The approach is intuitively clear and seems to perform well empirically, which increases confidence. However, I found the description of the implementation details of Algorithm 1 difficult to follow. Please consider including a more careful description of the implementation in Section 4. The issue is exacerbated by the absence of code. This is currently preventing me from giving the paper a higher score.
  - For example, the actual implementation of the algorithm in L15 of Algorithm 1 is unclear to me. I expect it to involve Eq 5 with some value of $\alpha$ like 0.7. But Eq 5 returns a real-valued prob estimate for a path pair (b_i, b_s). How is that turned into a boolean value (True / False) in L15? It's probably not real-valued equality. This is a key detail so please explain.
  - There are a number of other implementation details that are difficult to find or missing. See the questions for examples.

- Given that the primary contribution is algorithmic and empirical, I'd have hoped to see the source code included. Reproducibility is going to be challenging without it and since this paper seems to establish a new state of the art, I'd encourage the authors to release their code. - I had a number of questions about the exact implementation
  - What exactly is the implementation of the branching condition of L15 of Algorithm 1? How does it relate to Eq 5 and 6 which is defined as a function taking two inputs (b_i, b_j) and returning a probability?
  - What exactly is learned during offline learning vs online search? $d, v, p$ are likely learned offline. What about the abstraction function $\phi$? This seems online to me. Correct?
  - What is $l$ set to in the implementation? How does it value impact performance?
  - What is the implementation of the pruning function in L17 of Algorithm 1?
  - How are the legal actions for the abstracted state computed?
  - What is the size of $S_L$? How was it chosen? How does varying it affect performance?

- As described in L346-349, there seem to be a number of choices for the designer to make. These are not clear to me at the moment besides the obvious ones (e.g., $\alpha, N$). Please enumerate what exactly needs to be hand-designed or set manually for a given domain and what can be used off-the-shelf.

- Is there a reason to not include code? Will code be included in the final version? Yes",710,0,1,0.7654000000000001,0.1208551788,0.9153474569,216,53.4069,0.8231,neurips,0.0,5,5,4,5,factual,5,4,90,polite,5,positive,4,low,5,5,5,5,factual,5,5,85,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
7,Reviewer-GJR1,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","To accelerate MCTS, the paper proposed a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. 
They define states that are similar by using path transitivity and claim that such a method can have fewer mistakes. According to the results of Atari and Gomoku, the method can be 10% ~ 45% faster.
 1. The method provides some theoretical guarantee.
2. The experiments take place in many environments.
3. The ablation studies have tried many abstraction functions. 
 1. The intuition of the paper is weird.  The method required of all states on the paths needs to be similar. However, there are two problems here. First, the V value might be more incorrect at the beginning. Second, even if the V value is correct for the whole path, this method reduces the chance of pruning more nodes. For example, in Atari, agents can reach the exact same state with different paths. Since the environment is MDP, we should merge those two states. 
 
2. It is unknown for the performance when the simulation is higher. The abstract error normally increases when the simulation increase. The method might delete some good paths that can only be identified after numerous simulations.


 1. How do you prune a path from a tree? What will happen to those branches that are on the path?
2. Have you considered abstraction functions that also require the best action should be the same\[1\]?
\[1\] Are AlphaZero-like Agents Robust to Adversarial Perturbations? Stated in the weakness. ",250,2,7,0.7703,0.1869565217,0.9012311697,216,58.8964,0.1509,neurips,0.0,1,3,2,1,factual,1,1,40,impolite,1,negative,2,high,4,4,4,4,partially factual,5,5,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
7,Reviewer-CrLf,Accelerating Monte Carlo Tree Search with Probability Tree State Abstraction,"Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have achieved superhuman performance in many challenging tasks. However, the computational complexity of MCTS-based algorithms is influenced by the size of the search space. To address this issue, we propose a novel probability tree state abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A general tree state abstraction with path transitivity is defined. In addition, the probability tree state abstraction is proposed for fewer mistakes during the aggregation step. Furthermore, the theoretical guarantees of the transitivity and aggregation error bound are justified. To evaluate the effectiveness of the PTSA algorithm, we integrate it with state-of-the-art MCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental results on different tasks demonstrate that our method can accelerate the training process of state-of-the-art algorithms with 10%-45% search space reduction.","This paper suggests a method of abstracting the state space explored by a Monte Carlo Tree Search (MCTS) algorithm, in order to reduce the complexity of the search. We can create an abstraction of the state space for MCTS by considering an abstraction over entire paths in the tree - two paths of equal length, that start from the same node in the tree, can be aggregated into a single abstract state, thus reducing the search space. The paper proposes to use a probabilistic approach to the abstraction process, using the justification that this enables the algorithm to recover from aggregation errors that it commits early on. The specific probabilistic approach discussed relies on a divergence measure between the distribution of the value functions across the entire two paths, thus merging together with high probability actions that lead to similar distributions of the value function. This abstraction helps mitigate the worst weakness of MCTS - it reduces the large search space. Some theoretical guarantees are provided, as well as several experimental results for different game problems and for control tasks.  The paper deals with the very important challenge of improving MCTS techniques. This type of research is applicable in many domains, as this is a well known and well used algorithm. 

The experimental results looks extensive and well-presented, and are the main strength of the paper. I especially liked the comparison of different state abstraction functions, as it showcases the contribution of the paper in coming up with a specific one that seems to work well. Adding a successful novel approach to a well established algorithm is not a trivial task, and experimental results seem very promising. This seems like a strong enough reason to publish on its own.  I thought the main weakness of the paper is its readability. I had a tough time understanding the approach and the logic behind it, even though I have some experience with MCTS specifically (though admittedly, it had been awhile). Some more careful attention can be given to notation and explanations. The math in this paper requires close scrutiny and some of the explanations seem to assume a close familiarity with the specifics of MCTS, as well as state abstraction functions. This results in a reduced reading experience and lower comprehension. 
Some examples: 
1. In equation (1) Q is never explicitly defined, figure 1 appears much earlier in the paper than the definition of the probability state abstraction 
2. The complex distinction between paths, states and nodes is not explicitly stated, and sometimes ignored - table 1 is referenced during the general RL discussion that has a state notation (s1, s2) but uses a different notation, that is later used for nodes (v1, v2). 
3. Some of the notation within Algorithm 1 is never explained (e.g., actions like pruning/delete/add and the usage of a hidden state h). 
4. Q* usage is never explained 
5. In the explanation after definition 4.3 - encourages nodes that have the same candidate actions with similar value distribution expectations to be aggregated - should that be be encourages paths ? The entire definition seems to be about aggregating paths instead of specific states, but paths that start from the same node. 

It is fine to delegate some details to referenced works, but a paper should at least succinctly explain its own notations and be as self-contained as possible. I trust these weaknesses in explanations and paper organization can be fixed by the authors.  1. Are you planning to publish the code you used? 
2. Please check your math for some typos - eq. 19 in appendix A.
 Some limitations are briefly addressed, namely the need for hyper-parameter tuning and manually selecting the underlying abstraction function. I believe another limitation may lie in the added computational complexity of this method. ",632,0,5,0.788,0.0713583639,0.8781546950000001,216,44.8067,0.4553,neurips,0.0095238095238094,4,4,4,4,factual,4,4,80,neutral,4,neutral,3,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,2.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,4,3,4,4,factual,5,4,85,polite,5,positive,5,low,3,2,4,4,partially factual,4,4,75,polite,5,neutral,3,low
102,Reviewer-6dqz,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","The paper proposes distilling convolutional models for autoregressive sequence generation into recurrent (state-space) models. A key limitation of recently proposed convolutional models is that they use convolutional filters that extend potentially infinitely into the past, and the same techniques that yield efficient training do not transfer over to token-by-token generation. This paper proposes a post-training and data-free distillation step that replaces such convolutional models with a recurrent approach that uses constant time and memory at each step of generation during inference. The key strength of the paper is that proposes a novel and theoretically grounded distillation method, which achieves good approximation bounds without being tied to specific data or involving what amounts to an additional round of training.

Also, convolutional and state space models as a whole are an underexplored area in recent work when compared to Transformers and attention, and this paper puts forward a novel method of linking the two, both of which contribute to the originality of the paper. My biggest reservation based on my understanding of the paper is that I didn't get a qualitative sense of what might be lost as part of the distillation process. Is there some intuition of what is the worst-case qualitative behavior of a convolutional filter that can't be tightly approximated with LaughingHyena?

This question comes to mind because in the world of Transformers, it is no secret that most of the computational power spent on quadractic attention goes to waste. Just a small subset of the efficiency work there includes pruning entire heads, limiting each head to a head-specific attention context window, or even doing sliding-window attention with a fixed context length for the entire model. More in line with the present paper, work like Performer has developed an approximation for converting attention-based models into recurrent models -- at a cost. Notwithstanding the theory of the tightness of that last approximation, and equivalent performance of many to the Transformer that is demonstrated in some of the papers introducing these methods, there inevitably arises some situation where none of the approximations match the Transformer in quality. I worry that the present approach might fall into a similar pattern. A recurring theme in this area is that any method that sacrifices the ability to have long context, or to perform associative recall, is suspect. That's why when it comes to these approximations of convolutions, it would helpful to know whether the approximation is effectively some form of context-truncation in disguise, and if not what the qualitative cost is. How well does the LaughingHyena architecture perform on the associative recall task, especially as compared to Hyena (or MultiHyena)? Is the point beyond which the models fail to perform the task (in terms of sequence length or vocabulary size) different between the two?

For LM-Eval-Harness and HELM, have you tried a baseline of taking the impulse response from Hyena/MultiHyena and truncating it to a finite impulse response? A sliding window seems like one of the simplest approximations to try in the world of convolutions, and it would be helpful to know if maybe some defect of the tasks or the base model results in nothing more being required. This would, in fact, be a useful baseline to have in the paper. The paper could be improved with a little bit of additional discussion regarding limitations of the distillation/approximation.",555,0,0,0.7892,0.0370982143,0.8943858147,215,29.2188,0.1199,neurips,0.0119047619047618,4,4,4,4,factual,4,3,79,polite,4,positive,4,low,4,4,5,5,factual,5,5,88,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
102,Reviewer-wEPT,Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions,"Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.","This paper proposes LaughingHyena - an improvement to the Hyena model that can perform long-convolutions instead of attentions in transformers to avoid the quadratic scaling issues. One of the issues with the convolution sequence models is that they incur significant cost due to autoregressive inference. To avoid this, this paper seeks to come up with a techinique to have constant memory recurrent inference to increase generation thoroughput. This is achieved using the use of compact linear SSM and weight tying the filters across heads in Hyena architecture. The resulting performance improvements are impressive - 1.5x throughput improvement compared to Hyena. The model also achieves SOTA in the PILE dataset. - The perplexity on small-scale models on Table 1 and Table 2 outperform GPT, Hyena and establishes a new SOTA.
- The peak memory usage is also constant for different sequence lengths in Table 5.4 - The writing is a bit hard to follow.
- The performance of the model is still lacking compared to full transformer baseline such as Pythia in Table 5.3. Can the authors comment on this? Any idea on how much the performance degradation will be on very large scale models and datasets? - What assumptions do you use for the state-space model in Eq 3.1 to yield a good distillation results (d<<L)?
 I think the writing can be improved to provide a simple explanation of the method for readers who don't have a strong understanding of state-space models. Else, the text is hard to follow.",249,0,0,0.7824,0.1728084416,0.9072981477,215,51.1531,0.195,neurips,0.0,4,4,3,4,factual,4,4,85,polite,4,positive,3,low,4,3,4,4,factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,3,4,3,factual,4,4,75,polite,5,positive,4,low,2,2,3,4,partially factual,3,3,65,polite,4,neutral,4,low
82,Reviewer-2f9g,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper initiates the study of stochastic optimization with oblivious noise that might be biased and have unbounded variance, which is a generalization of the heavy-tailed noise setup. The key assumption regarding the oblivious noise is that it assumes a value of $0$ with a probability within the range of $0 \leq \alpha \leq 1$, which can be interpreted as the fraction of inliers. Notably, when $\alpha \leq 1/2$, it is proven to be information-theoretically impossible to find an approximate stationary point of the function. To address this challenge, the authors incorporate the concept of list-decoding into the framework of stochastic optimization, and focus instead on identifying a list of points where at least one of them is an approximate stationary point.

Technically, this paper presents an equivalence between list-decodable stochastic optimization with oblivious noise and list-decodable mean estimation problem leveraging a technique known as noisy location estimation. The analysis of list-decodable stochastic optimization with oblivious noise is conducted by examining the list-decodable mean estimation problem. This paper investigates an important setting of stochastic optimization introduces a fresh perspective by introducing the concept of list-decodable stochastic optimization. The definition of this new framework is not only intuitive and well-motivated by practical problems but also exhibits elegance from a theoretical standpoint, given how weak the assumptions on the noise model are. The algorithms presented in the paper, along with their corresponding proofs, are intricate and highly nontrivial from a technical standpoint. Nevertheless, the authors have succeeded in presenting the analyses in a well-organized manner, ensuring that they are generally not hard to follow. 1. As pointed out in the paper, an exponential dependence on $1/\eta$ is necessary in the list-size, which can mildly impact the overall appeal of the results. This dependency may introduce some considerations regarding scalability and practicality.

2. The framework presented in this paper is inherently abstract, and there is a lack of clarity concerning the algorithm's performance in concrete examples, including scenarios with more specific theoretical settings that incorporate additional assumptions, as well as practical problem domains. Correspondingly, I have the following questions that could possibly make the results even stronger if addressed:
1. Can the exponential dependence on $\eta$ be mitigated by making slight adjustments to the original definition of list-decodable stochastic optimization? For instance, are there additional assumptions that can be incorporated or specific parameter regimes that can be adjusted to reduce this exponential dependency?

2. Are there more concrete applications of list-decodable stochastic optimization methods? 

3. Minor comment: I saw the term ""convex"" in the caption of Algorithm 2. I assume this is a typo and convexity is not needed in the proof, right? Not relevant in my opinion.",445,0,4,0.7856000000000001,0.0264697571,0.920696795,215,16.5721,0.2025,neurips,0.034090909090909,4,5,4,4,factual,4,4,90,polite,4,neutral,4,low,4,5,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
82,Reviewer-5dvc,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper presents an algorithm for first-order stochastic optimization where the algorithm has access to an oracle that returns a noisy version of the gradient of the objective function. The considered noise model includes two components: A bounded-variance observation noise (which is the typical well-studied type of noise), and oblivious outliers noise $\xi$ satisfying $Pr\[\xi = 0\] >= \alpha$. Furthermore, the distribution of the oblivious noise \xi does not need to be symmetric.

It is shown that if the fraction of inliers is below 1/2, it is information-theoretically impossible to give a unique solution. This is why the authors consider a list-decodable learner where the learner returns a list of solutions, one of which is guaranteed to be good. The authors show that if the fraction of inliers is sufficiently close to 1, then the algorithm can recover a single solution. Designing learning algorithms which are robust against adversarial or semi-adversarial type of noise is very important. The setup that is considered in this paper is original (as far as I can tell). I found the paper to be generally not very well written. The notation is a bit confusing in several places (e.g., check the question regarding line 203 below), and the writing style can be sometimes too informal.

One thing that I found crucially missing is the clear and formal statement of the problem and the clear statements of the assumptions. For example, what are the properties of the function $f(\gamma,x)$? The only property that I found is that $f(x) = E_{\gamma}\[f(\gamma,x)\]$ must be $L$-smooth. However, this is clearly not enough to even guarantee the existence of a stationary point. For example, consider $x\in \mathbb{R}$ (i.e., one dimension) and define $f(\gamma,x) = x$. In this case, we have $f(x) = x$ and hence $\nabla f(x) = 1$ fo all $x$ and there is no stationary point.

Typos:
- Page 4, line 175: ""we can a generate list"" -> ""we can generate a list"" - What are the properties of the function $f(\gamma,x)$ which are needed for the main result to hold?
- Page 4, line 203: Is $\xi$ in $\xi + y' + t$ the same as the $\xi$ in $\xi + y$, or is it an independent instance? It seems from the following discussion that the authors consider an independent instance. If this is the case, please write $\xi' + y' + t$.
- Page 7, line 309: What is $L$? Is it the same as the $L$ of Section 2? But in Section 3 we don't have a parameter $L$ for location estimation.
- There doesn't seem to be a proof for Claim 3.3 (even in the appendices). No concerns regarding potential societal impact of this work.",451,0,0,0.7092,0.0572761905,0.9466682673,215,58.3546,0.2653,neurips,0.0117647058823529,2,4,5,3,factual,3,4,80,polite,4,negative,4,low,4,4,4,4,partially factual,4,4,75,neutral,5,negative,4,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,2,2,3,3,partially factual,3,4,65,polite,4,neutral,4,low
82,Reviewer-M73n,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","The paper considers the problem of stochastic optimization with oblivious noise. Here, one receives noisy gradients of a non-convex function $f(x) = \mathbb{E} \[f(x, \gamma)\]$ (where $\gamma$ is bounded variance observational noise) and the noisy gradient samples are generated as follows $\nabla_x f(x, \gamma) + \xi$ where $\xi$ is generated independently of $\gamma$ and $x$ with the only restriction that $\mathbb{P} (\xi = 0) \geq \alpha$. The paper specifically focuses on the setting where $\alpha \ll 1 / 2$. Under such mild restrictions on $\xi$, it is impossible to recover a single point which is guaranteed to be near-stationary. However, in line with recent results on list-decodable robust estimation, the paper shows that one can recover a list of estimates one of which is approximately stationary. 

Technically, the paper builds on two recent results on robust estimation. The first is SEVER which is a robust stochastic estimation algorithm focusing on the setting when $\alpha \to 1$. In this setting, it is possible to leverage recent robust estimation algorithms to clean the observed gradients and recover a good approximation to the true gradient (up to the degree determined by $1 - \alpha$) and then utilize this approximate gradient to find a stationary point. The second is a recent line of work on list-decodable mean estimation. Here, one receives corrupted samples from a high-dimensional distribution where $\alpha$ fraction of points are from the true distribution and the goal is to estimate its mean. While producing a single estimate is impossible, these algorithms return a list of size $1 / \alpha$, one of which is guaranteed to be accurate. In this paper, the authors essentially extend the SEVER framework to the list-decodable setting. However, this requires some novel technical contributions. A naive implementation would lead to exponential growth in the number of estimates (a list of size $l$ would have $l / \alpha$ many elements in the next iteration if each of its elements were queried and updated with the $1 / \alpha$ resulting gradients). Instead the authors introduce a novel technical tool that they term location estimation which when given samples from $z + \xi$ and $z' + t + \xi$ for some unknown $t$ (and $z$ and $z'$ have bounded covariance) can estimate $t$. With this tool, the algorithm starts by first generating $1/\alpha$ gradients at $0$ and initializing $1 / \alpha$ candidates each corresponding to one of the estimates. Then, each element of the list, $x$, is queried to produce gradient estimates. The location estimate procedure is then run on gradient estimates from $x$ and $0$ to essentially estimate $\nabla f(x) - \nabla f(0)$. Finally, from this each candidate is updated by estimating $\nabla f(x)$ using the particular estimate of $\nabla f(0)$ it corresponds to.

Overall, this is a really nice paper studying an interesting problem. My one concern is in the assumptions made in the paper. For instance, the assumptions don't capture the canonical estimation problem of list-decodable mean estimation. Here, one assumes that the true distribution has covariance bounded in spectral norm whereas this paper essentially assumes a bound on the expected squared length of a data point which could be larger by a factor of $d$. It would be interesting to see if these results could be extended to setting with weaker assumptions. Can we obtain similar results with $\mathbb{E} \[(\nabla f(x, \gamma) - \nabla f(x)) (\nabla f(x, \gamma) - \nabla f(x))^\top\] \prec \sigma^2 I$ as opposed to the stronger assumption in this paper of $\mathbb{E} \[\|\nabla f(x, \gamma) - \nabla f(x)\|^2\] \prec \sigma^2$ used here.
 See main review See main review See main review Yes",599,0,1,0.7426,0.0490403304,0.9423602819,215,39.8348,0.0548,neurips,0.0,1,3,2,3,factual,3,2,60,polite,3,positive,3,high,4,4,4,5,factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
82,Reviewer-FMyo,First Order Stochastic Optimization with Oblivious Noise,"We initiate the study of stochastic optimization with oblivious noise, broadly generalizing the standard heavy-tailed noise setup.
In our setting, in addition to random observation noise, the stochastic gradient 
may be subject to independent \emph{oblivious noise}, 
which may not have bounded moments and is not necessarily centered. 
Specifically, we assume access to a noisy oracle for the stochastic gradient of $f$ 
at $x$,  which returns a vector $\nabla f(\gamma, x) + \xi$, where $\gamma$ is 
the  bounded variance observation noise 
and $\xi$ is the oblivious noise that is independent of $\gamma$ and $x$. 
The only assumption we make on the oblivious noise $\xi$ 
is that $\Pr[\xi = 0] \ge \alpha$, for some $\alpha \in (0, 1)$.
In this setting, it is not information-theoretically possible to recover a single solution 
close to the target when the fraction of inliers $\alpha$ is less than $1/2$. 
Our main result is an efficient {\em list-decodable} learner that recovers 
a small list of candidates at least one of which is close to the true solution. 
On the other hand, if $\alpha = 1-\epsilon$, where $0< \epsilon < 1/2$ is sufficiently small
constant, the algorithm recovers a single solution.

Along the way, we develop a rejection-sampling-based algorithm to perform noisy location estimation, 
which may be of independent interest.","This paper introduces a new setup for stochastic optimization, where in addition to random observation noise, the stochastic gradient may be subject to independent oblivious noise. This noise might not have bounded moments and isn't necessarily centered. The authors propose a Noisy Location Estimation approach that estimates the gradient difference between two points, specifically \nabla f(x_t)-\nabla f(x_0). As such, they maintain robust estimations of the gradient at all points {x_t} as long as there is a reliable estimation of \nabla f(x_0). The new setup for oblivious noise introduced in the work is plausible, and the authors effectively discuss its relation to existing research. The Noisy Location Estimation proposed by the authors provides an innovative way to estimate gradient differences accurately, reducing the stochastic optimization problem to a mean estimation problem, which seems simpler in the setting. Also, as shown by the authors, the reverse of the reduction holds by simple arguments. The paper's presentation, particularly in the technical sections, lacks clarity.

1. Definitions should be more precise and self-contained. For instance, the work seems to require that oblivious noise be independent of the noisy gradient, but Definition 1.1 doesn't explicitly state this. In Definition 1.3, phrases like ""sufficiently small constant"" are too vague.
2. The methodology for mean estimation (Fact 2.1), isn't discussed in the main body. A brief discussion may be helpful.
3. The ""Rejection Sampling"" discussion on page 5 is difficult to follow and potentially misleading. From my understanding, the core intuition is to identify a large enough domain of size $i$, such that $i$ times the conditional expectation is robust and stable upon shifting the domain. 1. In Line 227, it appears that \[i- 4 · 12, i + 4 · 12) almost fully contains \[i - 4 · 12, -i + 4 · 12)\]. If that's the case, why is there a need for a \cup operation?
2. In Section 5, why is the exponential dependence on 1/\eta for the size of the list unavoidable?
3. Is it a requirement for the Noisy Location Estimation that alpha > 0? I didn't delve into the proof details, but it seems that if you're considering the conditional expectation, it might not require alpha > 0. Can you clarify this? As discussed in Weakness.",375,0,6,0.8073,0.0376226551,0.9388657212,215,44.5033,0.3634,neurips,0.0246913580246913,1,3,2,2,partially factual,3,2,55,neutral,3,neutral,3,high,4,3,3,4,factual,4,4,70,polite,4,neutral,4,moderate,2.0,3.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,4,low,3,2,3,4,partially factual,3,4,65,polite,4,neutral,4,moderate
153,Kevin-Garey,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,1,3,0,1,factual,2,2,20,neutral,2,neutral,2,extreme,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,2,0,0,factual,1,1,20,polite,3,neutral,0,extreme
153,Glen-Tillotson,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,0,0,0,0,unfactual,0,0,0,neutral,0,neutral,0,extreme,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,3,0,0,factual,1,1,20,polite,1,neutral,0,extreme
153,Vincent-Young,Recent Advances in the Diagnosis and Treatment of Clostridium Difficile Infection,"Clostridium difficile infection (CDI) has become the most frequently reported health care-associated infection in the United States [1]. As the incidence of CDI rises, so too does the burden it produces on health care and society. In an attempt to decrease the burden of CDI and provide the best outcomes for patients affected by CDI, there have been many recent advancements in the understanding, diagnosis, and management of CDI. In this article, we review the current recommendations regarding CDI testing and treatment strategies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions",67,0,0,0.8271000000000001,0.0641666667,0.5431773663,0,3.63,0.0999,f1000,0.0,1,3,2,2,unfactual,3,2,50,neutral,3,positive,1,high,0,1,0,0,factual,0,0,10,neutral,0,neutral,0,extreme,4.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,0,0,0,0,factual,0,0,0,neutral,0,positive,0,extreme,0,3,0,0,factual,1,1,20,polite,1,neutral,0,extreme
70,Sarah-Elizabeth-West,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The abstract results section could be clearer, in particular the sentence starting ‘ Even remembering’. I think it would be useful in the introduction to give the geographical context for your work, and figures about the size of the environmental volunteering sector in that country. I assumed UK, and it seems like the bulk of responses were from the UK, but I note that your survey was completed by people in 11 countries. It also needs some definition of environmental volunteering I think. I guess this includes things like practical conservation, environmental CS surveys, but what about someone delivering leaflets promoting Friends of the Earth activities for example? This example highlights why definition is important. And in your results, you talk about Biodiversity monitoring volunteers – is this your definition of environmental volunteers? Some justification of why PERMA was used as opposed to other multidimensional well-being measures would be useful. Some more info on why managers’ perceptions of their volunteers’ motivations is important is needed, I think this is missing. ‘Worldwide responses’ – how do you know that any difference in responses is due to the factors you are interested in, not due to the fact that they are in a different part of the world? Some justification for including these (relatively small number of responses) would be useful. The results text is very dense, and it is hard for those not very familiar with factor analysis (like me!) to understand what the key parts of the text are. I guess it’s the bottom of page 9 is it? I think some explanatory text at the beginning of results about what factor analysis is would be helpful. The 'External factors and volunteer well-being' section is clearer as you’ve said what the results are and then gone into the detail of how you came to that result, and means that people who are not au fait with statistics (as I guess will be many of your readers) can skip over it. Discussion – how did your volunteers and non volunteers compare to others using your well-being index? Or compared to other well-being indices? This would help to give your results more context.  Some of your sentences are a little long which makes them a bit hard to read, for example, the one starting However, this positive…on page 19.  Should your figures be in the discussion section, or would they be better placed in the results? It breaks the text up a bit too much I feel.",482,0,0,0.7688,0.0870438856,0.9067310691,13,50.36,0.1733,f1000,0.0194174757281553,3,5,4,2,partially factual,4,4,65,polite,4,positive,3,low,5,4,4,5,factual,5,5,88,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,3,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,3,low
70,Sabine-Pahl,Environmental volunteer well-being: Managers’ perception and actual well-being of volunteers,"Background: Environmental volunteering can increase well-being, but environmental volunteer well-being has rarely been compared to participant well-being associated with other types of volunteering or nature-based activities. This paper aims to use a multidimensional approach to well-being to explore the immediately experienced and later remembered well-being of environmental volunteers and to compare this to the increased well-being of participants in other types of nature-based activities and volunteering. Furthermore, it aims to compare volunteer managers’ perceptions of their volunteers’ well-being with the self-reported well-being of the volunteers. Methods: Onsite surveys were conducted of practical conservation and biodiversity monitoring volunteers, as well as their control groups (walkers and fieldwork students, respectively), to measure general well-being before their nature-based activity and activity-related well-being immediately after their activity. Online surveys of current, former and potential volunteers and volunteer managers measured remembered volunteering-related well-being and managers’ perceptions of their volunteers’ well-being. Data were analysed based on Seligman’s multidimensional PERMA (‘positive emotion’, ‘engagement’, ‘positive relationship’, ‘meaning’, ‘achievement’) model of well-being. Factor analysis recovered three of the five PERMA elements, ‘engagement’, ‘relationship’ and ‘meaning’, as well as ‘negative emotion’ and ‘health’ as factors. Results: Environmental volunteering significantly improved positive elements and significantly decreased negative elements of participants’ immediate well-being, and it did so more than walking or student fieldwork. Even remembering their volunteering up to six months later, volunteers rated their volunteering-related well-being higher than volunteers rated their well-being generally in life. However, volunteering was not found to have an effect on overall mean well-being generally in life. Volunteer managers did not perceive the significant increase in well-being that volunteers reported. Conclusions: This study showed how environmental volunteering immediately improved participants’ well-being, even more than other nature-based activities. It highlights the benefit of regarding well-being as a multidimensional construct to more systematically understand, support and enhance volunteer well-being.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title and Abstract: This is fine. I have some comments on the comparisons and causality below that the authors should consider.  Article content/ Conclusions: The article is well written and overall clearly structured. Using the PERMA model is a good addition. The specific research questions are very helpful in communicating the research. Nevertheless I have picked up two issues that are worth considering, and a few minor comments.  I wasn’t entirely convinced by the research question looking at managers’ perception of volunteer well-being. Why is it important that these correspond (p. 3)? Even if it is important, as far as I understand, the comparison is not straightforward. The volunteers rated by the managers are not the same people as rate their own well-being, are they? So the conclusion of non-correspondence is problematic, if you’re comparing the rated/perceived well-being of *different* people.  My second issue is around the language/interpretation in the article, specifically in the Implications section. You seem to assume these are causal effects i.e. the volunteering causes people’s improved well-being (and therefore it should be used more widely). But it’s not quite that straightforward, as you haven’t allocated people to different activities so there might be other differences between people who walk vs. volunteer for example, that could account for any effects you find. You can only make strong inferences about causality when you use a proper experimental research design. It would be good to note this in the discussion. (I think only the Wyles et al. article has tried this in the volunteering literature). You mention also that choice is important, which is a related consideration. This is where recommendations are a bit tricky, because you can’t (by definition) force people to ‘volunteer’ even it is good for them, and there may be selection effects that mean happier / healthier people are also the ones who do environmental volunteering. This is not a big problem but I feel should be acknowledged.  Minor points: I think a lot of space is dedicated to the different factor analyses (on pages 7-11) to establish questionnaire structure. While this is important and good practice it is not linked to any of the main research questions. Therefore I was wondering if (some of) this should be presented in an Appendix rather than the main text, as it distracts from the key questions and findings.  On p. 18 literature on the amount of time spent volunteering is reviewed but this all seems to be published in gerontology journals so I’m assuming uses older samples. Please add in the text if that’s the case. Data: Links to raw data are provided.",503,0,0,0.806,0.1353869896,0.8656298518000001,75,46.06,0.1441,f1000,0.0,2,4,3,3,partially factual,3,2,70,neutral,4,neutral,3,low,5,5,4,5,factual,5,5,95,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,3,low
126,Reviewer-qyRc,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","# Problem Statement
The paper addresses the challenge of neural policy learning methods struggling in long-horizon tasks, particularly in open-ended environments with multi-modal observations, such as the game NetHack. It was observed that symbolic agents significantly outperformed neural approaches in the NeurIPS 2021 NetHack Challenge.

# Main Contribution
The paper's main contribution is an extensive study on neural policy learning for NetHack. The authors analyzed the winning symbolic agent and extended its codebase to generate one of the largest available demonstration datasets. They examined the advantages of an action hierarchy, enhancements in neural architecture, and the integration of reinforcement learning with imitation learning. Their investigations resulted in a state-of-the-art neural agent that surpassed previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, they also demonstrated that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.

# Methodology and Experiments

## The Hierarchical HiHack Dataset
The authors create the HiHack dataset, which is a hierarchically-informed version of the NetHack Learning Dataset (NLD-AA), containing 3 billion recorded game transitions from over a hundred thousand games played by the AutoAscend agent.

## Hierarchical Behavioral Cloning
The authors extend the ChaoticDwarvenGPT5 (CDGPT5) model, a top-performing open-source neural model for NetHack, by introducing a hierarchical decoding module. The model consists of three separate encoders for different types of observations and an LSTM core module. The hierarchical extension replaces the linear decoder of the CDGPT5 model with a hierarchical decoder that predicts the strategy label and selects the appropriate low-level MLP for action prediction. The hierarchical LSTM policy and the baseline non-hierarchical LSTM CDGPT5 policy are trained using a simple cross-entropy loss. The results show that the introduction of hierarchy significantly improves the performance of LSTM policies trained with behavioral cloning, yielding a 40% gain over the baseline in mean NLE score and a 50% improvement in median score across seeds. The authors confirm that this improvement is due to hierarchy and not simply a result of the increased parameter count of the hierarchical LSTM policy.

## Architecture and Data Scaling
The authors explored scaling as a potential solution to improve the performance of the model, which was significantly behind the symbolic policy used to generate the HiHack demonstrations. They developed a novel base policy architecture for NetHack that introduces a Transformer module into the previous CDGPT5-based architecture. They also conducted data scaling experiments using subsets of the HiHack dataset to examine the relationship between dataset size and the test-time performance of BC policies. The results showed that both the non-hierarchical and hierarchical variants of the combined transformer-LSTM policy architecture yielded gains, but the larger model performed worse than the smaller one due to overfitting. This suggested that scaling of model capacity alone would not be sufficient to close the neural-symbolic gap. Additionally, brute force scaling of the dataset alone could not viably close the gap to symbolic methods.

## Combining Imitation with Reinforcement Learning
The authors explored combining imitation learning with reinforcement learning (RL) to bridge the performance gap with AutoAscend. They used a combination of behavioral cloning (BC) and asynchronous proximal policy optimization (APPO) for training. The results showed that RL fine-tuning significantly improved the performance of all models. The best-performing approach was APPO + BC using the hierarchical LSTM model, which achieved a new state-of-the-art for neural policies on NLE, surpassing the previous best result by 48% in mean NLE score and 25% in median NLE score. The Transformer-LSTM models performed worse due to their slower training speed and the fixed training time budget. The authors also observed that fine-tuning with RL improved the error-correction capability of models across all model classes compared to their purely offline counterparts. # Originality
The problem is interesting and the approaches are insightful.

# Quality
The analysis and experiments are comprehensive.

# Clarity
The article is overall well written and clear. 1. The current focus of the study is quite narrow, being primarily centered on the application of imitation learning for NetHack, limiting its influence. In the context of mastering the game, while this approach is interesting, it is unlikely to exceed the performance of experts that generate demonstrations, not to mention that the experts are already algorithms that can scale well. Furthermore, NetHack, despite being an excellent game, is somewhat niche and its real-world implications are relatively minimal. The techniques proposed in this study are specifically tailored for this game, which limits their potential for inspiring more universally applicable methods that could have a broader impact.
  - The availability of hierarchical labels is a strong assumption that does not often hold, which further limits the applicability of the proposed methods.

2. Even just for bridging the performance gap between neural models and AutoAscend, there is no promising direction revealed by the work as the various augmenting components seem to contradict each other. 1. When introducing Transformer to augment the capacity of the neural model, why did authors choose the architecture as shown in the article? Specifically, transformers are best known for their NLP and CV capacity, which could make them good replacement for the CNN and MLP encoders.
2. Why do the authors enforce the 48 hour training time cap instead of training all models till convergence? Given that this study does not appear to prioritize data efficiency or training efficiency, the necessity of such a computational time constraint is unclear. It would be beneficial to understand the rationale behind this choice, as it may not directly align with the study's primary objectives. The authors note that possible avenues for future exploration include: (a) methods for increasing the Transformer context length to give the agent a longer memory to aid exploration; (b) addressing the multi-modal nature of the demonstration data (i.e. quite different trajectories can lead to the same reward), which is a potential confounder for BC methods. Some forms of distributional BC (e.g. GAIL, BeT) could help alleviate this issue.

The aforementioned two points do not address the limitations raised in the ""Weakness"" section.",1010,0,4,0.7969,0.0384225531,0.9721859097,232,26.3989,0.0751,neurips,0.0,4,4,5,5,factual,4,4,90,polite,4,neutral,4,none,4,5,5,5,factual,5,5,88,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,5,5,4,factual,5,5,90,polite,5,neutral,5,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
126,Reviewer-PtAe,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","The paper improves the existing solutions in the NetHack Learning Environment (NLE). This is done by taking earlier solutions from a competition around NLE, collecting more data with the best available (symbolic) agent, and using that data to improve a neural only solution. The paper provides experiments with imitation learning (with or without RL tuning), larger models, hierarchical memory setup (LSTM + Transformers) as hierarchical behavioural cloning setup, using labels of the newly collected dataset. While there are improvements, it is still below the demonstrator results, which is then studied by scaling the model sizes and amount data collected. Paper concludes by providing the state of the art results in the task, but also noting that scaling alone is not enough to reach the expert demonstrator level (symbolic agent). - Provides more detailed dataset than the previous works (with hierarchical action labels)
- Sets an interesting premise/task for trying to reach the demonstrators' (AutoHack agent) performance with neural solutions.
- Different ablations to try to answer questions (data/model scaling, model architecture with hierarchy)
- Proposed hierarchical approach to imitate the demonstrator agent. While I enjoyed reading the paper, overall I think the results are interesting or applicable to most of the NeurIPS audience, even in the limited scope. The paper presents many results and provides some explanations for them, but does not verify these explanations with further experiments. I think proper answers to these issues would be insightful to many, and others could then use these insights in their work (e.g., where the trained agent failed to imitate the demonstrator? What was the cause of poorer performance? Why did bigger model perform worse?). Creating such insight in one environment would be sufficient, as by focusing on a single environment, you can create very specific scenarios to tease out these answers. 

- Limited scope of the work: experiments done in a single environment. Most of the paper is framed in a way that this is not a huge issue (e.g., ablations), but proposing new method just for playing NLE has limited impact. If a new method is proposed to generally improve RL/IL performance, it should be tested at least in two distinct environments.
- Limited improvement in the context of SOTA solutions: 2x over the baseline used in the paper with RL and proposed architecture included, but other neural agents in the NetHack Challenge had higher score. To be interesting in terms of performance, it should at least outperform the NetHack Challenge Neural solutions.
- Proposed method is limited in novelty, as evident by the previous work listed in the paper. If the hierarchical BC figured out the hierarchy automatically (or, if it was an emergent behaviour of the model), that would be more interesting.
- Paper outlines some assumptions on why things failed (e.g., ""model overfitted"" or ""learned to self-correct""), but these claims were not verified with results. The paper would be much stronger if you can give solid, verified answer that indeed, overfitting was to blame or that RL trained the model to ""self-correct"". Questions:
1) In multiple occasions paper says that the lower performance of bigger model is due to overfitting (e.g., line 229). However there are no results/experiments to show that this indeed was the case. A simple way to find this out is to do train-validation (or even train/validation/test) split, and testing on held out data as training progresses.
2) Regarding data scaling experiments: did you change any other settings of the training setup when increasing data amount? Previous work has demonstrated that the optimal model size and/or training compute depends on the amount of data (Hoffmann et al. 2020).
3) Regarding model scaling experiments: I assume only the number of layers in the transformer was changed? The bottleneck of the network may be elsewhere, e.g., one of the input layers or output layers. I would recommend scaling the whole network, similar to what OpenAI VPT work did, where ResNet blocks were ""widened"" in terms of filters, as well as increasing transformer size (Baker et al. 2022). Also, Hoffmann et al. (2020) changed number of layers, number of attention heads and transformer dimensionality when scaling models. This might be something you want to try.
4) Instead of LSTM + Transformer model, did you experiment with transformer model only? E.g., akin to VPT work (Baker et al. 2022), embed all inputs into one vector, stack vectors over timesteps, apply causal transformer, and predict actions from the transformer outputs. This type of model might scale better, as it reduces the amount of components that might interfere.

#### Comments (not questions)
- Fig1 right: weird scale. Any chance to get more points?
- Line 205: grammar error at the start of the line
- Explain/rename ""Dlvl"" and why ""Turns"" is good metric
- Figure 3: ""LSTM + XXL Dec"" is bit confusing naming, since ""decoder"" is not commonly used term in the paper. I'd recommend using something like ""LSTM (bigger)"" to simply reflect that it is the LSTM baseline but with bigger network
- Figure 3 (and others): add explanation to caption what is the error bar of the bar plots. Is it standard deviation or standard error (or something else)?
- Table 2 caption: starts with weird ""\[V4\]""

#### References

- Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas et al. ""Training compute-optimal large language models."" arXiv preprint arXiv:2203.15556 (2022).
- Baker, Bowen, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. ""Video pretraining (vpt): Learning to act by watching unlabeled online videos."" Advances in Neural Information Processing Systems 35 (2022): 24639-24654. No explicit sections for limitations or broader/societal impact was given. Authors bring up the future work ideas in the conclusion. While I think the work does not require societal impact section (no immediate impact), I urge authors still think through of any cases where the work or the insights could impact others. Or alternatively, what impact would _not_ including some results do (e.g., skipping some analysis).


## Rebuttal acknowledgement

I have read authors' rebuttal which did address my concerns, and I increased my rating from 4 to 7 to signal my vote to accept this paper (change was done before discussion period closed).",1043,3,5,0.8198000000000001,0.0860855389,0.8613269329000001,232,48.5199,0.1278,neurips,0.0,5,4,5,5,factual,4,4,95,neutral,4,positive,5,none,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
126,Reviewer-Ub8t,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.","This is an emergency review, and I regret that the paper is out of my expertise, which is why my review will rather stay at the surface level.

The paper is concerned with the NetHack challenge, a complex AI challenge that in 2021 reached headlines, because symbolic agents considerably outperformed neural agents. I see three main contributions in the paper:
 - The construction of a large-scale dataset, based on the best symbolic agent and its policy choices, that can enable training better neural agents
 - The training of better neural agents based on this dataset, and other improvements
 - A systematic analysis of the effect of different technical improvements (hierarchical BC, larger Transformer models, larger datasets, online fine-tuning with RL), notably finding that scaling training sets or model size alone will not bridge the gap to the best symbolic agent.

The problem is of very high interest to the AI community, and the technical investigation, results, and discussion appear thorough and insightful. The dataset might also enable further research. I find especially the results regarding scaling interesting, i.e., that performance increases logarithmic, and so more data or bigger models alone will not enable achieving parity with the symbolic approach.

Quality of writing is very good, and so the paper is easy to follow (subject to my lack of technical background).

Minor notes:
 - The paper appears to be missing a link to the dataset
 - The related work is not easy to access for someone not close to the field. E.g., paragraphs on ""imitation learning"" and ""hierarchical policy learning"" give too little detail about the basic ideas (do not start with descriptions of what they are for, but what they do)
 - ""The full observation space of NLE is far richer and more informed than the view afforded to human players of NetHack, who observe only the more ambiguous “text-based” components of NLE observations"" - I do not fully understand this sentence, please expand. What can systems observe in NLE, that humans don't receive in the original interface? Or do you mean that NLE aggregates the Ascii terminal characters into something more high-level?
 - Showing an excerpt from the dataset would be helpful, especially, as it is not quite clear what is added there, both strategies and substrategies? Or the more specific one only?
 See above. See above. See above. Yes, the authors critically discuss that scaling alone will not bridge the gap to symbolic agents on this challenge.",409,0,3,0.7943,0.1541088435,0.8796135187,232,39.1929,0.241,neurips,0.0,4,5,3,3,factual,5,4,80,polite,4,positive,3,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
3,Jouni-Tuominen,A Shape Expression approach for assessing the quality of Linked Open Data in Libraries,"Cultural heritage institutions are exploring Semantic Web technologies to publish and enrich their catalogues. Several initiatives, such as Labs, are based on the creative and innovative reuse of the materials published by cultural heritage institutions. In this way, quality has become a crucial aspect to identify and reuse a dataset for research. In this article, we propose a methodology to create Shape Expressions definitions in order to validate LOD datasets published by libraries. The methodology was then applied to four use cases based on datasets published by relevant institutions. It intends to encourage institutions to use ShEx to validate LOD datasets as well as to promote the reuse of LOD, made openly available by libraries.\n","This manuscript was submitted as 'full paper' and should be reviewed along the usual dimensions for research contributions which include (1) originality, (2) significance of the results, and (3) quality of writing. Please also assess the data file provided by the authors under “Long-term stable URL for resources”. In particular, assess (A) whether the data file is well organized and in particular contains a README file which makes it easy for you to assess the data, (B) whether the provided resources appear to be complete for replication of experiments, and if not, why, (C) whether the chosen repository, if it is not GitHub, Figshare or Zenodo, is appropriate for long-term repository discoverability, and (4) whether the provided data artifacts are complete. Please refer to the reviewer instructions and the FAQ for further information. The authors present a compact, focused experiment on applying ShEx validation to libraries' datasets to foster data re-use, with four exemplifying use cases on datasets provided by three individual libraries (and one non-library data). The presented methodology is quite straightforward application of ShEx. From purely technological perspective, the originality and significance of the contribution is not particularly high, but especially for researchers and practitioners working with (linked) data in GLAM institutions the paper would be relevant. Compared to the previous version of the paper: the authors have made improvements on the paper, extending it sufficiently on sections that needed further discussion. The comments I made in my previous review have been addressed sufficiently. The quality of the writing is good. The data file provided by the authors under “Long-term stable URL for resources” (A) is well organized and contains a README file, (B) appears to be complete for replication of experiments (based on the README file, file listing, and looking at couple of individual data files), (C) is stored on Zenodo, and (4) appears to provide complete data artifacts (based on the README file, file listing, and looking at couple of individual data files). I have one comment: - Page 14 ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - In such cases, why did you constrain the property value's datatype to ""xsd:string"" - instead of ""LITERAL"" (https://shex.io/shex-semantics/index.html#shexc) in the ShEx definition? For example, concerning NLF dataset's class Person, you have constrained the property schema:name's value to xsd:string (https://github.com/hibernator11/ShEx-DLs/blob/1.1/nlf/nlf-person.shex#L12). In the NLF data model the range of schema:name is defined as ""Literal"" (https://www.kiwi.fi/display/Datacatalog/Fennica+RDF+data+model), and in the schema.org vocabulary the range of schema:name is defined loosely: ""schema:name schema:rangeIncludes schema:Text"" (https://schema.org/version/latest/schemaorg-current-https.ttl). I would suggest loosening the constraint. Minor remarks: - Page 14: ""Table 6 provides an overview of the data quality evaluation. All the assessed repositories obtained a high score, notably the BNB and the BnF."" - Based on Table 6, NLF obtained as high score (mconRelat) as BnF. Mention NLF as well? - Page 14: ""Regarding the NLF dataset, a common problem is related with the property rdf:langString used for language-tagged string values that are validated against xsd:string"" - rdf:langString is not a property, but a datatype.",514,4,0,0.7396,0.0922263521,0.8461868763,31,32.22,0.3825,semanticweb,0.0319148936170212,3,4,3,3,factual,3,4,60,polite,4,positive,4,moderate,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,4,5,5,4,factual,5,5,90,polite,5,positive,5,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
11,Neng-Fisheri-Kurniati,"Anti-inflammatory activity and toxicity evaluation of 1,3-bis(p-hydroxyphenyl)urea","Background: Inflammation is a normal protective response caused by an injury or tissue damage, through physical trauma, damaging chemicals, or invasion of pathogenic microorganisms. One of the modified p-aminophenol compounds is 1,3-bis(p-hydroxyphenyl)urea, which was estimated to have more potent analgesic activity and fewer hepatotoxic side effects than paracetamol. When the lipophilicity of this compound increases between 1.8 to 4.4, it is observed to serve as an anti-inflammatory agent. Therefore, the determination of safety precaution is very necessary while testing for the toxicity effect of 1,3-bis(p-hydroxyphenyl)urea. This is due to the effectiveness and safety of suitable drugs. Methods: An anti-inflammatory test was carried out by measuring the percentage of inflammation in rats, after the administration of 1,3-bis(p-hydroxyphenyl)urea was previously induced by the carrageenan solution intraplantar and the analysis of neutrophil values through a plethysmometer and Hematoxylin-Eosin method. Also, an acute toxicity test was performed by administering this p-aminophenol compound to female rats for 24 h and observed for 14 days. In addition, a subchronic toxicity test was conducted on male and female rats for 28 days, with continuous observations carried out for 42 days. Results: The doses of 1,3-bis(p-hydroxyphenyl)urea at 50, 100, and 200 mg/Kg BW, had anti-inflammatory activity compared to diclofenac sodium at 2.25 mg/Kg BW. Also, there is no toxicity and animal death symptoms were observed in the acute and subchronic tests. Conclusion: This 1,3-bis(p-hydroxyphenyl)urea compound had an anti-inflammatory activity and relatively low toxicity.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article showed that 1,3-bis(p-hydroxyphenyl)urea had anti-inflammatory activity and safe to be used. However the results section in the abstract did not clearly show the efficacy and safety of the compound. Please provide the efficacy with number, percentage or else. Significance calculation should be shown in Figure 1 and 3 to make it easier for the reader to read the results. Legend of the figure and table should give more information, for example, the number of animals, magnification, etc. In Materials and Methods, many information have not been provided, such as the number of animal use for toxicity study, histology procedure, etc.  Please write a good introduction to the study. The first sentence of the paragraph should inform the primary information. Two or three next sentences should provide details information. Avoid repeated information. Furthermore, for the discussion section, please provide a more comprehensive discussion, such as comparing the data with the working hypotheses. Limitation of the study and future study should be mentioned as well.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? No",317,0,2,0.7553000000000001,0.2233870968,0.8138936162,85,30.77,0.378,f1000,0.0116279069767442,5,4,3,5,factual,5,5,88,polite,5,negative,4,none,5,4,4,5,partially factual,5,5,85,polite,5,neutral,5,low,1.0,4.0,4.0,2.0,partially factual,2.0,2.0,60.0,polite,3.0,neutral,3.0,low,5,4,4,5,factual,4,4,85,polite,5,neutral,4,low,4,4,4,4,partially factual,4,3,85,polite,5,neutral,4,low
155,Paul-Groth,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper describes an approach for reengineering Model View Controller (MVC) applications such that they expose Linked Data. The approach, named EasyData, focuses primarily on web applications. Summarizing the approach, the idea is to modify the Model of a typical web application (implemented typically by an Object-Relational Mapper) to also output data according to web vocabularies, to modify the View so that data is presented with RDFa / Microdata, and to modify the Controller such that the APIs are Linked Data compatible.  To help developers perform reengineering, two packages were developed for two popular web frameworks Ruby on Rails (EasyData_Rails) and Django (EasyData_Django). In terms of evaluation, the Rails package was applied to Redmine (redmine.org) an open source project management application. Secondly, software metrics were calculated for EasyData_Django and compared to 5 other software packages (e.g. Stanbol, D2Rq) that are also designed to help create Linked Data exposing applications.  First, I think this is an important problem to address. It’s not always straightforward to make applications Linked Data compatible and providing packages that focus on common development environments is a good one. The authors do a good job of defining the research methodology they are using from Oates. But I would have liked to see more details of what each of the steps actually required. Adding an additional paragraph describing what each of these steps require would be helpful.  There are two areas where the paper/tools need to be improved.  1) Evaluation I liked the approach of using a complex case study that’s open source, but the outcome of the application to redmine was not shown. What did the resulting project management software do? Figure 3 shows a service API but the namespace is published in example.org. It would be good to provide a place to download the updated version of the software and screenshots in the paper of what the results of the application of EasyData look like.  Again the authors provide a unique approach to evaluation with the application of software quality metrics. I really liked this approach. But it’s unclear why this validates the EasyData reengineering approach. This just says something about the EasyData software quality. While an interesting finding, the link is not made clear. Also, because EasyData Django is newer code one could argue that it will show less code complexity and code density than older software such as D2RQ.  What would have been of interest is a comparison of the software quality of software constructed using the multiple different approaches. Essentially, answering the question does the EasyData approach lead to better software than other existing approaches.  2) Software usability / evidence impact A key question for a Tools paper is the impact of the tool. Currently, no evidence is provided for large scale uptake. Another way to measure the potential of use is the quality of software itself. While the software metrics present give some indication of that, I think a key part of tool uptake is software usability this includes documentation. I would have liked to see a small tutorial at the GitHub repo which would have allowed someone to apply the approach to a small django app.  Overall, I think there is more work to be done in providing evidence for the tool and methodology’s applicability but there’s definitely something here.  Minor comments: - “Web of data” —> “Web of Data” - define LD at first use. - Look at combining footnotes 1 & 2 as footnote 2 relies on footnote 1’s definition. - You should provide a link to the redmine application website. - In the introduction, it is unclear the Linked Data (LD) principles that are being referred to. I assume it’s the classic Berners-Lee design principles https://www.w3.org/DesignIssues/LinkedData.html but Hausenblas is cited. )",623,1,0,0.7832,0.159952381,0.8893129826,43,46.67,0.0622,semanticweb,0.0,4,5,4,4,factual,4,4,91,polite,5,positive,5,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,moderate,3.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
155,Christoph-Lange,Reengineering application architectures to expose data and logic for the web of data,"This paper presents a novel approach to reengineer legacy web applications into Linked Data applications, based on the knowledge of the architecture and source code of the applications. Existing application architectures can be provided with linked data extensions that work either at the model, view, and controller layer. Whereas black-box scraping and wrapping techniques are commonly used to add semantics to existing web applications without changing their source code, this paper presents a reverse engineering approach, which enables the controlled disclosure of the internal functions and data model as Linked Data. The approach has been implemented for different web development frameworks. The reengineering tool is compared with existing linked data engineering solutions in terms of software reliability, maintainability and complexity.","This paper presents EasyData, an approach for reengineering legacy web applications to make them publish linked data.  The model, view or controller components of existing applications can be extended to publish linked data.  EasyData has been implemented for the popular Ruby on Rails and Django web application frameworks.  A comparison with other state-of-the-art linked data publishing platforms w.r.t. several software metrics shows that EasyData performs comparatively well. Let me first address the specific review criteria for a tool/system report: (1) Quality, importance, and impact of the described tool or system: Recency is also a quality criterion.  EasyData_Rails was last updated almost 5 years ago, EasyData_Django 3 years ago.  Other than README files and a few comments in the source code, I don't see much documentation.  Looking at the repositories, there are no signs of activity: few contributors, no issues, no forks (other than your own ones).  I don't see evidence that anyone has used EasyData, except yourself for adding linked data support to the Redmine project management tool.  Also, this extension of Redmine just seems to exist as an example within the scope of this article; I don't even see where it can be downloaded. (2) Clarity, illustration, and readability of the describing paper, which shall convey to the reader both the capabilities and the limitations of the tool: The paper itself is largely well-written.  Section 1 clearly states the importance of adding linked data support to web applications.  Section 2 gives a well-structured overview of existing reengineering approaches, with a focus on the model-view-controller (MVC) architecture.  One issue here is that MVC is rarely applied in a pure way.  Can you also address variants or partial implementations of MVC?  In Section 3, the capabilities of EasyData are presented clearly.  The evaluation by a proof-of-concept of adding linked data to Redmine, and by a comparison with other approaches, in Section 4 is comprehensible – but, and that's the weakest point of the paper, it could be a lot more convincing – that's basically what I require as ""major revision"".  Except for one minor observation, no lesson learned from Redmine is presented.  Also, I'm not convinced by Table 1.  Linking instances of a web app's data to linked data resources would have a much larger impact than linking the limited terminology of a software to DBpedia.  Regarding the comparative evaluation it is not sufficiently clear whether the competing linked data reengineering approaches are actually comparable to EasyData w.r.t. the chosen software metrics.  Competing tools might have a much broader functionality, and might thus have a larger code case while at the same time suffering from more vulnerabilities.  Any observations about EasyData should therefore be interpreted in relation to its small code base. For further details please find an annotated PDF with detailed comments at https://www.dropbox.com/s/ka2y2yvlpvd94jg/swj1681.pdf?dl=0.",464,1,0,0.8128000000000001,0.0626930502,0.9306222796,116,36.49,0.3587,semanticweb,0.0,2,5,3,2,factual,3,4,78,neutral,4,negative,4,low,4,5,4,4,partially factual,4,4,75,polite,5,negative,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,4.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
150,Reviewer-gFqX,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This article introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively.
Meanwhile DDGAN remains susceptible to performance drops caused by datasets that are corrupted with outlier samples.  
Through comprehensive evaluations, the RDGAN demonstrate that it outperforms vanilla DDGAN in terms of the aforementioned
generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets. Given the recent advancements in generative models such as Dalle2, stable diffusion, or diffusion GANs,  has effectively
addressed the challenges of generative modeling, namely producing high-quality
samples, covering different data modes. But it remains susceptible to performance drops caused by datasets that are corrupted
with outlier samples.
This article introduces a novel approach by employing a semi-unbalanced optimal transport to mitigate the impact of outliers effectively. The approach employed in the paper may appear to lack novelty. This is because the RDGAN simply replaces the loss function of diffusion GANs with a semi-unbalanced optimal transport, without conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives. 1. I would suggest that the author conducting a thorough analysis of the relationship between the semi-unbalanced optimal transport and diffusion GANs from both empirical and theoretical perspectives in the corrupted with outlier samples.

2. In Tables 1 and 2, RDGAN still does not achieve the best results when compared to other methods.

3. I recommend that the author present a comparison of results for different diffusion step values (T) in DRGAN, DDGAN, and the ablation study.

4. I recommend that the author provide a explanation of why the semi-dual UOT objective
ensuring that the fast sampling time of DDGAN is preserved in RDGAN?

5. ""In contrast, DDGAN’s
FID increases by more than 10 points, and the synthesized outlier ratio of RDGAN rises from 0.2
to 3.8 compared to DDGAN’s increase from 3.2 to 9.8.""
Does this mean that DDGAN is more likely to synthesize more samples in outlier data compared to RDGAN? Is this considered a desirable capability, or is it potentially problematic?

6. Can you present the outcomes achieved when training StyGAN2+Aug (Karras et al., 2020a) on mixed datasets, and include them in Table 4? Since stylegan2+AUG appears to generate a greater diversity of samples in Table 2, it would be valuable to assess its performance on mixed datasets as well.",406,0,6,0.7849,0.1768678161,0.9198342562,48,34.115,0.2174,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,neutral,4,none,4,3,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,3,4,4,partially factual,3,3,70,polite,4,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
150,Reviewer-P1f3,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper proposed a new method that combine UOT with diffusion GANs in order to permit more robust training while achieving high perception quality as well as fast inference.  The method was empirically evaluated on image generation. The strength of this paper is its technical soundness. The motivation is properly justified. The proposed method seems to make sense as a solution to the robustness problem to be solved. The main weakness of this paper is the novelty of the proposed method which seems to be a direct marriage of two existing ideas.  Indeed, this hasn't been done. However, I'm not sure how much this field of research will benefit from this obvious extension, especially when the practical side of this paper is also weak.  The experiments were done in very low dimensional settings, which is fine if the novelty of the approach is great.  When both are lacking, I'm reluctant to accept it to be published with the current version.  

**Minor**:  
The writing of the paper can be improved, including the organisation of the paper flow, as well as the typos such as a wrongly referenced equation (""equation 13"" in the paragraph before section 2.2),  the c-transform of v (following equation 6), and introducing ""Unbalanced Optimal Transport (UOT)"" twice.  

Some languages used need to be a bit more rigorous. For instance, in the paragraph 2 in the introduction - ""slow computational speed"" is confusing. The computational speed for training a diffusion model isn't slow in comparison.  It's only the inference/sampling speed that's the problem.  Second example is when you say ""Equation 13 can be reformulated as a **more** general optimal transport problem:"".  Obviously equation 13 is more general as D_{adv} can be one of the many distances/divergences. Can you comment on the performance of StyGAN2+ADA and StyGAN2+Aug in Table 2, in comparison to that of RDGAN and of DDGAN?  And how do you think about the disagreement in FID and Recall?",321,0,0,0.7219,0.0942557932,0.8881423473000001,48,51.005,0.0995,iclr,0.0377358490566037,4,4,4,3,partially factual,4,3,75,polite,4,neutral,3,none,4,4,4,4,partially factual,4,4,70,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
150,Reviewer-ov4k,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","This paper introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers. Through comprehensive evaluations, this paper demonstrates that the proposed RDGAN outperforms vanilla DDGAN in terms of the FID and recall, meanwhile being robust to outliers. The robustness of diffusion generative models is an important topic but is relatively less studied in the literature. This paper presents a simple modification to the existing DDGAN method, by using the semi-unbalanced optimal transport, to improve the method's robustness to outliers.

Empirically, a suite of numerical results is presented to show the robust performance. The contribution of this paper is limited. This paper replaces the reverse KL divergence in vanilla DDGAN with the unbalanced optimal transport. However, there are not much insights stated in the paper for using UOT.

Moreover, the experiments seem to be insufficient as well, there is a lack of comparison with the type of methods such as Wasserstein GAN, OT-GAN (and UOT-GAN if possible). In addition, in terms of the robustness of the proposed method and the ablation studies, this paper only compares with the vanilla DDGAN method, which is a bit limited. Please see the above in the weakness section. My main questions are: (1) is there any insight for using UOT objective within the DDGAN framework, why would it improve the overall image quality and convergence speed (even under the scenarios without outliers), and (2) is it possible to also compare with OT-GAN type methods since they also use OT type divergence for discriminators.",254,0,0,0.771,-0.0308035714,0.9117639065,48,36.4682,0.2191,iclr,0.0,1,4,3,1,partially factual,3,4,60,neutral,3,neutral,4,low,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
150,Reviewer-MvTD,ROBUST DIFFUSION GAN USING SEMI-UNBALANCED OPTIMAL TRANSPORT,"Diffusion models, a type of generative model, have demonstrated great potential for synthesizing highly detailed images. By integrating with GAN, advanced diffusion models like DDGAN \citep{xiao2022DDGAN} could approach real-time performance for expansive practical applications. While DDGAN has effectively addressed the challenges of generative modeling, namely producing high-quality samples, covering different data modes, and achieving faster sampling, it remains susceptible to performance drops caused by datasets that are corrupted with outlier samples. This work introduces a robust training technique based on semi-unbalanced optimal transport to mitigate the impact of outliers effectively. Through comprehensive evaluations, we demonstrate that our robust diffusion GAN (RDGAN) outperforms vanilla DDGAN in terms of the aforementioned generative modeling criteria, i.e., image quality, mode coverage of distribution, and inference speed, and exhibits improved robustness when dealing with both clean and corrupted datasets.","The paper proposes to replace the optimal transport formulation in DDGAN with Unbalanced optimal transport formulation. 1. The paper proposes a way to address the noisy dataset generative problem with the unbalanced optimal transport.

2. The paper is well-organized and well written. 1. The novelty. The method replaces the extsing optimal transport loss with the unbalanced optimal transport.  The technical novelty is limited.  Or authors may consider adding more in-depth analysis about the unbalanced optimal transport in diffusion model. section 4.3.1 is a good example.  

2. The noisy datasets are synthetic. Authors combines digits and CIFAR dataset, which are usually unlikely to happen in the real world. It would be better if authors could add experiments on some more real-world noisy datasets.

3. Some noisy-learning baselines need to be included. For instance, can we apply some noisy sample detection (a simplest way would be clustering and I think it should be easy to cluster the cifar images and digit image into two different groups.) before learning the datasets instead of using unbalanced optimal transport?  

Minor:

1. It would be better to introduce motivation to learning the generative model under noisy samples. as above",193,0,8,0.7317,0.2086080586,0.8585108519,48,39.8612,0.0866,iclr,0.0113636363636363,2,4,3,3,unfactual,3,3,65,neutral,3,positive,3,low,5,4,4,5,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,3,4,partially factual,4,3,75,polite,5,neutral,4,low
192,Giorgos-Stoilos,Warehousing Linked Open Data with Today’s Storage Choices,"This paper compares the performance of current storage technologies when warehousing Linked Open Data. This involves common CRUD operations on relational databases (PostgreSQL, SQLite-Xerial and SQlite4java), NoSQL databases (MongoDB and ArangoDB) and triple stores (Virtuoso and Fuseki). Results indicate that relational approaches perform well or best in most disciplines and provide the most stable operation. Other approaches show individual strengths in rather specific scenarios, that might or might not justify their deployment in practice.","The paper presents a benchmark and an evaluation of storage systems for linked data. The paper attempts to compare different storage models and approaches for RDF (linked) data like well-established and modern RBBMs, graph DBs, and triple-stores.  Although the proposed topic is indeed quite important (contrasting all these different storage models) is important and interesting and quite some engineering work has been conducted in collected data and evaluation various systems, in my opinion the paper does not succeed in providing sufficient fundamental or scientifically deep comparison or results.  On the one hand, the proposed benchmark is ill described. There are no details about the queries that have have been designed (how they have been designed, how large or complex they are, how many joins, etc.). A similar description is missing about the data model and complexity of the RDF dataset. Is the dataset highly interconnected or is it just small disconnected parts? On the other hand, with the massive number of RDF and SPARQL benchmarks out there and the evluations that have been conducted it is very difficult to justify why this is not just another collected dataset together with some queries and to show originality and novelty. Another key issue missing is a description about how the RDF data were converted and stored in the RDBMS and MongoDB. This is an important issue that needs a better description. It is especially important for MongoDB since key-value pairs are significantly weaker than triples. In the evaluation one should also present the number of tuples returned by each system. Perhaps I missed it but are they all returning the same number of answers for all queries. Comparing RDBMs systems with triple-stores is slightly unfair in the sense that the latter are also supposed to perform some kind of RDFS-reasoning either at loading or at query time (or at least they are supposed to be able to query interconnected graph-like data) hence it is not surprising that the RDBMs systems are faster. Especially if the dataset is quite loosly interconnected and the data can be easily mapped to the relational model then this is indeed the case. Overall it is not clear what are the results of this experiment. If one required some kind of RDFS reasoning then definatelly the RDBMs systems would be useless (even though faster) but if one does not need any kind of reasoning then obviously the RDBMs systems are the choice to go. It is not clear to which figures the observations in section 4 are referring to. Some conclusion is made but it is hard figure out out how and why this conclusion is produced. It would be good to add pointers, e.g., Fuseki did this (see Fig 4 (x)). Equation on page 6 should be clarified and made more precisely. What is the difference between queryscenario and testseries?",473,0,0,0.7558,0.0981728778,0.9203384519,167,50.57,0.0448,semanticweb,0.0,4,4,3,4,factual,3,3,75,neutral,4,negative,4,low,4,3,3,4,partially factual,4,4,65,neutral,4,negative,5,moderate,1.0,3.0,3.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,4.0,moderate,3,3,4,3,factual,3,4,70,neutral,5,negative,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
117,Reviewer-MDsd,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","Offline reinforcement learning (RL) suffers from the extrapolation error. There are numerous model-free and model-based offline RL algorithms that aim to tackle this challenge. Among them, model-based offline RL algorithms often learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, such quantifications are often inaccurate. This paper addresses this issue by training bidirectional dynamics models and rollout policies, and design a conservative rollout method that selects those synthetic transitions with the smallest reconstruction loss. The authors provide some theoretical analysis of their method and build their method upon some off-the-shelf model-free offline RL algorithms. # Strengths

The strengths can be summarized below:

- this paper is well-motivated, and the whole paper structure is clear

- the logic flow of this paper is clear, and it is easy to follow and understand

- the authors provide theoretical analysis to support their method # Weaknesses

Despite the aforementioned strengths, this paper has some flaws in novelty, empirical evaluation, and theoretical analysis. Based on these considerations, I can confirm that this paper is clearly under the acceptance bar of this venue. Please see the detailed comments below.

- (major) The core idea presented in this paper is NOT new. A highly relevant paper is published previously \[x\]. In \[x\], the authors also train bidirectional dynamics models and bidirectional rollout policies for offline data augmentation. Thus, the technical parts of this paper have a huge overlap with \[x\], making the contribution and significance of this paper quite weak. The differences are, that this paper selects the transitions with reconstruction loss while \[x\] selects reliable transitions via the proposed double check mechanism. It is doubtable whether the data selection approach adopted in this paper is better than the double check method, as intuitively, the reconstruction loss may not be reliable for forward/backward horizon larger than 1 (where no true next/previous states are available)

\[x\] Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination. NeurIPS 2022.

- (major) The empirical evaluations are limited and somewhat weak. The baseline algorithms this paper adopts are very old. It is somewhat confusing why the authors only choose to compare against these very weak algorithms. More advanced and recent offline RL algorithms ought to be included as the baselines (e.g., TD3BC, IQL, Decision Transformer, LAPO, etc.). The authors build their method upon CQL, BCQ, and BEAR. Can your method benefit more advanced offline RL algorithms?

- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper does not include any signal of statistical significance, e.g., std, IQM. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.

- (major) The theoretical analysis is also not new. Similar techniques are adopted in the MBPO paper. Specifically, one online model-based RL algorithm BMPO \[y\] theoretically shows that the error of the bidirectional models is smaller than unidirectional models, making the theoretical insights of this paper less appealing and unsurprising.

\[y\] Bidirectional model-based policy optimization. ICML 2020.

- (minor) The authors ought to specify the version of the D4RL datasets they use in the paper. In Table 1, your evaluated scores in halfcheetah-medium-expert are questionably low, why is that?

- (minor) This paper does not do a good job in the related work part, the authors include too few recent offline model-based/model-free offline RL papers Please refer to the the weaknesses part.",603,0,3,0.7867000000000001,0.0567165212,0.9565235972,53,32.288,0.3178,iclr,0.0,4,4,4,5,factual,4,4,90,polite,4,negative,4,moderate,5,5,4,5,factual,5,5,85,polite,5,neutral,5,moderate,1.0,4.0,4.0,2.0,partially factual,1.0,2.0,60.0,neutral,3.0,negative,3.0,low,4,5,5,3,factual,3,4,75,polite,5,negative,5,low,3,4,4,4,factual,4,4,85,neutral,5,negative,5,low
117,Reviewer-qiBS,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","This paper presents a new model-based method for offline reinforcement learning. The key technical contributions of the proposed model include: 1) It learns the bidirectional rollouts of the state transitions and the reward functions; 2) It learns forward and backward offline policies, following the BCQ method. With the learned bidirectional dynamics model and the corresponding policies, given a pivotal data point drawn from the offline dataset, the replay buffer can be augmented with the generated data trajectories. 

Additionally, the paper provides a theoretical analysis, establishing a tighter bound on the rollout error for the conservative bidirectional rollouts compared to unidirectional approaches. 

Finally, the empirical findings on the D4RL benchmark demonstrate the effectiveness of the proposed method. 1. The proposed method is simple, reasonable, and effective on the existing D4RL benchmark, showing great potential for practical offline RL applications. 
2. The paper is well-written and easy to follow. The overall design of the proposed method is presented in a clear and thoroughly motivated manner. 
3. The method seems to be a highly versatile framework. As shown in the paper, it can be easily integrated with existing model-free offline RL approaches. 1. My primary concern with this paper is about the novelty of the proposed bidirectional rollout technique. At NeurIPS 2022, a paper titled ""Double Check Your State Before Trusting It: Confidence-Aware Bidirectional Offline Model-Based Imagination"" by Lyu et al. introduces a conceptually similar idea. In both papers, forward and backward models are trained to augment the offline dataset. It is crucial for the authors to address this similarity and provide a comprehensive comparison between COBiMO and the method presented by Lyu et al., considering aspects such as model design and empirical results.
2. In the experiment section, the authors present averaged results of 6 random seeds. To enhance the statistical robustness of their findings, it would be better to include the standard deviations over multiple runs in Tables 1-3. 
3. The paper primarily compares COBiMO with approaches that were proposed 2-3 years ago. It would be beneficial for the authors to extend their comparisons to include more recent advances in offline RL to provide a comprehensive evaluation of COBiMO's performance in the context of the most current state of the field.
4. In Section 5.3, there is an absence of an explanation regarding the factors that lead to performance degradation in certain tasks when COBiMO is applied (which can be reasonable but needs more analysis). Besides, as claimed in Section 5.3, the proposed method outperforms the original algorithms significantly in 10/12 tasks. However, it's essential to ensure that all relevant results supporting this claim are presented, as only a partial subset of the results is currently shown in Table 3.
5. Typos:
- In the first paragraph of Section 5.1, ""...from three domain"" should be corrected to ""...from three domains"".
- In the third paragraph of page 4, ""...represents a gaussian distribution..."" should be ""...represents a Gaussian distribution..."". In summary, my primary concerns include the technical novelty in comparison to the missing reference (major), and some finer details of the provided experimental results (minor).",513,0,8,0.7682,0.1505058522,0.9512968659,53,35.9958,0.3011,iclr,0.0,4,4,4,4,factual,4,4,85,polite,4,neutral,4,low,5,5,5,5,factual,5,5,88,polite,5,neutral,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
117,Reviewer-7BFv,Model-Based Offline Reinforcement Learning with Conservative Bidirectional Rollouts,"Offline reinforcement learning (offline RL) learns from an offline dataset without further interactions with the environment. Although such offline training patterns can avoid cost and damage in the real environment, one main challenge is the distributional shift between the state-action pairs visited by the learned policy and those in the offline dataset. Prevailed existing model-based offline RL approaches learn a dynamics model from the dataset and perform pessimistic policy optimization based on uncertainty estimation. However, the inaccurate quantification of model uncertainty may incur the poor generalization and performance of model-based approaches, especially in the datasets lacking of sample diversity. To tackle this limitation, we instead design a novel framework for model-based offline RL, named Conservative Offline Bidirectional Model-based Policy Optimization (abbr. as COBiMO). First, we learn an ensemble bidirectional model from the offline dataset and construct long bidirectional rollouts by joining two unidirectional ones, thereby increasing the diversity of the model rollouts. Second, we devise a conservative rollout method that minimizes the reconstruction loss, further improving the sample accuracy. We theoretically prove that the bound of rollout error of COBiMO is tighter than the ones using the unidirectional models. Empirical results also show that COBiMO outperforms previous offline RL algorithms on the widely used benchmark D4RL.","This paper studies the model-based offline reinforcement learning problem. The authors propose to learn bidirectional model and bidirectional behavioral policies and use them to generate rollout trajectories. The output policy is obtained by a model-free offline reinforcement learning on the augmented dataset. The paper provides theory and empirical study to justify the proposed algorithm. 1. The paper is clearly written and easy to follow. 1. The Related Work misses important paper. For instance, this paper is not the first to use bidirectional model in offline learning. Confidence-aware Bidirectional Offline Model-based Imagination is the first to apply this idea to the best of my knowledge.
2. I cannot recognize the algorithmic novelty of the algorithm. Forward imagination is widely used in model-based offline learning and Reverse Imagination was first proposed in ROMI. This paper seems to just combine these two ideas directly without justifying why it can substantially improve the performance
3. The theory seems to be trivial.
4. The experiment misses important baselines, such as ROMI and Confidence-aware Bidirectional Offline Model-based Imagination which share similar ideas. Besides, the performance does not seem compelling if one also look at the performance in ROMI and Confidence-aware Bidirectional Offline Model-based Imagination paper. 1. What is the main intuition behind using bidirectional imagination? Why should we expect it provide substantial improvement?
2. What does the theory part tell us, is there any interesting insight?
3. How does the algorithm perform compared to other later model-based algorithms? How does the algorithm perform on other tasks in D4RL?",252,0,7,0.7828,0.1666666667,0.9260005355,53,30.2158,0.1199,iclr,0.0,0,4,3,0,unfactual,4,2,67,polite,2,negative,4,moderate,4,5,4,4,partially factual,4,3,75,neutral,5,negative,5,moderate,1.0,3.0,4.0,2.0,partially factual,2.0,2.0,60.0,polite,4.0,neutral,3.0,low,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,low,2,4,3,3,partially factual,3,3,60,neutral,4,negative,4,low
63,Reviewer-Qg2K,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the learning dynamics in the special case of two-layer ReLU neural network with small initialization. The results of this paper extend prior results from infinitesimal initialization to finitely small initialization.

The relation and difference with prior results are clearly presented. The paper is clearly written and easy to follow. The figures are helpful for understanding the argument. The setting of the neural network is unconventional. It requires the second layer weights to depend on the first layer weights in a way as shown in Eq.(3), instead of independently initialized.  This setting is used neither in practice nor in most theoretical analysis. According to the analysis,  I doubt that the results of this paper hold without this restriction on the second layer weights. 

> The paper has some discussion on this setting. However, it does not justify the validity of this setting. That it is commonly assumed in other papers does not directly justify. I would like to see some analysis, or at least some intuition, on why the results would hold on the natural setting.

The assumption on the data (Assumption 1) is strong, as it is not met by almost all real data. 

The significance of the results is limited, as it is an extension of similar results from $\epsilon \to 0$ to the finite but small $\epsilon$. In Eq.(5), why the R.H.S. is independent of the network output $f(x_i)$? Or, why there is no such term $y_i-f(x_i)$ which usually appears in the expression of gradients.",250,0,1,0.7221000000000001,0.0232363316,0.8568468094,49,53.8922,0.11,iclr,0.0,1,4,2,1,unfactual,4,1,55,neutral,3,negative,4,extreme,5,5,4,5,partially factual,5,5,92,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,3,partially factual,3,4,75,polite,5,neutral,4,low
63,Reviewer-5S56,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper improves on previous theoretical analysis of the early alignement phase of the neurons of a shllow neural network initialized with small weights. This allows them to prove quantitative bounds in terms of the initialization scale, time, number of neurons and number of datapoints required to guarantee convergence. The paper is easy to follow and explains well the previous issues and how they are solved. It is nice that the results apply to deterministic initialization of the weights, and do not require a random initialization (though they can of course be applied to this case). The assumption of positively correlated labels and balancedness are very strong, and usually are not true in practice. The description of Assumption 2 before the statement of the assumption does not match the statement of the assumption.",133,0,0,0.7155,0.0346338384,0.8429466486,49,43.7599,0.0999,iclr,0.0096153846153845,0,4,1,0,unfactual,3,1,20,polite,2,positive,0,extreme,2,5,4,4,partially factual,5,5,80,polite,5,neutral,5,none,2.0,5.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,3,low
63,Reviewer-osUg,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","The paper studies the problem of training a two-layer ReLU network for binary
classification using gradient flow with small initialization on well-separated input datasets, i.e. datasets $(x_i, y_i)_{i \in \[n\]}$ with $ \min \frac{\langle x_iy_i, x_jy_j  \rangle}{\Vert x_i \Vert_2 \Vert x_j \Vert_2 } \geq \mu$.
They show that in time $O(\frac{\log(n)}{\sqrt{\mu}})$ all neurons are well aligned with the input data, which means that positive neurons show in the same direction as the positively labeled points (or have a negative scalar prouct with all vectors of the dataset) (and an equivalent result for negative neurons).
Furhter they show that after the early alignment phase, the loss converges to zero at a $O(1/t)$ rate, and the weight
matrix on the first layer is approximately low-rank. Numerical experiments are provided. All claims are proven and illustrations are supporting the explanations. The assumption that the dataset is well seperated is very strong. I don't see why one would use a neural network on such a dataset rather than linear regression. -",167,0,2,0.7745000000000001,0.0306565657,0.9448035359,49,46.2312,0.1633,iclr,0.0,3,3,3,5,partially factual,3,2,45,polite,4,positive,3,moderate,2,4,4,2,factual,5,5,75,neutral,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,none,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,low,2,4,3,3,factual,3,4,75,polite,5,neutral,4,low
63,Reviewer-Qhf5,Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization,"This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.","This paper studies the problem of training a two-layer ReLU network classifier via gradient flow under small initialization. Training dataset assumes well-separated input vectors. Analysis of the neurons’ directional dynamics establishes an upper bound on the time it takes for all neurons to achieve good alignment with the input data. Numerical experiment on the MNIST dataset validate the theoretical findings. + Interesting alignment behavior of the gradient flow for training two-layer ReLU networks under small initialization and separable data
+ Rate analysis for the after-alignment-phase convergence - The results only hold for correlated data. 
- Only ReLU activation functions are analyzed.
- Some of the results have been previously known or observed, e.g., the solution of (stochastic) gradient flow finds in training two-layer ReLU networks on separable data is (almost) low-rank.
- How small the initialization shall be to ensure the two-phase convergence is not qualitiatively discussed? 1) Do the results/analysis extend to other activation functions? 
2) How small the initialization shall be to ensure the two-phase convergence? In general, since the training is nonconvex even with correlated/separable data due to the nonlinear relu activation function in the los. SGD/GD converges to a local minimum and indeed, this has been observed and numerically validated in the literature; see also \[R1\] Brutzkus et al. 2018. SGD learns over-parameterized networks that provably generalize on linearly separable data. ICLR. \[R2\] Wang et al. 2019. Learning ReLU networks on linearly separable data: Algorithm, optimality, and generalization. IEEE TSP. In \[R1\], SDG for two-layer ReLU networks under separable data converges to local minimum; yet for leaky ReLU networks, it finds global minimum. In \[R2\], it also shows that plain SGD on ReLU networks using separable data converges to local minimum numerically. Yet, a bit modification on the SGD helps SGD converge to a global minimum but with random initialization. It would be great if a comparison can be made between the approach in \[R2\] and the small-initialization SGD for training two-layer ReLU networks on e.g., MINIST. Moreover, is there any transition for such initialization value to go from the two-phase to single-phase gradient flow convergence to local minimum?
3) It would be great if more numerical tests are provided to demonstrate the two-phase convergence and provide the plots. 
4) If the second-layer weights are initialized not in a balanced manner (although not initialized according to (3)), I understand it would also work and guess that the imbalance between positive v_j and negative v_j values only influences the time it takes for the two-phases. More balanced initalization, faster convergence. It would be interesting to numerically validate if this is the case. 
5) There are some grammar issues and typos. Please correct.",445,0,8,0.7757000000000001,0.0692361402,0.9179714322,69,34.7577,0.5423,iclr,0.0106382978723403,1,3,3,2,unfactual,3,1,66,polite,4,positive,3,extreme,5,4,4,5,factual,5,5,88,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,3,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
64,Wing-Yin-Mo,"Effect of dietary protein level on growth, food utilization, food conversion and survival rate of giant trevally (Caranx ignobilis)","Background: Proper feed formulation is required for successful fish farming activities. Therefore, it is necessary for fish feed to provide optimal growth so that the cultivation business generates profits. Currently, there is very limited information about the appropriate feed for Caranx ignobilis, causing problems with its development. This study aims to provide feed with different protein levels to C. ignobilis. Methods: We will examine the protein levels’ effects on the daily growth rate (DGR), specific growth rate (SGR), absolute growth rate (AGR), feed conversion ratio (FCR), feed efficiency (FE), and survival rate (SR). This research was conducted for 35 days, from June to October 2017, at the Center Brackiswater Aquaculture Development (BPBAP) Ujung Batee, Ministry of Marine Affairs and Fisheries, Aceh Besar, Indonesia. This study used a completely randomized design method, with five treatment levels (30%, 40%, 50%, 60%, and 70% protein feed) and four replications. Results: The results showed that feeding with different proteins on C. ignobilis had a significant effect on the mean values ​​of DGR, SGR, AGR, FCR, FE, and SR. The 50% protein feed gave the best results for C. ignobilis, with a mean DGR value of 0.267 ± 0.005 g / day, a mean SGR of 1.722 ± 0.030% / day, a mean AGR of 0.081 ± 0.003 cm/day, a mean FCR of 1.290, a mean FE 77.755% and a mean SR was 86.667%. Conclusions: Furthermore, feed treatment with increased protein content between 30%–50% has a positive correlation with the growth of C. ignobilis. However, the ability to grow fish will decrease if the feed protein content is >50%.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ​​1. In the abstract, please revise the Methods from future tense to past tense, as the authors have already finished the experiment.  2. Professional editing is needed to improve the overall quality of the manuscript.  3. In the introduction, I think it will be more meaningful if the following information is provided: Production volume of the species, exported vs. domestic consumption.  Natural diet composition: Is the fish carnivorous fish? What are the natural preys?  4. One of the major issues of the manuscript is the feed formulation (Methods). More explanation should be provided to polish the manuscript. How did the authors produce the feed? Home-made or produced by manufacturers?  Where did the authors get the ingredients? Home-made or purchased?  Do the fish feed pellet float or sink? Does the diet fit the preference of fish?  The design seems like the authors were testing the suitability of blood meal (I prefer “meal” instead of “flour”) rather than testing the effects of protein levels. I did a quick check. There are many studies to replace fishmeal with blood meal. For omnivorous fish, 50% replacement is suggested (Kirimi et al. (20161)). For carnivorous species, such as Murray cod, partial replacement is possible (Abery et al. (20022)). Would it be true that high levels of blood meal affect growth of fish?  There are too many variables in the diets. To my best knowledge, high levels of carbohydrate is harmful to carnivorous fish. The author might check the paper published by Stone et. al. (20033), for more information about the effects of carbohydrates on different fish species. Thus, it would be possible that fed with Diet A and B resulted in inferior growth is related to the high levels of carbohydrate.  The diets were tested using juveniles and that should be reflected on the title.  5. The authors suggested fish mortality of groups D and E were related to feces accumulation and poisoning. However, the authors didn’t mention the depth of the experimental pond. In the pond used in the experiment, the authors suggested nets were used. Could the fish feed accumulate inside the cage and kill the fish because of that? Did the fish show any sign of intoxication? Also, is that pond equipped with any aerators? Is there any water treatment facility? 6. Amino acids and proximate compositions of the diets: The authors calculated the protein content of fish feeds. Did the authors measure the exact protein concentration? Would it be possible that the high level of blood meal resulted in inferior growth, because of insufficient amino acid(s)?  In addition to protein, other proximate compositions i.e. lipid, ash, moisture and carbohydrate contents should also be measured and presented.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? No  Are sufficient details of methods and analysis provided to allow replication by others? No  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",589,0,9,0.7455,0.1621666667,0.8303694725,150,49.72,0.2777,f1000,0.0,5,4,4,5,factual,4,4,85,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,65,polite,5,negative,5,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,5,4,5,5,factual,5,5,90,neutral,5,negative,5,low,5,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
76,Ghislain-Hachey,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","This paper investigates two different approaches to increase discoverability and connectivity of resources on the web: the Open Archives Initiative Object Reuse and Exchange vocabulary (OAI-ORE) which is based on RDF/XML and the use of Schema.org vocabularies for marking up HTML pages in a search engine friendly way. Although there as been a lot of work on automatic discovery and connectivity of web resources, not a lot of quality work has been done to rigorously evaluate the alternatives so I think this work is important and original as far as I know. That being said, I have several problems with the research as it is now. The results feel more like anecdotal evidence than rigorous scientific analysis. The paper started with a rather interesting (and essential) idea which was to investigate discoverability and connectivity of resources on the Web. For such a study I would expect a carefully crafted questionaire or at least a clear list of criteria to look for and to grade the three studied scientific projects with regards to the investigated approaches in a systematic manner. I understand this paper's purpose is not to conduct of survey, but it seems the main purpose is to ""investigate"" and as such I think maybe some elements of good evaluation papers such as [1,2,3] might provide insight into how to improve this research. The discussion section sort of integrates criteria for an evaluation, but too informally. The criteria should be presented early in the scientific method used. After which experiment can be conducted based on the criteria and *then* discuss results. I do find the discussions to be an interesting read but as they are now they can not be considered as scientific evidence. It is false to claim that Semantic web-enabled vocabularies are ""innumerable""; there is in fact a relatively small (but growing) set of them mainly in scientific communities. A simple wording re-adjustment would be better I think. The method of investigation would need to be more clearly explained too. The paper moves from giving some background information (which I really like as some of this was new to me) to providing results. There is a clear gap in outlining the Methodology used. It should be explicit (and repeatable) how results are to be compiled and right now I would find it hard to reproduce this evaluation/investigation. The style of writing is formal and appropriate, but there are several grammatical errors and typos that could have been easily avoided. Here's some examples: - p.6 filesfor missing space. - p.6 wasmanually missing space. - p.7 of of remove an 'of'. - p.8 reearch missing 's'. - p.9 regulariety ?!? Finally, this seems to be outside the scope of the SWI SWJ Special issue. I would encourage the authors to continue this important work though and maybe go through a conference first (if not done so already). However, I do not think it is ready for journal publication of original work. To summarise, the paper provides clear introductory text even for those new to the concepts discussed, but does not contain the required overall balance: almost half the paper is on background information and much missing in terms of methodology, results and discussions. The paper reads very well and I would definitely enjoy reading another more structured and rigorous version of it. It think the suject explored is critical, but the community would benefit from a more scientifically sound evaluation. [1] T. Dyba and T. Dingsøyr. Empirical studies of agile software development: A systematic review. Inf. Softw. Technol., 50:833–859, August 2008. [2] T. Dyba and T. Dingsøyr. Strength of evidence in systematic reviews in software engineering. In Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement, ESEM ’08, pages 178–187, New York, NY, USA, 2008. ACM. [3] T. Dyba, T. Dingsøyr, and G. Hanssen. Applying systematic reviews to diverse study types: An experience report. In Empirical Software Engineering and Measurement, 2007. ESEM 2007. First International Symposium on, pages 225–234, sept. 2007.",665,4,9,0.803,0.1443889444,0.8987319469,33,46.27,0.0376,semanticweb,0.0104166666666666,4,3,4,4,factual,4,3,80,neutral,4,negative,4,low,4,3,4,4,partially factual,4,4,65,polite,5,neutral,5,low,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,4,3,4,4,partially factual,3,3,70,polite,5,negative,4,low,4,3,3,4,partially factual,3,3,70,polite,4,neutral,4,low
76,Ian-Dickinson,Facilitating Data Discovery by Connecting Related Resources ,"In this study, we investigate two approaches to increase the discoverability and connectivity of resources on the web. The first approach is the use of semantic web data structures in RDF/XML, in particular the Open Archives Initiative Object Reuse and Exchange (OAI-ORE) vocabulary for creating compound digital objects. The second approach is the use of Schema.org vocabularies for marking up html web pages to increase their visibility to web search engines. Through applying these two mark-up approaches to three case studies within the geosciences, we identify factors that help to evaluate their applicability to research data archives. Our analysis points toward the most efficient and effective markup for aggregating resources within research data archiving settings. We focus on factors that can lead to increasing public discoverability of datasets. Our evaluations are based on the following characteristics of each mark-up approach: ease of use, the available standards and vocabularies, the ease of interoperability, and the relation to data citation tools and methods.","This paper has a number of minor flaws, but my principle reason for recommending rejection is that it does not live up to the premise that the authors establish. After a long and overly general preamble, the authors describe two efforts to annotate three different datasets with metadata in RDF and schema.org microdata. The premise is that doing so will make the datasets more discoverable and better connected, but this conjecture is never tested. It is not even discussed what ""more discoverable"" or ""better connected"" would mean in practice, nor are concrete, measurable objectives suggested. Moreover, the two methods discussed seem somewhat incomparable: schema.org can, as the authors note, be used to affect search rankings. RDF metadata, however, requires another tool - such as Sindice or something similar - to find and process the published RDF. Attempting to compare apparently incomparable approaches leaves the reader little the wiser; the more so when no conclusions are drawn. The paper has many minor errors, too many typos, and many places where claims are made without citation. Thorough proofreading is required. Among the more concerning errors: * ""in order to find something, it must be named"" (section 1). I disagree: anonymous things may be found, by their description. Perhaps it would be better to say ""in order to find something, it must be identified"", where identification is taken to include both naming and identifying reference expressions. * ""actionable identifiers"" (section 2). The action of an identifier is to identify; therefore ""actionable identifier"" is a tautology. Later in this section, the authors appear to mean ""resolvable"" rather than ""actionable"". * ""Web 3.0 is essentially a way to bridge the gap between human users and computerized applications"". I'm not sure quite what this means, but humans have been using computerized applications, successfully, for a long time. To the extent that Web 3.0 means anything (other than a rather vague marketing term), I don't believe that it means this. * "" Resource Description Framework ... is a standard"" (section 3.1). Not being an accredited standards body, the W3C is careful to state that it makes recommendations, not that it sets standards. This should perhaps read ""... is a specification"" * ""RDF is built from XML triples"" (section 3.1). This is most emphatically wrong. RDF and XML are completely orthoganal: one can encode RDF using XML, but XML is not fundamental to the definition of RDF. * ""RDF vocabularies are declared via namespace designations"" (section 3.1). Also incorrect. * ""Prior to ORE, groups of related resources could not be made visible on the web via URLs"" (section 3.2). I'm not sure what the authors are trying to convey here, but I disagree. Collections can be described in HTML as ul/li lists, or in RDF with seq and bag, or simply by publishing a list of URLs in a text file. * ""on a finite project"" (section 4). Are there infinite projects? * ""RDF requires a triple store, which may be overwhelming to [..] users. It is based on XML"" (section 6.1). Users do not need a triple store to publish and make use of RDF metadata, they only need a tool which can process it. Semantic web search engines, such as Sindice, can do this without the user ever creating a triple store themselves. Also, as noted above, RDF is not based on XML. * Section 6 is correctly labelled discussion, which is all that it does. It would be more helpful to the reader if it were labelled ""Evaluation"", and then proceeded to evaluate the different metadata and identification approaches against measurable criteria. It is not apparent to me that an dataset creator wishing to make their dataset more discoverable could use the results of this paper as anything other than general background to a decision about how, and where, to publish metadata on the dataset.",640,0,2,0.7908000000000001,0.1162368881,0.8519799113000001,34,47.59,0.2025,semanticweb,0.0,3,5,5,3,factual,5,5,95,polite,5,negative,5,none,4,5,4,4,factual,4,5,85,neutral,5,negative,5,none,1.0,4.0,2.0,2.0,unfactual,2.0,1.0,40.0,neutral,3.0,negative,3.0,moderate,3,5,5,3,factual,4,5,85,neutral,5,negative,5,none,3,4,4,4,factual,3,4,78,neutral,5,negative,5,low
85,Jennifer-Gaddy,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript by Natasha Thorn and colleagues entitled, “GBS vaccines in the UK: a round table discussion” presents a compelling discussion of the status of a protective vaccine against Group B Streptococcus, an important perinatal pathogen.  This manuscript is full of important information about disease risk from GBS infection and gaps in current treatment and prevention strategies.  There are many positive aspects about this manuscript that I would like to highlight.  First, the authors are extremely deliberate in their use of language, specifically referring to “pregnant patients” and “pregnant people”.  This is a subtle but important aspect of discussing these populations without introducing highly gendered language. Excellent work. The inclusion of stakeholders in the community such as Midwives was also a strength as these providers have the capacity to meet individuals who may be unaware of GBS risk and/or vaccine hesitant.  Buy-in from these groups will help with deployment in the future. Comparing/contrasting efficacy of other vaccination programmes deployed in pregnant patients was also a strength of this manuscript. I have a few comments to improve the quality of the manuscript. 1.  The authors mention AMR very briefly in the second paragraph of the Introduction.  It would be helpful to expand this section to acknowledge that the standard first line therapeutic choice for GBS is penicillin, but up to 10% of populations report penicillin hypersensitivity. Second line choice is often erythromycin or clindamycin and emerging clinical strains are exhibiting high resistance to these drugs (about 40% of strains are resistant).  2.  First line of the Introduction.  The authors refer to Group B streptococcus and italicize the word “streptococcus” but leave it lowercase.  If the authors are referring to the genus, this word should be capitalized and italicized. If they are referring to general morphology and arrangement of bacteria it can be lowercase but should not be italicized.  Most common references to GBS use the former (genus nomenclature).  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",464,0,4,0.7773,0.164210373,0.9360240698,78,34.97,0.6746,f1000,0.01010101010101,5,5,4,5,factual,4,5,85,polite,3,positive,5,low,4,5,4,5,factual,5,5,88,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
85,Lisa-Hanson,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  *Very well written article. Statistics and references are up to date and appropriate. Tables are very effective. A few suggestions for clarity. *""more crowded pregnancy vaccine space"" is unclear. *In the GBS3 trial description, more clarity is needed as to why participants in the routine testing arms receive either rapid PCR IP (versus 35-37 weeks). A reference here about the sensitivity and utiliy of rapid IP testing is needed-as this is not a usual strategy in culture-based EOGBS prevention approach recommended by the CDC and now ACOG (2019). *Table 3. The points about midwives having hesitancy to offer vaccines was interesting, as this is not the case in the USA. *Table 4 is redundant of the text on Potential Phase IV study designs.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",270,1,2,0.774,0.2182758621,0.8594522476,98,37.4,0.0999,f1000,0.0206185567010309,5,5,5,5,factual,4,4,86,neutral,5,neutral,5,low,4,5,4,4,factual,5,5,85,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
85,Rosana-Rocha-Barros,GBS vaccines in the UK: a round table discussion,"Background Group B streptococcus (GBS) remains a leading cause of infant sepsis, meningitis and death despite intrapartum antibiotic prophylaxis. A vaccine is urgently required, and two candidates are in advanced clinical trials. For successful GBS vaccine implementation, especially if a vaccine is licensed based on an immunological threshold, there must be cross-sector engagement, effective advocacy, robust plans for phase IV studies and equitable access.  Meeting A round-table discussion, held at St George’s University of London, reviewed the current position of GBS vaccines in the UK context, focusing on phase IV plans, convening a diverse group of stakeholders from across the UK, with a role in GBS vaccine licensure, advocacy, implementation or effectiveness evaluation. Presentations outlined the latest UK epidemiology, noting the rising infant invasive GBS (iGBS) infection rates from 1996 to 2021 for both early and late onset disease, with the highest disease rates in Black infants (1.1/1000 livebirths vs white infants (0.81/1000 livebirths). Potential coverage of the candidate vaccines was high (>95%). Regulatory input suggested that EU regulators would consider waiving the need for a pre-licensure efficacy study if a putative correlate of protection could be adequately justified. Phase IV study methodologies for a GBS vaccine were considered, largely based on previous UK maternal vaccine assessments, such as a nationwide cohort study design using a vaccine register and a maternal services dataset. Other strategies were also discussed such as a cluster or stepped-wedge randomised trial to evaluate implementation outcomes. Opportunities for advocacy, education and engagement with additional key partners were discussed and identified.  Conclusions With an approved GBS vaccine a near possibility, planning of phase IV studies and identification of critical barriers to implementation are urgently needed. Cross-sector engagement is essential and will facilitate a successful pathway.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Revision of the manuscript GBS vaccines in the UK: a round table discussion The manuscript is a comprehensive report of the round table held at St George University of London, that discussed the state of the art of GBS vaccines and planned phase IV trials. The manuscript brings the talks of different specialists, covering various issues regarding GBS vaccine background, vaccine implementation, and the follow-up after the beginning of vaccination. Overall, the text is very well-written and I have only an observation, as follows. Page 3 2nd paragraph. “IAP is not always deliverable, results in high antibiotic exposure...” This sentence seems a bit unclear. I suggest that the authors improve it.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Not applicable  Are all the source data underlying the results available to ensure full reproducibility? No source data required  Are the conclusions drawn adequately supported by the results? Yes",258,0,2,0.7639,0.1280357143,0.8627001643000001,128,27.42,0.2025,f1000,0.0,5,5,4,4,factual,5,5,90,polite,5,neutral,5,none,4,5,4,4,factual,5,5,85,polite,5,positive,4,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,5,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
36,S.G.-Lukosch,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The article introduces a semantic wiki system that supports collaborative multilingual knowledge management based on controlled natural language. The authors used a subset of Attempto Controlled English (ACE) implemented in Grammatical Framework (GF) to support bidirectional automatic translations between ACE and language fragments of a number of other natural languages in their semantic wiki. With this approach users speaking different languages can collaboratively build and manage a knowledge base.  The semantic wiki system was evaluated in a study with 30 participants speaking 3 different languages. For each language, there were 10 participants. In the study users had two tasks: users had to create articles in their native language or in a language they were fluent in as well as to read automatically translated articles to evaluate the truth or falsehood of the translation. The evaluation shows that users reach a high level of consensus. The evaluation also shows that the automatic translation does not have a negative effect.  In total, the article is well written and related to the state of the art. The authors clearly identify their contribution to the state of the art, i.e. making a semantic wiki environment multilingual, and evaluate whether their contribution addresses the identified problems around creating a multilingual ontology in a semantic wiki system. Conclusions are thus validated and future work is well based on the given findings. Concluding, I recommend to accept the article.",232,0,0,0.719,-0.0588461538,0.9578637481,18,28.03,0.0588,semanticweb,0.0,1,3,3,1,partially factual,2,3,30,neutral,3,positive,1,low,2,5,5,3,factual,5,5,85,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,2,5,4,2,factual,4,4,75,polite,5,positive,4,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
36,Eero-Hyvonen,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The paper extends the authors' earlier work on using controlled natural language (CNL) in semantic OWL-based wikis. The novelty in this paper is to investigate this in the multi-lingual case. CNL statements, transformed into OWL, can here not only be given in different languages (here in particular in Englishm German, and Spanish) but also translated arcross language boundaries facilitating using wiki CNL in different languages. The paper expands the authors' recent ESWC 2013 paper. The topic is clearly suitable for the topic of the special issue. The research problem and methods used for attacking it are clearly stated. Related work is discussed in a separate section, which seems adequate, although I am not an expert in this particular field.  The papers cover a great deal of work related to the underlying tools and new experiments, with illustrative examples and pointers to further sources. After presenting the framework, the quality of the translations arcross natural languages is evaluated and results analysed in careful way. The language and presentation is exceptionally well polished. In short, this looks like solid work worth publishing. My main concern about the paper is related to the general idea of using CNL as a basis in wikis in general. What would be the *realistic* use case problem for a system like this, and how well would it then actually solve the problem of collaboarative multilingual ontology creation? The paper concerns a toy example of countries, rivers etc. It is good to use such examples in a research setting, but it would be nice if the authors could shortly discuss this bigger question and e.g. motive the reader by examples of more serious CNL-based wikis and OWL ontologies - are there useful systems already and what are the challenges? It is a challenge, if a group of people start inputting CNL OWL expressions in a wiki, and this should coverge into something logically consistent and useful. Some challenges encountered in the evaluation section are discussed, e.g., different opinions people may have about geography, which leads to inconsistency. It is also said in the paper that 80% of the users could not express themselves as they liked in the experiment. In footnote 9 the authors point the reader to ""demo wikis"",  but I could not find any realistic applications or datasets there. The video there was for some reason not operational. Minor comments p. 2 Provide the reference to GF when it is first mentioned. Use mdash ""---"" without spaces at its ends. There are many occurrences of this. ""as already mentioned"" -- Remove, it is not good style to use expressions like this. ""[10] discusses a multilingual ..."" Using a reference as a word does not look nice. E.g. ""Davis et al. [10] discuss ..."" would be better. There are many occurrences of this. In Fig. 6 the ""proper name"" column contains adjectives ""Spanish"" and ""Swedish"". Explain or correct this. [25] Journal name ""Semantic Web"" is not complete. [36] Pages missing.",493,4,4,0.7982,0.1617001181,0.9298262,19,49.21,0.0743,semanticweb,0.0,3,3,4,3,factual,3,3,50,polite,3,positive,2,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
36,Prateek-Jain,Collaborative multilingual knowledge management based on controlled natural language,"User interfaces are a critical aspect of semantic knowledge representation systems, as users have to understand and\nuse a formal representation language to model a particular domain of interest, which is known to be a difficult task. Things\nare even more challenging in a multilingual setting, where users speaking different languages have to create a multilingual\nontology. To address these problems, we introduce a semantic wiki system that is based on controlled natural language to\nprovide an intuitive yet formal interface. We use a well-defined subset of Attempto Controlled English (ACE) implemented\nin Grammatical Framework. Our wiki system offers precise bidirectional automatic translations between ACE and language\nfragments of a number of other natural languages, making the wiki content accessible multilingually. Because ACE has a partial\nbut deterministic mapping to the Web Ontology Language, our wiki engine can offer automatic reasoning and question answering\nover the wiki content. Users speaking different languages can therefore build, query, and view the same knowledge base in\nan intuitive and user-friendly interface based on the respective natural language. We present the results of a user evaluation\nwhere participants using different languages were asked to write and assess statements about European geography in our wiki\nenvironment. Our results show that users reach a high level of consensus, which is not negatively affected by the presence of\nautomatic translation.","The work 'Collaborative multilingual knowledge management based on controlled natural language' presents a description, architecture and implementation details of a Controlled Natural Language based knowledge engineering in a semantic media wiki based environment. This system allows a Semantic Media Wiki to become multi lingual editing environment. The underlying technology relies on using ACE based controlled vocabulary. The authors have presented a comprehensive evaluation and a portal to download and play with the system.  I like the work as it (a) demonstrates capabilities which can be achieved just by using controlled language (b) Shows an actual system which can be used in multiple and real world settings.  Some minor remarks: The last years have shown great progress on the technical side towards the realization of what is called the Semantic Web -> The last few years ? Already in 2007 -> In 2007 proper names -> proper nouns",147,0,0,0.8194,0.0730769231,0.9267749786,295,33.85,0.2086,semanticweb,0.0,2,2,1,2,partially factual,1,2,20,neutral,3,positive,1,low,2,4,4,3,partially factual,3,4,75,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,4,3,2,factual,4,3,60,polite,4,positive,4,low,2,4,3,3,factual,3,3,75,polite,4,positive,4,low
176,Torsten-Hahmann,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","Review Summary: This paper describes the LOD that is developed as part of the OntoIOp registry. I'm confident that this will become an important linked data set in the future. While there is no doubt about the dataset's importance, improvements are necessary to make it easily accessible to a larger audience. The description of the dataset lacks sufficient clarity and detail to be useful to the novice user. The description of the dataset in Section 2 needs to be elaborated (adding detail and precision). Lists/tables and simple statistics could help address this issue (compare previous LOD papers in the journal). Furthermore, the figures need to be better tied in by explaining the depicted relationships and using them as examples in Sec. 2. The authors remain vague on the maturity of the dataset, which is a concern, though it might be less pressing once sufficient detail is provided. The current state (what is there, what is missing) should be stated more explicit. While some major rewriting/editing is necessary, I see no technical problems with the described data set. The raised issues about clarity/accessibility to the community at-large can be easily fixed. I support accepting this paper contingent on ""the lack of detail and clarity"" issue being addressed. More details on the 3 evaluation criteria: (1) Quality of the dataset.  I have no doubt that the relationships between the included logics and languages are correctly captured. However, the maturity/completeness of the dataset is an issue: as I understand it, not all mappings/relations between logics and languages are included yet. Be clear about which ones have been modeled and which are left for the future. As a side issue: While one cannot reasonably expect the dataset to ever be complete, some mechanisms for the inexistence of mappings/translations could be helpful to differentiate between non-mappability and incomplete knowledge. I'm not sure whether that is within the scope of the OntoIOp registry. (2) Usefulness (or potential usefulness) of the dataset.  The usefulness is not as clearly visible as would be desirable. Neither Hets nor Ontohub use the dataset, though potential future applications are hinted at. The authors do provide some example queries that help understand how the dataset may be useful by itself. (3) Clarity and completeness of the descriptions This is my chief concern. For a LOD description, I expect more detail than what is provided in Section 2. While the explanation of the provenance is sufficient, the explanation of what the dataset describes requires elaboration. This should be at a level that non-logicians can understand the basic ideas and use the LOD. For example, you need to explain the difference between logics and languages -- this will not be clear to most users (as often one language is associated with a single logic and vice versa). Also, a better explanation of the intuitions behind ""mapping"", ""translation"", ""serialization"", ""sublanguage"", etc. are needed. Explain why mappings/translations are modeled as types as opposed to binary relations.  The current scope of the LOD is a bit vague, some lists/tables to summarize the dataset would be very helpful: - explain the kind of items  (maybe each of the ""subdirectories"" of the URLs) from http://purl.net/dol/registry that are reflected in the directories in http://purl.net/dol/ - how many of each of the types of items and relationships does the dataset include? - list & briefly explain the kinds of mappings available, it wouldn't hurt to include the hierarchy of mapping relations from [13] - what languages and logics are currently included? Given the manageable scope of 29 logics, 43 translations, and 14 languages, it would be easily to list them in a table/figure. The figures could be more helpful by explaining what the depicted relations in Fig 1 and 2 are: most, I believe, are mappings (though I'm not sure whether sublanguage relations are mappings; at the beginning of Sec. 2 mappings are restricted to logics), but also serializations are included. Are the color coding of expressivity/decidability in Fig. 2 captured in the dataset? Some minimal working example would be very helpful: one (or more) logics with one (or more) languages and two serializations as well as mappings to other logics/languages and metadata (showing how VoID and SKOS are utilized). Lesser, though more general concerns about the described project/dataset: 1) The maturity/completeness of the LOD: the OntoIOp registry is still very much under development. While publication on the underlying research are very valuable, I'm note sure about the value of a description of the registry's LOD at this stage. It seems highly likely that the description will be outdated as soon as it is published. That defeats the purpose of describing the dataset to others for them to use/reuse. 2) ability for others to contribute: the purpose of the registry is to enable the community to contribute descriptions of languages, logics, and translations. However, for maintaining the registry, the authors propose to generate it automatically from Hets. This is counter to the desired openness: it would require others to first extend Hets instead of directly contributing to the directory/dataset. I personally think that the LOD should not be permanently tied to any specific software, which poses a significant barrier for the community to contribute. Other mechanisms for maintaining/updating the registry are needed.  Other things that need to be fixed in the final version: - given that the paper is less than 5 pages in content, the abstract is unnecessarily long. It includes much background information (2nd paragraph, 1st sentence of 3rd paragraph, last paragraph) that should better be placed in the main part. - p 4: last paragraph of Sec. 3 needs a rewrite to improve clarity - if possible, the wealth of technical terminology should be reduced to what is essential. This is not supposed to be a description of the entire OntoIOp project, but of the dataset only. You also need to more clearly separate and exlain differences between the DOL language, Lola vocabulary and the language of the OntoIOp registry at the beginning and clearly distinguish between what is a project (OntoIOp) vs. an artifact (registry, DOL, Lola) - I can't quite appreciate the relevance of the example on p. 2 as it only uses the language and syntax statements that relate to the registry. - The URl to Lola on p. 3 needs to be updated",1049,3,1,0.7569,0.0961049107,0.8967276216000001,35,44.85,0.8064,semanticweb,0.0309278350515463,4,4,5,4,factual,3,4,88,neutral,4,negative,4,none,5,4,4,5,factual,4,5,85,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,positive,5,low,3,3,4,4,factual,4,4,85,polite,5,neutral,3,low
176,Maria-Poveda,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper decribes a dataset for logics, translations and languages descriptions. In general, I find the dataset really interesting and promising for combining and integrating information from different ontology registries and translation between logics. For the organization of the review I will follow the dimensions established by the type of submission:  (1) Quality of the dataset.   One of the main shortcomings of the paper is that the SPARQL endpoint where one could try the queries in the paper or others is not explicitly referenced from the text nor in http://ontoiop.org/. It should be included in Table 1.  A VoID description of the dataset is claimed to provide metadata from the dataset in page 3 however I haven't been able to find it either. It would be nice to have a footnote with it or include it also in Table 1. Adding the dataset description to a dataset registry (for example http://datahub.io/) and providing the reference to the resource entry in the datahub would be also advisable.  In the text it is said ""the OntoIOp Registry, with LoLa being its main vocabulary, gets four stars"" and ""the OntoIOp Registry is unique in being a linked dataset covering the domain of OMS languages"" considering that the linked part of the 5-star ranking is precisely the 5th one these two sentences seems contradictory. Either the dataset is linked, being 5-star, or it should establish links to other dataset to be possible to claim the second sentence as for ""linked"". In general I would suggest reviewing the 5 star ranking and proof that the dataset is actually a linked dataset.  (2) Usefulness (or potential usefulness) of the dataset.   While thinking that the described dataset will be surely interesting and useful it would be welcome to read a bit more about motivation and potential uses apart from those in Ontohub and Hets. The current state of the paper gives me a feeling of the dataset were an ad-hoc development for these systems (Ontohub and Hets) and seeing some examples of uses out of this context would increase greatly the dataset value.  (3) Clarity and completeness of the descriptions.   Main concerns about clarity is the distinction between DOL and LoLa. It is not clear which ontology is used in dataset. At the beginning it seems like LoLa is the actual implementation of DOL for this dataset however in section 3 the URI of reference for LoLa contains ""dol"" and in the SPARQL query examples the prefix dol is used. In addition, the URI for LoLa gives a 404 errors (I tried to browse it several times in different weeks).   It would also be valuable including a diagram of the LoLa's main classes and properties as the current figures are example of instances from what I understand. --- Other comments --- Figure 2 is not referenced in the text. Is it nice to reference and describe within the text all figures and tables appearing in the paper. In the first query in page 5 the selected variable is ""?target-language"" that do not appear in the query, in the query body it appears ""?targetLanguage"" instead. I would like to see some concrete metrics about number of triples and outbound links to other datasets. The information about metrics in section 5 seems not clear about specific figures, see ""around three times as many triples as the core dataset"" Typo: Section 5 ""Thus, the expanded dataset has around three times as many triples as as.."" --> only one ""as""",579,2,0,0.7136,0.1711382114,0.8930797577,52,50.57,0.103,semanticweb,0.0,3,4,4,3,partially factual,4,3,77,polite,3,neutral,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
176,Mathieu-d’Aquin,"The OntoIOp Registry – a Dataset Supporting Ontology, Model and Specification Integration and Interoperability","  OntoIOp is an initiative for developing a standard for Ontology, Model and Specification Integration and InterOperability within the OMG (Object Management Group).\n  (We will henceforth abbreviate “Ontology, Model and Specification” as OMS.)\n  The OntoIOp working group, formed in 2011 and affiliated with the OMG since 2013, comprises a few dozen international experts representing all major communities on research and application of ontologies, formal modeling and formal specification.\n\n  The primary tangible output of the OntoIOp work will be DOL, the Distributed OMS Language, a meta-language that gives the combination of different OMS languages a formal semantics and enables writing OMS libraries consisting of modules written in multiple OMS languages, and of mappings between such modules.\n  The standardization of DOL's syntax and semantics is still in progress, there is already software that supports it, most prominently the Ontohub repository engine.\n\n  While the DOL conformance of the most widely used standard OMS languages, particularly OWL, Common Logic and RDFS, and of their underlying logics and of translations between them, is being established in annexes to the standard, the DOL framework is designed to be extensible to any future OMS language.\n  For this purpose, the standard provides for an open registry, to which the community can contribute descriptions of languages, logics and translations.\n  In the interest of enabling interoperability, this registry is published as a linked open dataset.\n\n  We present the initial population of the OntoIOp Registry, comprising 29 (sub)logics, 43 translations and 14 (sub)languages, each with rich descriptions, and the design of the LoLa ontology about logics and languages forming the core of its vocabulary, giving references to the literature based on which each part of the initial Registry and of LoLa were modeled.\n  As use cases we outline how queries and inferences over the Registry can support applications for managing OMSs and OMS libraries.\n\n  Looking into the near future, we draft the governance structures that will ensure sustainable maintenance of the OntoIOp Registry, and how large parts of it will be exported automatically rather than being maintained manually.\n","This paper presents the OntoIOP registry, which is a dataset based on an ad-hoc ontology for describing languages, the underlying logics, their serialisations and mappings between them. As a general comment, I thing the representation used is reasonably elegant, and I can see some value in having such a map of languages and logics available. However, it is very hard to extract, from the paper, how useful the dataset currently is, or what is its potential for impact. I also think that the a bit of additional work in improving access to the dataset, the scope of the content and the connections with external resources would help in improving and demonstrating the value of the dataset. In more details (1) Quality of the dataset The representation of the languages, logics and mappings seem reasonable. The authors argue that there is no other ontology covering these aspects, and I indeed don't know any myself. It would be good however to include more information, in the related work section, about some other metadata descriptions for ontologies/information resources, that overlap to an extent with the one presented here. For example, a clear explanation of what is added by the ontology compared to OMV or to the schema used by common ontology repositories would be useful. Generally, a more complete comparison with other works that are not intended for the same task, but that overlap (e.g. ontology repositories, VoID, etc.) would be useful. Although the information in the repository is modelled in a reasonable way, the content in itself is very small. That is not an issue in itself, but it certainly affects the usefulness as the scope of the dataset is very limited. One could argue that a dataset and a classification are different things, and that this is closer to a classification of languages. The paper mentions that their are links to other datasets included, but going through a few resources, I couldn't find any. More details about that would certainly be needed. Not directly related to the quality of the dataset, but to the ease of using it, it would have been good to also include other common forms of access to the data than resolving URIs to RDF, and a dump. A SPARQL endpoint as well as html documentation of the entities included (i.e. URIs resolving to human-readable documents too) would have been appreciated. (2) Usefulness The paper includes ideas about tools that could be using the dataset and an example query. This is interesting of course, but at the same time it is very hard to understand from what is written what is the real (current and potential) impact of the dataset. How much and how is it used currently? What is the demand for such information? How does the group plan to address this demand? The paper mentions sustainability, and honestly states that this is not a resolved issue. While this is understandable, and the case of many other datasets out there, it is also slightly worrying if the ambition for this is to become a reference point for others when describing resources related to languages, logics and their mappings. I can certainly see that happening, but again, as mentioned above, it would make the paper stronger if such an ambition was made explicit, with a clear view of how that might happen in the future if it has not done so yet. As an aside, I believe that this issue could be helped by extending the scope of the dataset a bit, importing from existing repositories of ontologies (TONES, BioPortal, Watson, etc.) their metadata and enriching them with information about the language/logics they rely on. This could certainly demonstrate a practical application of the dataset, and generate a valuable resource to go with it. (3) Clarity of the description The paper is reasonably easy to read, and besides a few slightly surprising formulations, it is well written in my opinion. As already described above, I think however that several sections (related work, usefulness, technical aspects and interfaces to the datasets) should be elaborated further.",677,0,1,0.7869,0.1082326007,0.9219363928,53,41.5,0.0743,semanticweb,0.010204081632653,3,4,3,2,unfactual,3,3,70,neutral,3,negative,4,high,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,3,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
57,Reviewer-Mh63,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The authors of this paper present an innovative methodology for out-of-distribution (OOD) detection. While existing methodologies typically involve the direct use of given OOD samples, this paper introduces a new approach that applies perturbations to OOD samples, allowing the model to experience a more diverse set of OOD samples during training.

The authors define these perturbations using an adversarial loss based on the uniform distribution loss commonly applied to OOD samples. This creates directional guidance for each instance, determining the direction of the gradient and thus defining the perturbations applied to OOD samples.

The results demonstrate that using perturbed OOD samples improves the performance of OOD detection across several key metrics, including False Positive Rate at 95% Recall (FPR95), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR), when compared to existing methodologies. This paper's novel approach of applying perturbations to a given data instance in order to utilize a broader array of samples and those tightly located at the decision boundary is indeed a reasonable and intriguing choice. The authors' concept of adversariality against the uniform loss, which implies a concentration of prediction towards a particular class, exhibits an interesting property worth exploring further.

Experimentally, the authors provide a substantial ablation study on the hyperparameters used in loss composition, which adds to the comprehensiveness of their methodology. Notably, the use of t-SNE based plotting allows for a clear visualization of how the OOD samples are perturbed to be situated very closely to in-class samples. It also arise the question of ""how would ood behave for the different perturbation types?"". The authors have presented an interesting methodology that applies adversarial perturbations based on a particular loss function to enhance the performance of out-of-distribution detection. However, it would be important for the authors to provide empirical evidence that demonstrates the superiority of this adversarial perturbation over other types of perturbations. This would require a systematic and rigorous experimentation design and would ideally be conducted across various datasets and under different conditions to ensure the results are robust and generalizable.

In the final implementation of the authors' methodology, it's noted that both the perturbed and unperturbed out-of-distribution (OOD) samples are simultaneously considered in the loss function. However, the manuscript doesn't sufficiently explain the rationale behind this particular choice. It would be particularly interesting to see a comparison of results when using only perturbed samples, only unperturbed samples, or both, in the loss function. 

a crucial aspect that needs further clarification and discussion is the ratio of in-distribution to OOD samples used in the training process. The choice of this ratio can significantly affect the performance and robustness of the model. For instance, a too high proportion of OOD samples could make the model overly sensitive to outliers, while a too low proportion might not adequately expose the model to OOD scenarios.

While the paper shows promising results in the specific contexts tested, it would be beneficial for the authors to provide a more extensive analysis covering a range of different OOD datasets. If the authors can provide empirical evidence demonstrating the consistent performance of their method across diverse OOD datasets, it would significantly strengthen their claims. 

it's unclear from the manuscript how each OOD sample is directed towards a specific class during this adversarial perturbation process. Specifically, they should explain how the gradient direction, which determines the perturbations applied to the OOD samples, correlates with the movement of these samples towards specific classes.

I am more than eager to increase my score if the questions above are adequately answered.

 Please see the section weaknesses. Please see the section weaknesses.",603,0,0,0.7914,0.1319551426,0.8588757515000001,230,20.6888,0.2586,neurips,0.0,5,5,4,5,factual,4,4,90,polite,5,positive,5,none,4,5,4,5,factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
57,Reviewer-1Wzd,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The manuscript studies image-wide OOD detection in presence of auxiliary negative data. The negative data is often limited and therefore cannot fully encompass the distribution of inliers. Consequently, contemporary learning procedures fail to deliver classifiers resilient to outliers. To overcome this issue, the manuscript presents a method for extrapolating the negative data towards all modes of the inlier distribution. The proposed method first calculates the gradient of arbitrary OOD score over the input. Then, the sign of the gradient is used to direct the negative input samples towards the inlier distribution. The final learning algorithm uses both initial and extrapolated auxiliary negatives to train the classifier resilient to outliers. The proposed method outperforms relevant related works on small image benchmarks. S1. The manuscript deals with an important issue.

S2. Extrapolation of auxiliary negative data towards modes of inlier distribution intuitively makes sense.

S3. The developed method achieves competitive results on considered benchmarks.

S4. The developed method can be combined with existing OOD detectors (e.g. Energy, MSP, ... ) W1.  The manuscript does not discuss the effectiveness of the method when there is only a small auxiliary dataset available. It seems that the developed method still requires a broad OE dataset (Tiny-ImageNet as stated in L236).

W2. The manuscript does not consider relevant related works which use synthetic negatives created by generative models \[a,b,c\]. Synthetic negatives are an effective way for augmenting the auxiliary dataset and the proposed method should outperform methods trained on a mixture of real and synthetic negative data.

W4. The manuscript does not reflect on the additional computational budget (time and memory) required by the method over the OE baseline.

\[a\] Shu Kong, Deva Ramanan: OpenGAN: Open-Set Recognition via Open Data Generation. ICCV 2021

\[b\] Matej Grcic, Petra Bevandic, Sinisa Segvic: Dense Open-set Recognition with Synthetic Outliers Generated by Real NVP. VISAPP 2021.

\[c\] Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin: Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples. ICLR 2018. C1. Can the proposed method work on large-scale experiments \[d\]

C2. Is the extrapolation of outlier data towards inliers always possible or there are some requirements that should be met?
Analysis similar to \[e\] could improve the manuscript.

\[d\] Haoqi Wang, Zhizhong Li, Litong Feng, Wayne Zhang:
ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022.

\[e\] Zhen Fang, Yixuan Li, Jie Lu, Jiahua Dong, Bo Han, Feng Liu:
Is Out-of-Distribution Detection Learnable? NeurIPS 2022 Although promised in Appendix D (L435), the limitations are not clearly stated. One possible limitation might be W1.
",415,0,11,0.8258000000000001,0.0130782313,0.8142668009,230,31.7369,0.0535,neurips,0.0,4,4,4,4,factual,4,3,80,polite,5,positive,5,low,4,4,4,4,partially factual,4,4,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
57,Reviewer-vD9G,Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation,"Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.","The paper tries to solve the problem that sampled auxiliary informative outliers may not be sufficient and diverse enough to recover the data boundary in the OOD detection setting. To achieve this, the authors propose a new learning objective with information extrapolation, where second term expands the surrogate OOD distributions towards a more diversified one. The authors have provided theoretical analysis to show that for Gaussian mixture model and binary classification problem, DivOE can extrapolate the
outlier boundary towards ID data. In the experiments, the authors adopt image ID datasets such as CIFAR10, CIFAR100, and results show that DivOE achieves better performances over several evaluation metrics. S1. The paper targets an important research problem within the OOD detection research community, and proposes a new auxiliary outlier generation and learning objective to target the research problem that surrogate auxiliary outliers are not sufficient and diverse enough.

S2. The learning objective is simple but adaptable to different post-hoc scoring functions, augmentation techniques and sampling techniques to auxiliary outliers.

S3. The paper has provided a solid theoretical analysis that shows the effectiveness of DivOE within a simplified binary classification setting. The result of Theorem 3.1 is consistent with the authors' explanations and observations in the previous sections.

S4. The paper provides good experiment comparison to exciting methods. They have used several evaluation metrics to demonstrate the effectiveness of the approach.

S5. The writing is very clear and presentation is easy to understand. W1. In Theorem 3.1, The hypothesis class of the binary classification problem is overly simplified to only linear decision boundary. It would be nicer if the authors can provide theoretical results that generalize to more complex hypothesis class.

W2. As Figure 4 shows, the extrapolation ratio and diversified strength are two variables that affect the OOD detection results. If extrapolation ratio is between 0.9~1.0, the performance may degrade and become worse than OE. However, the authors have not mentioned any general guidance to tune those two variables, especially for unseen OOD detection problems.

W3. The authors should consider to include a table comparing the number of outliers generated by DivOE and other benchmark methods in order to achieve similar detection performance scores during experiments.

W4. The authors should also include bold numbers for the best performing algorithms for Table 11, Table 12 and Table 13. Q1. The experiments are only performed on two simple image classification tasks with CIFAR10 and CIFAR100. The reviewer is wondering whether the authors have used other image datasets or tabular datasets for evaluation. 

Q2. Is there a general guideline in how to choose the extrapolation ratio, augmentation techniques, diversified strength, OOD scores when using DivOE on unseen OOD detection task? It seems that the four factors play an important role in detection performance.

Q3. How would $\eta$ and $\alpha$ play an effect in the training steps of DivOE? 

Q4. Could different data augmentation techniques be used in combination with the inner-maximization function? Would the augmentation function boost up the performance? The authors have adequately addressed the limitations and potential negative societal impact of their work, as listed in the NeurIPS checklist. The reviewer would appreciate if the authors can address the question and weakness section.",529,0,13,0.7985,0.1007839262,0.9079990983,230,29.1727,0.0751,neurips,0.0105263157894737,5,5,5,5,factual,4,4,95,polite,5,positive,5,none,4,4,4,4,3,4,4,75,polite,5,neutral,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
113,Reviewer-QTwQ,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings. Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see. Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6). All weaknesses listed above should be well addressed to improve the paper. The authors have shown some limitations of the proposd method, but more should consider other that the method itself.",505,0,5,0.7445,-0.0014520202,0.8663344979000001,215,41.2356,0.2383,neurips,0.0,4,4,4,4,factual,3,3,75,neutral,3,negative,4,low,5,4,4,5,5,4,4,85,polite,5,neutral,5,moderate,1.0,4.0,4.0,3.0,partially factual,2.0,2.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,5,negative,4,low,4,4,4,4,partially factual,3,3,78,neutral,5,negative,4,low
113,Reviewer-pJBT,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach.  1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.   I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''\[17\]that''
 1, Why can sub-prototypes benefit the universal domain adaptation scenario? 
I understand that, even within a domain, samples from the same class can be grouped into sub-classes. But, a critical part is missing why this helps the cross-domain association of common classes. which is the core problem for universal domain adaptation. An explanation or empirical justification is needed here, i.e., what is the pattern of retrieved sub-prototypes for common samples and private ones? 

2, Some technical details are not comprehensive enough. 
1) Is the memory learnable parameters? How to initialize them? This can be basic knowledge for people familiar with this, but it is still necessary to briefly detail this. 
2) After reading sec 3.5,  it is still unclear to be how the sub-prototypes help align the embeddings \hat{Z}. 

3, In Fig. 1 (c), does this method assume the sub-class of two domains can be matched? This seems unrealistic under the distribution shift. 

 Yes. ",311,1,1,0.7933,-0.0048850575,0.9108181,215,39.9698,0.0795,neurips,0.0108695652173913,5,4,4,4,factual,4,4,70,neutral,3,positive,4,low,4,4,3,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
113,Reviewer-YkYx,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score. Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments. mainly of the weaknesses. This paper has no limitation sections.",156,0,0,0.7433000000000001,0.1770634921,0.8554611802000001,215,45.59,0.088,neurips,0.0109890109890109,4,4,3,2,partially factual,3,3,50,neutral,3,positive,3,low,4,4,4,4,partially factual,4,4,75,neutral,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,positive,5.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
113,Reviewer-S9DQ,Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation,"Universal domain adaptation aims to align the classes and reduce the feature gap between the same category of the source and target domains. The target private category is set as the unknown class during the adaptation process, as it is not included in the source domain. However, most existing methods overlook the intra-class structure within a category, especially in cases where there exists significant concept shift between the samples belonging to the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. Moreover, from the interpretability aspect, it is unreasonable to align visual features with significant differences, such as fighter jets and civil aircraft, into the same category. Unfortunately, due to such semantic ambiguity and annotation cost, categories are not always classified in detail, making it difficult for the model to perform precise adaptation. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them. By doing so, our model learns a more reasonable feature space that enhances the transferability and reflects the inherent differences among samples annotated as the same category. We evaluate the effectiveness of our MemSPM method over multiple scenarios, including UniDA, OSDA, and PDA. Our method achieves state-of-the-art performance on four benchmarks in most cases.","This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases. S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section. W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as \[1, 2, ..\] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like \[3\]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
\[1\] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
\[2\] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
\[3\]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022 Please refer the weaknesses section for the related questions that need more clarification. A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.",695,4,0,0.8047000000000001,0.1297619048,0.9349661469,215,17.6354,0.1939,neurips,0.0,4,4,5,5,partially factual,3,4,80,polite,4,negative,5,low,5,4,4,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
42,Reviewer-qbM5,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper proposes an analysis of the the actor critic setting with function approximator and it proves the convergence using deep neural networks with an arbitrary number of hidden layers. The claims are ambitious. The analysis of the paper sidesteps many key elements from the literature that contradicts the possibility to have a critic that provably converges when using non-linear function approximators, let alone when combined with an actor. When learning with the Bellman iterations, a compounding of errors can occur due to the non-linearity of the function approximator with respect to the parameters, which can lead to divergence even with the continuity and Lipschitz assumptions as described in the paper.

The discussion from Section 4.2 is also not convincing. 

Additional comments:
- line 77: the reward function goes into $R$, but what is R? Did the authors mean the real numbers $\mathbb R$?
- Many discussion points lack a precise formalization, e.g. line 248: ""Such an analysis is inherently more technically challenging, since when the actor can wait for the critic to go through sufficiently many iterations, one could argue that the resulting Q-values are approximately accurate and the process resembles gradient descent."" Why do the examples of off-policy divergence not apply in your analysis (see for instance Sutton and Barto intro to RL book in Section 11.2 ""Examples of Off-policy Divergence""). Limitations are not really discussed and it might be that some of the claims (see questions above) are not correct.",243,0,0,0.8078000000000001,0.1497916667,0.9431985021,215,38.8276,0.0587,neurips,0.0306122448979592,1,3,2,2,factual,3,2,50,neutral,3,neutral,3,moderate,3,4,4,3,partially factual,4,4,65,neutral,5,negative,5,moderate,1.0,4.0,4.0,2.0,partially factual,1.0,2.0,60.0,neutral,3.0,negative,3.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,4,moderate,2,4,3,3,partially factual,3,3,65,neutral,4,negative,4,low
42,Reviewer-7mQ7,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The authors prove a bound on the approximation error of sample-based actor critic learning on a more general and realistic setting, namely allowing for a NN approximator of any depth. This is a very non-trivial result and an interesting contribution to the literature.

Note: I have not gone through the full proof in the appendix and thus cannot fully comment on the validity of the main contributions. The paper is very well written and the ideas are clearly communicated. I especially appreciate the tables and figures to aid in explaining the approach and how it fits into the literature.

The main theorem is a very interesting theoretical result. There are a few parts of the paper where the presentation could be improved a bit, but for the most part I thought the paper was very clearly written. Your final bound does not appear to be affected by the depth of the NN approximator, but clearly depth does improve approximation error. Does a tighter bound exist that takes into account depth or would depth mostly be captured in the \epsilon error term?

On line 135, you refer to \sigma_w, but I don't see that variable defined. What is that referring to?

In Assumption 2.6, what is the gradient of w with respect to?

Small notes:
I would recommend having all equation statements be numbered so it's easier for readers to refer to them.

I find the notation on the equation in line 161 a bit confusing, namely that Q refers to both a function that takes in weights and outputs a Q-function and the Q-function itself.

 I don't think there are significant potential negative societal impacts of this work.",278,0,0,0.7733,0.1502083333,0.8888005614000001,215,54.6915,0.8064,neurips,0.0,2,4,2,2,factual,3,3,65,neutral,3,positive,2,low,5,5,4,5,partially factual,5,5,95,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
42,Reviewer-FwfS,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This is a theoretical paper which studies actor-critic reinforcement learning algorithms in which both the actor and the critic are represented using deep neural networks with more than one hidden layer. The previous research addressed linear representations and neural networks with only one hidden layer. This paper derives convergence rates for both the actor and the critic for neural networks with more than one hidden layer.
 - Writing is of very high quality, with well-considered notation, and sensible flow.

- The problem statement is clear, and it is supported by required references.

- The problem is important, and the paper furthers our understanding of the behaviour of RL with function approximation using deep neural networks.
 - A few small typos can be found in the paper. E.g., in line 28 ""was consider in"". A few articles are also missing in various places.

- In the paper, the authors emphasize the need to stay close to the initial conditions of the critic (e.g. in Sec. 2.4). This makes sense from the regularisation point of view in general, but in RL, this may mean that the optimial policy may not be found if the algorithm is forced to say close to the initial conditions. Perhaps the motivation and the consequences of staying close to the initial conditions could be clarified.

- Consider Eq. (9), and assume that $\epsilon$ is small, but higher than 0. Even if $\epsilon$ is very small, the best action may change when a lover Q-value is allowed, i.e., the best action determined by $\theta_t$ may be different from the best action according to $Q(s,a)$. Do the smoothness assumptions made through the paper help to cope with the change of the best action in this case? Note that small $\epsilon$ may not be sufficient to make the result significant since the policy itself may be affected even when $\epsilon$ is tiny.

In line 201, the authors say that $C$, $\beta$, and $\mu_\min$ do not depend on $\theta$. But, I am not sure if this is true for $\mu_\min$ since the stationary distribution $\mu_\theta$ depends on the policy. When we have deterministic actions in the MDP, some states may even have probability of zero in the stationary distribution of the ensuing Markov chain.
 See previous box. One thing to note is that I don't prove convergence rates in my work, and I did not go through the proofs to verify their correctness.
",402,0,4,0.7103,0.0851666667,0.9396833777,215,55.0423,0.0831,neurips,0.0,2,4,3,3,factual,4,4,75,neutral,4,neutral,4,low,4,5,5,4,5,5,5,90,polite,5,neutral,5,low,2.0,4.0,5.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
42,Reviewer-GAW7,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper presents a convergence analysis of Actor Critic method with multiply layer networks.  The convergence analysis of AC with multi-layer networks is important, as AC methods with neural networks plays the core role of the success of DRL.  The paper is difficult to follow and the writing can be significantly improved. (More in Questions) I would ask the authors to clarify the following questions, which I believe can significantly improve the paper if addressed:
1. What are the differences for AC methods with single hidden layer networks and multiple layer networks, especially for proving the convergence? In the current version, these differences are not well explained. To improve the clarity and strengthen the paper's contribution, the authors should provide a more detailed comparison of these two methods, highlighting the specific challenges that arise when proving the convergence for each architecture. This will enable readers to better understand the significance of the proposed approach in tackling the convergence problem and its relevance in the context of existing research.
2. The proof of convergence presented in the paper is challenging to follow, and its integration within the main context is inadequate. To improve the overall readability and accessibility of the paper, I recommend including a sketch of the proof in the main body, i.e., Section 3. This will allow readers, especially those unfamiliar with previous work on the convergence analysis of AC methods, to grasp the high-level idea behind the proof. Providing a concise outline of the proof in the main paper will enhance the paper's accessibility and make it more appealing to a broader audience.
3. Expanding on the suggestions mentioned in point 1, once the key differences between AC methods with single hidden layer networks and multiple layer networks are clearly stated, it would greatly benefit the readers if the authors could elaborate on the key techniques utilized to address these differences and overcome the associated challenges (Section 4 in the current version is far from satisfactory). By doing so, the authors can provide valuable insights into the novelty and contributions of the proposed approach. Understanding the techniques employed to tackle specific obstacles will enable the readers to evaluate the significance of the paper more effectively and appreciate its potential impact on the field.


-------------------
After rebuttal, as the authors address most of my concerns, I would increase my score to 6. I would still suggest the authors to carefully revise the paper to make it more readable if accepted.  No. The limitation is not discussed. As there are many assumptions, I suggest that the authors should discuss whether these assumptions can be relaxed, as well as the cases where these assumptions cannot hold. ",445,0,3,0.7604000000000001,0.1671732523,0.8855220079,215,35.8282,0.1507,neurips,0.0283018867924528,2,4,3,3,factual,4,3,65,neutral,4,neutral,4,low,5,4,4,5,factual,4,5,85,polite,5,negative,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,5,3,4,5,factual,4,4,85,polite,5,neutral,4,low,5,3,4,5,factual,4,4,85,polite,5,neutral,3,low
42,Reviewer-X8JD,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","The paper is a well written and clear result demonstrating the convergence of the actor critic algorithm. To my knowledge, this is the first example of global convergence of the actor critic algorithm using neural network parametrization. It is a good extension of the actor critic sample complexity analysis given in works such as Xu et.al (2020) from a linear function approximation to a neural network approximation for the value function. Extends well established analysis of single timescale actor critic for a linear function approximation to one with a neural network approximation for the value function. 

Provides better convergence bounds than current analyses of actor critic using neural network approximations, while not having the restriction in the depth of the networks that existing results have.

The paper is well written, concise and easy to follow.
 Since a finite state space is being assumed here, the comparison to existing results  such as Wang et. al (2019) and Cayci et. al. (2022) does not seem to be valid. Both these works assume an infinite state space. Since the constant c1 is a multiple of the cardinality of the state space, an infinite state space does not seem to work for the analysis given here. 

The upper bound on the norm of the Hessian of the neural network in Liu et.al (2020) is stated as a probabilistic bound. This bound is stated as deterministic in the lemma B.1.

Assumption 2.7 while being obvious for a linear function approximation, has not been assumed in the works cited where a neural network approximation has been used such as Cai. et. al (2019) and Xu and Gu (2020). Thus the validity of the assumption has not been established for the setup being analyzed. Can the existing result be extended to an infinite state space? That is not immediately clear from the analysis done here.

As a consequence of the finite state space, what is the advantage of assuming a neural network approximation here and not a tabular form of the value function?
 Since this is a theoretical work which analyses existing algorithms negative societal impact is limited.",351,6,2,0.6994,0.0870238095,0.9289754629,215,50.1232,0.1262,neurips,0.0,2,4,4,4,factual,4,4,75,neutral,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,5.0,5.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,5,4,4,partially factual,4,4,85,polite,5,positive,5,low
42,Reviewer-akFw,Convergence of Actor-Critic with Multi-Layer Neural Networks,"The early theory of actor-critic methods considered convergence using linear function approximators for the policy and value functions. Recent work has established convergence using neural network approximators with a single hidden layer. In this work we are taking the natural next step and establish convergence using deep neural networks with an arbitrary number of hidden layers, thus closing a gap between theory and practice. We show that actor-critic updates projected on a ball around the initial condition will converge to a neighborhood where the average of the squared gradients is $\tilde{O} \left( 1/\sqrt{m} \right) + O \left( \epsilon \right)$, with $m$ being the width of the neural network and $\epsilon$ the approximation quality of the best critic neural network over the projected set.","This paper establishes the convergence of single-timescale actor-critic with neural networks representing the value and policy with > 1 layer, strengthening over prior results in the linear setting and two-scale approaches. 
 I will preface my review by saying that I have little background on convergence of actor-critic methods or in analyses of deep networks -- and thus not much to say about the significance of the technical advances. 

I found that despite this lack of background, this paper was an absolute pleasure to read. The paper is very well-written, and the authors do a great job of motivating the problem and the technical approach. Each assumption is well-motivated, and the two tools -- nonlinear gradient splitting and the nonlinear small gain theorem -- are also described in a way that is easy to understand. I wish that more theory papers were written like this!

While I briefly looked through the proofs in the appendix, I unfortunately do not have the expertise to gauge correctness.
 While the mechanism of the proof was very well explained in the paper, I would have liked to see some more discussion about the significance of the result and it's implications for future work. Why is this result interesting? What does it enable? Perhaps it would be useful to spend a little more time discussing the applicability of the proposed tools and theory beyond their application to AC -- what other places may these technical tools be useful?  1. I would have loved to see a little more discussion on future directions. What are the next steps to relax? Or is it to more tightly characterize the convergence?

2. How does this play with value learning when the TD objective does not correspond to a gradient descent (e.g. in off-policy learning)?  N/A",296,0,3,0.784,0.1785533911,0.9276584387,215,52.6077,0.0762,neurips,0.0,2,4,2,2,factual,4,2,55,polite,2,positive,2,moderate,4,5,3,4,partially factual,4,4,85,polite,5,positive,4,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,3,3,factual,4,3,75,polite,4,positive,4,low,2,5,3,3,partially factual,4,3,75,polite,5,positive,4,low
78,Sang-arun-Isaramalai,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. Incongruency on the philosophical basis of the research methodology between qualitative and quantitative has existed- using the term ""sample"" in results - page 4 and in Limitation in page 8 including terms, representative & confounders. 2. Introduction- gap of knowledge was unclear-why need to explore those influencing factors, what have known and what need to be explored for resolving the problem. 3. Using qualitative data analysis, grounded theory, themes are expected to be emerging from the data themselves not from known categories.  4. The Procdures, page 4 need to take out the subject, the researcher.  5. Figure 1, Need to include influencing factors in the diagram and provide discussion on how those factors mediate or moderate the decision. 6. Discussion - page 7 need to explain why on the study findings not part of literature review. 7. Discussion page 8 - study results from qualitative research are not ready for utilization or designing intervention. 8. Conclusion - Not summary of the results, but need to focus  on what was new knowledge emerging from the study, what confirmed existing knowledge.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",325,0,8,0.7509,0.1459498834,0.6593110561000001,86,25.8,0.0999,f1000,0.0,4,5,4,5,factual,4,4,70,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,85,polite,4,neutral,4,low,1.0,4.0,3.0,2.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,2.0,low,4,3,4,4,factual,4,4,4,neutral,5,neutral,3,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
78,Marta-Talavera,Factors influencing the decision to commit violence in Thai male juvenile offenders: A phenomenological study,"Background: Violence is a social problem that affects the physical and mental health of adolescents. For a long time, Thailand has adopted strategies formulated by the World Health Organization to reduce violence but has been unsuccessful. The aim of the current qualitative study was to understand the decision of adolescents to commit violence and to identify factors contributing to violence among male juvenile delinquents. Methods: Data were collected from 50 male juvenile offenders at the Department of Juvenile Observation and Protection detention facilities located in 5 regions of Thailand through in-depth interviews focusing on delinquent violence committed in the past year. Results: Adolescents who decide to use violence have been associated with and live in environments where they face conflicts in their neighborhood and violence in their community. Mostly, juveniles were found to drop out of school, engage in abuse and supply of drugs, consume alcohol, and experienced domestic violence problems and family divorce. Juvenile offenders typically experience and learn about violence from family and peers, which creates a positive attitude toward violent behavior in them. These offenses can be categorized into intentional violence, which involves seeking revenge or resolving prior conflicts and requires premeditation, and unintentional violence, which results from a situation escalating quickly and usually requiring no preplanning, such as insults, conflicts, power struggles, self-defense, or protecting peers. Conclusions: A violence prevention model and guidelines need to be introduced into Thailand’s youth health care system. This study identified a lack of both decision-making skills and socially adequate adjustment to difficult situations among adolescent perpetrators as precursors to violent behavior.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The theme is interesting and relevant, but the sample size is too small to be able to generalize results.  Also, it would be necessary to provide a better social and economic contexualization. The bibliography needs to be updated with more recent references. The methodological description is not clear. The exhibition is not detailed as well as the subsequent analysis, so the results do not have sufficient foundation for the statistics.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Partly  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",215,0,1,0.7398,0.1799479167,0.6117376089000001,118,27.93,0.0999,f1000,0.0099009900990099,1,3,2,1,unfactual,2,1,30,neutral,2,negative,2,high,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,3,3,partially factual,3,3,60,neutral,4,neutral,2,moderate,2,3,3,3,partially factual,3,3,60,polite,4,neutral,2,moderate
199,James-W.-MacDonald,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The iSEE package was developed to allow people to easily perform exploratory data analysis with data that are stored in a Bioconductor SummarizedExperiment object. A SummarizedExperiment container allows researchers to store one or more matrices of data, where the columns represent samples, and the rows represent either genomic positions or genomic features (genes, exons, transcription start sites, etc). In addition to the matrices of data, the SummarizedExperiment also contains two additional objects that describe the samples (the colData) and the rows (the rowData or rowRanges). iSEE allows users to interactively plot the underlying data from a SummarizedExperiment, and also choose subsets of the data based on either interactive selection of data in a plot, or by selecting samples or genomic regions based on the colData or rowData. The chosen subsets can then be linked to other plots in the Shiny Dashboard. This simplifies what could be a complex process, allowing both experienced R users a quick way to check over their data, and allowing less experienced R users the ability to do things that they otherwise might not have been able to do. All the underlying code generated while making interactive changes is saved and can be printed out later, in order to make the exploratory data analysis reproducible. This is an excellent feature, particularly for those who want to share observations with colleagues that may not be local.  The only negative for this package is that, being based on the Shiny framework, to allow a colleague to explore the data requires that the colleague either have R, iSEE, and all its dependencies installed, or that you have a server running all necessary packages that you can point the colleague to. This limits sharing with people who are not R savvy, but is a function of how Shiny works, rather than the iSEE package. This is a high quality package, and given the generalizability of the SummarizedExperiment package, is applicable to a whole range of different data types. Given the ease of use, self documenting features, and applicability to multiple data types, this package will likely become very popular for exploratory data analysis.  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",506,0,1,0.755,0.1682376396,0.918002069,5,26.03,0.1585,f1000,0.0,0,5,4,0,factual,3,4,80,neutral,4,positive,5,low,3,5,5,5,factual,5,5,95,polite,5,positive,5,none,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,3,5,5,3,factual,5,5,90,polite,5,positive,5,none,3,4,4,4,factual,4,4,92,polite,5,positive,5,low
199,Lorena-Pantano,iSEE: Interactive SummarizedExperiment Explorer,"Data exploration is critical to the comprehension of large biological data sets generated by high-throughput assays such as sequencing. However, most existing tools for interactive visualisation are limited to specific assays or analyses. Here, we present the iSEE (Interactive SummarizedExperiment Explorer) software package, which provides a general visual interface for exploring data in a SummarizedExperiment object. iSEE is directly compatible with many existing R/Bioconductor packages for analysing high-throughput biological data, and provides useful features such as simultaneous examination of (meta)data and analysis results, dynamic linking between plots and code tracking for reproducibility. We demonstrate the utility and flexibility of iSEE by applying it to explore a range of real transcriptomics and proteomics data sets.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Authors show an interactive visualization tool for a very common data type used for many of the packages in Bioconductors (SummarizedExperiment). It has enough flexibility to explore all kind of information the object can contain, an interactive tool based on Rshiny, is customizable so it can be adapted to each user. I only have minor some comments: Tutorial 2: step 10 gets the text box in the upper left of the windows, but I think it should be at other position since it says to change the y-axis of the plot. I think this happens when the user doesn't follow the instruction to click on to some button that should expand the menu with more options.  It would be nice the tour re-start from the position it was left, with an option to start over. It happened many times that I click accidentally outside the box and I had to start over.  In the cases the object doesn't have reducedDim for more than the 2 dimensions shown in the plot. I tried to use 3, and it gave an error. Maybe a more informative error would help the user to understand that there is no that information.  I am not totally sure how to use the rintrojs package to generate a tool. It would be nice a reference to some documentation on how to do it or clarification if I am not understanding this correctly.  For the features mentioned like code tracking and additional functionality, it would be nice to have a link to the vignette in the paper so the user can jump into how to get it done.  I think it would be nice to make available a docker image with all the requirements to run iSEE installed. It would promote the use of the tool a lot among bioinformaticians working with non-computational researchers.  It is nice to change the color for all the variables. I would add an example on how to change the palette for all categorical since the code would be slightly different than the one for continuous variables. It would make the user quickly using that option and avoid silly errors.  I don't know if this is possible as it is right now, but it could be an option to load a RDA/RDS file containing the SE object instead of creating an app only for that data? That would open the door to deploy the tool independent of the data. For instance, I can see a scenario where iSEE is installed in a docker container, where the user just starts the image and when opening the browser at localhost:8787, there is an option to load a file with the object.  Congrats on the tools!  Is the rationale for developing the new software tool clearly explained? Yes  Is the description of the software tool technically sound? Yes  Are sufficient details of the code, methods and analysis (if applicable) provided to allow replication of the software development and its use by others? Yes  Is sufficient information provided to allow interpretation of the expected output datasets and any results generated using the tool? Yes  Are the conclusions about the tool and its performance adequately supported by the findings presented in the article? Yes",603,0,1,0.7697,0.1625646946,0.8846502304,6,48.84,0.11,f1000,0.0,5,5,5,4,factual,4,3,95,polite,5,positive,5,none,5,5,5,5,factual,5,5,100,polite,5,positive,5,low,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,4,4,4,5,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
108,Reviewer-euBm,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper presents a new approach called Merlin for goal-conditioned reinforcement learning, which is inspired from diffusion models. The authors introduce a new approach to construct a ""forward process"" by stitching trajectories if there are two states from different trajectories are close. The forward process outputs a augmented dataset, and authors propose to learn the corresponding backward process. They validate their method on offline goal-reaching tasks and show competitive results with state-of-the-art methods. Overall, the paper proposes a new class of goal-conditioned RL algorithms, 1. I like the high level idea of this work which is inspired from diffusion models: constructing a simple forward process to enlarge the training set by injecting noise, and learning the reverse process. Specifically, they use the Nearest-neighbor Trajectory Stitching to generate more data. The algorithm is somewhat novel and might work well on some tasks. 

2. Competitive results: The authors validate their approach on offline goal-reaching tasks and show competitive results with state-of-the-art methods. This demonstrates the effectiveness of their approach and its potential for real-world applications. 1. Weak theoretical justification: diffusion models enjoy strong theoretical foundations, the forward and the backward process are proven to share the same marginal distribution. However, it is not clear to me whether the backward process of  Nearest-neighbor Trajectory Stitching still has similar theoretical guarantees.

2. Limited range of applications: Nearest-neighbor Trajectory Stitching seems to be designed for some specific applications. The generalizability remains unclear.

3. Misleading title: Diffusion models have a relatively clear definition now. While there are ""forward"" and ""backward"" processes in this paper, this algorithm does not fall into the class of diffusion models. See weaknesses.",271,0,5,0.8161,0.0717140796,0.9331908226,47,30.1209,0.1695,iclr,0.0113636363636363,2,5,3,3,factual,4,4,75,polite,4,neutral,4,none,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
108,Reviewer-5ke8,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This paper models the offline GCRL problem in offline data in a diffusion process-like paradigm called merlin. The authors consider three choices for the noise model to replace Gaussian noise in diffusion including reverse play from the buffer, reverse dynamics model, and a novel non-parametric trajectory stitching. This is an improved behavioural cloning paradigm without the need to learn an additional value function, which achieves excellent results in offline control tasks. 1.Novel perspective of framing goal-reaching as a diffusion process. 
2.Trajectory stitching technique seems useful for generating diverse state-goal pairs from offline data.
3.Strong empirical results on offline goal-reaching tasks compared to prior methods. 1.Although the paper seems to describe a feasible diffusion-like process to model the GCRL problem, I think merlin is essentially a variant of constrained GCSL. From this perspective, merlin has only limited novelty. Start with the cleanest method, merlin build policy upon $s, g, h$ instead of $s, g$ by GCSL. Although the merlin shows better results in the motivation example, I think it's because of the inclusion of a more stable time guide.
2.I observe that Merlin-NP and Merlin-P show better results in the experiments, but they can be considered as GCSL + temporal constraints + reverse dynamics model (Wang et al.) + trajectory stitching (a commonly used data augmentation method in OfflineRL). These other components can be easily combined with the universal GCRL approach, so the performance gains are no surprise. 
3.The approach seems sensitive to hyperparameters like time horizon and hindsight ratio. I'm not sure that good performance comes from hyperparameter tuning. 1.What metric and distance threshold works best for the trajectory stitching? Is there a principled way to set this?
2.In appendix table 5, I have observed that there is not much difference in success rate between Merlin and DQL, GCSL and other methods, whereas there is a bigger difference using reward metric, why is that? Success rate should be a common metric for evaluating a GCRL algorithm.

Overall this paper proposes interesting ideas for offline goal-conditioned RL as diffusion process. The empirical results are strong but there are some open questions (see above weakness and questions). Addressing some of the weaknesses and questions raised would strengthen the paper further. I think the central problem is that the article overclaimed the design of the approach to solving the GCRL problem by a diffusion process and I vote reject for current version.",399,0,0,0.8320000000000001,0.1665223665,0.9278346896,47,43.6593,0.11,iclr,0.0114942528735632,4,4,5,5,factual,5,4,85,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,75,polite,4,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,60.0,polite,5.0,neutral,3.0,low,3,3,4,3,factual,3,3,65,neutral,4,negative,4,low,4,4,4,4,partially factual,4,3,78,neutral,5,negative,5,low
108,Reviewer-6Zbj,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","The paper proposes a diffusion based method for goal-conditioned Reinforcement Learning. It is assumed that a dataset of offline demonstrations is given (which also indicate a goal variable g). This dataset is then used to train a diffusion-model-based policy. The idea is to inverse-diffuse the current sample to eventually arrive at the goal point. Hence, the noising process is in state space, where the idea is that the goal state is continuously noised (generating a reversed trajectory). - The problem setting is interesting
- The figures are nice and intuitively explain the ideas presented in the paper
- The analogies to behavior cloning are interesting
- Reduction in need for denoising steps is beneficial - The introduction could be improved by making the exact problem setting more clear from the beginning
- The nearest neighbor based approach makes the assumtion that close states are connected/ can be accessd from each other, this should be discussed. This could also be evaluated by designing a more complex toy environment based on the environment in Figure 2.
- The related work description of Janner et all is not exactly correct, as it is not full trajectories that are noised but just trajectory segments
- A comparison to the related works such as \[A\] would be appreciated
- An ablation on trajectory stitching is only implicitly done (by defining different algorithms)
- A motivation for the goal-conditioned problem setting (instead of starting with just a single goal setting) would be beneficial  
 
-.

- The description of the method is rather confusing. First, it is explained that the trajectory is denoised, which would result in a denoising of states. However, in the following sections, suddenly the action is denoised (see section 4.1)
- Is the policy a diffusion model? It is mentioned that BC is performed at the end of section 5.2.
- The paper would definitely benefit from an algorithm description of the method
- It appears that the dataset extension through trajectory stitching is not performed for the baseline methods, which makes the comparison unfair
- The fact that methods based on inverse dynamics model approaches did not work weakens the method, as trajectoriy stitching has obv. downsides and likely only works in state spaces that resemble physical environments


Related work:

\[1\] ""Goal-Conditioned Imitation Learning using Score-based Diffusion Policies"", Reuss et al. 2023 See weaknesses",392,1,1,0.7325,0.0553199405,0.9015843272,72,34.2575,0.1041,iclr,0.0106382978723403,3,4,2,3,partially factual,3,3,70,polite,3,neutral,3,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,3,4,4,partially factual,3,3,70,neutral,5,neutral,4,low,3,3,4,4,partially factual,4,4,78,polite,5,neutral,3,low
108,Reviewer-i457,Learning to Reach Goals via Diffusion,"Diffusion models are a powerful class of generative models capable of mapping random noise in high-dimensional spaces to a target manifold through iterative denoising. In this work, we present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of diffusion modeling. Analogous to the diffusion process, where Gaussian noise is used to create random trajectories that walk away from the data manifold, we construct trajectories that move away from potential goal states. We then learn a goal-conditioned policy analogous to the score function. This approach, which we call Merlin, can reach predefined or novel goals from an arbitrary initial state without learning a separate value function. We consider three choices for the noise model to replace Gaussian noise in diffusion - reverse play from the buffer, reverse dynamics model, and a novel non-parametric approach. We theoretically justify our approach and validate it on offline goal-reaching tasks. Empirical results are competitive with state-of-the-art methods, which suggests this perspective on diffusion for RL is a simple, scalable, and effective direction for sequential decision-making.","This presents a method for sequential decision making with diffusion. It frames sequential decision making as the reverse process in diffusion. In this case the initial state is “noise” and the final state is the result of denoising. For a particular goal state the policy will “denoise” the initial state. An additional contribution of this work is their “trajectory stitching method”. If there are states that are nearby to one another in two different trajectories then the dataset can be augmented by concatenation of trajectory segments (making sure to relabel the goal state for the swapped trajectories). Interesting dataset augmentation technique that might improve performance on some control tasks. The trajectory stitching method is only usable if distance between two states can be defined. What if states are observed via images? Additionally what if distance between states is not indicative of their relation to one another in a sequential process. What if there are discontinuities in states?

Transition from 3.2 to 4 is abrupt. No additional information on issues with offline reinforcement learning.

GCSL seems to be a very important concept which is used as a baseline algorithm in this paper. Yet there is no description of it in related work. How is GCSL different from GCRL?

Figure 7 is referenced in the main text but appears in the appendix. is the method applicable with partially observable states e.g., images?",230,0,0,0.7707,0.1163095238,0.8710092306,47,49.2569,0.4095,iclr,0.0206185567010309,3,4,3,3,factual,3,3,80,neutral,3,negative,4,low,3,4,3,4,partially factual,4,4,72,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,2,3,3,2,factual,3,3,60,neutral,4,neutral,4,moderate,3,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
135,Clement-Jonquet,Ontology alignment for wearable devices and bioinformatics in professional health care,"Web Ontology Language (OWL) based models and triple stores hold great potential for access to structured information. Not only are OWL-based ontologies extremely versatile and extendable, but triple stores are robust against changes to ontologies and data. The biomedical field illustrates this value insomuch as it employs vast amounts of information distributed across different models and repositories. This paper presents a case study that sought to demonstrate the real-world value of linking disease, symptom, and anatomical models with wearable devices and physical property models and repositories. Integrating these models is both necessary and problematic; necessary to provide undifferentiated access to health care professionals, problematic because although the biomedical ontologies and repositories exist, they aren't semantically aligned and their designs make alignment difficult. This case study demonstrated that manually linking multiple biomedically-related models can produce a useful tool. It also demonstrated specific issues with aligning curated ontologies, specifically the need for compatible ontology design methodologies to ease the alignment. Although this study used manual ontology mapping, it is believed that systems can be developed that can work in tandem with subject matter experts to reduce mapping effort to verification and validity checking.","The papers presents an application of semantic web approach for integrating health/medical data with wearable device information. The idea is to offer an integrated model for: device, physical property, Anatomy, Symptom and disease. The authors explain the ontologies they have used and how they have connected them. The paper has significant issues to be considered good candidate for a journal publication. Mainly, the study presented in this ‘application report’ appears not finished, with negative conclusions in terms of scalability/expansion of the approach and impact. Although the position of the study is never clear between data integration vs. data interoperability, the paper does present an integrated model for 5 ontologies: DOID, SYMP, FMA, QUDT and SSN. Such models is supposed to help to represent device related data (at least Vandrico) and answer queries such as the one given in beginning of introduction. Such a model is an interesting contribution which certainly has value and should be evaluated by the community. However, the biggest lack is that the model proposed is not really experimented/tested/used in the paper. Its impact is not demonstrated. By the end of the paper, 4 diseases (from DOID) are mentioned, but no information about the numbers of concepts/relations from other ontologies are given. The methodology for generating manually symptoms for disease is purely manual and such a task should not be given to non-medical expert. And if medical experts would be in the loop, they would know the disease-symptoms relations without having to search manually the web. In addition, as stated by the authors this approach totally prevent to be extended for more diseases and syndromes. Overall in the paper the clarification about what the authors called ‘alignment’, ‘mapping’, ‘semantic bridge’ etc. is not clear. The notion of alignment is pretty clear in the community and I am not sure I will call ‘connecting ontologies to integrate them in a common schema’ an ontology alignment. What you are doing is a good example of designing a new small schema or ontology with strong reuse of other existing ones. Which is a good practice, but it’s not ‘ontology alignment’ rather ontology reuse. Such point would have be avoided if the paper would provide a real state of the art related to: the use of wearable device and relevant ontologies for them and previous work that have proposed an integration with biomedical ontologies. This is a strong lack of the current paper. In conclusion, I will say that the current paper does not offer convincing evidence of the impact and importance of the application. The core of the contribution (i.e., the integrated model) might be useful (assuming it does not exists, what a state of the art on that aspect would have said) but the application of that model does not convince the reader of the results one can obtain by using such a model. Semantic web technologies are used at least by the fact of offering the new model as an ontology. But nothing related to semantic web data technologies is mentioned (eg., RDF etc.). Major comments by sections: -	Abstract: “undifferentiated … professionnals”. = unclear -	Abstract: “a useful tool” : such what? -	Use of section numbering and structure is obscure. Unique subsections are used. -	Section 1: You should discuss that the query the doctor is asking in the case of diabetes II will be asked only once… then the doctor will have the knowledge that diabete => deviceX. -	Section 1 should rather concentrates on wearable device rather than on the impact of ontologies and semantic web technologies. The audience of the SWJ will know this. -	Beginning of section 2 is unclear. Your goals are described with words that haven’t been clarify to the reader yet. Maybe come back on this in conclusion. -	If you assume a device is always something that measure a property, then say it explicitly. -	You could give examples in beginning of section 2 to illustrate your speech. -	Section 3.1: explain what you mean by semantic bridge. If your contribution is a “semantic bridge ontology” define this introduction, give it a name and refer to it by its name. -	Use namespace abbreviation in your figure, this will help figuring out what is existing, what is yours. Provide your own namespace. -	You need to tell us more about Vandrico data source. Size, format, importance in the field, why this one, etc. -	Section 4 is not a relevant state of the art for your application. This section must allow to answer what have been done in the semantic web for medical and wearable device integration? Nothing on mapping (also it is not necessary if you don’t call your work mapping/alignment anymore). Nothing on device. -	Mission conclusion that comes back on the contributions and discuss them before detailing the perspectives. Minor comments: -	Section 1: “locate” : do you mean “find out” -	Section 2: ‘4’ => four -	‘Figure 1:’ => ‘Figure 1.’ -	Section 2.1 exists without section 2.2. Idem for 3.1 -	Section 2.1: ‘OBO Disease Ontology’ => don’t need OBO. Idem after. -	Fig 2 is important in your paper but totally unreadable. -	Section 3.1 ‘be exist’ => English -	Fig 3 is also too small.",875,0,1,0.7801,0.0872040207,0.9324635863,60,47.99,0.0391,semanticweb,0.0404040404040404,5,5,4,5,factual,4,4,90,polite,4,neutral,4,low,4,3,4,4,partially factual,4,3,65,neutral,5,negative,5,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,moderate,3,3,5,3,factual,4,4,75,neutral,5,negative,4,low,3,2,4,4,partially factual,4,3,65,neutral,5,negative,3,low
52,Reviewer-eZuw,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","- This paper proposes a novel part-based algorithm for 3D novel class discovery (NCD). Authors propose Decompose Novel Into Known parts (DNIK) that leverages knowledge about parts of known objects to discover novel classes.
- Authors identify that the main problem with learning 3D features for object discovery is that the features are heavily biased towards the known classes. This work shows that this can be prevented by using the well known part-based modeling approach. 
- DNIK is trained to learn a part concept bank that can be used to compose different known and novel objects. Three different regularizations are proposed to prevent the collapse of this part bank.
- Extensive experiments show the deficiencies of existing 2D NCD literature and the effectiveness of DNIK to overcome these issues. - This paper builds on an age old, part-based models in visual recognition and shows impressive improvements over single holistic representations currently in use in the 2D category discovery methods. As shown in the experiments this has significant merits for identifying and grouping new classes. 
- The paper writing was smooth and was very easy to follow. An experienced engineer would be able to reproduce the work with the given details.
- Authors support all the claims made in the paper with experiments on real world datasets or on toy problems. Sec 3.1 and Fig. 4.a were particularly interesting to understand what the authors were trying to convey.
- The effectiveness of the method was shown with the impressive experimental results. - While the problem tackled by the authors is relevant and important, the setup adopted by the authors is outdated. Generalized category discovery, as done in \[1\]\[2\] is a more realistic setting and it is not clear why the current method is not suited for this setup or the authors advice against it? It is my strong suggestion to the authors to answer this question and compare with the relevant work (cited below) to justify this work among existing literature.
- The toy example in Sec. 3.1 is not fair for the following reason. In L86, the setup states that the classes in known and unknown sets share some similarities, but in the example authors choose {table, sofa, stool} as known and {chair, bench, bathtub, plant} as unknown objects. In this case, bathtub and plant do not share any commonalities with the known objects. It looks like the authors intentionally exaggerate the problem to make their point. While this is acceptable, it is not clear how much of a serious issue is the ""overfitting to the known classes"" problem. 
- Author propose to use supervised contrastive loss to learn more parts from the known shapes. The motivation and explanation doesn't justify why this would be the case. In L204 authors pool the features along the last dimension which basically is a ""shape"" feature as opposed to a ""part"" feature. It is not clear how the contrastive loss helps learn more part features when the loss is being applied on the ""shape"" features.
- Table 4 demonstrates the performance improvement for each of the proposed components but experiments to demonstrate that show that the regularizations on the part features actually operate as the authors claim is missing. What happens when diversity loss is missing? Do all the part features in the bank collapse to fewer representations? This can be quantified by cosine similarity between the part features. Similar analysis on the remaining two regularization terms is warranted.

\[1\] Sagar Vaze, Kai Han, Andrea Vedaldi, Andrew Zisserman, Generalized Category Discovery, CVPR 2022.
\[2\] Sai Saketh Rambhatla, Rama Chellappa, Abhinav Shrivastava, The pursuit of knowledge: Discovering and localizing novel categories using dual memory, ICCV 2021. - Can Fig. 5b, 5c be combined in to one plot? Two separate plots makes it hard to understand what values of K, Q are being used for each of these. 
- Legend for Fig. 6 is missing.  Authors have addressed the limitations adequately. ",656,4,2,0.7643000000000001,0.1391305916,0.9319424629,220,50.7302,0.0437,neurips,0.0,4,5,4,5,factual,4,5,95,polite,5,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,5.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,low,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
52,Reviewer-dey2,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","In this work, they address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, the authors propose to decompose novel into known parts, coined DNIK, to mitigate the above problems.  1. The paper is well written and motivation (separate instances into repeatable parts) is pretty good. 
2. The model design is reasonable and the improvement is satisfied. 
3. The experimental analysis is sufficient.  1. This paper does not consider hierarchical part representation. 
2. why does Part Relation Encoder work for novel classes ? 
3. Does the improved representation works for some scene level tasks, such as novel class segmentation for point cloud ?  see the weakness  yes",151,0,6,0.7591,0.1863636364,0.9325822592,220,42.9518,0.0945,neurips,0.0,2,4,3,2,partially factual,3,2,65,polite,3,neutral,3,moderate,3,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,4.0,none,2,3,3,2,factual,3,3,60,neutral,4,positive,4,moderate,3,4,4,4,partially factual,4,3,85,polite,5,positive,4,low
52,Reviewer-mbrM,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper tackles the problem of novel category discovery in the 3D shape recognition domain, a framework leveraging the 3D parts and the part-wise relation is proposed which the motivation is learning the parts from the known classes could help the model capture more transferrable features or concepts for the novel categories.
This motivation is validated using experiments, and overall the framework shows better performance than some baselines. 1. The idea of decomposing a category into parts is interesting.
2. I like the organization of this paper, starts with an analysis of the problem of previou method, and then proposed new ones based on the analysis.
3. The paper also explored a bit on the design choices for 3D novel category discovery, which could be helpful. 1. This paper still considers the novel category discovery problem while a more generlized setting exists, generalized category discovery \[R1, R2\], I would suggest the paper to include more discussion and experiment on this generalized setting.
2. It seems that the total number of categories in the datasets are quite small compared to 2D NCD, I am wondering if Objaverse \[R3\] can be used for this task?


\[R1\] Generalized Category Discovery, CVPR 2022
\[R2\] Parametric Classification for Generalized Category Discovery: A Baseline Study, arXiv.
\[R3\] https://objaverse.allenai.org/ 1. In the v1 version of SimGCD fig 10 \[R4\], it is shown that the accuracy on known classes first increases and then drops while the novel class accuracy keeps improving, this contradicts the observation made in this paper, I am wondering if this is because of the setting (generalized category discovery v.s. novel category discovery), the data (2D v.s. 3D), or the number of categories(200 v.s. 7)? Consider this observation is the motivation for this paper, this question will be the biggest concern of mine.


\[R4\] https://arxiv.org/pdf/2211.11727v1.pdf I think the main limitation is that the number of categories is small, thus the conclusion made based on these small datasets may not be generalizable to larger scale datasets.

Overall I think this paper is very clear, and could be of interest for the community, however the concerns I raises in the questions should be addressed first.",358,2,7,0.746,0.1578253119,0.9263672829,220,32.9482,0.1262,neurips,0.0188679245283018,2,4,3,2,partially factual,3,3,60,polite,3,positive,4,low,4,5,4,4,partially factual,4,4,80,polite,5,neutral,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,positive,4.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,3,85,polite,5,positive,4,low
52,Reviewer-qzzG,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This work presents a framework, called Decompose Novel Into Known parts (DNIK), that addresses the challenge of 3D Novel Class Discovery (NCD) – identifying new classes from an unlabeled dataset using the knowledge of known classes. Current methods, heavily biased towards known classes, struggle to generalize to novel classes. By leveraging more generalizable geometric parts across different classes, DNIK mitigates this issue. It constructs a part concept bank encoding rich geometric patterns from known classes, which is used to represent novel 3D shapes as part concept compositions, thus facilitating cross-category generalization. DNIK also leverages part-wise spatial relations for improved recognition. The method has been tested through three 3D NCD tasks, consistently outperforming state-of-the-art baselines.


---- after rebuttal ----

As the author's rebuttal resolved some of concerns, I raised my score to 5. However, I still feel the studied task is a bit simple, and also there are several spaces to improve for the current manuscript. I will not fight for its acceptance.  The studied direction is important as we need to understand parts well to play with 3D objects generalizable. This paper takes a step towards open 3D object recognition via part understanding. Overall, the components used in the proposed framework are sound and reasonable. The paper is easy to follow. 


Extensive results shown in Table 1 & 2 demonstrate the strength of the proposed method. The proposed DNIK generally achieved state-of-the-art performance. Some detailed ablation studies are included in Table 4. The cross-domain task is interesting to see the transfer performance.  Utilizing the part are sharable across different 3D object categories are studied in the previous literature \[1,2\]. In those paper, they exploited ""harder"" task, such as segmentation. As the proposed framework can address novel class classification via known part concepts. Can the framework be extended to ground where is those known parts in novel object? Or other applications beyond object recognition?

\[1\] Learning to Group: A Bottom-Up Framework for 3D Part Discovery in Unseen Categories
\[2\] 3D Compositional Zero-Shot Learning with DeCompositional Consensus

How the framework handle two different object categories with limited shared parts, such as airplane and chair? Will the framework train on multiple object categories benefit novel object discovery? It would be good to include some failure cases to analyze and provide readers a sense for the limitation of the proposed framework. 

L46~47 said the framework can help use part relation features. Can the framework be extended to discovery part relationship?  Please address the concerns raised above.  Line 319~320 analyzed one minor limitation. I feel there are potential more:

1. if the part are not sharable between different categories, such as lamp -> chair, table -> faucet, can the framework still handle?

2. the only shown application is recognition which limits the practical use of the proposed framework.",463,3,1,0.8174,0.100393028,0.9493124485,220,43.6879,0.1933,neurips,0.0,2,4,3,2,partially factual,2,2,50,neutral,3,neutral,3,low,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low
52,Reviewer-zZT9,Decompose Novel into Known: Part Concept Learning For 3D Novel Class Discovery,"In this work, we address 3D novel class discovery (NCD) that discovers novel classes from an unlabeled dataset by leveraging the knowledge of disjoint known classes. The key challenge of 3D NCD is that learned features by known class recognition are heavily biased and hinder generalization to novel classes. Since geometric parts are more generalizable across different classes, we propose to decompose novel into known parts, coined DNIK, to mitigate the above problems. DNIK learns a part concept bank encoding rich part geometric patterns from known classes so that novel 3D shapes can be represented as part concept compositions to facilitate cross-category generalization. Moreover, we formulate three constraints on part concepts to ensure diverse part concepts without collapsing. A part relation encoding module (PRE) is also developed to leverage part-wise spatial relations for better recognition. We construct three 3D NCD tasks for evaluation and extensive experiments show that our method achieves significantly superior results than SOTA baselines (+11.7%, +14.1%, and +16.3% improvements on average for three tasks, respectively). Code and data will be released.","This paper addresses the problem of 3D NCD (novel class discovery). The objective is to discover novel classes by leveraging information learned from the known classes. This paper proposes a novel framework, DNIK, for 3D novel class discovery (3D NCD) by leveraging part concepts and part-wise relations learned from known classes to reinforce the recognition of novel shapes. The framework consists of a learnable part concept bank, a local geometric aggregation module, a part relation encoding module, and three constraints to facilitate effective part concept learning.  (1) The proposed part concept bank and part relation encoding module can effectively bridge the gaps between known and novel shapes and mitigate feature bias.

(2) The experiments show that the proposed method outperforms all baselines consistently and significantly on all metrics. 

(3) The paper is generally well-written and easy to follow. (1) The unseen class number is assumed to be known, which makes the method less practical.

(2) The effectiveness of the proposed PRE module is not well demonstrated. The performance is not shown by using only the part position feature from the PRE. Therefore, it is not clear about the individual role of PCB and PRE. It would be good to at least ablate the effectiveness that only uses PRE in Table 4.

(3) The study of NCD has been extended to consider the case where the unlabelled data contains objects from seen and unseen classes \[A\]. It is more convincing to also show results under this more general and practical case. 

\[A\] Vaze et al, Generalized Category Discovery, CVPR 2022

(4) Each part in Part Set Q has the same number of points, that is, K neighbors, which may be dataset dependent and affected by the scale of the objects, while a fixed value of K=64 is selected in the paper. This is unlikely to generalize well to other datasets and objects of different scales. It would be good to show how the method works on instances from the same categories but of different scales. More investigation on this is expected.

 (1) How to ensure the features from PRE include the position relationship of each part? The feature extraction by PRE seems like a process through a black box.
(2) How are the Nq parts like in the initial point cloud? How different/similar are they? The initialization may also heavily affect the results. e.g., if the initial parts are too similar, they are unlikely to be well separated in the end. However, in the beginning, we have little (or no) control over this. The paper has described the potential limitation of multi-scale objects, not mentioning much about the societal impact, but I did not see any major problem here.",448,0,0,0.7433000000000001,0.0980769231,0.9567717314,220,53.5703,0.1199,neurips,0.0,3,4,4,4,factual,3,4,75,polite,4,neutral,4,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,4.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
164,Reviewer-2sTV,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","The paper presents a theoretical analysis of the sample complexity in goal-conditioned hierarchical reinforcement learning (HRL) and establishes a lower bound using hierarchical decomposition to quantify it. Additionally, the paper empirically validates the theoretical results by examining the sample complexity of the proposed hierarchical algorithm on several toy grid-world tasks. I commend the authors for addressing an important theoretical problem in the field of HRL and deriving a lower bound to quantify the sample complexity of goal-conditioned HRL. One significant aspect that the paper lacks is a thorough theoretical analysis regarding the selection of sub-goal spaces in continuous environments or sets in discrete environments. Q1. The selection of the sub-goal space plays a vital role in the efficiency of the HRL algorithm, as different choices of sub-goal spaces can result in varying sample efficiencies, such as in HIRO \[1\], HRAC \[2\], HIGL \[3\], and others \[4\]. Unfortunately, in Theorem 3.1, the authors do not analyze the impact of different sub-goal spaces on the sample efficiency of HRL. Therefore, I find the theoretical results to be trivial.

\[1\] Nachum, Ofir, et al. ""Data-efficient hierarchical reinforcement learning."" Advances in neural information processing systems 31 (2018).

\[2\] T. Zhang, S. Guo, T. Tan, X. Hu and F. Chen, ""Adjacency Constraint for Efficient Hierarchical Reinforcement Learning,"" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4152-4166, 1 April 2023.

\[3\] Junsu Kim, et al. ""Landmark-guided subgoal generation in hierarchical reinforcement learning. "" Advances in Neural Information Processing Systems, 34: 28336–28349, 2021.

\[4\] Lee, Seungjae, et al. ""DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning."" Advances in Neural Information Processing Systems 35 (2022): 13668-13678.

Q2. Another limitation of the paper is the absence of a comparison between the proposed method and other existing HRL algorithms in the experimental section. It would be valuable to include such a comparison to provide a more comprehensive evaluation of the proposed approach See Questions",323,10,9,0.7885000000000001,0.0493421053,0.9271934032,217,28.2965,0.1858,neurips,0.0,4,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low,5,5,4,5,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
164,Reviewer-hRCv,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","The paper takes an important step toward quantifying the benefits achieved due to hierarchical decomposition of an MDP by deriving lower bound on sample complexity of goal-conditioned HRL algorithms. The paper also proposes a novel hierarchical Q-learning algorithm that exploits goal-based hierarchical decomposition of an MDP into a high-level and a low-level sub-MDPs and jointly learns their policies. The authors evaluate hierarchical policies for different decompositions on original MDPs to validate when decomposition provides benefits and whether it aligns with the derived bound. 

The paper is well motivated and clearly written. The ideas of the paper to quantify benefits of hierarchical decomposition are novel. The derived theoretical guarantees on the lower bound are sound. I suggest clarifying that the derived bounds only apply to tabular setting of RL i.e. discrete state space problems in the Introduction. The theoretical findings are  backed by empirical results on maze environments of different sizes with convincing insights. The empirical evaluation would benefit from diversification of domains and tasks not restricted to navigation, and an investigation of bounds when using bad and good decompositions for the same environment.  (I) The paper provides strong theoretical guarantees on the lower bound of the number of episodes given a decomposition needed to learn an epsilon-accurate hierarchical policy, which also serves as a lower bound to learn an epsilon-accurate optimal policy. 

(II) The paper also identifies properties relating to state and temporal abstractions and the size of the high-level action space from the derived bound that can improve sample efficiency.
 (I) The assumptions regarding the scope of the derived bounds restricted to a tabular setting need to be clarified in the Introduction. 

(II) All experiments are on maze environments of different sizes. While the current analysis is convincing for navigation tasks, it will be interesting to see if the benefits of decomposition and the derived bounds align for more diverse domains and tasks that not restricted to just navigation e.g. officeworld \[2\], taxiworld \[3\] etc.

(III) It is not clear how the bounds will identify when a decomposition is bad enough and would degrade the performance compared to non-hierarchical algorithms. 

Minor errors:
(I) Line 48: proposes -> propose (I) How are the ideas of the Stationary Hierarchical Q-learning to overcome non-stationarity of the high-level policy related to the ideas in \[1\]? 

(II) Can you elaborate what it means to separate the original state space evenly between the two level of hierarchy? 

(III) Does the method apply only to dense reward functions?

(IV) Would the bounds identify when a decomposition for an environment would degrade performance of the proposed algorithm compared to Qlearning? 

References:

\[1\] Levy, A., Konidaris, G., Platt, R. and Saenko, K., 2017. Learning multi-level hierarchies with hindsight. arXiv preprint arXiv:1712.00948.

\[2\] Rodrigo Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith. Using reward machines for high-level task specification and decomposition in reinforcement learning. In International Conference on Machine Learning, pages 2107–2116. PMLR, 2018.

\[3\] Thomas Dietterich. State abstraction in maxq hierarchical reinforcement learning. Advances in Neural Information Processing Systems, 12, 1999. (Included in the weaknesses)",508,6,5,0.7992,0.0943627451,0.8967522979,217,29.7299,0.2852,neurips,0.0114942528735632,4,4,4,4,factual,4,3,75,polite,3,neutral,4,low,5,5,5,5,factual,5,5,95,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
164,Reviewer-VzQJ,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.","Provides a sample bound on the complexity of goal-conditioned HRL algorithms based on the two MDPs they are decomposed into, and a Q learning algorithm to leverage these findings. Formulates HRL as a two-level problem, where the upper level passes actions to the lower level policy. The work proves a lower bound on the complexity of the HRL formulation which pivotally scales according to the size of the high level action space and the reusability of the low level space. A new algorithm is introduced which identifies the need for a consistent low level action space, and this method is asssessed in four-room gridworld domains.  This work provides a clean proof with a highly understandable sketch and a strong intuition. Together, this provides an extremely clear and easy-to-read description of the sample complexity of HRL In addition, the theory provides some clear insights into how to understand other HRL work.

This work provides a simple idea applied to the existing framework of HRL in the description of the high-level training based on low-level performance. It also seems like adding the intuition of the shared upper-level complexity term would be useful for keeping the size of the upper policy action space small (by somehow limiting the goals), which is empirically verified in other HRL work.
 Figure 1 is difficult to comprehend, somehow managing to be simultaneously overly simple (this is a basic construction of hierarchical RL) and overly complicated (what is the intuition for the equations on the right-hand side?

This work contrasts against the Options framework in the first paragraph of the background, without specific Ying what the options framework is. As a framework itself, the options framework also makes no assumptions about prior knowledge, no prevents from state abstraction, both statements made about the framework. 

This work spends much of the earlier part justifying the context of the goal-based hierarchy, but it appears that other than the state-based complexity term, there is no strict requirement that the hierarchy be goal based as opposed to simply parameter based. As long as there exists a measure of the performance of the lower-level policy, it seems like the same reasoning would apply. 

The empirical results are somewhat lacking. In particular, while the proof should apply generally to HRL contexts, the work only empirically verifies in maze environments, and maze environments which are constructed to amplify the advantages of the upper-level policy. A different kind of environment such as a mountain car or multiple inverted pendulum would have been interesting, notwithstanding an environment that requires a deep RL method. See the weaknesses section Limited empirical assessment in multiple domains

Additional evidence of how the terms of the bound translate to empirical results would be insightful",452,0,0,0.7528,0.0711098379,0.9008852243,217,34.4183,0.1041,neurips,0.0,3,4,2,3,partially factual,3,3,65,neutral,3,negative,3,moderate,4,4,4,4,factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,4,3,partially factual,3,3,70,polite,4,positive,4,moderate,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
46,Ifeoma-P.-Ijei,Cost-effectiveness of invasive devices versus non-invasive devices for screening of anemia in field settings in India: A study protocol,"In India, an estimated 53% of women and 58% of children are anemic.  The accuracy of Sahli’s hemoglobinometer, commonly used for detecting anemia in public health settings, is questionable. This study presents the protocol for assessment of cost and cost effectiveness of devices for screening of anemia using invasive devices (HemoCue 301 and True Hb), and non-invasive devices (AJO Spectroscopic Test and Masimo Pulse Oximetery test) compared to automated auto-analyser (reference test). The study population will include all adult patients attending the outpatient department in urban/rural health centres for routine investigations. Each included patient will undergo either one or two index tests apart from the reference test, on a predefined weekly schedule to avoid bias. The total and incremental costs of the intervention will be measured prospectively by measuring both screening and provider costs.  Since the priority of the national program is detection of severe anemia, detection rates of anemia and severe anemia will be considered to calculate effectiveness. Cost comparisons of median, average and range of costs across the invasive and non-invasive devices will be calculated. Cost-effectiveness analysis will be compared for four devices within time horizon of 1 year. Ethics approval for the study has been obtained from the institutional ethics committees of the hospitals. The study protocol will generate evidence on the use of cost effectiveness of medical devices to influence policy decisions.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors reiterate the public health significance of anaemia in LMICs and the complexities of the modalities available for haemoglobin measurement in various clinical settings, from the capital-intensive, technically demanding haematology analyzers to hand-held, point-of-care testing devices. The cost-effective determination of anaemia remains pertinent and relevant especially in LMIC settings with high prevalence of anaemia and this, they have demonstrated in their introductory comments. The authors have sought to compare the cost effectiveness of available technology for screening of anaemia in the field. The rationale aligns with clinical and policy needs for a quick, accurate, reliable and cost-effective method for the detection of anaemia which is highly prevalent in the study population and this is borne out in the primary objective of this study. The measurement of HRQoL using EQ-5D tool however, may be reflective of the underlying aetiology of the anaemia detected rather than the nature of the technology (invasive and/or non-invasive devices) utilized in the detection of anaemia. Its determination as a secondary objective may not be relevant to this particular study. The strategy for analysis of cost does not fall under the reviewer’s area of expertise but their exploration of the factors contributing to the evaluation of cost-effectiveness and factors impacting on end-user utilization is quite comprehensive. However, the opening statement for the study design denotes an evaluation of diagnostic accuracy of testing methodology which is not reflective of the title and main text. It is also unclear if the authors wish to demonstrate the cost-effectiveness or otherwise of the gold standard/reference test (auto-analyzer) against the index invasive and non-invasive test techniques or if the latter are being compared to each other. Adequate steps have been taken to satisfactorily address ethical concerns. The authors should indicate the potential levels of policy impact of this manuscript as this could be a significant milestone deliverable from this study. The article is well written and makes for an interesting read. The authors would be well served by providing clarity to the aforementioned observations. Reviewer Guidelines: Title: Is it reflective of the objective and design of the study?  Yes, the title is reflective of the objective and design of the study. Are the keywords searchable? Yes, the keywords are searchable.  Abstract: Are the contents a comprehensive representation of the full text in terms of methodology, findings and conclusion? Is the volume satisfactory? The content of the abstract represents the text and is satisfactorily voluminous. Introduction: Is the literature rich with global, regional and local literatures and perspectives? The literature provided predominantly reflects a local perspective. Is the gap in knowledge that the study is attempting to close obvious? Yes. Methods: Are ethical issues (consent, concealment of subject identity, institutional ethical clearance) well addressed? Yes, these are satisfactorily addressed. Is the study design consistent with the stated objective? Partly. Results: Is there internal consistency – do figures add up? Are there unexplained missing data? Are the Tables and figures simple and clear to understand? This information is not available for review.  Discussion: Are differences or similarities between comparison studies well explained? Are the issues discussed consistent with the findings in the study? This information is not available for review. Conclusion: Are the conclusions based on the findings in the study? Are the recommendations based strictly on the findings in the study? This information is not available for review. References: Are the references adequate and satisfactorily current? Yes. General: Is the entire text satisfactory in terms of spellings, use of punctuation, grammar and style of expression? Yes. Does the manuscript make a substantial contribution to knowledge? Yes, the manuscript has the potential to impact policy direction with some revision.  Verdict: Accept with major revisions. Guidelines for making a Verdict: Acceptance with major revisions - Need for thorough copy-editing for spellings and grammar, data re-analysis, need for more tables or graphical expressions, need for more references or reduction of references, more discussion points, extensive reduction or expansion of the text.  Is the rationale for, and objectives of, the study clearly described? Yes  Is the study design appropriate for the research question? Partly  Are sufficient details of the methods provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? No",772,0,5,0.7193,0.118963964,0.8825360537,1252,29.45,0.1041,f1000,0.010752688172043,4,4,4,3,factual,3,3,70,polite,3,positive,3,low,4,4,4,4,factual,4,4,80,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,3,4,4,3,factual,4,4,75,polite,4,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
22,Reviewer-Qwcz,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper proposes CO2, a new approach that enables efficient distributed training of large language models on clusters with limited bandwidth. CO2 introduces local-updating and asynchronous communication to the distributed data-parallel training, allowing for full overlap of communication with computation. The approach achieves 100% scalability even on clusters with limited communication bandwidth. The paper also introduces staleness gap penalty and outer momentum clipping techniques to improve convergence and training stability. The proposed approach is validated through extensive experiments on computer vision and natural language processing tasks as well. + The paper is well-written and comprehensible.
+ The code is available in this work.
+ The utilization of local updating and asynchronous communication makes a full overlap of computation and communication. 
+ The paper provides enough theoretical explainability and empirical validation. 
+ The experimental results are sound and promising. I do not have much to comment on the weakness, as this work goes beyond my acceptance threshold. How many runs for each task? I understand that training a Language Learning Model from scratch can be quite costly. However, conducting the experiment only once may not yield persuasive results.",187,0,0,0.8098000000000001,0.1653896104,0.9581924677,51,23.0455,0.145,iclr,0.0,1,4,3,1,partially factual,3,2,29,polite,3,positive,2,high,3,5,4,3,partially factual,4,4,85,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,3,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
22,Reviewer-rgZH,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper proposes an approach called CO2 to improve throughput of distributed model training by overlapping computation and communication. Building on prior work that perform multiple training iterations with local updates before each global model synchronization, CO2 enables further throughput improvement by making the global synchronization asynchronous and overlapped with the next round of local updates. CO2 proposes two techniques for addressing the convergence issues of the asynchrony: (i) staleness penalty gap, and (ii) outer momentum clipping. The paper presents theoretical analyses of the convergence guarantees of these two techniques. The evaluation results show that CO2 can achieve convergence results comparable to baselines that are fully synchronous (e.g, Adam) and better than those using local updates (e.g, LocalSGD). The experimental results also show the throughput and scalability of CO2 are better than Adam. The paper is tackling an important problem since communication is a major bottleneck for scaling model sizes and training hardware, and so approaches for reducing communication overheads are very relevant to the community. 

The idea of overlapping communication with computation is reasonable given the cost-effectiveness. I also liked the fact that the paper attempts to quantify and fix the resulting staleness update problem. 

The evaluation considers a diverse and important set of workloads and hardware environments, which helps to understand the generality of CO2. I observe some critical problems in the draft that raise the question of whether CO2 can simultaneously achieve good convergence and high throughput. 

1. The convergence and throughput trade-off of inner loop step count ($\tau$) is not clearly reported in evaluation. In particular, the convergence results in Tables 1 & 2 should include the corresponding $\tau$ and throughput. I was unable to determine whether the good convergence results are achieved with $\tau$ that also provides throughput benefits. 

2. The paper is silent on the memory overheads of CO2 relative to baselines, even though Algorithm 2 suggests that multiple copies of the model is required to support asynchronous communication. 

3. Equation 3 assumes learning rate decay in the inner loop which is not true for learning rate schedules, such as cyclic, which involve learning rate increases. 

4. It is unclear to me whether CO2 can achieve expected throughput benefits in scenarios with parallelism techniques (e.g., tensor slicing, sequence parallelism, and zero stage 3) that introduce communication to forward/backward passes. It seems these (synchronous) communication operations would interfere with the overlapped communication and hurt overall throughput. Evaluating such scenarios could help to better understand the generality of CO2. See weaknesses.",415,0,5,0.7924,0.0957478632,0.8365457058,63,26.0033,0.2025,iclr,0.0,4,4,4,3,factual,3,4,80,polite,4,neutral,4,none,4,5,4,4,factual,4,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
22,Reviewer-m91N,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","To address the communication problem in large-scale distributed training of deep neural networks, the paper proposes a combination of local-SGD and asynchronous communication to derive a new distributed training algorithm named CO2. In CO2, two novel approaches are developed to ensure that CO2 aligns the convergence performance with conventional distributed data-parallel algorithms. Experiments are conducted on a 64-GPU testbed, showing that CO2 outperforms existing methods significantly. The studied problem is timely and important. The paper is also well-written. - Propose a new distributed training algorithm, CO2, using local updates and asynchronous communication to alleviate the communication problem in conventional synchronous data-parallel distributed training. 
- New tricks to address the convergence problem in stale gradients.
- Comphesive experiments to show the effectiveness of CO2. - Some stale parallel algorithms (e.g., SSP \[ref1\]), whose key ideas are quite similar to CO2, were not included in the discussion and comparison. The survey paper \[ref2\] may help find SSP-like methods for comparison.
- It seems that 100% scaling efficiency is over-claimed. The scaling efficiency highly depends on $\tau$. Higher $\tau$ has better scaling efficiency but has worse convergence performance. Thus, achieving 100% scaling efficiency with a $\tau>1$ while sacrificing the convergence performance cannot conclude the algorithm has true 100% scaling efficiency.

\[ref1\] More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server, NeurIPS 2013.
\[ref2\] Communication-efficient distributed deep learning: A comprehensive survey, arXiv 2020. - How about comparing with SSP-like algorithms in terms of theoretical convergence bound and empirical scaling efficiency? 
- How $\tau$ is set in Table 1?
- How about the end-to-end training performance (i.e., time to accuracy)?
- How to choose $\tau$ in a new distributed GPU cluster?",278,0,0,0.8098000000000001,0.0605264378,0.8219593763,51,24.2808,0.0751,iclr,0.0,4,4,4,4,factual,3,4,80,polite,4,neutral,4,low,5,4,4,5,5,5,5,85,5,5,neutral,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,3,4,75,polite,5,neutral,4,low
22,Reviewer-vfwG,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","The paper introduces CO2, a framework for improved communication/computation overlap for distributed deep learning, especially in the case of limited network bandwidth. CO2 leverages local SGD, performing a fixed (tunable) number of local iterations while allreduces perform synchronization in the background, allowing communication to almost always be hidden. To ensure good convergence, CO2 computes a staleness gap metric and uses this to scale updates, as well as a clipping mechanism to limit the variance of updates. A convergence bound is proven and experiments on a variety of network architectures and datasets show convergence matches that of standard SGD and other communication-avoiding algorithms; in the low-bandwidth regime, CO2 additionally offers significantly improved performance and scalability. 1. This paper is addressing an important situation: communication-bound training workloads. This can occur due to both large models and slower interconnects. I appreciate that the paper specifically and clearly calls out lower-bandwidth networks as an area it is focused on. While the idea is relatively straightforward, it includes some details to get it to work well in practice.
2. The paper adequately specifies its proposed algorithm and includes some theoretical justification to support its claims.
3. There are extensive experiments on a variety of models, including relatively large ones, demonstrating roughly equivalent convergence curves, indicating that the method does not compromise learning.
4. Scalability studies are also conducted, showing slightly improved performance on high-bandwidth networks and significantly improved performance on low-bandwidth networks relative to a standard allreduce implementation. 1. I think the claims of ""perfect 100% scalability"" are a bit oversold. This relies on appropriately selecting $\tau$, the number of local steps between global communications; it seems clear that if you can arbitrarily set the amount of computation done to hide communication, you can easily hide it. (Though I wish to be clear that the paper is clear that you can't make $\tau$ arbitrarily high and still achieve good convergence.) This also neglects other aspects of training which may limit scalability (e.g., I/O for data ingestion).
2. The paper does not provide guidance on selecting an appropriate $\tau$, and in its experiments searches over a small set of potential values. This seems like a challenging parameter to tune in practice, as it could significantly increase hyperparameter tuning costs.
3. It is not clear to me how the paper improves upon existing communication-efficient works which try to tune the communication frequency to achieve both good learning and runtime performance. In particular, works like Wang & Joshi, ""Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD"", or Haddadpour et al., ""Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization"", seem like relevant points of comparison.
4. The paper lacks implementation details. Specifically, it does not specify how the asynchronous allreduce is implemented (e.g., is it using a NCCL allreduce on a separate CUDA stream?). It is also not clear whether the asynchronous allreduce is operating on a separate weight/gradient buffer from the one being used for computation; or what the memory overheads of the method are.
5. While I appreciate that the experiments were run multiple times (Section 4.1), the results do not include any measure of variance. This makes it hard to understand whether CO2 amplifies the variance between runs and how much methods actually differ.
6. Scalability is only evaluated on one model. I would be interested to see how models other than the TransNormer-LLM scale; in my experience, smaller models tend to benefit less from communication optimizations as they are already often able to hide most communication.
7. The scaling study in Section 4.3 does not include any comparisons with other communication-efficient methods. Given that SlowMo demonstrates very similar convergence curves, it seems prudent to see whether CO2 offers better scalabiltiy.
8. From a performance perspective, the paper is missing a detailed analysis substantiating its claims. In particular, the communication/computation overlap achieved is never actually measured. 1. I think the paper would be stronger if the claims of ""perfect 100% scalability"" were toned down and better contextualized. (See above for some details.)
2. How should $\tau$ be selected? Is hyperparameter tuning the only way to do so?
3. How does the paper improve upon prior works which tune the communication frequency (see above for some references)? Could these approaches be used to tune $\tau$ automatically?
4. Please add implementation details and a discussion of memory overheads. I think memory may be especially relevant for larger models such as LLMs.
5. Please add the observed variance to the accuracy results. It would also be good to include error bars in the scaling performance results.
6. How do other models considered in the paper (e.g., ResNets or ViTs) scale?
7. How do other communication-efficient (e.g., SlowMo) methods scale on the fast and slow network?
8. How much communication/computation overlap is actually achieved by CO2, particularly at scale?
9. A more minor point: The paper refers to gradient bucketing as a way to overlap communication and computation (e.g., in Section 1). I think this is not quite correct; rather, gradient bucketing is a latency/bandwidth tradeoff (performing fewer allreduces on larger buffers). While this can be more efficient, and consequently improve communication/computation overlap, it does not itself enable overlap.

-----

In light of the authors' response and promised updates, I have raised my score. They have addressed a number of points above.",888,0,21,0.8352,0.157113842,0.8250510693,63,34.2758,0.7721,iclr,0.0,4,4,5,5,factual,4,4,89,polite,4,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
22,Reviewer-cSfz,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","This work proposes a new distributed training algorithm called CO2, which aims to improve the communication efficiency of data-parallel training by overlapping local training iterations with parameter averaging from the previous global step. The proposed method is tested across multiple machine learning tasks and achieves better scalability than the baseline approaches while maintaining comparable convergence properties.

---
Post-rebuttal update: after reading authors' responses and other reviews, I decided to keep my weakly positive score unchanged and increase the confidence of my review. I think that the contributions of the study are solid and I am in favor of accepting the submission, but I am not fully sure that the work will have significant impact on the field in light of prior closely related publications. * Overall, the proposed method is conceptually simple yet shows promising results.
* The paper has a broad range of experiments, covering 5 setups with models that are widely used in practice.
* Authors conduct a detailed ablation study for the components of CO2, as well as measure its scalability in different environments. * While I am not an expert in distributed optimization, to my understanding, similar methods allowing full overlap of communication and computation have been proposed previously. See, for example, \[1\] from the related work section: on page 17, they state that ""as long as the number of local updates τ is large enough, the communication can be fully overlapped with the local computation."" This appears to be quite close to the primary contribution of this work, therefore I believe that the submission needs to describe the key distinctions from prior work in more detail.
* I think that the experimental setup description could benefit from more details. For example, while the authors mention that their hyperparameters were tuned ""to find the optimal balance between efficiency and performance"", we do not see neither the exact values of $\tau$ for each experiment nor the exact description of the tuning procedure. Also, authors mention that they leverage ZeRO for TransNormer experiments, but do not state the exact type of the optimizer within that family.
* Lastly, the majority of model sizes used in this work have quite small parameter counts (fewer than 1B), and therefore it is a bit surprising to see communication as the bottleneck for training even on 80Gbps networks. I think that it would be beneficial to provide more detailed breakdowns of computation and communication times (for example, the time to process 1 microbatch and 1 batch of data, as well as the time to exchange parameters) in each setting to demonstrate the necessity of large $\tau$.

\[1\] Cooperative SGD: A unified Framework for the Design and Analysis of Communication-Efficient SGD Algorithms. Jianyu Wang, Gauri Joshi. JMLR 2021 * What were the values of $\tau$ for each experiment?
* Which stage of ZeRO have you used for the TransNormer experiment?
* In Table 2, it is somewhat surprising to see that CO2 (an asynchronous method) obtains consistently lower perplexity than a non-asynchronous adaptive method (AdamW). Do you have any explanations of that phenomenon?",510,2,1,0.8132,0.1600292438,0.8966901302,69,34.7875,0.2889,iclr,0.0097087378640776,4,4,4,4,factual,3,4,80,polite,4,neutral,4,none,4,5,4,4,partially factual,4,4,88,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
22,Reviewer-8oQm,CO2: Efficient Distributed Training with Full Communication-Computation Overlap,"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","This paper proposed a novel distributed training method: CO2, which can overlap communication and computation in distributed training. This technique is particularly useful when there are a large number of GPU nodes and the inter-connection between nodes are very slow. Compared to previous works, this paper introduces (1) penalty on stale momentum (2) momentum clipping. Empirical ablations show these two techniques are crucial to improve the training convergence performance. The authors also conducted extensive empirical studies, including experiments on image classification, large language model training, to demonstrate the effectiveness of the proposed method. - The paper has extensive empirical studies across different learning tasks as well as different network environments.
- The authors also provided a convergence analysis for the proposed method. - The idea of overlapping communication and computation is not new, as mentioned in the paper. The key contribution of this paper would be introducing the staleness penalty and momentum clipping mechanisms. They also present solid experimental resutls.
- The comparison with previous works are not enough. For example, totally overlapping communication and computation has already been achieved. like Wang et al, 2020. Overlap-Local SGD. The authors should include more discussions on the differences. or even include this method as a baseline.
- It is not very clear the convergence analysis was performed on which algorithm. Does the analysis consider staleness penalty and momentum clipping? Also, the convergence analysis looks like following previous works. It'd be better to cite few at the very beginning of the analyses. See the above section.",253,0,3,0.8169000000000001,0.0210642691,0.8847193122,51,29.1425,0.0513,iclr,0.0,2,3,2,2,partially factual,3,4,50,polite,4,neutral,3,moderate,4,4,4,4,partially factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
90,Reviewer-iUr9,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","This paper studied frequency estimation and learning-augmented frequency estimation. CountMin and CountSketch are the most popular algorithms for this task. With the addition of learning augmentation, an algorithm is given access to a learned prediction, in this case the prediction of the heavy hitters. This paper focuses on the stream being from a Zipfian distribution, which are well-studied, well-motivated distributions with heavy tails.

In the learning-augmented algorithm, if an element is predicted to be heavy, it is given a unique bucket so that a more accurate frequency can be computed for it. If it isn’t predicted to be heavy, it is simply input into a sketching  algorithm. They prove bounds on the weighted error of algorithms, including,  CountSketch, CountMin, and a novel algorithm. For CountSketch and CountMin, the paper gives a tight analysis. The new algorithm is studied both with and without predictions, though predictions give the largest advantage in low space settings. 

Experiments justify the theory is predictive of performance.  - Learning-augmented frequency estimation is itself a very nice question, I was looking forward to reading this paper in my pile. 
- The algorithm is clean, straight-forward. I believe the results are correct. 
- The paper is grammatically well-written.

 - I am confused about the prediction model. Normally, in learning-augmented algorithms, we measure an algorithm’s performance based on the error in the prediction. Here, as far as I could tell, all of the theoretical results only held when one assumed the predicted heavy hitters were correct. I expected to see some trade-off between the quality of prediction and the weighted error bounds. The experiments briefly mentioned that the prediction quality might be poor, thus leading to worse empirical performance (as expected), but there was no theory discussing the robustness of the predictions. Robustness in the prediction error is what differentiates learning-augmented algorithm from all these other BWCA frameworks (data-driven algorithms, algorithms with advice, etc). 
Perhaps because of the heavy tail distribution assumptions, it’s reasonable to assume that one learns the heavy hitters perfectly? Or can you offer another explanation for this choice in the model?

- This paper does not clearly lay out its improvements on prior work. I would like to see a lot more comparison to the most relevant previous work \[Hsu et al. 2019\]. Can this be more clearly stated  in the introduction? Concretely, it would help to have previously known results listed in a column in your table 1 for that we can see your improvement.  (see Weaknesses, please) na",415,0,2,0.7866000000000001,0.0921696557,0.9061119556,215,43.6815,0.4482,neurips,0.032258064516129,5,5,4,5,partially factual,2,2,75,polite,5,positive,4,low,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,3,4,partially factual,3,4,78,polite,4,neutral,4,low
90,Reviewer-UWwz,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Summary of the Paper
==================
* This work follows (Hsu Indyk Katabi Vakilian 2019) in trying to improve the performance of hashing-based frequency estimation algorithms (such as Count-Min, CountSketch) by making use of ""advice"" in the form of a learning model's predictions which classify the input elements as ""heavy-hitters"" or otherwise based on the input distribution.
* Just as (HIKV2019), the theoretical analysis assumes a Zipfian (heavy-tail) property for the data distribution, and they provide guarantees for the expected weighted estimation error $\frac{1}{N} \sum_{i=1}^{n} f_i \cdot |f_i - \hat{f}_i|$.
* They improve on the (HIKV2019) analysis of Count-Min and Learned-Count-Min algorithms to get tight bounds on the expected estimation error when there are multiple hash functions ($k \geq 2$).
 * They also provide tight bounds for the expected estimation error of CountSketch, with and without learning.
* Finally, they propose a better frequency estimation algorithm --- both plain (Algorithm 1&2) and learning-augmented (Algorithm 3&4) --- and prove bounds on the expected estimation error in both cases, showing that the learning-augmented algorithm outperforms both Plain-CS and Learned-CS in all regimes, whereas the plain (no-learning) algorithm outperforms the Plain-CS algorithm in the low-space regime ($B = {\rm polylog}(n)$).
* They also propose a parsimonious variant of the algorithm (limited number of queries) and do an experimental evaluation. 
* The problem setting is already studied in the literature and thus the improvements shown in this work are clearer. The Zipfian (heavy-tail) property for the data distribution is known to hold for many real world datasets (if approximately).
* This work provides tight bounds for the expected estimation error of CountSketch and CountMin, both with and without learning. In the case of CountMin, it improves upon the existing bounds from (HIKV2019).
* The proposed ""better frequency estimation algorithm"" provides tangible improvements over CS and CM, both wiith and without learning-augmentation.
* They also consider a variation of the algorithm with worst-case guarantees, even when the data distribution is not Zipfian, and the variant nicely generalises from the Zipfian case.
* The work includes the implementation of the algorithms and experimental evaluation.
* A reasonable level of proof-sketches are provided in the main paper. * The experiments should ideally have also considered the worst-case variant of the algorithm (Algorithm 6 in the supplementary) in both the Zipfian and non-Zipfian cases. None Not applicable.",388,0,1,0.7424000000000001,0.0704212454,0.8556281328,215,26.5102,0.0999,neurips,0.0151515151515151,1,3,2,2,factual,2,2,30,polite,3,neutral,4,high,4,4,4,4,factual,5,5,85,neutral,5,neutral,5,low,3.0,4.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,3,5,5,3,factual,5,5,90,polite,5,positive,5,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
90,Reviewer-TyeE,Improved Frequency Estimation Algorithms with and without Predictions,"Estimating frequencies of elements appearing in a data stream is a key task in large-scale data analysis. Popular sketching approaches to this problem (e.g., CountMin and CountSketch) come with worst-case guarantees that probabilistically bound the error of the estimated frequencies for any possible input. The work of Hsu et al.~(2019) introduced the idea of using machine learning to tailor sketching algorithms to the specific data distribution they are being run on. In particular, their learning-augmented frequency estimation algorithm uses a learned heavy-hitter oracle which predicts which elements will appear many times in the stream. We give a novel algorithm, which in some parameter regimes, already theoretically outperforms the learning based algorithm of Hsu et al. *without* the use of any predictions. Augmenting our algorithm with heavy-hitter predictions further reduces the error and improves upon the state of the art. Empirically, our algorithms achieve superior performance in all experiments compared to prior approaches.","Authors study frequency estimation algorithms CountMin and CountSketch
and propose their modifications tailored for heavy tailed distributions.
They first analyze CountMin and CountSketch, showing that the second
one achieves better theoretical bounds on such distributions which explains
experimental results in previous work.
They propose a different algorithm with significantly better performance
bounds on heavy tailed distributions which also satisfies worst case guarantees
(for the case when the input does come from a considered heavy-tailed distribution)
which are comparable to CountSketch.
They also propose an ML-augmented variant of their algorithm which assumes that
there is an oracle which correctly identifies half of the heavy hitters. This algorithm
also works in parsimonious setting where it is allowed to receive only a few predictions.
 * They consider problem important both in theory and practice in a setting which occurs often in practice
* They show limitations of the existing algorithms and design new ones overcoming these limitations
* the ML-augmented version of their algorithm can work in a parsimonious regime: only very few predictions are needed and I believe that this is a good sign of usability in practice * I did not see lower bounds for the problem in their setting. It is not clear whether better algorithms are possible
* It is not clear how their algorithm's performance depend on precision of the predictor, e.g., what if it identifies too many or too few items as heavy hitters  * if your algorithm reports too many items as heavy hitters, what does your algorithm do?
* requirement that the predictor perfectly identifies the top B/2 heavy hitters seems rather strict. Can it be made weaker, e.g. that it identifies 90% of the top B heavy hitters, or that it correctly identifies $i$th heavy hitters with some probability depending on $i$? assumptions clearly stated in the theoretical results",305,0,0,0.7563000000000001,0.0656060606,0.8871167302,215,30.0305,0.3011,neurips,0.0104166666666666,3,2,2,2,partially factual,3,2,40,polite,3,neutral,3,moderate,3,4,4,4,partially factual,4,4,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
12,Reviewer-hGLR,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","They present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities.  Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, they also propose to align modalities in both the input and output space. 1 One model that takes any combination of modalities as input or output is novel and promising.
2 As the lack of training data, the alignment of different modalities is very difficult. The proposed method for the alignment is very interesting. 1 The simple weighted interpolation of different representations is not so convincing. Why does this method work? see above not addressed",143,0,1,0.7243,0.0889508929,0.948985219,215,28.0155,0.068,neurips,0.0,0,4,0,0,unfactual,1,0,12,neutral,1,neutral,0,high,2,4,3,3,partially factual,4,4,55,polite,5,neutral,3,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,2,3,3,2,partially factual,3,3,50,neutral,4,neutral,2,moderate,1,3,2,2,partially factual,3,2,45,polite,4,neutral,3,moderate
12,Reviewer-mZPw,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","This paper presents a method that can generate any combination of output modalities, including language, audio, image, or video, from any combination of input modalities. The idea here is to align four modalities in a shared feature space first, and then learn to generate one or more modalities based on the shared feature space. This design enables many combinations of modalities despite the lack of training datasets. Since the feature space is shared, it also flexible to extend to other modalities. * The idea of any-to-any generation is interesting, and it enables many different tasks in one model.
* The framework is flexible and customizable to many other potential modalities, such as semantic maps, heat map, depth map and so on.
* The performance of the proposed method achieves comparable or better results than previous SOTA methods.
 * The method part is not clear. The relation among image diffusion model, video diffusion model, vision encoder and vision unet is confusing. Since 4 diffusion models are introduced and only 3 types of encoders and unet are shown in Figure 2, It s not clear whether image and video models share the parameters or not.
* The evaluation of Table 3 is not sufficient. Only the text-video faithfulness (CLIPSIM) is evaluated, while the video quality (FVD) is not evaluated.
* The proposed framework enables many different tasks. However, it does not outperform previous SOTA methods in many tasks, such as text-to-video generation, text-to-image generation, image captioning and video captioning.
 * From Table 8, using both text and audio as input achieves higher FID compared to using each single modality as input. Could you explain why model achieves worse performance with more information as input?
* From table 2 and table 3, CoDi does not outperform previous SOTA results. Do you think a model that can do all tasks need to sacrifice its performance on each specific task?
* During training, the text encoder weights are frozen after training with images, would it result to a suboptimal problem when training with other modalities?
* In Sec 3.3, image diffusion model and video diffusion model are introduced separately. However, in Figure 2, only vision UNet and Vision Encoder are shown. Does it mean image diffusion model share parameters with video diffusion model during training?
* In table 4, why CoDi can outperform other diffusion-based method in image captioning? The authors adequately address the limitations and potential negative sosietal impact.",405,0,0,0.7395,0.0743082368,0.906255722,215,37.9574,0.3617,neurips,0.0,3,4,4,4,partially factual,3,3,73,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
12,Reviewer-dG2H,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper introduces Composable Diffusion (CoDi), an innovative generative model capable of producing any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities simultaneously and is not limited to a subset of modalities like text or images. To address the challenge of lacking training datasets for many modalities combinations, the authors propose a modality alignment approach in both the input and output space. This enables CoDi to condition freely on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a unique composable generation strategy that establishes a shared multimodal space through alignment in the diffusion process. This allows for the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Offering high customization and flexibility, CoDi achieves impressive quality in joint-modality generation and either outperforms or matches the state-of-the-art unimodal models for single-modality synthesis. 1. The paper is addressing an important problem of mapping modalities from any domain to any domain without fully paired data.
2. The proposed method is novel and reasonable. It is good to see that each different component can be trained separately.
3. The proposed bridging alignment is interesting. The proposed method shares some similarities with previous works. Nevertheless, this paper still contributes to the community in my opinion. It could be better to have a more specific discussions on the difference with the related work. Please refer to weakness. Yes.",257,0,4,0.8107000000000001,0.2638203463,0.9874250889,215,21.2322,0.1572,neurips,0.0,0,4,1,0,unfactual,3,0,40,neutral,2,neutral,2,high,4,5,4,4,factual,5,5,88,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
12,Reviewer-7M7p,Any-to-Any Generation via Composable Diffusion,"We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.","The paper presents a new generative model called Composable Diffusion (CoDi). This model is capable of generating any combination of output modalities from any combination of input modalities, including language, image, video, or audio. Unlike other models that are limited to a subset of modalities like text or image, CoDi can generate multiple modalities in parallel.

The authors have designed CoDi to align modalities in both the input and output space. This allows the model to condition on any input combination and generate any group of modalities, even if they are not present in the training data.

A key feature of CoDi is its novel composable generation strategy. This involves building a shared multimodal space by bridging alignment in the diffusion process. This feature enables the synchronized generation of intertwined modalities, such as temporally aligned video and audio.

The paper reports that CoDi achieves strong joint-modality generation quality. It either outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. 1. Originality: The paper introduces Composable Diffusion (CoDi), a new model in multimodal generation. This model is designed to process and generate modalities across text, image, video, and audio simultaneously. This is a novel contribution as it enables the generation of various output modalities from different combinations of input modalities.

2. Quality: The authors have conducted extensive experiments to demonstrate the capabilities of CoDi. The results show that CoDi can generate single or multiple modalities from a wide range of inputs. The model's performance is competitive with state-of-the-art models in tasks such as image and video generation, video captioning, and image synthesis from multiple input modalities.

3. Clarity: The paper is well-structured and provides clear explanations of the model's architecture and its generation strategy. The use of figures and tables helps to understand the model's capabilities and performance.

4. Significance: This work represents a step towards more comprehensive human-computer interactions by enabling the generation of multiple modalities in parallel. CoDi has potential applications in various areas, from content creation to human-computer interaction. The authors also provide a basis for future research in generative artificial intelligence.

In summary, the paper presents a significant and original contribution to the field of multimodal generation, demonstrating high-quality research and clear presentation. The paper presents a novel approach to multimodal generation, but there are several areas where it could be improved:

1. Evaluation Metrics: The evaluation of the model's performance is primarily based on quantitative metrics such as Frechet Inception Distance (FID) and CLIPSIM. These metrics, while useful, may not fully capture the perceptual quality or coherence of the generated outputs. Incorporating user studies or other qualitative evaluations could provide a more comprehensive understanding of the model's performance.

2. Quality of Generated Results: The quality of the generated results could be improved. The generated videos are relatively short, the quality of the images is perceptually low, and the generated text is often short and discontinuous. These factors could limit the practical utility of the generated outputs.

3. Preservation of Input Modality: The model primarily focuses on understanding between modalities, but it does not always preserve the faithfulness of the input modality. For instance, the output video and images do not consistently preserve the identity of the input image. This could limit the model's ability to generate accurate and coherent outputs across different modalities.

4. Cross-Modality Benefits: The paper does not convincingly demonstrate that the generation results benefit from cross-modality conditions. For example, Table 8 shows that the quality of image generation can even degrade when using conditions from two modalities. Similarly, Table 9 shows only marginal improvements in video quality when using multiple modalities. The authors should establish a benchmark that clearly demonstrates the benefits of using multiple modalities for generation. Without such evidence, the necessity of the proposed architecture could be questioned.

5. Omission of Baselines: In Table 2, the authors omit the StableDiffusion v1.5 baseline, which is the image Latent Diffusion Model (LDM) they used. Including this baseline could provide a more comprehensive comparison of the model's performance. 1. Evaluation Metrics: Could you provide more details on why you chose FID and CLIPSIM as the primary evaluation metrics? Have you considered incorporating user studies or other qualitative evaluations to assess the perceptual quality and coherence of the generated outputs?

2. Quality of Generated Results: Could you elaborate on the factors that might be contributing to the short and discontinuous text, short video length, and perceptually low-quality images? Are there potential improvements or modifications to the model that could address these issues?

3. Preservation of Input Modality: How does the model ensure the preservation of the identity or characteristics of the input modality in the generated outputs? Are there specific mechanisms in place to ensure this, or is it an area for future work?

4. Cross-Modality Benefits: Could you provide more evidence or a clearer explanation of how the generation results benefit from cross-modality conditions? The results in Tables 8 and 9 suggest that the benefits might be marginal or even negative in some cases. Could you clarify this?

5. Omission of Baselines: Why was the StableDiffusion v1.5 baseline omitted from the comparisons in Table 2? Including this baseline could provide a more comprehensive view of the model's performance relative to existing methods.  The authors have adequately addressed the limitations and potential negative societal impact of their work.",886,0,13,0.7738,0.0881843647,0.9791465998,215,19.9989,0.3617,neurips,0.011111111111111,5,4,5,5,factual,3,5,95,polite,5,neutral,5,none,5,5,5,5,factual,5,5,100,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
0,Reviewer-HFRa,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","This paper introduces ν-ensembles, a novel deep ensemble algorithm that achieves both efficiency and conceptual simplicity. When presented with an unlabeled dataset, ν-ensembles generate distinct labelings for each ensemble member and subsequently fit both the training data and the randomly labeled data. The strength of ν-ensembles lies in their ability to enhance deep ensemble diversity and calibration without significantly increasing computational demands. Key strengths include improved calibration in both in-distribution and out-of-distribution settings, achieved without complex implementation or extensive hyperparameter tuning. This method maintains the efficiency of standard deep ensembles, ensuring diversity through a straightforward process of assigning random labels to unlabeled data points. The theoretical grounding via PAC-Bayesian analysis provides a guarantee of diversity, accuracy, and calibration on test data, making ν-ensembles a promising and efficient technique for enhancing deep neural network ensembles. 1. The paper lacks the related works of other calibration method such as train time calibration loss, and post hoc calibration which is very important in this domain.
2. From my experience, the ECE measurement could be very unstable when classification accuracy is low. For experiments in table 1 for CIFAR100, the accuracy is very low, and the results may not reliable.
3. The experiments lack the comparison with SOTA methods such as Focal Loss Calibration and Adaptive Label Smoothing. In table 1, how many times does the author run the experiments? Since the ECE measurement can be very stable among low prediction accuracy models, the ECE reported in Table can have very large variance. Please report the variance of multiple runs to verify the effectiveness of your method.

The experiment is limited to CIFAR10 datasets. Since the authors mention that the small dataset regime often happens in medical area. It is better to verify your algorithm on the small medical datasets.",296,0,3,0.7848,0.0529183673,0.9382253885,47,22.8589,0.2519,iclr,0.0,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,5,5,4,5,factual,5,5,92,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,4,neutral,4,low,4,4,3,4,partially factual,3,4,78,polite,5,neutral,4,low
0,Reviewer-i38b,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The paper proposes a very neat method for improving the diversity of deep ensembles: It assigns random labels to a set of unlabelled data and lets each ensemble component fit different random labels such that these ensemble components can be diverse. The paper further provides theoretical guarantees for the resulting ensembles' behavior on test samples. The empirical results further show that the method acquires significantly better calibration on small training dataset regime, without sacrificing accuracy. Importantly, the method only introduces little extra training overhead while outperforming baseline approaches that are way more complicated. Overall, I think the proposed idea is novel, interesting, easy-to-use, and could be of great impact. - The proposed method is easy! It is much easier and efficient to implement than other methods for enhancing ensemble diversity, such as Stein-based methods.

- The proposed method comes with theoretical guarantees: Although the method sounds like some heuristic, the author provides PAC-Bayes bounds for its performance on test data.

- The empirical performance improvement is significant: The results show that the proposed method improves the calibration error to a great extent for both in-distribution test data and out-of-distribution data (i.e. corrupted data), without hurting the accuracy. - The method ""Sample y randomly without replacement"", however, when the number of ensemble is larger than the number of classes, it is unclear to me how the method should be applied.

- Since the method assumes having access to a validation dataset, a baseline worth considering would be temperature scaling.

- The presentation of the results can be improved: There is no legend for the lines in Figure. 2; The usage of bold font is not consistent and confusing in Table. 1 Why the method becomes less effective when we have access to more data?

If I understand correctly, the method assigns random labels to **in-distribution** data, this sounds weird to me, as it implies that the ensemble would have high uncertainty on these in-distribution samples. I think one can also consider introducing OOD samples into training and assigning random labels to them for each ensemble member.",345,0,0,0.7883,0.0617635659,0.9469445348,47,33.8655,0.1932,iclr,0.01,3,4,3,3,factual,3,4,65,neutral,4,neutral,3,moderate,4,4,4,4,partially factual,5,5,85,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,3,85,polite,5,positive,4,low
0,Reviewer-My8L,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The authors present a method for improving the calibration of deep neural network ensembles in the small data regime when access to an unlabelled data set is assumed. In particular, they propose the counterintuitive idea of randomly labelling the unlabelled dataset (distinctly for each ensemble member) and training the deep ensemble on the joint supervised and randomly labelled data. The randomly labelled data promotes ensemble diversity. A PAC bound which relates generalisation performance to ensemble diversity is derived while the diversity of the ensemble is demonstrated to be related to the ensemble size. Experiments on various slices of CIFAR-10 and CIFAR-100 show that while the method does not improve accuracy relative to standard ensembles, there are substantial gains on calibration. Calibration does not improve consistently over more complicated/expensive diversity promoting ensemble methods. - The paper is very well written and clear.
- The idea for the method, of using randomly labelled unsupervised data to promote ensemble diversity is simple, cheap and easy to implement and in so far as promoting diversity makes sense.
- Some theoretical results are presented in which the ensemble diversity is related via a PAC bound to the generalization performance (I have some other comments on these results below).
- The experimental results are convincing that at least in the small data regime with relatively little unsupervised data the calibration relative to standard ensembles is significantly improved. Please see my questions in the section below for potential weaknesses that can be addressed through further experiments.

- The method is targeted solely at the small data regime, gains in calibration go to zero as the amount of labelled data increases.
- The method introduces a new $\beta$ hyperparameter which must be tuned.
- The experiments are presented without error bars and it is unclear if they come from a single run or are averaged over multiple seeds, standard practice, especially when considering the relative small datasets considered in this paper is to run experiments with multiple random seeds and present averages and standard deviations of the metrics of interest (or better yet other forms of statistical test of the significance of the results).
- Experiments are conducted on small slices of CIFAR-10 and CIFAR-100, while performance in the large data regime is alluded to in the paper, an experimental evaluation of this setting (for example ImageNet is fairly standard in the ensemble literature) would be much appreciated.
- From equation 3, it seems to be the case that as the number of classes (c) increases the gains in ensemble diversity go to zero, so the method is both likely to give no gains in the large data and large number of classes regime.
- The primary theoretical motivation for the method is equation 1, which is a PAC bound on the generalization performance, it is difficult to get a sense of how tight this bound is and to what extent there is a competition between the various terms in the bound.

Small things (didn't effect rating):
- Typo: ""coincides we standard weight decay"" -> ""coincides with standard weight decay""
- It took me a while when reading the paper printed out to realise that there are two colours plotted in the left hand side of Figure 2 - as the orange is almost fully hidden by the red, making this clear in the figure or caption would be helpful to readers. - While the experimental results do not show big drops in accuracy, I am quite concerned that given vastly more unlabelled data the method would lead to overfitting the random labels and thereby harm test set accuracy (as is a well known phenomenon in the noisy label literature). More formally one could imagine that vast amounts of unlabelled data would promote the diversity term in the RHS of equation 1, but I given results in the noisy labels literature, I would find it hard to believe that this would not come at a corresponding cost in the first term on the RHS of equation 1. Could the authors please comment on this concern? Experimentally, I would be interested in seeing an experiment on ImageNet, for example, where the labelled set is of size 50k and the unlabelled set is 950k examples, a standard resnet50 or similar capacity model is used with 4 ensemble members (as per other papers in the literature) and a comparison to standard ensembles in terms of accuracy and calibration is given. This is a significant concern for me, as usually with methods that make use of an unsupervised dataset, the expectation is that as the unlabelled dataset grows, the gains from using it grow to. I fear this will not be the case for this method, which would limit the method to the small dataset, small number of classes and small unlabelled dataset regime. I recognise that the $\beta$ hyperparameter can to a certain extent control this trade-off, so if further experiments are conducted to address this concern, please report the results over the $\beta$ hyperparameter range.",835,0,0,0.7756000000000001,0.0024741462,0.9451873302,47,28.0723,0.5162,iclr,0.0,5,5,5,5,factual,4,5,95,polite,5,positive,5,moderate,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
0,Reviewer-3iBP,$\nu$-ensembles: Improving deep ensemble calibration in the small data regime,"We present a method to improve the calibration of deep ensembles in the small data regime in the presence of unlabeled data. Our approach, which we name $\nu$-ensembles, is extremely easy to implement: given an unlabeled set, for each unlabeled data point, we simply fit a different randomly selected label with each ensemble member. We provide a theoretical analysis based on a PAC-Bayes bound which guarantees that for such a labeling we obtain low negative log-likelihood and high ensemble diversity on testing samples. Empirically, through detailed experiments, we find that for low to moderately-sized training sets, $\nu$-ensembles are more diverse and provide better calibration than standard ensembles, sometimes significantly.","The paper introduces a method to enhance the calibration of deep ensembles, particularly in situations where there is a small amount of labeled data and some unlabeled data. For each point in the unlabeled dataset, the ensemble members are trained with different randomly selected labels. The authors provide a theoretical justification for this approach, drawing on PAC-Bayes bounds to argue that it leads to lower negative log-likelihood and higher ensemble diversity on test samples. Empirically, they demonstrate that ν-ensembles outperform standard ensembles in terms of diversity and calibration, especially when the training dataset is small or moderate in size. - The paper gives a method to improve calibration error for deep ensembles using unlabeled data. The use of unlabeled data to improve calibration error of deep ensembles has not been explored much before as most of the works have focused on joint training approaches which can be memory and computationally expensive.
- The paper is overall well written and easy to understand. 
- The paper presents supports their method with both theoretical and experiments. - One major weakness of the paper is that their method only improves calibration error not accuracy but they have not compared to any other calibration technique like temperature sampling. 
- The other issue is that the method appears very similar to the Agree to disagree work mentioned in the paper where they also use unlabeled data to maximize diversity and the idea seem incremental. Can the authors please explain in detail how exactly Agree to disagree maximizes diversity on the unlabeled set?
- Another limitation is that this method only improves calibration in the small data regime. 
- Another limitation is that there are only two datasets used in the paper - CIFAR-10 and CIFAR-100. It would be nice to have additional datasets. - The paper says that the labels for unlabeled data points are chosen without replacement. What happens if we sample with replacement? One should expect the same empirical results to hold but maybe the theoretical argument will not hold?
- I understand the text written at bottom of the Figure 1 but I don’t understand the figure. What are the 3 columns in the figure?
- One part that is not clear to me is when we are forcing the models to make random predictions on unlabeled data which is from the same distribution, why we are not hurting the accuracy or the cross entropy loss of the model? When training data is small and unlabeled data set is bigger, can the authors share their regularization parameters and if they had to give small weights on the regularization term?
- The colors used in figure 2 and 3 are very similar and it is hard to distinguish different lines. 
- There are other works which also use this idea of diversifying using unlabeled datapoint for other problems. For example, DIVERSIFY AND DISAMBIGUATE: OUT-OF-DISTRIBUTION ROBUSTNESS VIA DISAGREEMENT. Can the authors please compare to this work also?
- Did the authors try using the unlabeled data from different distributions like random Gaussian noise. One benefit would be that fitting random labels on this dataset will not interfere with the learning on the original distribution.",530,0,0,0.7822,-0.0265522876,0.9537856579,47,40.043,0.1256,iclr,0.0,4,4,4,4,factual,4,4,90,polite,4,negative,4,low,4,4,4,4,partially factual,4,5,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
92,Reviewer-fL1b,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The paper proposes a method to learn low-dimensional continuous-time representations of network nodes, based on the collection of interaction events among them. More precisely, the events are in the form of $(i,j,t)$, where $(i,j)$ is the pair of nodes involved in the interaction event, and $t$ is the occurrence time. The proposed method first estimate the intensity function $\lambda_{i,j}(t)$ of events between each pair of nodes $(i,j)$ at every time instant $t$, then project the intensities of each node at time $t$ onto a learned lower dimensional subspace to obtain a representation. Theoretical results on the recovery error of the representation is provided. Numerical experiments using real data shows the effectiveness of the proposed method. The paper proposes to estimate the representation of nodes using continuous-time events, which seems to be a novel type of data. I find the presentation of the paper generally vague and hand-wavy. See the following.

1. The introduction is way too high-level. The authors should be more specific about the problem setting in this paper, for example, why we care about dynamic models, continuous-time event data, low-dimensional representation of nodes etc.

2. The related work is not specific. The authors should use a sentence to summarize the contribution of the mentioned papers and explain the difference from your work.

3. Lemma 1 is not correct. $\widehat U_d$ minimizes the residual sum of squares at $B$ chosen time instants, but not the integrated one. 

4. In Section 3, notation part, what is the difference between $\gg$ and $\gtrsim$? Also is the universal constant multiplicative or additive?

5. It's not clear what `$\approx$' means in Section 4.  . .",272,0,7,0.7208,0.08125,0.8962726593,215,43.4477,0.3676,neurips,0.0,4,4,3,3,factual,3,4,75,neutral,4,neutral,4,low,5,5,4,5,factual,4,5,85,impolite,5,negative,4,low,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,moderate,3,3,3,3,partially factual,3,3,60,neutral,4,negative,4,moderate,4,3,3,3,partially factual,3,3,60,neutral,4,negative,4,moderate
92,Reviewer-L5U9,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The paper presents a framework called Intensity Profile Projection (IPP) for continuous-time representation learning in dynamic networks. The authors aim to address the challenge of capturing temporal dynamics and evolving relationships in dynamic networks with both high statistical precision and interpretability. The model leverages the concept of intensity profiles, which encode the temporal changes and interactions between nodes in a network. The model provides a uniform error bound for learned node representations and preserves a novel ""temporal coherence"" property compared to existing baselines. Empirical results on real-world dynamic network datasets demonstrate that IPP outperforms existing methods in various tasks such, highlighting its ability to capture continuous-time representations and uncover temporal patterns in dynamic networks. 1. The paper introduces the Intensity Profile Projection (IPP) framework, which offers a unique and innovative approach to continuous-time representation learning for dynamic networks. It introduces the concept of intensity profiles and effectively utilizes them to capture temporal dynamics. 

2. Theoretical analysis towards the model shows that the model can achieve high statistical precision and preserve interpretability in terms of """"temporal coherence"".

3. The paper is in general easy to follow. 1. Lack of comparison with state-of-the-art methods: Although the paper claims improved performance over existing methods, it does not provide a comprehensive comparison with some existing continuous models such as GraphODEs\[1,2,3,4\] which combines neuralODE with GNNs to model network evolution over time.

2. Scalability: The scalability of the IPP framework is not extensively discussed. It would be valuable to address the computational requirements and scalability limitations of the proposed approach, especially when dealing with large-scale dynamic networks.

3. The related work section is too short to provide a comprehensive background of the research topic.


\[1\] Huang, Zijie, Yizhou Sun, and Wei Wang. ""Learning continuous system dynamics from irregularly-sampled partial observations."" Advances in Neural Information Processing Systems 33 (2020): 16177-16187.

\[2\] Song Wen, Hao Wang, and Dimitris Metaxas. 2022. Social ODE: Multi-agent Trajectory Forecasting with Neural Ordinary Differential Equations. In Computer Vision–ECCV 2022: 17th European Conference.

\[3\]Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting system dynamics. In
401 ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 705–715, 2021.

\[4\] Zang, Chengxi, and Fei Wang. ""Neural dynamics on complex networks."" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 892-902. 2020. 1. What would be the time complexity of the proposed method?
2. How would the model performance be affected by different network topology? The authors have discussed the limitations of their work.",420,6,11,0.8181,0.0620555556,0.9237517715,215,25.273,0.0751,neurips,0.0,4,4,4,5,factual,4,4,78,neutral,5,neutral,4,low,4,5,4,4,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
92,Reviewer-QsMj,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","To represent the continuous dynamic network, authors provide the framework based on the intensity profile. First, the intensity between nodes is estimated, which produces the intensity profile. Low dimension reduction via SVD is applied on the intensity, and then each node embedding is obtained by the low dimensional subspace.
Author also provide various theoretical analysis about the error bound and the bias-various trade-off. Theoretical analysis as well as empirical analysis on the simulated data demonstrates that the proposed method capture structural preserving and temporally coherent properties. Case study on the real data is conducted to explain the outcome of the proposed framework qualitatively - Simple but powerful method is proposed
- Based on the mathematical model, theoretical bound is analyzed and explained.
- IPP can capture the behavior of a bifurcating block model. - The proposed method is not novel enough. SVD decomposition is a very common technique for the reduction of dimensions, and it often suffers from the long-tailed singular values. 
- Comparison is too limited. The analysis has been made only for the simulated data with figures. More experiments as well as some qualitative results would be great to have.
- SVD decomposition does not prevent producing negative values at the reconstruction.
- The proposed projection space is very dependent on the fixed dataset. At least, how to leverage the given embeddings for predictions is not straightforward. Given this, the potential application value is not very clear. - Figure numbers are all wrong. 
- Section 4 is true for any global subspace projection. Also, both properties could be debatable, not necessarily ideal. For instance, when \Labmda_{i}(s) = \Lambda_{i}(t), X_{i}(s) = \alpha * X_{i}(t) could be more ideal, depending on the interactions among the other nodes. 
- It would be great if authors compare the embedding trajectory for more real data, beyond the specific simulated ones.  Often, the meaning of each dimension from the SVD decomposition is not clear. This interpretability is not necessarily required for the representation, but this should be addressed when presenting the case study.",339,0,0,0.7807000000000001,0.0777465827,0.8251838684,215,34.1479,0.0999,neurips,0.0,4,4,4,4,factual,4,4,88,polite,4,positive,4,low,4,3,4,4,partially factual,3,3,65,neutral,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,3,4,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,4,partially factual,3,3,70,polite,4,neutral,4,low
92,Reviewer-SnVH,Intensity Profile Projection: A Framework for Continuous-Time Representation Learning for Dynamic Networks,"We present a new representation learning framework, Intensity Profile Projection, for continuous-time dynamic network data. Given triples $(i,j,t)$, each representing a time-stamped ($t$) interaction between two entities ($i,j$), our procedure returns a continuous-time trajectory for each node, representing its behaviour over time. The framework consists of three stages: estimating pairwise intensity functions, e.g. via kernel smoothing; learning a projection which minimises a notion of intensity reconstruction error; and constructing evolving node representations via the learned projection. The trajectories satisfy two properties, known as structural and temporal coherence, which we see as fundamental for reliable inference. Moreoever, we develop estimation theory providing tight control on the error of any estimated trajectory, indicating that the representations could even be used in quite noise-sensitive follow-on analyses. The theory also elucidates the role of smoothing as a bias-variance trade-off, and shows how we can reduce the level of smoothing as the signal-to-noise ratio increases on account of the algorithm `borrowing strength' across the network.","The authors propose an approach for learning time-varying node embeddings from continuous-time dynamic network data, which consist of a set of instantaneous timestamped relational events between nodes (e.g., messages from one social media user to another). Their proposed approach learns a projection that minimizes reconstruction error of the pairwise intensities between nodes and comes with theoretical guarantees on estimation error. They also show that their approach generates embeddings that both preserve network structure at a given time and is temporally coherent. They demonstrate strong empirical performance on simulated data compared to other dynamic network embeddings. Furthermore, they use their approach to analyze a real network data set on face-to-face interactions of primary school students, which is quite enlightening due to the interpretability of their model.

*After rebuttal:* The authors have clarified the one question I had about the meaning of ""inductive"" in their setting. I continue to strongly support the paper. - Proposed approach learns time-varying node embeddings from continuous-time networks with theoretical guarantees, which is among the first, if not the first, in the literature.
- Proposed embeddings can satisfy two good properties of structure preservation and temporal coherence.
- Very well written and organized paper that provides highlights of theoretical analysis in the main paper followed by details, including proofs, in the supplementary. - There's a large body of related literature on probabilistic generative models for continuous-time networks using point process models such as Hawkes processes that should be discussed. Many of these models are based on stochastic block models or latent space models and are thus also learning node embeddings. See suggested references below.
- No quantitative evaluation. This is only a minor weakness in my opinion because I view the main contribution to be theoretical.

Typos and minor issues:
- Supplementary Section C heading: Visualsation -> Visualisation

References:
- Arastuie, M., Paul, S., & Xu, K. S. (2020). CHIP: A Hawkes process model for continuous-time networks with scalable and consistent estimation. In Advances in Neural Information Processing Systems 33 (pp. 16983-16996).
- Corneli, M., Latouche, P., & Rossi, F. (2018). Multiple change points detection and clustering in dynamic networks. Statistics and Computing, 28(5), 989-1007. doi:10.1007/s11222-017-9775-1
- Huang, Z., Soliman, H., Paul, S., & Xu, K. S. (2022). A mutually exciting latent space Hawkes process model for continuous-time networks. In Proceedings of the 38th Conference on Uncertainty in Artificial Intelligence (Vol. 180, pp. 863-873).
- Junuthula, R. R., Haghdan, M., Xu, K. S., & Devabhaktuni, V. K. (2019). The Block Point Process Model for continuous-time event-based dynamic networks. In Proceedings of the World Wide Web Conference (pp. 829-839).
- Matias, C., Rebafka, T., & Villers, F. (2018). A semiparametric extension of the stochastic block model for longitudinal networks. Biometrika, 105(3), 665-680. doi:10.1093/biomet/asy016
- Yang, J., Rao, V., & Neville, J. (2017). Decoupling homophily and reciprocity with latent space network models. In Proceedings of the Conference on Uncertainty in Artificial Intelligence. 1. The authors mention several times that their approach is inductive, allowing one to obtain a node representation profile outside of the training sample. If the task is to obtain the node representation for the future, how would the Intensity Profile Projection approach handle it? Would it require some data from other nodes at that future time? Limitations are thoroughly discussed in Section 6. I commend the authors for being very forthcoming with these limitations. I don't view the limitations as weaknesses, because they are mostly limitations that apply to all unsupervised problems.",576,8,12,0.8295,0.098241342,0.8879517317000001,215,35.1135,0.1953,neurips,0.0,4,3,4,4,factual,3,4,78,polite,4,neutral,4,none,3,5,4,4,factual,5,5,90,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,3,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,92,polite,5,positive,5,low
175,Sabbir-Rashid,The Numerate Web: Mathematical Formulas and Computations on the Web of Data,"Ontologies and related Semantic Web technologies are applied in many areas where\nmathematical relationships are essential to the domain knowledge.\nHowever, unlike ontologies and logical rule languages, mathematical expressions\nand calculation rules are not an intrinsic part of the linked data\nrepresentation. Therefore, additional mapping processes between semantic domain\nmodels and the programs executing the mathematical computations are usually\nrequired.\nThe Numerate Web is an approach to representing mathematical models with RDF,\nlinking them to RDF resources and properties, running computations, and finally\nalso making the results available as part of the RDF representation.","SWJ Review In this article, the author presents the Numerate Web, an approach that leverages and extends earlier work to advance the support for the representation of mathematical models in RDF. This work has a significant potential impact, is well-motivated, and is supported through the demonstration of examples. The syntax and incorporated shorthand notations for incorporating mathematical equations are well explained and several algorithms for calculation execution are provided. Nevertheless, despite the numerous strengths of this article, the major shortcoming is the lack of a rigorous quantitative evaluation of the approach. Instead, how this work can be leveraged in the context of two case studies is provided. Additionally, the mathematics in the examples included were relatively straightforward. Could this approach be used for calculus or solving differential equations? There is a mention regarding the incorporation of time-varying behavior as future work, but the discussion on the limitation of this approach should be extended. In terms of mathematics, it should be made very clear what this approach can and cannot do. Listed below are many of the grammatical issues found within the article. Several issues were likely missed, so it is highly recommended that the author addresses the following and also carefully proofreads the article afterward. For example, I didn't comment on the use of Oxford commas, but you mostly use them but in some places do not. Whether or not to use Oxford commas is debatable, but whatever you decide, it should be consistent throughout the paper. Section 1 Page 1 Line 42-43 - Single sentence paragraph, should be combined with the following paragraph. Line 48-49 - Single sentence paragraph, should be combined with the previous paragraph. Line 49 - footnote should go after the punctuation: ""...that both have RDF serializations^1."" -> ""...that both have RDF serializations.^1"" Page 2 Line 12-14 - Single sentence paragraph, should be combined with the following paragraph or the thought should be expanded upon. Line 37-38 - Single sentence paragraph, should be combined with the previous paragraph. Line 39-40 - Single sentence paragraph, should be combined with the following paragraph, which is also a single sentence paragraph. Section 2 Line 50 - Missing comma: ""In 2003 Marchiori..."" -> ""In 2003, Marchiori..."" Page 3 Line 22 - Missing comma: ""In 2011 Lange..."" -> ""In 2011, Lange..."" Line 25-26 - phrasing and missing comma: ""Additional to OMDoc the work introduces..."" -> ""In addition to OMDoc, the work introduces..."" Line 29 - Missing comma: ""In 2012 Ferre..."" -> ""In 2012, Ferre..."" Line 45-46 - Unnecessary comma: ""For example, constants, and variables are only..."" -> ""For example, constants and variables are only..."" Line 49 - Missing comma: ""In 2014 Munoz..."" -> ""In 2014, Munoz..."" Section 3 Page 4 Line 15-16 - Single sentence paragraph, should be combined with the following paragraph. Line 45-46 - Single sentence paragraph, should be combined with the previous paragraph. As noted for these first 4 pages, many single-sentence paragraphs are included and continue to be included in the remainder of the paper. The use of single-sentence paragraphs is not technically grammatically incorrect. It can serve a stylistic purpose typically for emphasis in story-telling, but that is not the case here so we recommend that such occurrences should be corrected. The remainder of this review will not continue to include comments for single-sentence paragraphs, but that is not because they went unnoticed. We leave it to the authors to remedy this issue. Section 4 Page 6 Line 25 - Figure 5 caption, typo and missing article: ""Example for representig a gear motor as RDF model"" -> ""Example for representing a gear motor as an RDF model"" Section 5 Line 45 - missing comma and article: ""As mentioned in Section 1 these objects may be represented using Content MathML as markup language."" -> ""As mentioned in Section 1, these objects may be represented using Content MathML as a markup language."" Page 7 Line 16 - missing comma: ""Therefore an OWL ontology for OpenMath..."" -> ""Therefore, an OWL ontology for OpenMath..."" Page 8 Line 46 - footnote should go after the punctuation: ""...within the POPCORN definition^2."" -> ""...within the POPCORN definition.^2"" Section 6 Page 9 Line 40 - missing comma: ""Analogous to connecting programming languages to SPARQL endpoints via APIs a hypothetical Content"" -> ""Analogous to connecting programming languages to SPARQL endpoints via APIs, a hypothetical Content"" Page 10 Line 14 - missing comma: ""In [30] we already proposed..."" -> ""In [30], we already proposed..."" Line 16 - footnote should go after the punctuation: ""...is reviewed and available on the OpenMath website^3."" -> ""...is reviewed and available on the OpenMath website.^3"" Line 42 - missing comma: ""With rdf:resource and rdf:resourceset it is possible to select..."" -> ""With rdf:resource and rdf:resourceset, it is possible to select..."" Line 43 - missing comma: ""However, for traversing the edges further operators are necessary."" -> ""However, for traversing the edges further, operators are necessary."" Line 43-44 - phrasing can be improved and it is not clear what is meant here. Why does it say ""with one"" when it seems from the examples that both operators expect multiple values? It should be clarified that ""one and multiple"" is referring to the output of the functions rather than the input: ""For this purpose, two additional operators for RDF properties with one and multiple values are defined: rdf:value and rdf:valueset."" -> For this purpose, two additional operators for RDF properties with the ability to return a single value or multiple values, respectively, are defined: rdf:value and rdf:valueset."" Page 11 Line 7 - missing comma: ""Complementary to the operator rdf:value the operator rdf:valueset is able..."" -> ""Complementary to the operator rdf:value, the operator rdf:valueset is able..."" Line 41 - the quotes don't match up: 'A literal with the content ""‘This is an English text.""’ and the language label ""‘en""’ is representable...' -> 'A literal with the content ""‘This is an English text.’"" and the language label ""‘en’"" is representable...' Line 48 - footnote should go after the punctuation: ""...and reduce the amount of data required for encoding^4."" -> ""...and reduce the amount of data required for encoding.^4"" Page 12 Line 1 - missing comma: ""For the RDF operators defined in the previous sections short forms for URIs are not necessary for the functionality."" -> ""For the RDF operators defined in the previous sections, short forms for URIs are not necessary for the functionality."" Line 3 - typo: ""...to assign parts of of URIs to..."" -> ""...to assign parts of URIs to..."" Line 4-5 - incompletes sentence: ""In this case, the prefixes...ontology about persons."" -> ""In this case, the prefixes...ontology about persons are used."" Line 5 - typo and phrasing: ""As can be can be seen,..."" -> ""As shown,..."" Line 17 - missing comma: ""In order to support prefix declarations in OpenMath semantic attributions could be used, comparable to..."" -> ""In order to support prefix declarations in OpenMath, semantic attributions could be used, comparable to..."" Line 25-26 - redundancy: ""It is possible to overwrite a prefix within a child object is possible."" -> ""It is possible to overwrite a prefix within a child object."" Line 35 - tense agreement: ""...the inheritance of the prefixes to child objects itself."" -> ""...the inheritance of the prefixes to child objects themselves."" Line 45 - spelling: ""...elements fulfil a certain..."" -> ""...elements fulfill a certain..."" Page 13 Line 1 - missing word: ""...the example shown the efficiency..."" -> ""...the example shown of the efficiency..."" Line 2 - typo: ""...has to be loaded from the from the RDF database."" -> ""...has to be loaded from the RDF database."" Line 3 - missing comma: ""If the filter condition could be pushed down to the database then this would allow..."" -> ""If the filter condition could be pushed down to the database, then this would allow..."" Line 35-36 - missing comma and unnecessary comma: ""Therefore it can be checked for consistency by OWL reasoners, and it can be..."" -> ""Therefore, it can be checked for consistency by OWL reasoners and it can be..."" Line 41 - incorrect pluralization: ""In order to improve the usability of mathematical expressions input and output when..."" -> ""In order to improve the usability of mathematical expression inputs and outputs when..."" Section 7 Page 14 Line 33 - typo: ""...their linkage with with RDF resources..."" -> ""...their linkage with RDF resources..."" Line 33 - missing comma: ""On this basis the creation..."" -> ""On this basis, the creation..."" Page 17 Line 26 - unnecessary article: ""The Algorithm 1..."" -> ""Algorithm 1..."" Page 18 Line 44 - unnecessary article: ""The algorithm 2..."" -> ""Algorithm 2..."" Page 19 Line 20-21 - unnecessary article: ""...(line 12 of the Algorithm 2)."" -> ""...(line 12 of Algorithm 2)."" Page 20 Line 1-2 - unnecessary article: ""To support this, the algorithms 1 and 3 must be adapted..."" -> ""To support this, Algorithms 1 and 3 must be adapted..."" Line 4 - phrasing: ""An example depicts Figure 7, which shows..."" -> ""An example is depicted in Figure 7, which shows..."" Line 25 - footnote goes after the punctuation: ""...Ontology^9 (MUO)."" -> ""...Ontology (MUO).^9"" Line 29-30 - phrasing: ""...with QUDT contains [56, pp. 294]."" -> ""...with QUDT is contained in [56, pp. 294]."" Line 42 - unnecessary article: ""...into the algorithm 3..."" -> ""...into Algorithm 3..."" Line 46 - footnote goes after the punctuation: ""An example is shown in Listing 13^11, where..."" -> ""An example is shown in Listing 13,^11 where..."" Page 21 Line 40 - missing comma: ""For this purpose the conversion..."" -> ""For this purpose, the conversion..."" Line 42 - missing commas: ""For the given example therefore the conversion..."" -> ""For the given example, therefore, the conversion..."" Page 22 Line 8 - missing comma: ""...via OWL restrictions as shown in Listing 14."" -> ""...via OWL restrictions, as shown in Listing 14."" Section 8 Line 29 - missing commas: ""The first case study OpenMath Content Dictionaries (Section 8.2) investigates..."" -> ""The first case study, OpenMath Content Dictionaries (Section 8.2), investigates..."" Line 33 - missing commas: ""The second case study process chain planning and evaluation (Section 8.3) investigates..."" -> ""The second case study, process chain planning and evaluation (Section 8.3), investigates..."" Line 39 - typo: ""...described insection 8.1 was..."" -> ""...described in Section 8.1 was..."" Line 49 - footnote goes after the punctuation: ""...representation of mathematical objects and the execution of calculations^12."" -> ""...representation of mathematical objects and the execution of calculations.^12"" Page 23 Line 37 - redundancy: ""For example, the KOMMA ontology editor, for example, supports textual..."" -> ""For example, the KOMMA ontology editor supports textual..."" Line 42 - capitalization of proper noun: ""As already described in section 5, OpenMath..."" -> ""As already described in Section 5, OpenMath..."" Page 24 Line 4 - footnote goes after the punctuation: ""...platform eniLINK^14, an extension..."" -> ""...platform eniLINK,^14 an extension..."" Page 26 Line 2 - typo: ""...sums or products in in any..."" -> ""...sums or products in any..."" Line 5 - footnote goes after the punctuation: ""...SPARQL query^19."" -> ""...SPARQL query.^19"" Line 44 - typo: ""...calculations wer developed with..."" -> ""...calculations were developed with..."" Page 31 Line 46 - footnote goes after the punctuation: ""...into the Schema.org vocabulary^21."" -> ""...into the Schema.org vocabulary.^21"" Line 46-47 - phrasing: ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment gives [67]."" -> ""An example of the use of the GoodRelations ontology for the domain mountain sports equipment is given in [67]."" Page 32 Line 36 - unnecessary comma: ""...the integration of external data in mathematical models is possible, if it is available in an RDF..."" -> ""...the integration of external data in mathematical models is possible if it is available in an RDF..."" Line 41 - capitalization: ""...in section 8.1 extended..."" -> ""...in Section 8.1 extended..."" Line 47 - unnecessary article: ""The figure 18..."" -> ""Figure 18..."" Page 34 Line 18 - phrasing: ""Questions are here the embedding..."" -> ""Questions include the embedding...""",1975,4,2,0.5918,-0.0331199225,0.8533676863,223,51.95,0.1878,semanticweb,0.0,5,4,5,5,factual,3,5,97,polite,5,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,low,5,4,4,5,factual,4,4,85,polite,5,positive,4,low,3,3,4,4,factual,4,4,85,polite,5,neutral,5,low
148,Reviewer-Fii6,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","In Graph Drawing Objective (GDO) and the generalized GDO, the optimization problem in Equation 1 and 3 are used to find the Laplacian representation, but this formulation allows symmetries, which lead to hyper-parameters that can lead to potential issues. The proposed method, Augmented Lagrangian Laplacian Objective (ALLO) in Equation 6, requires no hyper-parameters. In Theorem 1, they show a theoretical result on how there is a guarantee of the stability of the proposed objective function for finding Laplacian representations. The paper concludes with some experiments. - interesting formulation and solution
- motivated problem
- having experiments - some parts (e.g., Section 1 and 2) are hard to follow - How do you compare the complexity of the proposed objective function optimization problem with previous cases?





---------------------------------------------
After the rebuttal: I appreciate the authors for their response. They fully addressed my question and I decided to keep my acceptance score.",149,0,0,0.8237,0.0046296296,0.7480648756,61,30.7324,0.7922,iclr,0.0128205128205127,3,3,1,2,partially factual,4,3,50,polite,3,positive,3,moderate,3,3,3,3,factual,4,4,65,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,2,3,3,2,factual,4,3,60,polite,4,positive,4,moderate,2,3,3,3,partially factual,3,3,65,polite,4,neutral,4,low
148,Reviewer-nARE,Proper Laplacian Representation Learning,"The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.","The authors propose a method to approximate the true eigenvalues and eigenvectors of a graph Laplacian relying on an unconstrained max-min problem solved by gradient-based optimization. This can be used to learn good representations for the states in reinforcement learning problems. In the experiments, the efficiency of the method is demonstrated together with an ablation study. - This is an interesting and novel approach to the challenging problem of unsupervised representation learning.
- The technical part of the paper seems to be solid and reasonable, but I have not verified the theoretical results in detail. 
- Both the theoretical results and the experiments support the claims.
- The paper is relatively well written. I think that the proofs could have been in appendix and instead use the space for more examples, demonstrations, and clarifications. Q1. While in the paper the approach focuses on the eigenvectors of the graph Laplacian, in the experiments it is used for finding eigenfunctions. I think that further information should be provided for the actual formulation/solution of this problem.
Q2. I find Corollary 1 and the paragraph above a bit unclear. Why does an optimum of (2) and (4) imply that the constraint must be violated? 
Q3. Perhaps, an experiment to test the stability of the equilibrium with respect to permutations.
Q4. Why rotated eigenvectors do not provide a good representation?",225,0,4,0.7319,0.2205882353,0.8910561204,48,41.1356,0.1507,iclr,0.0106382978723403,4,5,3,4,factual,4,3,70,polite,4,positive,4,low,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
53,Reviewer-A92f,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper presents a new method that combines set-valued prediction with out-of-distribution detection in multi-class classification problems. The central idea is a risk minimization framework with a loss function that consists of three parts. The first two parts trade off set size and accuracy, while a weight parameter controls which of the two terms is more important. The third term allows to exclude atypical examples from acceptance regions. In addition, the penultimate layer of the neural uses random fourier features to approximate a Gaussian kernel. 

The authors present theoretical results that present (a) the quality of the random Fourier feature approximation (b) the convergence to Bayes risk when sample size increases. In the experiments the proposed methods is compared to three baselines on three datasets and three metrics. The metrics evaluate the set-valued prediction and OOD detection performance. - The presented method is novel
- Overall the paper is well written (but some parts are unclear, see below)
- I agree with the authors that combined set-valued prediction and OOD detection is a key concept in satefy-critical applications of AI. 
- I liked that the authors describe the problem setting and the assumptions formally (Assumptions 1 and 2). This is often missing in OOD detection papers. This is a quite technical paper, and I am afraid that I don't understand the method very well, despite having a background on the topic and spending quite some time to read the paper. The last part of objective function (1) is unclear to me. What are lambda_k and rho_k? Are these explained in the paper? The authors explain that rho_k is used to exclude atypical examples from acceptance regions, but I don't see yet how that's going on. Also, why is a Frobenius-penalty needed for the parameter matrices? This is quite atypical for deep learning methods, where regularization is typically done via early stopping in SGD.

Furthermore, the need for random Fourier features in the penultimate layer of the neural network is also unclear to me. What does this component add to the method, compared to just propagating the embedding to the output layer? 

I also find the connection to existing literature a bit weak. The literature on OOD detection is vast, so I understand that the authors cannot discuss every paper, but some essential papers are definitely missing. Assumption 1 clearly motivates why generative models / density-based models are a good approach to represent P(x|y) as a first step for combined set-valued prediction and OOD detection. The authors discuss a few methods that model P(x|y), such as the unpublished work of Hechtlinger et al. However, there are many other papers that also model P(x|y), such as:
Charpentier et al. Posterior network: Uncertainty estimation without ood samples via density-based pseudo-counts, ICLR 2021
Van Amersfoort et al. Uncertainty estimation using a single deep deterministic neural network, ICML 2020
Lee et al. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks, Neurips 2018

Perhaps those papers don't evaluate set-valued prediction, but they can be immediately used for such purposes. From that perspective, I would argue that such methods are also better baselines than the current baselines. These methods are deep learning methods, and they are published, unlike two of the three papers that are currently used as baselines. If I would have to do simultaneous set-valued prediction and OOD detection for a specific application, I would be more tempted to try these methods first instead of the method proposed here, because for methods that model P(x|y) it is more clear what they are doing. For the proposed method I am not sure whether it is modelling a class-specific density P(x|y). This might be realized via the random Fourier features, but more explanation would be needed. 

Furthermore, in set-valued prediction there are also quite some methods that consider loss functions that consist of two parts: a part that minimizes accuracy, and another part that minimizes set size, see e.g. 
Mortier et al. Efficient set-valued prediction in multi-class classification, Data Mining and Knowledge Discovery, 2020. 
Titouan Lorieul, Uncertainty in predictions of deep learning models for fine-grained classification, PhD thesis, University of Montpellier, France. 

The proposed method has a lot of connections with such methods, but there are two differences: (1) the proposed method has an additional third part in the loss, (2) the other methods typically fit a probabilistic model first, and optimize set-based utility scores during the inference phase. Perhaps these two differences are enough to behave good for OOD detection as well, but that's still unclear to me. See above.",757,0,2,0.7928000000000001,0.0294517671,0.8858969212000001,48,42.5887,0.1507,iclr,0.0,4,4,5,4,factual,4,5,90,polite,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
53,Reviewer-5AV7,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","This paper proposes a novel way to learn a set-valued classifier, called Deep Generalized Prediction Set (DeepGPS); the proposed method is capable of identifying ambiguous observations and detecting out-of-distribution observations. Also, it is the first set-valued classification with a theoretical guarantee and scalable to large datasets. In theory, this paper provides that DeepGPS attains the optimal expected prediction set size, while achieving the user-prescribed class-specific accuracy. The efficacy of DeepGPS is demonstrated by using MNIST/CIFAR10/Fashion-MNIST datasets and multiple baselines. This paper proposes a learning approach for DeepGPS along with its theoretical properties in Thm1-3. I appreciate the authors' careful analysis on the algorithm. I was initially surprised that the proposed approach can achieve a user-prescribed class-specific accuracy \gamma without training a base model and additional set predictor in a decoupled way. However, I found that it is simply due to the hyperparameter C tuning in a validation set. In this regard, I found that the stated guarantee in (1) of Thm3 is largely disconnected to the empirical results. 

Afterward, I was not convinced whether we actually need this complicated way in training the entire neural network. I’d rather simply use conformal prediction for the specified task (e.g., BCOPS). 

Also in representing the main results in Table 1, I think the class-specific accuracy results should be bolded if they are close to the desired level; however, the maximum values are bolded. 1. Based on the appendix, the hyperparameter C is chosen to achieve the desired class-specific accuracy, i.e., “The tuning parameter C is determined such that the prediction set is smallest on the unlabeled part in the validation data when the misclassification rate is close to γ on the labeled part in the validation data.” Then, what’s the meaning of (1) of Thm3? I think without this theorem, we can heuristically achieve the desired class-specific accuracy via hyperparameter tuning over a validation set. 

2. Related to the above question, can you re-evaluate the benefit of DeepGPS compared to BCOPS? I think simple training in a decoupled way provides a stronger guarantee. 

3.  In Table 1, is there a specific reason that the accuracies are highlighted when it is the largest number? Otherwise, please use bold numbers if the class-specific accuracy results are close to the desired level.",376,0,3,0.783,-0.0018571429,0.9310600758,48,29.803,0.8641000000000001,iclr,0.0,4,4,3,4,partially factual,3,4,75,neutral,4,neutral,4,moderate,5,5,4,5,factual,5,5,90,polite,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
53,Reviewer-nfKV,Deep Generalized Prediction Set Classifier and Its Theoretical Guarantees,"A standard classification rule returns a single-valued prediction for any observation without a confidence guarantee, which may result in severe consequences in many critical applications when the uncertainty is high. In contrast, set-valued classification is a new paradigm to handle the uncertainty in classification by reporting a set of plausible labels to observations in highly ambiguous regions. In this article, we propose the Deep Generalized Prediction Set (DeepGPS) method, a network-based set-valued classifier induced by acceptance region learning. DeepGPS is capable of identifying ambiguous observations and detecting out-of-distribution (OOD) observations. It is the first set-valued classification of this kind with a theoretical guarantee and scalable to large datasets. Our nontrivial proof shows that the risk of DeepGPS, defined as the expected size of the prediction set, attains the optimality within a neural network hypothesis class while simultaneously achieving the user-prescribed class-specific accuracy. Additionally, by using a weighted loss, DeepGPS returns tighter acceptance regions, leading to informative predictions and improved OOD detection performance. Empirically, our method outperforms the baselines on several benchmark datasets.","The authors explore set prediction, or conformal prediction, within the context of out-of-distribution (OOD). In this prediction paradigm, rather than offering a singular classification result, a predictor provides a set of labels. This set is expected to encompass the true label with a high degree of certainty. When dealing with OOD, predicting an empty set becomes a significant indication, suggesting the assignment of an OOD label to the test data point. The authors introduce an algorithm that employs Random Fourier Features, ensuring scalability in relation to sample size. Furthermore, they present the adaptive weighted hinge loss and offset penalization techniques to boost classification efficiency. The paper theoretically investigates the expected prediction set size for their algorithm, showing that it approaches the optimal size as the sample size grows. Experimental outcomes underscore that their algorithm surpasses existing methods. Moreover, the components of the adaptive weighted hinge loss and offset penalization play pivotal roles in enhancing classification efficiency. 1. The paper is well-written and easy to follow.

2. Addressing set-valued classification issues in OOD scenarios is both demanding and imperative. Issues of trustworthiness and OOD can stymie the deployment of machine learning algorithms in real-world applications. Developing an algorithm for set-valued classification within OOD scenarios augments the applicability of machine learning techniques.

3. The proposed elements—adaptive weighted loss and offset penalization—are astutely crafted to evaluate the accuracy constraint more rigorously and to minimize the expected set size.

4. Theoretical insights guarantee that the classifier obtained by the proposed algorithm will attain the optimal expected set size achieved by the ideal classifier. This underscores the rationale behind the algorithm's design.

5. Experimental findings robustly attest to the proposed method's dominance over existing techniques in terms of the metrics evaluated. 1. The rationale behind incorporating Random Fourier Features is ambiguous. Attaining scalability can be realized by merely employing a fixed-width network as the penultimate layer. Resorting to infinite-dimensional kernel features as the penultimate layer seems unnecessary without a clear justification, making the algorithm's design seem somewhat ill-advised.

2. The theoretical findings seem to be direct derivations from the generalization bound established through the Rademacher complexity. Their technical significance remains dubious. Furthermore, given that the core contributions revolve around the introduction of adaptive weighted loss and offset penalization, the impact of these components on generalization error remains unexplored. Consequently, the results offer limited support for the algorithm's design.

3. There is likely an intrinsic trade-off between OOD recall and Efficiency as gauged in the experiments. Thus, assessing this trade-off's efficiency becomes crucial. A comprehensive superiority assertion for the proposed algorithm necessitates comparative analyses of such trade-off efficiencies.

4. It is also vital to assess the trade-off between OOD recall and Efficiency within the ablation studies.

5. The authors seem to incorporate the adaptive weighted loss with an aim to enhance the precision of class-wise error assessments. Therefore, to ascertain the efficacy of this component, evaluations of the precision of class-wise errors should be undertaken. 1. Would the authors shed light on the imperative of integrating the Random Fourier Features into their methodology?",507,0,11,0.7986000000000001,0.0256258503,0.9049091339,48,15.1946,0.1041,iclr,0.0210526315789473,4,4,4,4,partially factual,4,4,80,polite,4,neutral,4,low,5,5,5,5,partially factual,5,5,90,polite,5,neutral,5,low,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
128,Reviewer-TLX1,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","This paper proposes a method, Online Sparse Residual Tree (OSRT) for
handling streaming multivariate scattered data. The proposed
method is built on the sparse residual tree (SRT) method proposed in \[Xu & Luo, 2022\] and extended to deal with
evolving data efficiently in an online fashion.

The proposed OSRT model dynamically updates the tree structure by adding or deleting neurons and by splitting nodes as a new training sample arrives.
Experiments demonstrate that the ORST method has superior performance to other online algorithms. - With the proposed online extension, the SRT framework can now learn streaming data in an online fashion to predict future data.
- The experiments demonstrate the proposed method outperforms the state-of-the-art base-line methods in the literature. - There are some imprecise parts which make it difficult to evaluate the feasibility of the proposed method. For example, in Section 2.2, on page 5, the sentence ""then we set the."" is incomplete. Algorithm 1 is not fully explained in the text. For example, FindLeaf() in step 3 is not defined in the text. The step 9 seems to contradict what they say in the text. I supporse if the condition is NOT satisfied then it should do splitting. On page 5, the authors state that ""We have mentioned ... as $N_{max} = 1.2 N_{\chi}$,"" but they never mentioned it earlier.
- The SRT, which is the previous work, is treated as if originally proposed in this paper. The authors should clearly split Section 2 into two separate sections, one for explaining the previous SRT as background and the other for the proposed online extensions. 
- The details of the hyperparameter settings used in the experiments are missing completely. The hyperparameters include the maximum tree depth $d_{max}$, the factor $\theta_s$, the stack size $N_l$ and the error threshod $\Delta_1$. Changing their values may influence their performance and setting them to appropriate values may be non-trivial. However, none of their concrete values nor
their robustness to the performance in the experiments is reported. Because OSRT is an extension of SRT, I would like to know the performance difference
between the original SRT and its online version OSRT. The ORST is an online algorithm and evaluates each
sample only once according to Algorithm 1 on page 7. Therefore 
some performance degradation is expected against SRT, while OSRT is more
computationally efficient. The extent of the performance degradation is important
information to understand the potential of the proposed method and should be reported.

Minor comments:

In Section 2 on page 2, the Gaussian kernel is defined as $\theta_j(x)$ that includes $c_j$ as its center vector but a different
symbol $\phi_j(x)$ is used in the following equation. 
On page 4, $\phi_{\delta_l}(X_{li} -\chi_j)$ is used, where the definition of $\phi_{\delta_l}(x)$ does not include $c_j$ and the suffix of $\phi_{\delta_l}$ is the shape parameter, while the suffix of $\phi_j$ is the node index.

On page 4, $\sum_{i=1}^{t_q}$ should be $\sum_{i=1}^{q}$.

In Section 2.1, $\prec t_q$ is defined but $t_q$ is not defined at all and is still used in a couple of places.

In Equation (8), the notation $r_l(x)$ is misleading. It should be $r_l(X_l)$ as  used
later in $Q^T_{q+1}r_l(X_l)$.

In Section 2.3 on page 6, the definition of $S_m$ is unclear. $S_m$ is supposed to be a vertex of Voronoi diagram.

The right hand side of Equiation (1) : $\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x)$ is confusing because 
$\sum_{i=1}^{N_{\chi}} \alpha_i \phi_{\delta_l}(x) = \phi_{\delta_l}(x)\sum_{i=1}^{N_{\chi}} \alpha_i $",568,0,0,0.7098,0.067958153,0.9277796745,49,56.0736,0.2552,iclr,0.0,5,5,5,5,factual,4,4,90,polite,4,negative,5,none,5,4,4,5,partially factual,4,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,5.0,neutral,5.0,moderate,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
128,Reviewer-nWJS,OSRT: An Online Sparse Approximation Model for Scattered Data,"Online learning is a crucial technique for dealing with large and evolving datasets in various domains, such as real-time data analysis, online advertising, or financial modeling. In this paper, we propose a novel predictive statistical model called the Online Sparse Residual Tree (OSRT) for handling streaming multivariate scattered data. OSRT is based on online tree decomposition and online adaptive radial basis function (RBF) exploration. OSRT dynamically expands its network depth as more data arrives, and incorporates a sparse and appropriate RBF refinement at each child node to minimize the residual error from its parent node. OSRT also uses an incremental method to explore the central node of the RBF function, ensuring both sparsity and accuracy of the model. When the network reaches its maximum depth, the OSRT model updates the RBF approximation of its final layer based on the most recent data. This ensures that the model captures the latest trends in the evolving data. We evaluate our algorithm on several datasets, and compare it with existing online RBF methods. From the results, it is shown that OSRT achieves higher efficiency and accuracy.","This paper presents a predictive statistical model OSRT for handling streaming multivariate scattered data. The OSRT model can dynamically expand its network depth with the arrival of data. A RBS refinement is also incorporated into the OSRT model to minimize its residual error. Moreover, the paper proposes an incremental method to explore the central node of the RBF function, ensuring the sparsity and accuracy of the model. Theoretical analysis and Empirical results are provided to demonstrate the effectiveness of the proposed OSRT mode. S1. The paper focuses on online regression analysis, which is an important problem especially considering the growing necessity to process large-scale data in the era of Big Data.

S2. The paper proposes several approaches to minimize the residual error. The effectiveness of the proposed method is theoretically proved and empirically demonstrated. My main concern is the presentation of the paper. 

1. There is no formal problem definition in the introduction, which makes it almost impossible for non-experts to understand the paper. 

2. The introduction part is too short and not very informative. The authors should at least illustrate some of the backgrounds of online regression analysis and highlight existing challenges. 

3. The authors did not clearly state the technical contributions of the work. The related work part is also messy, which makes it very hard for me to identify the contributions of the paper. 

4. the author did not present any intuition for the proofs, which makes it hard to verify the correctness. 

5. the current manuscript contains numerous typos, unclear sentences, and undefined notations. For instance: 

- Page 1: For example, The partition

- Page 1: with more and more data is generated

- Page 1: have deriving

- Page 1: too large a network may bring in ...

- Page 1: takes the growing strategy first, it adds

- Page 2: It separate

- Page 2: represented blow

- Page 2: Where

- Page 3: Where

- Page 3: Most regression trees grown by 

- Page 3: $r_{l+1, j}$ combined into

- Page 3: the notation $\varphi$ requires clarifications

- Page 3: $i \neq j$ Then -> $i \neq j$. Then

- Equation (4): $\mathbb{I}$ and $1_{\Omega_{L_i}}$

- Page 4: then the problem (??)


In general, I think the paper is promising. However, the presentation of the paper does not meet the high standards of ICLR. Please refer to the Weaknesses part for details.",399,0,5,0.7301000000000001,0.0372081413,0.9088370204,49,43.15,0.1508,iclr,0.0,3,4,5,3,factual,3,4,79,polite,4,negative,4,moderate,5,5,5,5,factual,5,5,90,neutral,5,negative,5,low,1.0,3.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,4,3,4,4,factual,4,4,75,polite,5,neutral,4,low,3,3,4,4,factual,4,4,78,polite,5,neutral,3,low
49,Reviewer-Uwik,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","The paper introduces the Cross-Guided Ensemble of Tokens (CrossGET), which is designed to enhance the efficiency of vision-language Transformers. It tackles the significant challenge of mitigating the computational costs and latency associated with vision-language models. Within this framework, two essential components come into play: Cross-Guided Matching and Ensemble, orchestrating the fusion of tokens guided by cross-modal cues, and Complete-Graph Soft Matching, contributing to the refinement of token matching outcomes. 1.Comprehensive Experimentation and Solid Theoretical Foundation: The paper's strength lies in its extensive and well-documented experiments, combined with a rigorous theoretical underpinning for the proposed method. This makes the work sound and reliable, both in terms of its theoretical framework and practical applicability.
2. Relevance of the Addressed Problem: The choice of the problem addressed in the paper holds significant value, especially in the context of the substantial computational overhead associated with many state-of-the-art multimodal models. This highlights the practical importance of the research. However, it is recommended that the authors extend their analysis and experimentation to encompass a broader range of models, moving beyond the initial exploration with BLIP-2. This would further enhance the paper's contribution and generalizability. 1. Cross-Modal Guidance Utilization: In the paper, the emphasis is placed on the ability of CrossGET to be applied to modality-dependent models like BLIP and BLIP2. The approach involves learning a cross-token to serve as guidance for another modality. However, there are concerns about this approach. Taking BLIP as an example, it appears that it may not fully harness textual guidance. In scenarios like visual grounding, where different textual descriptions highlight various aspects of the same image, it raises questions about how CrossGET selects tokens from different texts to focus on.
2. Unfair Experimental Comparisons: The paper contains instances of unfair comparisons in the experiments. For example, in section 4.1, the authors directly compare retrieval results of models such as TRIPS and UPOP. Yet, these models vary significantly in terms of training data and model parameter sizes, making the comparison less meaningful. To provide a clearer perspective, the paper should emphasize how much TRIPS, or similar acceleration methods, improve over the baseline, and how much the proposed method accelerates and enhances performance compared to the baseline.
3. Limited Model Performance Improvement: The paper reports only marginal improvements in model performance while introducing a relatively complex method. Moreover, the acceleration achieved by the proposed method appears similar to that of ToMe. Given the relative complexity of the proposed approach, the effectiveness of this work may be questioned, especially if the gains in performance and acceleration are not substantial. 1. Implementation of Token Reduction in BLIP-2: It would be beneficial for the authors to provide more detailed information on how they specifically implemented token reduction in BLIP-2 within the context of their method. A more elaborate explanation of the process and its impact on BLIP-2's performance would enhance the clarity and completeness of the paper.
2. Impact of CrossGET on OPT in BLIP-2: A notable aspect of this work is the introduction of CrossGET into the frozen OPT component of BLIP-2 for token reduction. However, it's important to consider that OPT is a decoder-only model. The paper should address how this approach might affect the inference capabilities of OPT and whether any experiments were conducted to analyze and verify why image captioning performance appears to be minimally impacted. Further insight into this aspect of the methodology would enhance the paper's robustness and contribute to a better understanding of the results.Im glad to improve my score if my   concerns be addressed.",586,0,6,0.7977000000000001,0.1171066253,0.94465065,48,25.0653,0.1262,iclr,0.0,2,5,4,3,factual,3,4,78,polite,4,neutral,4,moderate,4,4,4,4,partially factual,4,3,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,3.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
49,Reviewer-oYGw,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","This paper proposes cross guided matching and cross guided ensemble as cross-modal importance indicator. Besides, a Complete-Graph Soft Matching algorithm is proposed as an improved version of ToME's bipartite soft matching. 1. Both Cross Guided Matching (CGM) and Complete-Graph Soft Matching (CGSM) is well motivated and proved to be effective.
2. Extensive experiments are conducted on several vision language tasks for both modal indenpendent VL model (CLIP) and modal dependent VL model (BLIP2). I do recognize the amount of work that went into this submission. 1. The proposed approach is named as Cross-Guided Ensemble of Tokens, however, I find that the proposed Cross-Guided Ensemble (CGE) is not that useful as illustrated in Table 1. So, I think the paper should re-organize the structure and highlight the really useful designs.
2. The proposed Complete-Graph Soft Matching is not specialized for cross-modal tasks, so does it outperform the ToMe algorithm in general visual recognition tasks? The proposed method can improve the model efficiency after training with little performance loss, and I am curious if the proposed method can also accelerate the training of multi-modal tasks.",183,0,4,0.7942,0.08515625,0.9441901445,48,40.5737,0.0529,iclr,0.0,4,4,2,3,unfactual,3,2,75,neutral,4,negative,4,high,4,4,4,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
49,Reviewer-Yt1v,CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers,"Recent vision-language models have achieved tremendous progress far beyond what we ever expected. However, their computational costs are also dramatically growing with rapid development, especially for the large models. It makes model acceleration exceedingly critical in a scenario of limited resources. Although extensively studied for unimodal models, the acceleration for multimodal models, especially the vision-language Transformers, is relatively under-explored.  To pursue more efficient and accessible vision-language Transformers, this paper introduces \textbf{Cross}-\textbf{G}uided \textbf{E}nsemble of \textbf{T}okens (\textbf{\emph{CrossGET}}), a universal acceleration framework for vision-language Transformers. This framework adaptively combines tokens through real-time, cross-modal guidance, thereby achieving substantial acceleration while keeping high performance. \textit{CrossGET} has two key innovations: 1) \textit{Cross-Guided Matching and Ensemble}. \textit{CrossGET} incorporates cross-modal guided token matching and ensemble to exploit cross-modal information effectively, only introducing cross-modal tokens with negligible extra parameters. 2) \textit{Complete-Graph Soft Matching}. In contrast to the existing bipartite soft matching approach, \textit{CrossGET} introduces a complete-graph soft matching policy to achieve more reliable token-matching results while maintaining parallelizability and high efficiency. Extensive experiments are conducted on various vision-language tasks, including image-text retrieval, visual reasoning, image captioning, and visual question answering. Performance on both classic multimodal architectures and emerging multimodal LLMs demonstrate the effectiveness and versatility of the proposed \textit{CrossGET} framework. The code and models will be made public.","The paper proposes CrossGET to accelerate VLM by token merging. Specifically, this work introduces complete-graph matching to partition tokens and merge/reduce tokens based on similarities. The experimental results on common vision-language tasks demonstrate some effectiveness of the proposed method. The paper is well-organized and the presentation is good. The motivation of accelerating VLMs is clear. 1. The major issue is novelty. CrossGET is incremental over ToMe by replacing ToMe's matching algorithm, adding learnable tokens and adapt unimodal ToMe to the multimodal setting.
2. As shown in Table 1, the newly proposed matching algorithm has marginal improvements.
3. CrossGET is proposed to accelerate heavy VLMs. However, majority of experiments are carried out on relatively light-weighted BLIP. There's only a small section for the truly heavy BLIP2, which is a stronger VLM that really needs acceleration.
4. CrossGET requires fine-tuning of VLMs. (1) In most cases, when models need fine-tuning, they are relatively small (acceleration is not demanding). (2) Huge VLMs that are really heavy can be used as zero-shot in different tasks or different datasets of a same task. In this sense, CrossGET which does not apply to pre-training stage is a bottleneck.
5. The paper fails to compare or adapt relevant works \[1\]\[2\].

\[1\] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification, NeurIPS 2021

\[2\] Not all patches are what you need: Expediting vision transformers via token reorganizations. ICLR 2022

**Final recommendation**: I agree the paper is improved by additional experiments and extensive analysis, and thus I raise my rating to 5. When CrossGET is applying to Flamingo or BLIP2 which uses frozen LLMs, it reduces to accelerating only vision encoders? Then, there will be a bunch of alternative approaches in accelerating ViTs?",283,4,5,0.8172,0.0279545455,0.9102016687,74,38.8176,0.049,iclr,0.0,4,4,3,4,factual,3,4,86,polite,5,positive,4,low,5,5,4,5,factual,4,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,neutral,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
32,Denise-Battaglini,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Not Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This study investigates the predictors of hospital length of stay in COVID-19 patients in Jordan.  The study is well written and interesting. However, it has a lack of novelty and should be improved. I would suggest to add more information: 1) Hospital length of stay is often made by different wards and eventually ICU. I think it is important to understand which patients were admitted to ICU, if some of them were endotracheally intubated, tracheostomize, if some patients had hemorrhage, thrombosis, infections, other complications, which PaO2/FiO2 on admission, if they were non-invasively ventilated (CPAP, NIPPV, High flow), if CPR, D-dimer, previous antibiotic therapy, SOFA on admission, Charlson comorbidity index, steroidal therapy, sedation, analgesia, myorelaxants, etc. and other factors that could have been predictors of hospital stay.  The study aims to investigate only predictors but I believe that there is a lack of some important factors which could have changed patients' clinical course.  I suggest to extend the analysis to other important factors and, if possible to divide between those who survived and those who did not OR those who were admitted to ICU/those who did not.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Partly",330,0,1,0.7733,0.1701041667,0.7571926117000001,174,32.22,0.2025,f1000,0.0,5,4,3,5,partially factual,3,2,50,polite,4,negative,4,moderate,5,5,5,5,partially factual,5,5,75,polite,5,negative,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,3,4,factual,4,4,75,polite,4,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
32,Omar-Soliman-Mohamed-El-Masry,Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan,"Background: On March 11th, 2020, the World Health Organization (WHO) declared coronavirus disease 2019 (COVID-19) as a global pandemic. Healthcare systems in low- and middle-income countries may face serious limitations during a pandemic, for which understanding the predictors of prolonged hospital stay are crucial in decreasing the mortality rate. The aim of this study was to investigate the predictors of increased length of hospitalization among COVID-19 patients. Methods: In this prospective study, we investigated the effect of presenting symptoms and laboratory investigations on the duration of hospitalization of 131 COVID-19 patients at a tertiary hospital in Jordan from March 17th to April 9th, 2020. Results: Patients median age was 24 years [interquartile range (IQR): 8-39], of which 67 (51.15%) were males and 64 (48.85%) were females. Smokers had shorter in-hospital stay (OR: -3.52; 95% CI: -6.73 to -0.32; P=0.03). Taste loss (OR: 5.1; 95% CI: 1.95 to 8.25; P<0.01) and chills or rigors (OR: 4.08; 95% CI: 0.73 to 7.43; P=0.02) were the symptoms significantly associated with increased in-hospital stay, while those who had malaise (OR: -4.98; 95% CI: -8.42 to -1.59; P<0.01) and high white blood cell (WBC) count (OR: -0.74; 95% CI: -1.31 to -0.17; P=0.01) had faster recovery. Conclusions: Our study found that the most common presenting symptoms of COVID-19 are cough, malaise, and headache. Smoking, presenting with malaise or elevated WBCs were associated with shorter hospital stay, while loss of taste and chills or rigors at presentation were associated with a longer in-hospital stay.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article titled ""Clinical characteristics and predictors of the duration of hospital stay in COVID-19 patients in Jordan” represents an attempt to assess clinical factors that might be associated with COVID-19 patients' hospitalization in Jordan. The article rationale is good, however, it cannot reflect the figure in the whole country as data being taken from five centers and included a limited number of patients. I would suggest revising the title unless the data at this time represents the total number of patients in the whole country. In the introduction, the authors started to recount history of the beginning of COVID-19 observation in China, but the year was not mentioned (December 2019). Please, add 2019. The introduction should include a background section on factors reported in the study that might affect patients’ hospitalizations that were reported, at least, for similar diseases (MERS, for example). In addition, the authors should discuss the other factors that could affect this parameter; such as comorbidities. In the material and methods’ section, the following sentence “It is noteworthy that, in Jordan, all patients diagnosed with COVID-19 were admitted to hospital during the study’s timeframe, regardless of the severity of their illness” needs further clarification; does this mean that those were all COVID-19 patients reported in the whole country? If yes, it would be very early to generalize the findings of the current study and this must be clearly indicated as a limitation. In the results section, smoking status was not found as a predictor for the length of the hospital stay, which is odd knowing that COVID-19 patients suffer from serious lung problems. Did the author investigate confounding factors with smoking status; such as age, for example? Also, I think calculating odd ratio is not suitable for the study design. I think the findings were concluded from a premature study, which was conducted at the very beginning of the Corona crisis; therefore, the conclusions are premature and cannot reflect the logical and the expected conclusions regarding COVID-19. Thus, the study should be revised by including data of a larger sample size to be more representative and provide evidence-based conclusions. Having said that, the study rationale is good and interesting; but when supported with robust design, it will be of more interest to the scientific community and will better reflect the real situation.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",531,0,2,0.7698,0.1330880952,0.8107333779,183,30.09,0.2674,f1000,0.0202020202020202,5,4,4,5,factual,3,2,60,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,80,polite,4,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,low,4,4,3,4,partially factual,4,3,75,polite,5,neutral,4,low
79,Reviewer-LDAP,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The authors introduce a dataset distillation (DD) method called Farzi Data for data with a ""left to right"" (autoregressive) causal structure. Their algorithm has two novel elements: 1) the parameterization of the synthetic distilled data, which allows them to apply it to discrete data (such as the tokens in language modeling); and 2) a method for computing the outer loop gradient for DD when the inner loop is performed with Adam, which has a constant memory footprint independent of the number of inner optimization steps. They conduct extensive experiments with their proposed method on language modeling and sequential recommendation tasks. Compared to existing DD methods (adapted to discrete data via their parameterization), they obtain improved performance across the tested datasets, often obtaining downstream performance better than training a model on the entire original dataset. **Algorithmic Contribution.** Algorithm 1 for computing the gradient through the inner-loop optimization with Adam using constant memory is a significant contribution. Among existing dataset distillation methods, those which take into account the entire training trajectory on the distilled data tend to obtain better accuracy (as compared to other methods which use surrogates for this objective such as the gradient matching objective in dataset condensation). However, the computational burden of these methods (specifically the memory requirement, which necessitates keeping the entire computation graph) renders them infeasible for application to larger datasets. Farzi Data takes a significant step towards addressing this problem by introducing an algorithm for differentiating through an inner loop optimized with Adam, whose memory does not scale with the number of steps in the inner loop (see Fig. 5). This is an important improvement for DD to be practically useful in real ML applications.

**Empirical Results.** The empirical results are also impressive. The authors obtain better performance than competing methods across several different real-world benchmarks. There are even scenarios where their distilled data consistently outperforms training on the entire original dataset (cf. Table 1), indicating that Farzi Data implicitly promotes some sort of ""data cleaning"" whereby samples that *hurt* model performance are removed or discounted. This is similar to, e.g., removing mislabeled points or data with negative Shapley values, but Farzi Data is not explicitly trained for this task. **Presentation and Clarity.** While the actual prose of the paper was generally clear and easy to read, there are some major concerns with notation/presentation that limit understanding of some of the main contributions of the paper.

P1. There are many cases where important notation is not defined. For instance, $\mathrm{Rep}(\mathcal{F}, \mathcal{D})$ is defined in the Appendix, but not the main text, and is critical to interpreting Theorem 3.1. It is not stated what the terms $d\mathbf{m}$, $d\mathbf{x}$, and $d\mathbf{w}$ in Algorithm 1 are supposed to be, so it is impossible to determine if the expressions are correct or not. How to construct the output of the algorithm from these quantities is also not clear. What is the correspondence of the quantities in Alg. 1 to the DD problem, i.e., what will we actually update using the meta-gradient once we know how to compute it? Some (but not all) of these details can be found in the Appendix, but as they are critical to being able to understand the results, they should be moved to the main text and given appropriate explanations.

P2. Stylistically, there is also some nonstandard notation. For instance, $\mathcal{O}(100)$ (3rd bullet point, pg. 2). I suppose the authors meant ""on the order of 100x"", but big-O notation has a mathematically precise meaning that doesn't make sense here. Another instance is Proposition 3.2. ""Correctness of Algorithm 1, Line 13"" is not a complete mathematical statement (or a complete sentence). The result should be stated completely and precisely.

**Theoretical Results.** There are also issues with the theoretical results.

T1. The most critical problem is that the proof of the main theorem (Theorem 3.1) is not mathematically sound. Specifically, the authors want to show that the expected representativeness of their low-rank synthetic data parameterization is strictly less than the expected representativeness of a naive synthetic data parameterization, under some suitable conditions and for quadratic classifiers: $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < \mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$. ($\mathcal{D}_F$ and $\mathcal{D}_N$ stand for Farzi and naive data, respectively.) In their proof in Appendix B.1, they show that $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_F)\] < B_1$ and $\mathbb{E}\[\mathrm{Rep}(\mathcal{F}, \mathcal{D}_N)\]$ for some bounds $B_1$ and $B_2$. Then, since $B_1 < B_2$, they conclude the desired result. This is not valid: $a < b$, $c < d$, and $b < d$ does not imply that $a < c$. There needs to be a _lower_ bound on the representativeness for the naive parameterization.

I remark that I believe the _result_ is (at least ""morally"") correct. The theorem essentially reduces to saying that the Rademacher complexity resulting from the low-rank parameterization is smaller than the Rademacher complexity from a general parameterization, which is intuitively obvious. However, the _proof_ has a fatal error and must be corrected somehow.

T2. For Lemma B.3 to hold, there must clearly be some assumptions on the loss function $l$; in order to apply the lemma from Shalev-Shwartz, the Rademacher complexity of the loss composed with the models in $\mathcal{F}$ must be considered, not $\mathcal{F}$ itself. As stated, I believe this lemma is not correct and the loss must be accounted for. Apart from the logical error, the motivation for the use of quadratic classifiers in the theorem wasn't clear to me. What connection do such models have to the auto-regressive tasks that Farzi Data is applied to?

T3. This is related to the presentation problems regarding the notation used in Algorithm 1, but the proof of Proposition 3.2 is also suspect. What is meant by $d\mathbf{m} = d\mathbf{m} + \frac{\partial w_t}{\partial m_t} \cdot d\mathbf{w}$? Is $w_t$ supposed to be $\mathbf{w}_T$, or is this expression meant to be a recursive formula? What about the formulas for the other quantities, and how are these combined to compute the meta gradient?

If these issues can be satisfactorily addressed, along with the questions in the section below, I would be willing to raise my score to accept, given how promising the empirical results are. Q1. The authors mention that training with the reference trajectories $\Omega$ is important for obtaining the best performance, as compared with training only from randomly initialized networks. However, it wasn't clear to me if this might just have been the result of a greater number of training steps when learning the distilled dataset. That is, are the results in Fig. 6(b) with the total number of meta-gradient steps constant, or do the additional precomputed trajectories result in more meta-gradient steps?

Q2. On a related note, it was not clear to me exactly how the precomputed trajectories were used. My assumption was that instead of training the network in the inner loop only from random initializations, instead the network from the inner loop will be initialized with parameters from one of the training trajectories. Is this correct?

Q3. Why isn't FMLP also used as a teacher network in Table 1?",1161,0,13,0.7745000000000001,0.0963321995,0.9078657031,47,38.9031,0.0977,iclr,0.0,5,5,5,5,factual,3,5,98,neutral,5,neutral,4,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,5.0,neutral,5.0,low,5,4,5,5,factual,5,5,95,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
79,Reviewer-THWQ,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper provides an extension of dataset distillation to sequence modeling along with a few other innovations, such as a low rank approximation of the distilled dataset and an efficient trick to save memory during meta-learning. Overall, the paper contains strong (albeit limited) empirical results on the sequence modeling (penn tree bank) and recommendation systems datasets. * The high level motivation of the problem is quite the need of the hour, as with larger models we need to better understand their dependencies on the data
* Pursuit of this research direction could potentially yield methods that enable us to train SOTA transformer models for a fraction of the input cost
* Empirical results are thorough, although a bit limited in terms of number of datasets for sequence modeling (only PTB is used) A number of points about the approach were unclear to me from the writeup, and I would appreciate clarifications from the authors:

* It is said that the complexity of the dataset distillation algorithm scales by the size of the vocabulary (page. 4) and the size of the sequence that we wish to model. I can see the latter to be the case, since the loss will now be summed over the entire sequence as opposed to one forward pass (so the complexity of the forward pass is increased). However, I do not see how the time complexity increases with the vocabulary size. Do we mean space complexity? Also, more than the forward pass the dominant factor in dataset distillation is the computation of a bunch of hessian vector products in the meta gradient. Those terms do not depend on the vocabulary size either… please clarify..
* It would be nice to provide an intuition for what is saving the memory, making things O(1) in memory.  Currently the big algorithm block does not provide an intuition for how this approach is O(1) in memory regardless of the number of timesteps of unrolling. This is important to clarify, since this is an important contribution, if clearly explained. If this approach is essentially gradient checkpointing, then it is worth noting that Deng and Russakovsky already implement a version of this in their code. 
* Looking at Eqn. 2, I am a bit puzzled as to how \Omega, namely the trajectories from the real data are incorporated in the DD process. From what I am able to understand, \theta_0 \sim Omega -- namely the init is sampled from the pretrained trajectories, and then from the right hand side of eqn. 2 I understand that the rest of the trajectory is obtained using Adam on the synthetic data. Where is the role of the pretrained trajectories then? Please explain..

* Rank regularization has been done in the previous work (Deng and Russakovsky) for dataset distillation. It should be cited that this has been done, and not be presented as a novelty.. My major questions concern the clarifications about the approach listed above, without which it is really hard to judge the technical correctness / soundness of the paper.",506,0,0,0.7575000000000001,0.0487258687,0.8768354654,47,45.1043,0.7308,iclr,0.0196078431372549,4,4,4,3,factual,4,4,80,polite,3,neutral,4,none,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
79,Reviewer-htDK,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","This paper proposes a method for distillation of ""auto-regressive data"", in this case meaning any data that is represented as event sequences. This can include natural language text, but also general time-series data. Their method aims to summarize a dataset into a sequence of latent embeddings (which can subsequently be decoded) given a downstream task such that they achieve similar performance to training on the complete dataset. They do this through a meta-learning procedure, optimizing directly through Adam for data which lowers downstream task loss. My review comes from the point of view of someone familiar with training on natural language (and associated downstream evaluation), but not general event forecasting problems. I was not familiar with the benchmarks used by the author prior to reading this paper. 

**Originality and Significance**

- The paper seems original. Aspects of this work (e.g. using meta-learning/second order methods) for distillation have been touched on in the past, but usually for smaller datasets, and generally not for auto-regressive tasks. Most past works I have seen which work on large corpuses revolve around finding mixing coefficients for existing datasets \[1\]. This method doesn't work on datasets of that size, however this shows an improvement in scaling. 
- Getting a meta-learning approach to work on such dataset sizes is quite difficult, given difficulties with estimating second-order components over the full dataset. Scaling this to even larger language-style datasets would be an interesting (future) contribution.



**Quality and Clarity**

This paper is quite well-written. Experimental details are clear, and the method is properly motivated. Diagrams clarify the algorithm and the key difficulties to this method are highlighted appropriately.

\[1\] The Pile: An 800GB Dataset of Diverse Text for Language Modeling, Gao et al. 2021 **Weaknesses**

- The authors touch on language datasets as a motivation, however do not study this (or other large-sequence tasks) due to practical model/sequence length scaling constraints. Are there reasonable paths forward that would allow this to scale to longer sequence lengths/larger models? 
- Given that the outer loop evaluates across the full original dataset, and the inner loop needs to be run several times to get updated parameters (Figure 5), what's the overall cost saving versus just training a model on the original dataset for more time (until matching student performance), if any? 
- Have the authors thought about cases where there is significant noise in the training corpus? Given that the loss is computed with respect to the original dataset, it seems like this could be a problem if one ever tried to directly filter a noisy web-crawl. All questions have been included in the ""Weaknesses"" section above.",435,2,1,0.8452000000000001,0.0972619048,0.9140241742,47,38.7268,0.103,iclr,0.0,3,4,3,3,partially factual,4,4,72,polite,4,neutral,3,low,4,5,4,5,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,5,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
79,Reviewer-FjiL,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","The paper introduces FARZI, a data distillation framework for machine learning tasks. The goal is to condense the original large dataset into a much smaller number of synthetic sequences, so that downstream performance on the synthetic data matches (or even improves) performance on the full real dataset. The authors cast the problem using a bi-level optimization formulation, similar to meta-model matching based dataset distillation. The naive formulation is infeasible due to the very large token vocabulary and the maximum sequence length. To address this, the authors propose to factorize the synthetic dataset into a latent data summary and a token-decoder matrix. This renders the optimization continuous (as opposed to discrete), while it provides flexibility to sample synthetic sentences from a distribution (as opposed to having a fixed small set of synthetic sentences). Furthermore, the authors suggest to replace SGD in the inner loop by the Adam optimizer. To mitigate the large memory footprint, they derive an efficient approximation for reverse-model differentiation of the Adam optimization. The authors assess FARZI on sequential recommendation and language modeling tasks, where they manage to match or even exceed the downstream full-data performance using as little as 0.1% of the original dataset. The authors conduct several experiments and ablation studies to shed light on various aspects of their framework. The paper makes several interesting contributions. The meta-model matching based dataset distillation was originally proposed for continuous data (e.g., image data), as opposed to language data that use discrete tokens. The use of a latent space addresses this challenge by ensuring that the optimization can be performed in a continuous space, but by also allowing us to sample the synthetic sentences from a compact distribution. Furthermore, the observation that the Adam optimizer is a much better choice for the inner loop optimization (compared to SGD) is very interesting and dramatically improves downstream performance. To address the large memory footprint, the authors derive an efficient approximation of the reverse-mode differentiation of the Adam optimizer, which nicely complements their finding that Adam is better than SGD. Interestingly, this may be more broadly applicable in other bi-level optimization tasks (e.g., in a meta-learning context).

The paper is well written and the related work is covered quite extensively. The authors describe in detail the various insights of their framework. When it comes to the experimental evaluation, they provide a lot of information on the metrics, datasets, hyperparameters, objectives, and even architectures.

The experimental evaluation is quite convincing and supports the claims made by the authors. It is very interesting that FARZI can even outperform downstream performance on the full original dataset, which could indicate the improved robustness with dataset distillation. I liked the fact that the authors investigated various aspects of FARZI, such as the versatility of the synthetic data, the cross-architecture generalization, the performance of different meta-objectives, the cold start problem, and the impact of pre-trained trajectories. 1. Even though this paper makes interesting contributions to the DD literature for autoregressive tasks, it is not so obvious that it would be 
very helpful for much larger text corpora and large language models with millions or billions of parameters. The memory footprint might end up being very large, rendering the whole framework infeasible. Furthermore, a compression rate of 0.1% may not be extremely helpful for very large datasets consisting of billions of sentences. This may limit the applicability of FARZI to settings consisting of ""reasonably large but not very large"" language corpora.

2. It was not clear to me how time-consuming the FARZI dataset generation process is. For example, how long did it take to generate the synthetic datasets for the tasks considered in this work? In particular, did FARZI improve the total runtime? For instance, if generating the synthetic data takes very long, then there may be very little benefit (if any) from this process. Furthermore, it is not automatically obvious that a smaller dataset can be trained faster than a larger one. There is the added question of the number of epochs required to reach convergence. The synthetic dataset may require more rounds. This was not obvious in the experimental evaluation. If I am not mistaken, I feel that the subject of runtime was only superficially touched in this work, and a more thorough discussion (with detailed pros and cons) would be needed.
(Theoretically, this may not be a big issue if the same synthetic dataset could be successful used on several downstream tasks, but this is not immediately true. If we need dataset distillation for each separate task, then we may end up performing FARZI several times.) 1. Could the authors elaborate more on the total runtime (total time for synthetic dataset generation + total time for downstream training with synthetic vs. full data)? It would be helpful if the authors could shed light on the various questions/comments raised in Weakness (2) above.

2. In Equation (2), \Omega is a set containing initializations for the inner loop, if I understand correctly. But instead of picking the initialization randomly, these come from a small number of training trajectories on the full dataset. If that is true, then the \theta_i in the definition of \Omega has nothing to do with the update rule for \theta_t in Equation (2). This may still be confusing to some readers though because the same symbols are used (theta with a subscript, so the authors may want to clarify this point (i.e., what exactly is in \Omega).

3. I was not clear how exactly the authors chose the final hyperparameters for each setting. Did they exhaustively try all corresponding combinations in the hyperparameter table and picked the best one?

4. Is a new synthetic batch created at the beginning of each outer-loop step based on the latent factorization?",954,0,6,0.7977000000000001,0.1470528605,0.865883112,65,37.7545,0.1429,iclr,0.0,4,4,5,5,factual,3,4,90,polite,4,neutral,4,none,4,5,4,4,factual,4,4,88,polite,5,positive,5,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,5,4,factual,5,5,90,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
79,Reviewer-At7H,Farzi Data: Autoregressive Data Distillation,"We study data distillation for auto-regressive machine learning tasks, where the input and output have a strict left-to-right causal structure. More specifically, we propose Farzi, which summarizes an event sequence dataset into a small number of synthetic sequences — Farzi Data — which are optimized to maintain (if not improve) model performance compared to training on the full dataset. Under the hood, FARZI conducts memory-efficient data distillation by (i) deriving efficient reverse-mode differentiation of the Adam optimizer by leveraging Hessian-Vector Products; and (ii) factorizing the high-dimensional discrete event-space into a latent-space which provably promotes implicit regularization. Empirically, for sequential recommendation and language modeling tasks, we are able to achieve 98 − 120% of downstream full-data performance when training state-of-the-art models on Farzi Data of size as little as 0.1% of the original dataset. Notably, being able to train better models with significantly less data sheds light on the design of future large auto-regressive models, and opens up new opportunities to further scale up model and data sizes.","This paper proposes FARZI, a data distillation method for auto-regressive ML tasks/event-sequence datasets. The method summarizes a large dataset into a set of synthetic sequences in latent space which can be decoded later. They show that model performance is upheld/enhanced when compared to training on the complete dataset on the downstream tasks of sequential recommendation and language modeling. For data distillation, the paper shows Adam to be better than SGD as inner loop optimizer, and derives an efficient reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps. - Originality and Significance: The latent parametrization that makes FARZI optimization friendly, and the proposed trick that enables reverse mode differentiation of Adam such that its memory complexity is independent of the number of inner loop steps are great contributions and of practical value.
- Quality and Clarity: The paper is well written with extensive experiments whose details and evaluations are that are clearly described. The results are impressive. The method is able to achieve better performance on downstream tasks compared with using the full dataset. - It is not clear whether this method will be practical and scale for larger language models and larger datasets. It would be great if the authors can elaborate on this.
- There is not a clear analysis of the total time gains of this method in comparison with training from scratch. Providing some values would make the case for this method more compelling. Listed in weakness section.",251,0,0,0.7685000000000001,0.2299744898,0.9208657742,47,39.2431,0.0945,iclr,0.0,1,2,2,1,unfactual,4,3,55,neutral,1,neutral,1,high,4,5,4,4,factual,4,4,85,polite,5,positive,5,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
40,Reviewer-PzSz,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper studies the contextual bandit and imitation learning problem with preference-based feedback. The authors propose an oracle-based contextual bandit algorithm, which attains both worst-case and instance-dependent regret bounds. Besides, the algorithm has an instance-dependent guarantee on the querying numbers of the preference-based information. Furthermore, the proposed bandit algorithm is extended to the imitation learning setting with provable guarantees. - the proposed method has strong theoretical guarantees on the regret (both worst-case and instance-dependent bound) and query complexity. Although the oracle-based algorithm proposed shares similar techniques with MinMaxDB \[Saha and Krishnamurthy, 2022\] and AdaCB \[Foster et al., 2020\], the authors provide enough discussion to highlight the difference.
- lower bounds are provided to justify the upper bounds on regret, and query complexity is tight up to logarithmic factors
- the paper is well-structured and written - about the practical implementation of the proposed method: one of my main concerns about the paper is from the practical side. Similar to the oracle-based algorithm for the standard contextual bandit problem (e.g., SquareCB \[Foster et al. 2022\]), the proposed method is established on an online regression solver with regret guarantees. However, I'm not sure to what extent such an online regression solver can be obtained with the preference-based feedback model. For instance, as shown in example 1, $f(x, a,b) = r(a,x)-r(x,b)$, the function $f(\cdot)$ is not convex even $r:\mathcal{X}\times\mathcal{A}\rightarrow\[0,1\]$ is a convex function, and the algorithm developed for online convex optimization is not applicable. I think it would be beneficial if the authors could provide some concrete examples (for example, the reward function has a linear structure?) that the online regression oracle is available. 

-  about the instance-dependent bound: the proposed instance-dependent regret bound as an $O(\Upsilon^2)$ dependence on the regret of the oracle and an  $O(\Upsilon^3)$ on the query complexity. There seems still some room for improvement. In the finite function space case, AdaCB attains an $O(\log \vert\mathcal{F}\vert/\Delta)$ bound for a standard contextual bandit problem, but the result obtained in this paper implies an $O(\log^2 \vert\mathcal{F}\vert/\Delta)$ regret bound. 


 - could you provide concrete examples of the online regression oracle for the preference-based feedback model? It would be even better if the author could provide more detailed discussions on to which extent such an online regression solver can be established.

- could you provide more discussion on the tightness of the instance-dependent bound, especially on the dependence of $\Upsilon$?

- The expert policy $\pi_e$ is not formally defined. Does $\pi_e$ refer to the policy that can maximize the value function? I am confused by the claim, ""our algorithm not only competes with the expert policy but can also surpass it to some extent"" in line 343. What is the formal definition of ""surpass."" Do you mean the regret would go negative due to the term $Adv_T$? However, it is unclear to me when the negative term is large enough to cancel the $O(\sqrt{T}, A/\Delta)$ term. The paper has discussed the limitation and potential future work in the conclusion. Another issue is that it imposes a realizable assumption for $f^\star$. It is unclear whether extending the analysis for standard contextual bandit (Section 5 in \[Foster et al., ICML 2020\]) to the contextual dueling bandit setting is possible.",533,1,0,0.7521,0.0431166056,0.9522576332,215,40.3366,0.5388000000000001,neurips,0.0,5,5,5,5,factual,5,5,100,polite,5,neutral,5,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
40,Reviewer-K3eU,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","The paper gives “best-of-both-worlds” results for an imitation-learning problem in contextual bandits and MDP settings. With small orthogonal changes to assumptions, the algorithms primarily improve over prior work by considering instance-optimal bounds both in regret and queries, and require only ordinal preference feedback rather than explicit rewards (similar to the “dueling bandits“ literature).  - The paper is easy to read, the algorithms and notation are well-explained, and the results are appropriately contextualized in prior work.
- The examples given for the functions in the model are quite useful for grounding the problem in more concrete applications. Related work is discussed thoroughly.
- Conceptually, the model draws nice connections between contextual bandits and modern topics in finetuning models (e.g. LLMs) from preference feedback, where the emphasis on “instance-optimal” style results is particularly well-motivated. - While the application of techniques from online reinforcement learning to obtain the instance-optimal bounds in this setting is clever, it is unclear how much of this follows directly vs what technical innovation is required. It would be helpful to highlight the methodological contributions used.
- Given the applications discussed, it would be beneficial to give experimental results for preference finetuning (even in a toy setting) to demonstrate the importance of instance-optimality in practice.
- While the instance-optimal rates seem reasonable, it would be nice to include (partially) matching lower bounds for some results, or discuss barriers to obtaining such results. - Can the rates on $d$ or $\Delta$ be shown to be asymptotically tight for either queries or regret? 
- What does the notation $P_t\[a_t, b_t\]$ on line 146 refer to? - Connections to prior RL work which makes use of eluder dimension could be discussed in greater detail.
- Some hyperlinks are broken in the PDF.",290,0,0,0.7843,0.1495748299,0.9330165386,215,31.3783,0.072,neurips,0.0,2,4,3,3,factual,4,3,70,polite,3,neutral,3,low,5,4,4,5,factual,4,5,85,polite,5,neutral,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,5,low,3,5,4,4,factual,4,4,88,polite,5,positive,5,low
40,Reviewer-FECY,Contextual Bandits and Imitation Learning with Preference-Based Active Queries,"We consider the problem of contextual bandits and imitation learning, where the learner lacks direct knowledge of the executed action's reward. Instead, the learner can actively request the expert at each round to compare two actions and receive noisy preference feedback. The learner's objective is two-fold: to minimize regret associated with the executed actions, while simultaneously, minimizing the number of comparison queries made to the expert. In this paper, we assume that the learner has access to a function class that can represent the expert's preference model under appropriate link functions and present an algorithm that leverages an online regression oracle with respect to this function class. For the contextual bandit setting, our algorithm achieves a regret bound that combines the best of both worlds, scaling as $O(\min\\{\sqrt{T}, d/\Delta\\})$, where $T$ represents the number of interactions, $d$ represents the eluder dimension of the function class, and $\Delta$ represents the minimum preference of the optimal action over any suboptimal action under all contexts. Our algorithm does not require the knowledge of $\Delta$, and the obtained regret bound is comparable to what can be achieved in the standard contextual bandits setting where the learner observes reward signals at each round. Additionally, our algorithm makes only $O(\min\\{T, d^2/\Delta^2\\})$ queries to the expert. We then extend our algorithm to the imitation learning setting, where the agent engages with an unknown environment in episodes of length $H$, and provide similar guarantees regarding regret and query complexity. Interestingly, with preference-based feedback, our imitation learning algorithm can learn a policy outperforming a sub-optimal expert, matching the result from interactive imitation learning algorithms [Ross and Bagnell, 2014] that require access to the expert's actions and also reward signals.","This paper develops the provably efficient algorithms AURORA and AURORAE, which are able to achieve the optimal regret bound under contextual dueling bandit setting, and imitation learning respectively, at the same time minimizing query complexity. The key idea behind is that the algorithm only makes a query when the algorithm is very uncertain about the optimal action ($Z_t 1_{|A_t| > 1}$). The algorithm decides the sampling distribution of action pairs to make a query by considering whether the estimated cumulative regret exceeds the carefully designed threshold. If it does not exceed, the algorithm does exploration and sample action pairs from the uniform distribution. If it exceeds, the algorithm uses a technique similar to inverse gap weighting to achieve better balance between exploration and exploitation. For imitation setting with horizon H, the algorithm treats MDP as a concatenation of H contextual bandits and runs AURORAE, which is a stack of multiple AURORA instances. This work is original and well-motivated. It is crucial to design an online learning algorithm that achieves optimal regret while using minimal query complexity. Although I did not get a chance to read the complete proofs in the supplementary material carefully, given the discussion of intuition, all technical results seem reasonable to me. 

This paper is well presented and is a pleasure to read. An example for illustration follows every definition. All materials are well organized in a logical manner. 
 I have several concerns regarding the proposed algorithms. First, P5 I 5, the computational complexity for the candidate arm set might be very large, even if F is assumed to be a d-dimensional linear class. The computational complexity might be $O(dT\log(T)|A|)$. Also, in reality, F might be very complex, which might even worsen the computational complexity. Can we use a simple function class F for approximation while still achieving a similar regret bound?
 Please see the review in weaknesses.  The authors address their limitations of not having any experiments on real data or simulations. I believe the work will be much more convincing if the theoretical bounds are supported by experiment results.",344,0,0,0.8049000000000001,0.0822619048,0.9381507635,215,30.0936,0.3201,neurips,0.021978021978022,4,3,2,3,partially factual,2,3,55,polite,3,neutral,1,high,5,5,5,5,partially factual,5,5,90,polite,5,positive,5,moderate,2.0,4.0,5.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,4,factual,4,4,80,polite,5,positive,5,low,3,5,4,4,partially factual,4,4,85,polite,5,positive,5,low
139,Bhamini-Krishna-Rao,Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study,"Background Healthcare, like other industries, emphasizes performance, quality, and consumer experience while also attempting to reduce costs. However, high-quality healthcare remains paramount for vulnerable and ill patients. This study aimed to investigate parents' and caregivers' level of satisfaction with physiotherapy services provided to neuropediatric outpatients on the United Arab Emirates (UAE).  Methods This descriptive cross-sectional study included 103 parents/caregivers of children with neurological disabilities that were randomly selected from different Emirates Health Services Hospitals in the UAE. Data was collected using the long-form Patient Satisfaction Questionnaire (PSQ-III).  Results The overall mean satisfaction was 159±7.73 (out of 250 points). Communication (20.36/25), interpersonal factors (20.17/35), and doctor-patient time (20.17/35) had the highest mean satisfaction scores (8.06/10). The lowest mean satisfaction scores were for access/availability/convenience (34.60/60), technical quality (33.17/50), and economic elements (23.83/40).  Conclusion Despite participants’ overall satisfaction scores being positive, some service domains require improvement to improve satisfaction, specifically the access/availability/convenience, technical quality, and economic elements. These areas should be prioritized by service providers and managers to improve patients’ experiences and clinical outcomes.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript titled ""Parents’ satisfaction with physiotherapy services for neuropediatric outpatients in government and private hospitals in the United Arab Emirates: a cross-sectional study"" presents a valuable exploration of parental satisfaction with physiotherapy services for children with neurological disabilities in the UAE. The study design, which utilizes a cross-sectional correlational approach, is appropriate for the research objectives and provides a comprehensive overview of the satisfaction levels among parents and caregivers.  The methods section is detailed and well-structured, clearly outlining the study design, participant recruitment, data collection, and analysis procedures. The choice of the Patient Satisfaction Questionnaire (PSQ-III) is justified and its reliability is well-documented, making it a suitable tool for this study. The ethical considerations are thoroughly addressed, ensuring the integrity and ethical soundness of the study. However, providing more details on the sampling process, including the selection criteria and any potential biases, would enhance the transparency and replicability of the methodology. The results are presented clearly and concisely, with comprehensive tables that effectively illustrate the key findings. The analysis is robust, and the interpretation of the data is logical and consistent with the study's objectives. The sociodemographic characteristics of the participants are well-documented, providing important context for understanding the results. The correlation analysis between demographic variables and satisfaction scores is particularly useful, highlighting the factors that influence parental satisfaction. Including more detailed subgroup analyses could provide additional insights into these factors. The discussion effectively interprets the results in the context of existing literature, highlighting both the strengths and areas needing improvement in the physiotherapy services. The identification of areas requiring improvement, such as access, technical quality, and economic elements, is particularly valuable for informing future service enhancements. The discussion could be further enriched by exploring potential strategies for addressing these areas and by discussing the implications of the findings for policy and practice in more detail. Additionally, a comparison with similar studies in other regions could provide a broader perspective on the findings and underscore the study's relevance in a global context. In conclusion, this study sheds light on the crucial aspect of parents' satisfaction with physiotherapy treatment for neuropediatric outpatients in the UAE. The findings underscore the overall positive satisfaction reported by parents and caregivers regarding various aspects of physiotherapy services, particularly in communication, interpersonal factors, and doctor-patient time. However, it is evident that there are areas in need of improvement, notably access, technical quality, and economic elements. These findings emphasize the importance of continuous assessment and enhancement of healthcare services to meet the evolving needs of patients and their families. Addressing the identified areas of concern is paramount to enhancing patient experiences and ultimately improving clinical outcomes. Therefore, it is imperative for service providers and managers to prioritize these domains in their efforts to optimize the quality of care provided to neuropediatric outpatients and ensure the delivery of patient-centered healthcare in the UAE. Suggestions for Improvement: The abstract can be reorganized to suit the title of the study by giving importance to parents whose children receive long term rehabilitation services. The introduction can emphasize more on how caregiving is difficult in neuropediatric population rather than giving too much importance to general aspects of patient satisfaction Provide more details on the sampling process and potential biases in the methods section. Include more detailed subgroup analyses in the results section to provide additional insights into factors influencing satisfaction. The results section can highlight parents' or caregivers' characteristics and then compare it with the patient satisfaction scores. Explore potential strategies for improving areas of low satisfaction in the discussion. Compare findings with similar studies in other regions to provide a broader context. Include specific recommendations for future research and practice in the conclusion. Recommendation: Approve for indexing with minor revisions.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",763,0,1,0.7771,0.1780205905,0.9188451767,8,7.66,0.0999,f1000,0.011111111111111,1,4,4,1,unfactual,3,1,30,polite,1,neutral,4,extreme,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,4.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,5,5,5,5,factual,5,5,95,polite,5,positive,4,low,4,4,4,4,factual,4,5,88,polite,5,positive,4,low
44,Slobodan-M-Janković,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors made an observational study trying to correlate MTX PG levels with disease activity of RA (as measured by a clinical score). The topic is of general interest, and the study brings results with practical significance. The manuscript is well written, and merits acceptance for publication. However, there are a few issues that should be corrected: In the Methods section the authors should state precisely how they measures the MTX PG levels in erythrocytes. As it is written now, it is not clear whether the MTX PG levels were measured in erythrocytes or in full blood.  Number of patients is small, so it is critical that statistical methods were used properly. The authors should state whether assumptions of multivariate logistic regression were met. Also, what was the categorical outcome used as dependent variable of the regression? Finally, quality of the regression model should be stated (Hosmer Lemeshow test, Cox and Snellen...).  Something should be said about adherence of the patients to the therapy. Was there any method used to check for adherence? If not, mention this in the Limitation paragraph.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",324,0,1,0.7684000000000001,0.1401785714,0.8017649651000001,6,37.2,0.0999,f1000,0.0103092783505154,2,3,2,3,partially factual,4,4,40,polite,3,positive,4,low,5,5,4,5,partially factual,5,5,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,5,5,4,5,factual,4,4,92,polite,5,positive,5,low
44,Andri-Frediansyah,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The researchers looked at 34 people with rheumatoid arthritis (RA) to see if there was a link between MTX-PG levels and how active their RA was. There were two women and 32 men in the study. The subject matter is of general interest, and the study yields useful information. There are, however, a few issues that should be addressed: 1) Please specify the date, duration, and months of the experiment. 2) Please verify the following statement: ""low disease activity, <3.2–5.1"". Is this correct? 3)The methods section is unclear. Please describe it in detail. Is there a particular type of blood (whole blood, red, or white blood cells) that you used in the study? Additionally, please provide detailed information about the centrifugation parameters, such as time, temperature, and g-force/RCF (g). Prior to analysis, is the blood subjected to any special treatment? 4) Please rewrite the section on chromatography measurement and analysis in detail. Include the HPLC specification and brand; column details (including particle size, pore size, inner diameter, and length); ammonium hydrochloride concentration and pH; solvent B composition (or A, if any); and the reference you cited. 5) Did you combine ammonium bicarbonate and ammonium chloride, and if so, in what proportion? Which detector (UV/CAD/MS) did you use? If UV/DAD, at what wavelength did you adjust the detector? 6) Please specify the brand of the MTX-PG3 standard and the R2 (nmol) value of the standard you used.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? I cannot comment. A qualified statistician is required.  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",388,0,1,0.7661,0.1242921493,0.7883067727,8,39.43,0.5077,f1000,0.021505376344086,4,4,3,4,factual,4,4,70,polite,4,neutral,4,low,5,4,4,5,partially factual,4,5,75,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,neutral,4.0,none,5,4,4,5,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
44,Talha-Bin-Emran,Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study,"Background: Rheumatoid arthritis (RA) is one of the most common autoimmune diseases, characterized by systemic inflammation, joint destruction and disability. Methotrexate (MTX) is used as the primary treatment for RA patients. However, the response to MTX therapy is highly varied and difficult to predict. This study sought to determine the role of MTX by measuring the MTX polyglutamate 3 (MTX-PG3) levels and the disease activity score 28 based on C-reactive protein (DAS28-CRP) of RA patients. Method: A prospective cohort study was conducted at the Rheumatology Polyclinic of Dr. Cipto Mangunkusumo General Hospital. Thirty-four patients with RA were included and followed up to 12 weeks. The RA patients were treated with MTX 10 mg per week and an increased dose of 5 mg per week every month. DAS28-CRP and MTX-PG3 level were assessed at week 8 and 12. Multivariate logistic regression analysis was used to determine the correlation between MTX-PG3 and DAS28-CRP. Result: A total of 34 RA patients were followed and the MTX was well tolerated in which no increase of serum glutamic oxaloacetic transaminase (SGOT), serum glutamic pyruvic transaminase (SGPT) and glomerular filtration rate (GFR) were observed. The mean scores of DAS28-CRP decreased following the MTX-treatment: 3.93, 3.22 and 2.82 at week 0, 8 and 12, respectively. In contrast, the median concentration of MTX-PG3 increased from week 8 to week 12 followed by increasing the dose of MTX. Our analysis suggested there was a moderate positive correlation between MTX-PG3 levels and DAS28-CRP score at week 8 and week 12 post-MTX treatment. Conclusion: The level of MTX-PG3 is correlated with DAS28-CRP score suggesting that MTX-PG3 could be used as an indicator to assess the disease activity in RA patients. Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Title: Correlation between serum methotrexate-polyglutamate 3 (MTX-PG3) level and disease activity in rheumatoid arthritis patients: A prospective cohort study Minor comments: Although the article has scientific rigor, several minor flows need to be improved before publication: 1. The abstract section is unsuitable—no focus point in the abstract section. 2. ""Nevertheless, a prospective study with a higher number of patients is needed to confirm this finding."" Is this necessary? 3. Authors are suggested to use the full form when used for the first time throughout the manuscript. 4. The aim of the study should be written as the last paragraph of the introduction. 7. MTX treatment and follow-up: How was this selected? 8. Receiver Operating Characteristics (ROC) analysis: Please describe in further detail. 9. ""Further analysis using the ROC curve showed that MTX-PG3 level…"" needs more insights with relevant references. 10. Presentation of figures is good. 11. Figure legends are appropriate and self-explanatory. 12. The conclusion needs to address future perspectives. 13. Spacing, punctuation marks, grammar, and spelling errors should be reviewed thoroughly.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",317,0,11,0.7843,0.1904411765,0.8207837343000001,15,29.96,0.1939,f1000,0.0121951219512195,2,3,2,3,partially factual,4,3,40,polite,2,positive,2,low,5,4,5,5,factual,5,5,88,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,4,5,factual,4,4,85,polite,5,neutral,5,low
146,Reviewer-LTFo,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","This paper presents Prometheus, an open-source language model that provides fine-grained evaluation capabilities comparable to GPT-4. The authors aim to overcome the challenges of using GPT-4 as an evaluator, such as its closed-source nature, uncontrolled versioning, and high cost. Prometheus is trained on a new dataset, the Feedback Collection, which includes a wide range of user-based evaluation criteria. The model shows strong correlation with GPT-4 evaluation on seven benchmarks and outperforms ChatGPT in human evaluation. Remarkably, Prometheus demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. 1. Prometheus can assess responses based on novel and unseen score rubrics and reference materials provided by the user. This flexibility makes it applicable to a variety of real-world criteria.
2. Prometheus can be freely used and further enhanced by the academic community, facilitating transparency and reproducibility.
3. Prometheus shows remarkable performance in comparison with GPT-4 in terms of evaluation capabilities and the quality of generated feedback.
4. The creation of the Feedback Collection, a dataset designed specifically for the task of teaching fine-grained evaluation to language models, is a significant contribution. 1. One of my concerns about this work is whether can Prometheus be generalized to other fields since the downstream benchmarks are close the the training data. More results on unseen data and more specific domains can better improve this work. 

2. Potential bias of Prometheus. Can Prometheus be attacked by some adversarial attack methods? Does it have stronger biases like length bias compared with GPT-4?

3. Dependency on GPT-4 Feedback: The training of Prometheus relies heavily on feedback generated by GPT-4. The model's ability to generalize beyond the feedback patterns of GPT-4 is unclear. See weaknesses.",288,0,8,0.7815000000000001,0.2704617605,0.954846859,48,25.7747,0.1213,iclr,0.0,2,4,3,3,factual,3,3,55,polite,3,positive,3,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,4.0,4.0,4.0,4.0,factual,4.0,5.0,85.0,polite,5.0,positive,3.0,none,3,5,4,3,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
146,Reviewer-B7Vr,Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models,"Recently, GPT-4 has become the de facto evaluator for long-form text generated by large language models (LLMs). However, for practitioners and researchers with large and custom evaluation tasks, GPT-4 is unreliable due to its closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose PROMETHEUS a fully open-source LLM that is on par with GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. For this purpose, we construct a new dataset – FEEDBACK COLLECTION – that consists of 1K fine-grained score rubrics, 20K instructions, and 100K natural language feedback generated by GPT-4. Using the FEEDBACK COLLECTION, we train PROMETHEUS, a 13B evaluation-specific LLM that can assess any given response based on novel and unseen score rubrics and reference materials provided by the user. Our dataset’s versatility and diversity make our model generalize to challenging real-world criteria, such as prioritizing conciseness, child-readability, or varying levels of formality. We show that PROMETHEUS shows a stronger correlation with GPT-4 evaluation compared to ChatGPT on seven evaluation benchmarks (Two Feedback Collection testsets, MT Bench, Vicuna Bench, Flask Eval, MT Bench Human Judgment, and HHH Alignment), showing the efficacy of our model and dataset design. During human evaluation with hand-crafted score rubrics, PROMETHEUS shows a Pearson correlation of 0.897 with human evaluators, which is on par with GPT-4-0613 (0.882), and greatly outperforms ChatGPT (0.392). Remarkably, when assessing the quality of the generated feedback, PROMETHEUS demonstrates a win rate of 58.62% when compared to GPT-4 evaluation and a win rate of 79.57% when compared to ChatGPT evaluation. Our findings suggests that by adding reference materials and training on GPT-4 feedback, we can obtain effective open-source evaluator LMs.","Paper presents a new benchmark for building evaluation systems with LLMs. Although the paper contribution is promising, there are some serious problems in the paper. Many of the figures are missing and unvisible. The paper contribution, whether this is a novel LLM, or a data set generated by gpt-4 is unclear. The model is advertised as open-source but how the data will be shared is unstated. If an LLM is built on this data, which is described as a 100K synthesized data set, how is it an 13B LM is unclear. Paper cannot be published in such state with so much missing information. Proposes open-source LLM for evaluation Model implementation is not described.
Experimental methodology not clear or supported.
Most figures missing.
Contribution too small (not any new data, model or any advertised contribution is clearly described).
Data is synthetic and not corrected by humans for any potential errors. Where is Figure 2?
Where is Figure 4?",157,0,1,0.7629,0.0292929293,0.7373477221,48,52.1175,0.0999,iclr,0.0404040404040404,3,4,2,3,partially factual,3,3,50,neutral,3,negative,3,moderate,2,3,4,2,partially factual,2,2,45,impolite,4,negative,4,low,1.0,4.0,2.0,2.0,unfactual,2.0,1.0,40.0,neutral,3.0,negative,3.0,moderate,2,3,3,2,partially factual,2,3,45,neutral,4,negative,3,moderate,2,4,3,3,partially factual,3,3,60,neutral,4,negative,4,low
110,Cliff-Ragsdale,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  There are a growing number of protocols for differentiating stem cells into particular neural cell types. This paper demonstrates the great potential of RNAseq technologies for assessing the identities of such differentiated cells in culture. The authors’ goal is an in vitro population of 'glutamatergic cortical neurons'. Although many of the genes catalogued show the anticipated profiles across the differentiation process (Otx2 abundance decreases with DIV while Kcnh5 reads increase), the dataset also demonstrates that this culture protocol may not be the best for 'glutamatergic cortical neuron' study as transcripts for the predominant cortical vesicular glutamate transporter gene, Vglut1/Slc17a7, are barely detected in the differentiated cell populations.",174,0,0,0.8164,0.1857843137,0.9288681746,33,2.31,0.0999,f1000,0.0098039215686274,2,4,3,3,partially factual,3,2,70,neutral,4,neutral,4,low,3,5,3,4,partially factual,4,4,75,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,2,4,3,2,factual,4,4,60,neutral,4,neutral,4,low,2,4,3,3,partially factual,3,4,75,polite,5,neutral,4,low
110,Joyce-van-de-Leemput,Longitudinal RNA sequencing of the deep transcriptome during neurogenesis of cortical glutamatergic neurons from murine ESCs,"Using paired-end RNA sequencing, we have quantified the deep transcriptional changes that occur during differentiation of murine embryonic stem cells into a highly enriched population of glutamatergic cortical neurons. These data provide a detailed and nuanced account of longitudinal changes in the transcriptome during neurogenesis and neuronal maturation, starting from mouse embryonic stem cells and progressing through neuroepithelial stem cell induction, radial glial cell formation, neurogenesis, neuronal maturation and cortical patterning. Understanding the transcriptional mechanisms underlying the differentiation of stem cells into mature, glutamatergic neurons of cortical identity has myriad applications, including the elucidation of mechanisms of cortical patterning; identification of neurogenic processes; modeling of disease states; detailing of the host cell response to neurotoxic stimuli; and determination of potential therapeutic targets. In future work we anticipate correlating changes in longitudinal gene expression to other cell parameters, including neuronal function as well as characterizations of the proteome and metabolome. In this data article, we describe the methods used to produce the data and present the raw sequence read data in FASTQ files, sequencing run statistics and a summary flatfile of raw counts for 22,164 genes across 31 samples, representing 3-5 biological replicates at each timepoint. We propose that this data will be a valuable contribution to diverse research efforts in bioinformatics, stem cell research and developmental neuroscience studies.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The depth and temporal nature of the dataset presented in this paper will be beneficial to any researcher interested in cortical development in general, and potentially lead to many new insights and avenues to pursue. A point of note, in my experience differences in passage number of the cells used for differentiation can affect gene expression levels throughout. The authors state “ESCs were differentiated into neurons between 5-30 passages after adaptation to suspension culture.”, I wonder if that is why the DIV21 samples cluster in between the DIV16 and DIV28 when performing a PCA analysis on the transcript read counts (obtained from Data File 2)? Related question, how raw are the transcript read counts in Data File 2, as I thought raw counts would have to be integers whereas the counts given have decimal points? Finally, with regard to the previous Ref Report (Ragsdale and Albertin; 12 March 2013), have you considered comparative analysis using the Allen Brain Atlas/ Mouse Brain expression data for the thalamic and cortical areas and see which region your samples resemble most?",244,0,0,0.8079000000000001,0.069039294,0.8534755111000001,96,19.13,0.1969,f1000,0.0196078431372549,3,3,3,4,partially factual,3,4,70,neutral,4,neutral,4,moderate,4,4,4,4,partially factual,5,5,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,3,3,factual,4,4,70,polite,4,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
131,Reviewer-fLMf,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","The paper discusses recent advancements in differentially private (DP) deep learning, focusing on large vision and language models with millions to billions of parameters. The authors find that different group-wise clipping styles offer an accuracy-memory trade-off. While all-layer clipping is commonly used and provides better accuracy, it requires more memory compared to group-wise clipping. The paper formalizes this trade-off through convergence theory and complexity analysis. Importantly, it demonstrates that the accuracy gap between group-wise and all-layer clipping decreases with larger models, while the memory advantage of group-wise clipping remains, allowing DP optimization of large models with high accuracy and low peak memory usage. The paper addresses an important aspect of DP deep learning, namely gradient clipping, which is crucial for privacy-preserving training of large models. It thoroughly explored the design space of group-wise clipping styles for various learning tasks.

Empirical Experiments: The paper includes a good set of experiments to support its claims. **Motivation of Group-Wise Clipping**: In the abstract, the paper claims The paper lacks a clear and strong motivation for why group-wise clipping is a necessary or valuable alternative to all-layer clipping as **all group-wise clipping enjoy almost the same training speed as the standard non-DP optimization**. Meanwhile the memory cost does not differentiate too much across various grouping choices, either (see Table 3 and Figure 5).

**Confusing measures**: There are several terms used across the paper, e.g., time complexity, training speed, memory cost. The paper should define them clearly whether they are theoretically or empirically computed. If empirically, the training speed and the memory cost are jointly affected by the setup of the batch size, model size and the model architecture. Book-keeping technique would store the backward gradients on the output of each operation, the same as storing the activations, which may have memory problem when the batch size is large. 

As a following weak point, the paper does not talk about the implementation detail and wall-clock training speed comparison.  This is because the non-uniform grouping is complex to implement and the wall-clock training speed is the ultimate measure for different choices.
The cost of searching the best non-uniform grouping is not counted.

**Relevance of Theory**: The theoretical analysis may not provide sufficient insights into practical scenarios. The upper bound gets sub-linearly (sqrt) worse as the number of the groups increases, which is not reflected in real experiments. Theorem 2 is a bit trivial and does not convey much information related with the target of the paper. 


**Experiments presentation**: The experiments are cherry picked in the main text. It seems that the results of the paper are not as good as the result of He et al. 2022 in Appendix C, which are excluded from the main text. Moreover, all the experiments consider the fine-tuning setting, which is not clearly stated in the main text. There lack training scratch experiments for full comparison. Questions about the experiment results.  In Table 3, the memory cost increases as you increases the number of groups for QNLI RoBERTa-base. This contradicts with theory analysis and all other experiments. Can the authors explain why this happens?",514,0,0,0.7683,0.1138977072,0.9602714181,49,39.831,0.1647,iclr,0.0,3,4,4,3,factual,4,4,70,polite,4,neutral,4,low,4,4,4,4,5,5,5,85,neutral,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,neutral,5,neutral,4,low,3,4,4,4,partially factual,4,3,75,neutral,5,negative,5,low
131,Reviewer-yntr,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies group-wise clipping for optimization under differential privacy. The issues discussed in this article regarding optimization under DP are timely and critical. The performance loss caused by DP necessitates urgent solutions for these problems. The paper lacks novelty as the proposed clipping method is an extension of the existing Book-Keeping
technique Bu et al. (2022c). Furthermore, the convergence analysis relies on smoothness assumptions.

I also disagree with the authors' perspective that ""Differentially private (DP) optimization of deep learning models has enjoyed amazing accuracy and rigorous guarantees against privacy risks."" From my knowledge, accuracy loss remains a significant obstacle, which is also the problem this paper aims to address. Are there any hyperparameters that need to be tuned for the proposed clipping methods? If so, do these adjustments come at an additional privacy cost? Has the paper reported these associated costs?",142,1,1,0.8899,0.2458333333,0.8636165857,49,31.5628,0.1858,iclr,0.0,3,4,1,3,partially factual,3,3,50,neutral,4,negative,2,moderate,4,4,3,4,partially factual,4,3,70,impolite,5,negative,4,low,1.0,4.0,4.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,3.0,low,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,moderate,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,low
131,Reviewer-nK9w,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","This paper studies the group-wise clipping approach in DP, and gives analysis on its convergence and its algorithmic relation to back-propagation. The authors also analyze the system wise metrics such as peak memory profile usage. Empirical results are given on GPT2 and ViT models. * The paper provides detailed analysis to the group-wise clipping technique in DP domain, some of the conclusions are interesting to this field.
* The authors give both insights from theory and system perspectives.
* The authors also set up new baseline results, which could potentially be a good reference for further work in this space. * From the peak memory profile results, i.e. Table 3 and Figure 5, it looks like the peak memory usages for different boundaries are pretty close (in general less than 2 GB). I'm not sure how much this can lead to faster training and larger batch sizes. For example, what is the new batch size that can be used, and how much speed up we gain? Some real-world numbers here could be beneficial.
* From Theorem 1, it looks like the AUTO algorithm obtains the same convergence speed compared to the standard SGD. However, the standard SGD does not require per-sample gradient to be symmetric about the oracle gradient as shown in Assumption 4.3. I wonder if this is critical for AUTO to get on-par convergence speed to SGD? What will the convergence rate be like without such assumption?
* In the paper, the authors object to the conclusion of https://arxiv.org/pdf/2212.01539.pdf with a self-designed group-wise clipping algorithm for faster training speed. However, I don't see too much evidence supporting this. Could you show a convergence curve? Please refer to the weaknesses section.",282,1,0,0.8236,0.1419191919,0.8059782982,49,57.6519,0.6015,iclr,0.0202020202020202,4,4,4,4,factual,4,4,85,polite,4,neutral,5,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,78,polite,5,neutral,4,low
131,Reviewer-RBj1,On the efficacy of group-wise clipping in differentially private optimization,"Recent advances have substantially improved the accuracy, memory cost, and training speed of differentially private (DP) deep learning, especially on large vision and language models with millions to billions of parameters. In this work, we thoroughly study the per-sample gradient clipping style, a key component in DP optimization. We show that different clipping styles have the same time complexity but instantiate an accuracy-memory trade-off: while the all-layer clipping (of coarse granularity) is the most prevalent and usually gives the best accuracy, it incurs heavier memory cost compared to other group-wise clipping, such as the layer-wise clipping (of finer granularity). We formalize this trade-off through our convergence theory and complexity analysis. Importantly, we demonstrate that the accuracy gap between group-wise clipping and all-layer clipping becomes smaller for larger models, while the memory advantage of the group-wise clipping remains. Consequently, the group-wise clipping allows DP optimization of large models to achieve high accuracy and low peak memory simultaneously.","Recent advances in differentially private deep learning have improved accuracy, memory efficiency, and training speed for large models. This paper focuses on per-sample gradient clipping methods in DP optimization. It finds that different clipping styles have similar time complexity but trade off accuracy and memory usage. All-layer clipping offers better accuracy but requires more memory than group-wise clipping. As models grow larger, the accuracy gap narrows, while the memory advantage of group-wise clipping remains, making it suitable for efficient DP optimization of large models. + It's an interesting paper that leverages memory-accuracy tradeoff of group-wise dp optimization with different granularity. 
+ The key observation about dS doesn't depend on dW so that the computational time doesn't depend on m provides great ml-sys type of insights. 
+ The ViT experiments on Cifar100 is convincing. - The presentation needs some work. The paper contains multiple contributions and a lot prior work / settings, which was clear in the introduction, but very confusing in later sections. For example, I was very confused about the equal time efficiency part because authors wrote this contribution directly so I thought that was the previous design. Specifically, if this is the contribution, I would sign-post it at the beginning of section 3 what are the conventional wisdom and why a simple analysis on computational dependency graph (you don't need dW to derive dS) would do the work. It requires many passes of reading and reasoning to get the point. 
 - The presentation of experiment section is poor. Also ImageNet is mentioned at the beginning but the experiments don't have it? In addition, cifar10/100 (better imagenet) are convincing Image baselines, but why using E2E dataset in the last experiment 1) it is not popular for decoder only model 2) you didn't benchmark the peak memory for gpt. Also I understand you benchmarked peak memory before, but table 5 and 6 better have acc and peak mem side by side. I'm curious in authors' view, is this 1-2 GB memory difference significant? Or in another word, is this an important tradeoff worth studying to begin with?",347,0,0,0.7952,0.1283511905,0.9418504238,49,36.785,0.0866,iclr,0.0,4,3,4,3,factual,3,3,70,polite,4,positive,4,low,4,4,4,5,factual,4,4,80,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,3,4,3,factual,4,4,75,polite,5,positive,5,low,3,3,3,4,partially factual,3,4,75,polite,4,neutral,4,low
129,Reviewer-vTRX,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","This paper explores the efficiency of training parameterized quantum models, from the perspective of backpropagation scaling. By leveraging some recent developments in shadow tomography and accessing multiple copies of a quantum state, the authors propose an algorithm that matches backpropagation scaling in quantum resources and reduces additional classical computational costs. The results provide valuable insights into the reusability of quantum information and the results are potentially meaningful for the future of quantum machine learning. - The paper investigates a timely and relevant topic in quantum machine learning, comparing the efficiency of training parameterized quantum models to classical neural networks.
- The authors leverage recent developments in shadow tomography, providing a novel approach to study a meaningful problem on quantum neural networks.
- The proposed algorithm matches backpropagation scaling in quantum resources and reduces classical auxiliary computational costs.
- The angle of this work to study quantum neural networks is novel. - The primary analysis is limited to quantum neural networks based on variational quantum circuits, which restricts the scope of the paper as many other types of quantum neural networks exist.
- The application of the results to general quantum machine learning algorithms is not convincingly demonstrated.
- The paper lacks a clear and well-motivated example demonstrating the application of the proposed methods, making it difficult to assess its practical implications and usefulness. - Could the authors provide more insights into the practical implications of the results and its potential applications?
- How do the results of this work extend to other quantum neural networks?
- There are a certain number of existing works on the gradients of quantum neural networks. How does Proposition 7 advance the known works?
- What is the relationship between this work and the problem of the barren plateau? NA.",295,0,1,0.7624000000000001,0.0951298701,0.9247617126,216,22.886,0.068,neurips,0.010204081632653,3,3,3,3,factual,4,4,65,neutral,4,neutral,4,moderate,4,4,4,4,factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,3,partially factual,3,4,75,polite,4,neutral,4,low
129,Reviewer-mok7,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The paper studies the scaling of computing the gradient of a quantum neural network. While in the classical case we can use backpropagation, which gives the same linear scaling for computing the gradient and the forward pass, in the quantum case, we would naively have to run a circuit for each component of the gradient, leading to a squared complexity in the number of parameters, which prevents studying quantum models with large number $M$ of parameters.
The authors formulate this problem in the language of shadow tomography and apply ideas from that field to the problem at hand.
This shows that while an $M\log M$ scaling is possible using polylog copies of the input state. This comes at a drawback of classical cost that scales as $2^n$ with $n$ the number of qubits. Resolving this exponential scaling would resolve some open problems in shadow tomography. - Relevant problem in quantum ML.
- Connection with shadow tomography and application to scaling of gradient computation is new and can lead to new ways to think about the problem
- Rigorous statements supporting scaling 
- Well written paper - The paper relies on quantum information concepts that are not necessarily familiar with the ML audience at the conference.
- When talking about memory requirements of backprop in classical neural networks, one needs to store activations for reverse mode autodiff. This leads to memory that scales with the number of layers, while in the quantum case by analogy the number of qubits does not scale with the number of layers. The authors could comment about this.
- I was confused by Prop. 3: is the proof considering the case of a number of parameters $4^n$? I am not sure what we learn from this example since it does not seem to be part of the quantum neural networks we would like to train.
- The classical scaling as $2^n$ required for the proposed algorithm restricts a lot the class of problem for which this protocol can be useful. 
 - Can you add a related work section to highlight the novelty with respect to previous work?
- Can you explain what is rotate/threshold check in figure 1? Can you add more intuition around proposition 7 to see how gentle measurements are used, e.g. what is alpha in this case? What is the role of $\sigma$? 
- Can you comment on what problems could benefit from the proposed protocol, namely small $n$ and large $M$?
- Can you explain why approximations to $\sigma$ using for example tensor networks could be more robust than just simulating the quantum neural network with tensor network? - As the authors say several time, the main limitation is that their algorithm come with a classical exponential scaling that limits its applicability. ",460,0,0,0.7696000000000001,0.0542147667,0.8965290785000001,216,52.7329,0.3398,neurips,0.0096153846153845,4,4,4,4,factual,3,4,85,neutral,4,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,neutral,4,moderate,2.0,4.0,4.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
129,Reviewer-Hoi6,"On quantum backpropagation, information reuse, and cheating measurement collapse","The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters -- which can now be in the trillions.  Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion.  Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state.  With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.","The authors go over the backpropagation scheme for both classical and quantum machine learning methods. They also propose a novel quantum backpropagation algorithm based on quantum shadow tomography to reuse information and reduce the time complexity.  1. This paper provides a link between quantum backpropagation and quantum shadow tomography, which are both important in quantum computing.
2. This paper provides a thorough background check on quantum backpropagation, information reuse scheme in QST, and backpropagation scaling problem.
3. This paper is technically sound. 1. This paper is more like a report paper than a research paper to me, since the main contribution is to discuss in detail how reusing information can benefit quantum backpropagation, and the proposed algorithm seems quite trivial. 1. Whether this level of space complexity on the classical device is acceptable?
2. Could you be more explicit about and also highlight the potential impact of this paper on the quantum machine learning society?
 The major limitation is whether this paper fits the scope of the research paper in NeurIPS. ",171,0,6,0.7516,0.225462963,0.9025430679,216,34.1816,0.3617,neurips,0.0,3,3,3,3,partially factual,3,3,50,neutral,3,neutral,3,low,3,3,3,3,partially factual,4,4,55,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,2,3,3,2,partially factual,3,3,60,neutral,4,neutral,4,moderate,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
37,Barend-L.-van-Drooge,Comparison of the oxidative potential of primary (POA) and secondary (SOA) organic aerosols derived from α-pinene and gasoline engine exhaust precursors,"Background: Primary (POA) and secondary (SOA) organic aerosols, deriving from both anthropogenic and biogenic sources, represent a major fraction of ambient particulate matter (PM) and play an important role in the etiology of respiratory and cardiovascular diseases, largely through systemic inflammation and cellular oxidative stress. The relative contributions of these species to the inhalation burden, however, are rather poorly characterized. In this study, we measured the in vitro oxidative stress response of alveolar macrophages exposed to primary and secondary PM derived from both anthropogenic and biogenic sources. Methods: POA and SOA were generated within an oxidation flow reactor (OFR) fed by pure, aerosolized α-pinene or gasoline engine exhaust, as representative emissions of biogenic and anthropogenic sources, respectively. The OFR utilized an ultraviolet (UV) lamp to achieve an equivalent atmospheric aging process of several days. Results: Anthropogenic SOA produced the greatest oxidative response (1900 ± 255 µg-Zymosan/mg-PM), followed by biogenic (α-pinene) SOA (1321 ± 542 µg-Zymosan/mg-PM), while anthropogenic POA produced the smallest response (51.4 ± 64.3 µg-Zymosan/mg-PM). Conclusions: These findings emphasize the importance of monitoring and controlling anthropogenic emissions in the urban atmosphere, while also taking into consideration spatial and seasonal differences in SOA composition. Local concentrations of biogenic and anthropogenic species contributing to the oxidative potential of ambient PM may vary widely, depending on the given region and time of year, due to factors such as surrounding vegetation, proximity to urban areas, and hours of daylight.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The work of Lovett et al. presents interesting data on the possible inflammatory effects of SOA from traffic as well as biogenic (pinene) origin. The method setup is well designed, although the variables, such as conditions of relative humidity and temperature, could have been studied in different values. On the other hand, the results show that the biogenic SOA have similar high inflammatory effect in the test compared to traffic SOA, which is an important fact, and indicates that effects have been observed in real-world data. However, the study could have been more complete and higher quality if the researchers would have made an effort to detect and quantify the molecular chemical compounds that are present in the SOA fractions.  The work is suitable for indexing.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",269,0,1,0.7683,0.1806060606,0.8230857849000001,120,33.54,0.0999,f1000,0.0204081632653061,3,4,3,2,factual,3,2,40,polite,3,positive,4,moderate,4,4,4,4,factual,4,4,85,polite,5,positive,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,3,4,3,3,factual,4,4,75,polite,5,positive,4,low,3,4,3,4,factual,4,4,85,polite,5,positive,4,low
35,Reviewer-kmpt,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","This paper explores adding long-range modulatory feedback connections to deep CNNs (specifically AlexNet, evaluated on ImageNet). It explores 2 ways of incorporating the feedback: a default mode and a cognitive steering mode. The results show improvements on ImageNet accuracy, adversarial robustness, and a composite image recognition task. - The paper is high-quality: it's well-written, clear, the visualizations seem to have been very thoughtfully prepared, etc. The motivation for the modulatory connections is well-explained, and the empirical results (ImageNet accuracy, robustness, and cognitive steering effects) are compelling.
- The authors anticipated most of my questions and responded to them in the body of the paper. For example, Section 2.2 has a great description of why certain decisions were made and which options were explored. 
- The experiments are explained clearly, and the visualizations really enhance the presentation.
- I think this is a very significant question (how to incorporate long-range feedback into deep neural networks), which has been studied for many years but hasn't quite become mainstream yet. I applaud the authors for thoughtfully probing this question and taking a step toward bringing long-range feedback connections into modern neural networks, which I expect will be a quite impactful addition to NNs when it finally lands. - In Section 2.1, I would have liked to see either mathematical equations or pseudocode to remove any ambiguity regarding the implementation of the feedback connections. The descriptions are decent, but I'm having to guess what the exact computations are. The implementation is the essence of the paper, in some sense. It would be nice to make this very precise in the body of the paper.
- It would be good to have a thorough discussion of the costs associated with incorporating these connections (time, memory, etc.). Right now, the paper kind of reads like there's no reason *not* to use them, which probably isn't entirely fair. What am I losing if I incorporate these connections?
- Although the Related Research section is nice, I would like to better understand which 1-3 works are most closely related to this one, with a more detailed description of how this implementation differs from these 1-3 most closely related prior works. - What is the precise definition of ""modulatory"" used here? It seems like one could argue that any feedback connections are ultimately modulatory. What's the exact definition you're using such that other types of feedback connections *aren't* considered modulatory?
- This isn't essential, but I'm curious (and I suspect some readers might be) -- is there something more biologically plausible about this version of feedback connections than some of the prior work?
- How important is the fact that the steering is global vs. local? It might be worth discussing this more.
- In Figure 3, what do you mean by ""full branch""?
- In Section 3.2, what label is used?
- At the end of Section 3.1, could you further spell out why relevant features are amplified and irrelevant features are suppressed? It might be helpful to connect this more precisely to the mathematics/implementation, if added to Section 2.1 (as discussed in Weaknesses).
- nit: typo in the second paragraph of Section 3.2 (where --> were)
- In Section 3.3, you're using ""target"" as the label whereas it's previously used differently in the ""source/target"" description, right? If so, this dual use might not be optimal. Could you find a way to use 2 different words?
- Is there possibly a better phrase than ""target absent category""? It took me a little while to parse this. Do you have any ideas to help clear this up? The paper includes a nice description of the limitations of this work, including limited exploration of different architectures and a lack of mechanistic analysis into why the long-range connections help.",628,0,0,0.7887000000000001,0.237199793,0.9079948664,216,47.996,0.2814,neurips,0.0120481927710843,4,4,5,4,factual,4,4,95,polite,4,positive,4,none,5,4,4,5,partially factual,5,5,88,polite,5,positive,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,90.0,polite,5.0,positive,4.0,none,4,5,5,5,factual,5,4,90,polite,5,positive,5,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
35,Reviewer-sqng,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","This work introduces a recurrent modulation module that can be added to visual models so that the top layer can project to and influence the earlier layers. The authors find that the model with the feedback projection layers outperforms the baseline feedforward model in both the categorization performance and adversarial robustness. The model is further analyzed to test whether the feedback modulation can be controlled to meaningfully steer the representations. The authors find that the top-layer steering yields significant performance increase when mixed targets are presenting in the same image in a side-to-side or overlaying fashion. The paper is well-written and easy to read. The significant improvement of the feedback-augmented model compared to the baseline model is also interesting. The steering analysis is done on both side-to-side and overlaying settings. The idea of adding feedback modulation from top layers to earlier layers is not new. The authors should more clearly discuss the difference between their work and other models.

The performance and robustness evaluation also needs to be more careful. The feedback connection adds more computation and trainable parameters. But the performance is still compared to the baseline model with less computation and trainable parameters. The authors should compare their model to a larger or deeper architecture.

Although the steering analysis shows that the model can reach higher performance, this analysis is kind of circular as the target signal is explicitly used to generate the modulation signal, which makes the performance improvement unsurprising. This steering property of the new model has its potential, as the proposed visual model is now available to be tested in attention experiments just as how humans can be asked to attend to different parts or features in their input. But more tests and experiments to compare models to humans are needed to illustrate this potential. See weakness. Yes.",303,0,2,0.7442000000000001,0.2003176931,0.8941668272000001,216,36.8412,0.0513,neurips,0.031578947368421,2,4,4,2,partially factual,4,3,65,polite,4,neutral,4,moderate,4,5,4,4,factual,4,4,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,5,4,3,factual,4,4,80,polite,5,neutral,4,low,4,5,3,4,partially factual,3,3,78,polite,4,neutral,4,low
35,Reviewer-esAd,Cognitive Steering in Deep Neural Networks via Long-Range Modulatory Feedback Connections,"Given the rich visual information available in each glance, humans can internally direct their visual attention to enhance goal-relevant information---a capacity often absent in standard vision models.  Here we introduce cognitively and biologically-inspired long-range modulatory pathways to enable `cognitive steering’ in vision models.  First, we show that models equipped with these feedback pathways naturally show improved image recognition, adversarial robustness, and increased brain alignment, relative to baseline models. Further,  these feedback projections from the final layer of the vision backbone provide a meaningful steering interface, where goals can be specified as vectors in the output space.  We show that there are effective ways to steer the model that dramatically improve recognition of categories in composite images of multiple categories, succeeding where baseline feed-forward models without flexible steering fail. And, our multiplicative modulatory motif prevents rampant hallucination of the top-down goal category, dissociating what the model is looking for, from what it is looking at. Thus, these long-range modulatory pathways enable new behavioral capacities for goal-directed visual encoding, offering a flexible communication interface between cognitive and visual systems.","Authors study ways of incorporating cognitive steering in vision neural network models. They add a top-down feedback mechanism to Alexnet with which they report improved performance. Further, they test other steering mechanisms, including prototypes, language-based signals etc. These tests are over image composite tasks where the approaches show greatly improved performance.  The paper is interesting, the experiments and the controls are convincing. Cognitive steering in deep CNNs is novel as far as I know, especially with language signals. Some parts of the paper are well written. 

 * The paper lacks benchmarking. There are several methods of incorporating feedback in deep learning models from previous years that weren't tested. Look at \[1\] for a survey. Although I am sympathetic about this since cognitive steering in itself is interesting but the paper needs to be clear that the contributions are in studying various steering methods/signals and not introducing feedback itself. 

* Side-by-side composition is not a straightforward task setting - putting images side by side reduces the scale of the objects and since CNNs are not scale invariant that poses a challenge. So I am not quite convinced it is a good test for steering.  



\[1\] Kreiman, G., & Serre, T. (2020). Beyond the feedforward sweep: feedback computations in the visual cortex. Annals of the New York Academy of Sciences, 1464(1), 222–241. https://doi.org/10.1111/nyas.14320 Improvements to text and minor corrections:
* Please make it clear in the text what ""target absent"" control means. Only place it is explained is the Figure 5 caption. I had to spend a lot of time trying to understand that until I stumbled on Fig 5 caption. 
* Please consider updating Figure 3 to say ""target unit"" or ""target neuron"" or ""target logit"" instead of ""target"" 
* Line 169: where &rarr; were

Questions:
* Why does LRM models have higher accuracy than alexnet at 0th modulatory pass? They should be same?
* In the ""target absent"" control - how is the absent target chosen? Is it average of every other (998 other target classes not present in the composite or some random class?).
* How many modulatory passes were they trained for and tested for? Is it the same number of passes in training and testing? NA",369,4,2,0.8012,0.10507087,0.9063189626,216,55.7114,0.1517,neurips,0.0,4,4,5,5,factual,4,4,100,polite,4,positive,5,none,4,4,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
80,Reviewer-hEF2,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","In this paper, the author(s) propose a federated learning benchmark dedicated to artificial intelligence of things. In particular, the benchmark includes eight extant datasets collected from IoT devices and applications. The proposed benchmark also contains an end-to-end framework, which consists of five main modules: non-IID data partitioning, data preprocessing, IoT-friendly models, FL hyperparameters, and IoT-factor emulator. Importance of contribution: The solution is proposed to resolve the lack of a proper benchmark for IoT-specific federated learning. The author(s) validate the feasibility of this benchmark.

Soundness: The author(s) explain the benchmark in detail, and conduct evaluation on the different modules in the framework. 

Quality of presentation: The paper is well-organized, and the language is technical yet understandable for readers with domain knowledge.

Comparison with related works: The author(s) introduce extant studies on federated learning benchmarks for computer vision, natural language processing, medical imaging, etc., and clarify the research gap between this study and related work. The methodology can be elaborated for better clarity of the overall research step. - Figure 1 is not explicitly referred to in the manuscript.
- The author(s) can consider elaborating the methodology of how to collect and choose the datasets. What are the metrics to select and finalise the eight datasets?
- The author(s) can specify the definition of small, medium and large datasets.
- A proof-reading is needed as there are some typos. For instance, Section 3.1: “… FedAIoT.These datasets …”, a space is needed.",239,0,0,0.7615000000000001,0.0212585034,0.9212706089,49,29.1699,0.068,iclr,0.0,1,4,2,1,partially factual,3,2,20,polite,3,positive,2,moderate,5,5,4,5,5,4,4,85,polite,5,positive,5,moderate,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,none,4,4,4,4,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
80,Reviewer-N2ym,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","Abstract Summary
The paper introduces FedAIoT, a novel federated learning framework tailored for IoT applications. The framework aims to address the unique challenges posed by IoT ecosystems, such as data privacy, limited computational resources, and network constraints.

Key Contributions
Novel Framework: The paper presents the architecture and design principles of FedAIoT, which incorporates distributed data storage and decentralized learning algorithms to enable IoT devices to participate in machine learning tasks without compromising data privacy.

Mathematical Formulation: The authors provide rigorous mathematical models to describe the federated learning process, focusing on optimization algorithms and convergence properties.

Experimental Validation: Through extensive experiments using real-world and synthetic datasets, the authors demonstrate that FedAIoT outperforms traditional centralized learning methods in terms of accuracy, privacy preservation, and computational efficiency.

Applicability: The framework is designed to be adaptable to various IoT applications, from smart homes to industrial automation.

Methodology
The paper employs a federated learning approach where IoT devices can train machine learning models locally on their own data and then share only the model parameters with a central server for global aggregation. This preserves the privacy of the data while allowing for a collective learning experience.

Results
The experiments show that FedAIoT achieves comparable or superior performance to centralized approaches while ensuring data privacy and reducing the computational load on the central server. The framework also exhibits robustness to non-IID data distributions and network delays.

Conclusion
The paper concludes by asserting that FedAIoT offers a scalable, efficient, and privacy-preserving solution for implementing machine learning in IoT networks. It also identifies avenues for future research, including optimization of communication overhead and integration with other emerging technologies like edge computing. Thus the paper makes a compelling case for the adoption of federated learning in IoT environments, providing both the theoretical foundation and practical validation for the proposed FedAIoT framework. Strengths Assessment of the Paper
Originality
The paper presents an innovative framework—FedAIoT—for federated learning in the context of Internet of Things (IoT) applications. The originality of the work lies in the seamless integration of federated learning techniques with IoT devices to achieve distributed, privacy-preserving learning. The novelty also arises from the unique problem formulation that caters specifically to the challenges posed by IoT environments, such as limited computational resources and data privacy issues.

Quality
The paper is of high quality in multiple aspects:

Methodological Rigor: The mathematical formulations and algorithms are soundly developed. The paper thoroughly validates the proposed framework through a series of experiments, complete with baseline comparisons and varied settings.

Data Quality: The choice of datasets and the justification for those choices are clear and appropriate for validating the model. The paper also employs robust statistical methods to analyze the results.

Citation and Contextualization: The paper provides an extensive literature review, situating its contributions aptly within existing work.
The significance of the paper is manifold:

Theoretical Contribution: The paper addresses a critical gap in federated learning by tailoring it to the specific needs of IoT applications, thus extending the theory of federated learning to a new domain.

Practical Impact: The FedAIoT framework has the potential to revolutionize how machine learning models are deployed in IoT networks, thereby having broad applicability and impact. Abstract and Introduction
The paper proposes a unified end-to-end Federated Learning (FL) framework for Artificial Intelligence of Things (AIoT) named FedAIoT. The framework is benchmarked across multiple IoT datasets and incorporates a variety of data partitioning schemes, preprocessing techniques, models, and FL hyperparameters. Despite its comprehensive approach, the paper lacks a comparative study with existing state-of-the-art solutions. Moreover, while the paper mentions the inclusion of popular schemes and models in its framework, it doesn't substantiate why these were chosen over other potential candidates.

Equations and Mathematical Formulations
The paper briefly touches upon the Dirichlet distribution for creating non-IID data partitions and mentions metrics like accuracy and Mean Average Precision (MAP-50). However, it lacks mathematical rigor. For instance, the Dirichlet distribution is mentioned but not defined. A formal definition, perhaps along with its probability density function, would have given more depth. Furthermore, there are no equations to represent the FL optimizers like FedAvg and FedOPT, which makes it difficult to appreciate the nuances or compare them.

Tables and Figures
Table 1: While useful for a cursory comparison, this table lacks depth. For example, it could include a comparison based on performance metrics to provide an analytical foundation for its claims.

Figure 1 and 2: These figures provide an overview but lack detail. For example, Figure 2 could be improved by including the types of IoT-specific preprocessing techniques or by detailing the architecture of the proposed IoT-friendly models.

Table 4: This table summarizes the performance metrics but lacks confidence intervals or p-values, which are essential for ascertaining the statistical significance of the results.

Table 5: While this table attempts to show the impact of client sampling ratios, it doesn't explain why only two ratios (10% and 30%) were chosen for comparison.

Dataset and Experimental Design
The paper includes a wide range of datasets, which is commendable. However, it doesn't provide any rationale for the specific choice of datasets. Furthermore, no information is given about the train-test split methodology. Was it random or stratified? The partitioning schemes for these datasets are discussed, but there is a lack of empirical justification for why these schemes are effective or superior to existing methods.

Algorithms and Techniques
The paper discusses various FL optimizers, data partitioning schemes, and IoT-friendly models, but there is a lack of justification for the chosen methods. For example, why were FedAvg and FedOPT selected as FL optimizers? Are they computationally less expensive or do they converge faster?

Results and Discussion
The paper presents a broad range of results but lacks a discussion comparing these results to existing benchmarks or state-of-the-art methods. The paper would benefit from including such a comparative analysis.

Insufficient Empirical Validation
The experiments conducted are somewhat limited in scope and scale. Only a few datasets are considered, and they seem to belong to similar domains. This raises questions about the model's generalizability. Moreover, the paper lacks ablation studies, making it difficult to understand the contribution of each component of the proposed method.

Actionable Insight: Include a broader array of datasets from varying domains to validate the model. Conduct ablation studies to quantify the impact of each component or parameter.

Absence of Comparative Analysis
While the paper aims to introduce a novel methodology, there is an absence of a comparative analysis with state-of-the-art methods. Without this, the paper falls short of convincingly establishing the proposed method's superiority or novelty.

Actionable Insight: Include comparisons with state-of-the-art methods in both qualitative and quantitative terms. This could be in the form of performance metrics, computational efficiency, or even qualitative assessments based on real-world applicability.

Mathematical Rigor
The paper would significantly benefit from a more rigorous mathematical treatment of the proposed algorithm. Currently, it seems to rely more on empirical observations. Given your stated goals of developing proper mathematical models, this is an area that requires attention.

Actionable Insight: Introduce formal proofs or derivations that can substantiate the algorithm's properties, such as stability, convergence, or robustness. Include theoretical justifications for the choices made in the algorithm's design.

Lack of Discussion on Limitations
Every model has its limitations, and acknowledging them not only adds credibility but also helps in guiding future work.

Actionable Insight: Devote a section to discuss the limitations of the proposed method and potential avenues for future research. Data Assumptions: Could the authors clarify the specific assumptions made about the data distribution? How do these assumptions align with the real-world scenarios where the model is expected to be deployed?

Methodological Choices: What was the rationale behind the selection of specific hyperparameters and architectural elements in the proposed model? Some clarification on this could strengthen the paper's methodological grounding.

Evaluation Metrics: The paper uses a particular set of metrics for evaluation. Could the authors elucidate why these metrics are most suitable for assessing the model's performance? Are there any other metrics that were considered but not used?

Computational Complexity: How does the computational complexity of the proposed method compare with existing state-of-the-art methods? Could the authors provide a detailed analysis in this regard?

Scalability: The paper does not discuss how well the proposed method scales with the size of the dataset. Could the authors provide insights or supplementary experiments that address this?

Ablation Study: The absence of an ablation study leaves some questions about the necessity of each component of the proposed model. Could the authors provide such an analysis in the rebuttal or an extended version of the paper?

Theoretical Guarantees: Are there any theoretical guarantees, such as convergence or bounds, that can be associated with the proposed algorithm? If yes, this would be a valuable addition to the paper.

Limitations: Every model has its shortcomings. Could the authors elucidate the limitations of the proposed model and how they plan to address these in future work?",1480,0,0,0.8076,0.0647396694,0.8674352765000001,49,18.1151,0.1211,iclr,0.0,2,4,4,2,factual,4,2,40,polite,3,positive,4,moderate,5,4,5,5,factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,2.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
80,Reviewer-Vfdi,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The authors describe a new IoT FL benchmark suite based on several datasets they have curated and demonstrate that it can be used to compare FL optimizers. + Benchmark curation work doesn't receive the credit it deserves, given its impact on advances in the field. The authors are doing something important, here. + The writing quality is poor, even in the abstract.

+ The authors claim this is the first IoT FL benchmark but a Google Scholar search turns up ""FLBench: A Benchmark Suite for Federated Learning"" by Yuan Liang, Yange Guo, Yanxia Gong, Chunjie Luo, Jianfeng Zhan, and Yunyou Huang, which includes an AIoT benchmark domain. It isn't clear whether this is competing work, or work by the authors of the submitted paper. In either case, it seems highly relevant but does not appear to be cited. One way to deal with this problem within the blind review process is to cite the work, but use an entry such as ""Redacted for blind review"" in the bibliography during the review process. Another paper, ""FedML: A Research Library and Benchmark for Federated Machine Learning"" claims to support IoT devices. I am not claiming that those papers are identical to this work, but they seem close enough to merit contrasting them with the author's benchmark. I view this as a substantial weakness, but one that might be resolved via rebuttals and simple revision.

+ The modifications to the curated datasets are not well justified. They are reasonable, but explicit justification or basing the approach on well justified approaches from prior work would be best. 1) Does the similarity matrix used for noisy labeling depend on the particular centralized learning approach? If so, does that mean that centralized training and evaluation must be redone to enable noisy labeling whenever an algorithm changes? Or is there something fundamental about the confusion matrix, i.e., is it unlikely to change much when models change?

 2) Why not leave the sounds in raw format instead of converting to the frequency domain with particular parameters? Isn't this sort of raw data to feature conversion part of the approaches your benchmarks will be used to evaluate? If so, why build one particular approach to feature extraction into the benchmarks?

3) What is the purpose of Section 4? To demonstrate that the benchmarks can be used to compare optimizers? Enabling comparison doesn't imply enabling comparison yielding correct ranking of optimizers. If you could demonstrate that the findings using your benchmarks differ from those using the most closely related existing (perhaps even non-IoT) benchmarks, and your benchmarks are more typical of applications in the IoT domain, that would support your claim that your benchmarks are more useful in this domain than prior work.",453,0,0,0.8017000000000001,0.0920518284,0.8552749157,61,50.4888,0.1443,iclr,0.0,5,3,4,5,factual,4,4,85,polite,4,negative,5,none,5,4,4,5,factual,4,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
80,Reviewer-MJiJ,FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things,"There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most of existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, a FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight well-chosen datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. In addition, FedAIoT includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope that FedAIoT could serve as an invaluable resource for researchers and practitioners to foster advancements in the important field of FL for AIoT.","The paper introduces a new benchmark for Federated Learning (FL) specifically aimed at Internet of Things (IoT) applications. The contributions include the curation of eight (already available) datasets spanning different applications and modalities, an end-to-end FL framework for AIoT, some novel ideas on handling noisy labels and extending quantized training to the client side. The main strengths and contributions of the paper are the following:

1) The limitation of existing benchmark datasets in their application to IoT applications is a real one, and the contribution of this paper is curating important publicly available datasets to create a single benchmark for evaluating FL algorithms is an important step.

2) The important FL issues of noisy labels in classification tasks and quantized training due to the resource constraint of IoT devices has been addressed. The paper has the following weaknesses:

1) Although the paper does well in introducing a new benchmarking framework for FL for IoT, it still largely builds upon curating from existing datasets introduced by prior works.

2) The introduced end-to-end FL framework also seems to be a collection of standard machine learning and FL ideas such as non-IID data partitioning, normalization, etc. The novel contributions of addressing noisy labels (non uniform addition of noise) and quantized training at the client side seem limited.

3) The discussion on the details of non-IID partitioning using Dirichlet allocation seems limited, with no further details provided either in the main paper or in the supplementary material. Below are some comments and questions:

1) The authors mention that in real-life settings, individuals may not carry a smartphone and wear a smartwatch at the same time, and hence WISDM dataset was partitioned into two. However, this conclusion does not always hold true and better partitions of the WISDM dataset can be made that include both smartphone and smartwatch data in some realistic manner.

2) For non-IID partition over output distribution that implements quantile binning, how is the value 10 for the number of groups chosen? This seems arbitrary or heuristic.",335,0,0,0.7825000000000001,0.1332491582,0.888387382,49,25.7145,0.1041,iclr,0.0,3,4,3,3,factual,4,4,75,polite,4,neutral,3,low,4,5,4,4,5,5,5,85,5,5,5,5,4,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
18,Rodrigo-Lopez,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article by Moreews et al. describes a registry of bioinformatic tools images that are portable using Docker technology. The manuscript is well written and describes well the aims of the BioShaDock registry and it's possible interactions with the ELIXIR Tools and Data Services Registry as the means to find Docker containers in the wild. As pointed out in the abstract, other Docker registries exists, such as Docket HUB, but lack of curation and user engagement hampers their progress. Furthermore,BioShaDock provides user management at a level required for ensuring that the interoperability between the registries,  images and local environments is secure, auditable and effective.The article describes well the overheads associated with typical software installations and maintenance and presents a balanced view on the advantages of using Docker to manage this processes. Although not perhaps within the scope of this article, this reviewer feels it would be useful to inform the readership of other alternatives to Docker; e.g. Rocket, DrawBridge and LXD from Canonical and FlockPort, as it is clear that Docker is still maturing and it is certainly not the only container available today.",251,0,0,0.7973,0.0616459627,0.9242030978,1,26.24,0.1041,f1000,0.01010101010101,1,4,2,1,partially factual,3,3,40,polite,4,positive,3,moderate,4,5,4,4,factual,5,5,85,polite,5,positive,5,none,3.0,4.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
18,Björn-A.-Grüning,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,"Linux container technologies, as represented by Docker, provide an alternative to complex and time-consuming installation processes needed for scientiﬁc software. The ease of deployment and the process isolation they enable, as well as the reproducibility they permit across environments and versions, are among the qualities that make them interesting candidates for the construction of bioinformatic infrastructures, at any scale from single workstations to high throughput computing architectures. The Docker Hub is a public registry which can be used to distribute bioinformatic software as Docker images. However, its lack of curation and its genericity make it difﬁcult for a bioinformatics user to ﬁnd the most appropriate images needed. BioShaDock is a bioinformatics-focused Docker registry, which provides a local and fully controlled environment to build and publish bioinformatic software as portable Docker images. It provides a number of improvements over the base Docker registry on authentication and permissions management, that enable its integration in existing bioinformatic infrastructures such as computing platforms. The metadata associated with the registered images are domain-centric, including for instance concepts deﬁned in the EDAM ontology, a shared and structured vocabulary of commonly used terms in bioinformatics. The registry also includes user deﬁned tags to facilitate its discovery, as well as a link to the tool description in the ELIXIR registry if it already exists. If it does not, the BioShaDock registry will synchronize with the registry to create a new description in the Elixir registry, based on the BioShaDock entry metadata. This link will help users get more information on the tool such as its EDAM operations, input and output types. This allows integration with the ELIXIR Tools and Data Services Registry, thus providing the appropriate visibility of such images to the bioinformatics community.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This article describes very well the current state of bioinformatics Linux container adoption and arising problems. It offers solutions to these and also describes real-world use-cases with an existing integration into systems like Galaxy. Especially interesting is the rich annotation system, that involves ELIXIR ontologies as well as the ELIXIR registry.This is needed and a big step forward.Personally, I would like to see stronger collaborations between the mentioned other registry and Docker-build projects. I still feel we have a lot of redundant work inside of the bioinformatics community. For example I think it would be relatively easy to configure travis in biodocker to push automatically into BioShaDock, if biodocker counts as trusted partner. On the other hand biodocker can profit largely by the rich annotation system.The manuscript is well written and I would encourage everyone to participate in this project. I certainly will.",210,0,0,0.8595,0.1349378882,0.9337836504,49,26.71,0.2552,f1000,0.0,1,4,1,2,partially factual,3,2,30,polite,2,positive,3,moderate,4,5,4,4,factual,4,4,88,polite,5,positive,5,low,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,3,75,polite,5,positive,4,low,3,4,4,4,factual,4,3,85,polite,5,positive,5,low
123,Reviewer-EGJf,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","**Summary:** 
This paper presents an open-source toolkit based on LoRa. I believe this work might be more appropriate for the ""benchmarking and datasets"" track. Positioned here, it's challenging for me to evaluate the innovation this paper offers. **Remarks:** 
While the improvements and variants on LoRa are relatively straightforward, the theoretical part of the paper seems sound. **Recommendation:** 
I would advise the authors to provide clear insights through experiments and offer some specific suggestions. I cannot evaluate this paper because I believe it is proper for a benchmarking and dataset track, not the main track.",94,0,0,0.7561,0.2401515152,0.7697365284000001,75,42.4333,0.2025,iclr,0.0374999999999999,3,4,3,2,partially factual,3,2,30,polite,2,negative,4,moderate,4,5,3,3,partially factual,3,3,55,polite,3,neutral,3,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,1,3,2,2,partially factual,3,2,40,polite,2,neutral,2,moderate,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
123,Reviewer-DWom,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This paper proposes a comprehensive library for evaluating text-to-image finetuning methods, typically based on LoRA. In addition to different algorithms, it also provides comprehensive evaluation criteria. Finally, some experimental results provide some insight about different finetuning methods. 1. This is a good engineering paper that provides a library for text-to-image finetuning methods evaluation.
2. It support different matrix factorization techniques such as LoRA, LoHa, LoKr, DyLoRA, GLoRA, GLoKr and so on.
3. This paper also consider comprehensive evaluation metrics, including fieldity, controllability, diversity, base model preservation and image quality. 1. This paper mainly focus on LoRA-based finetuing strategies, can it be expanded to other parameter-efficient finetuning methods such as \[1\] and \[2\]? It doesn't provide a clear explanation.
2. The conclusion about the performance of different finetuning methods is not clearly presented in the experimental section. Maybe some tables can more straightforwardly represent your final conclusions. 

\[1\] Qiu, Zeju, et al. ""Controlling Text-to-Image Diffusion by Orthogonal Finetuning."" arXiv preprint arXiv:2306.07280 (2023).
\[2\] Xie, Enze, et al. ""DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning."" arXiv preprint arXiv:2304.06648 (2023). Please refer to the weakness section.",187,6,9,0.8365,0.0530612245,0.911740303,52,16.9695,0.2131,iclr,0.0,2,4,2,2,partially factual,3,3,50,polite,3,negative,3,moderate,4,4,3,4,5,4,5,75,4,5,neutral,4,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,3,3,factual,4,4,70,polite,4,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
123,Reviewer-PnHf,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","This author introduces LyCORIS, an open source library dedicated to fine-tuning of Stable Diffusion, which integrates a comprehensive range of finetuning methods. For rigorous comparisons between the implemented methods, the author proposes a comprehensive evaluation framework that incorporates a wide range of metrics. Based on the evaluation framework, the author performs extensive experiments to compare different fine-tuning algorithms and to assess the impact of the hyperparameters (i.e, training epochs, learning rate, trained layers, et al). Overall, the experiments, comparisons, analyses, and results of the entire paper are very well-rounded and thorough. 1. Developing an open-source library is of great significance in fostering the advancement of a particular field. After comparing the existing open-source libraries available online, the LyCORIS library offers a relatively more comprehensive set of algorithms.

2. The author has developed a comprehensive benchmark to evaluate various algorithms from multiple perspectives, addressing a significant gap in the text-to-image field. This thorough evaluation and comparison of existing finetuning methods have been lacking in the domain until now.

3. The author conducted comprehensive experiments for different algorithms and parameters; in addition, the author also provided a detailed analysis of the current mainstream fine-tuning algorithms. 1. HuggingFace has also released the PEFT library, which supports a wider range of pre-trained models and includes the methods mentioned in the paper. Therefore, what are the advantages of the LyCORIS library compared to PEFT?

2. The paper conducted a multitude of experiments and comparisons on existing methods and various hyperparameters, leading to certain conclusions. Based on these findings, could there be a more optimal algorithm or design compared to previous ones? For this kind of paper that builds benchmarks based on a certain field, I would recommend the author to submit to a journal.",289,0,5,0.7676000000000001,0.1721428571,0.8675829172,52,20.4212,0.1213,iclr,0.0,4,4,4,4,factual,3,4,70,polite,5,negative,4,low,4,4,4,4,partially factual,4,3,75,polite,5,positive,5,moderate,3.0,4.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
123,Reviewer-ekPo,Navigating Text-To-Image Customization: From LyCORIS Fine-Tuning to Model Evaluation,"Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts.  Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field.  However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation.  Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion.  Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories. Through this comprehensive approach, our work provides essential insights into the nuanced effects of fine-tuning parameters, bridging the gap between state-of-the-art research and practical application.","The authors propose LyCORIS, an open-source library that contains multiple fine-tuning techniques for Stable Diffusion. The authors also explore many improved fine-tuning techniques such as LoCon, LoHa and LoKr. This paper also presents evaluations for different fine-tuning techniques using multiple metrics and prompt types. (1) The theory and experiments are both solid. The paper has over 57 pages devoted to analyzing the fine-tuning techniques.
(2) The details for experiments are very clear.
(3) In addition to the framework, the authors also explore other fine-tuning techniques. (1) The results of this framework combined with ControlNet can be presented in this paper.
(2) Efficiency (time and GPU memory cost) of different approaches are not provided and analyzed. (1) Please refer to the main questions in the weakness section.
(2) A minor question: It will be better if the authors provide the results on other versions of stable diffusion, such as SD2.0 and SDXL.",151,0,0,0.7539,0.0711904762,0.7818619609,52,48.9543,0.1844,iclr,0.0,3,4,4,3,factual,3,3,65,polite,4,positive,4,low,5,4,4,5,partially factual,5,5,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,75,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
138,Reviewer-z7DA,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","The paper proposes MEET, a method to train a LLM to generate ""good"" and ""bad"" answers to a given question / task by conditioning the model computation with an adapter (LoRA or Soft Prompt). To do so, they adopt a two-step training procedure. First, they train a ""good control adapter"" and a ""bad control adapter"" on good answers and bad answers respectively while keeping the base LM fixed, then they fine-tune both the control adapters and the base model. The authors show that this two step procedure is important to achieve gains over the Chain of Hindsight baseline (basically a baseline where control adapters is just a handcrafted prompt ""A good/bad conversation is:"") and DPO on two datasets OpenAI Summary and HH-RLHF from Anthropic. - The paper is well-written, the details of the experimental setting are clear.
- The two-stage training procedure is interesting and its importance is validated by the ablation study.
- Results seem to suggest that the two-step optimization method delivers gains w.r.t. DPO. - It feels like the authors are a bit confused on where the novelty of their paper really lies, they seem to suggest that it is in using adapters to control generation, but imho, the interesting bit is more on the two-step training procedure that guarantees information is captured by the adapters and thus they are not ""information-starved"" by the full LM fine-tuning (easy to fix)

- The more problematic bit is that authors' confusion seems to have affected the overall experimental methodology; for example, the authors seem to tie their method to the specific loss function used (i.e. MLE) and compare to DPO, while their method can be used on top of DPO. Moreover, the baselines numbers are a bit concerning and some important baselines are missing (overall harder to fix) About novelty:

The paper proposes to learn ""attributes"" conditional models with adapters, which have been proposed in https://arxiv.org/pdf/2302.08453.pdf for diffusion models for example. So, here, the novelty might reside in 1/ applying this general idea to textual generation tasks and 2/ the two-stage training approach proposed to train these adapters. The current stance of the paper is that the main novelty is to apply LoRA adapters for generation instead of hard prompts. I feel like 2/ is a more interesting and impactful contribution but currently it is a bit understated in the paper, so it feels like it should be the central focus of the paper. I feel like the paper can be an interesting set of experiments showing that the two-stage approach prevent adapters from being ""information-starved"" from full model fine-tuning.

About experiments:

Confusion about the contributions seem to appear in Section 3, where the authors tie their method to MLE loss (1) and (2) and compare in the experiments with a DPO baseline. This is a bit surprising to me given that their method can be deployed on top of DPO, i.e. Eq (1) and (2) can use DPO instead of MLE (to train each good and bad expert), so I am not sure why DPO would be a baseline in the experiments. On the contrary, I would have expected to see two versions of their method in the experiments: with Eq. (1) and (2) using DPO (MEET-DPO) and Eq. (1) and (2) using MLE (MEET-MLE).

From all experiments, one straightforward baseline is missing in addition to CoH: SFT -- which just trains on positive data.

Similarly, for MEET-MLE, what is the impact of integrating negative data? i.e. what is the gap between MEET-SFT, which just trains the controllable adapter of positive data and MEET-MLE, which trains on both positive and negative data with Eq. 2?

In the first dataset, DPO underperforms CoH on OpenAI/Summary dataset. The fact that DPO underperforms CoH on this dataset is a bit suspicious. Did you tune the \beta parameter for DPO on both datasets ?

How do you do cross-validation in these two datasets? Are you searching for the best HPs for each method on the validation set?

Taken together, your results currently show that DPO is useless in these two datasets and severely underperform MLE training with MEET. I am not sure this result can be published without further ablations and baselines as I suggest above, especially it appears to me that MEET can be further improved with DPO training.

Please, do not consider my score as final, I am willing to increase the score substantially if the authors can give answers to my questions.",743,1,2,0.7414000000000001,0.0964416896,0.8564590812,47,50.4638,0.087,iclr,0.0,4,4,4,4,factual,5,4,90,polite,4,positive,4,low,5,5,5,5,partially factual,4,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,3,4,4,4,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
138,Reviewer-Q2P6,Parameter-Efficient Tuning Helps Language Model Alignment,"Aligning large language models (LLMs) with human preferences is essential for safe and useful LLMs. Previous works mainly adopt reinforcement learning (RLHF) and direct preference optimization (DPO) with human feedback for alignment. Nevertheless, they have certain drawbacks. One such limitation is that they can only align models with one preference at the training time (e.g., they cannot learn to generate concise responses when the preference data prefers detailed responses), or have certain constraints for the data format (e.g., DPO only supports pairwise preference data). To this end, prior works incorporate controllable generations for alignment to make language models learn multiple preferences and provide outputs with different preferences during inference if asked. Controllable generation also offers more flexibility with regard to data format (e.g., it supports pointwise preference data). Specifically, it uses different control tokens for different preferences during training and inference, making LLMs behave differently when required. Current controllable generation methods either use a special token or hand-crafted prompts as control tokens, and optimize them together with LLMs. As control tokens are typically much lighter than LLMs, this optimization strategy may not effectively optimize control tokens. To this end, we first use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens and then fine-tune models for controllable generations, similar to prior works. Our approach, alignMEnt with parameter-Efficient Tuning (MEET), improves the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two well-recognized datasets compared with prior works.","This paper proposes to use parameter-efficient tuning (e.g., prompting tuning and low-rank adaptation) to optimize control tokens
and then fine-tune models for controllable generations. The MEET aims to improve the quality of control tokens, thus improving controllable generation quality consistently by an apparent margin on two datasets. 1. This paper studies a parameter-efficient way to improve the language alignment. It is an interesting direction to explore.

2. It studies several aspects of the proposed method such as prompt length, rank, and temperature. 1. This paper conducted several experiments. However, I don't think the baselines the paper compares with are sufficient. Several works focus on a similar idea about incorporating the reward into text learning, such as RLPrompt \[1\] and AutoPrompt \[2\]. Those should become the baselines to compare the method proposed in the paper. Also, For controllable text generation, there is an interesting direction to utilize the diffusion process, such as the Diffusion-LM \[3\]. However, none of these are included and compared in the paper. Thus, I am not convinced with the experimental results shown in the paper.

2. The performance of the proposed method does not show enough improvements compared to the baseline mentioned in the paper. It highly correlates to the hyperparameter settings. It would be good to include the detailed ablations of those hyperparameters. 

3. The proposed method seems to be not novel. We know the impact of LoRA, and the proposed method seems just a direct implementation of the LoRa with parameter-efficient tunning with some specific designs. Could authors provide more justification about the novelty of the proposed methods?

4. For the ablation section, what would be the efficiency comparison between the proposed method and the baselines? Such as the running time and computation latency.



\[1\] Rlprompt: Optimizing discrete text prompts with reinforcement learning

\[2\] AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts

\[3\] Diffusion-LM Improves Controllable Text Generation Please refer to the Weaknesses section.",321,6,6,0.7649,0.1105,0.8929747939,47,38.6383,0.2968,iclr,0.0,3,3,2,1,factual,2,2,50,neutral,2,negative,4,moderate,5,5,4,5,partially factual,5,5,90,polite,5,negative,5,none,2.0,4.0,4.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,3,4,70,neutral,5,negative,4,low,4,4,3,3,partially factual,3,3,70,neutral,4,negative,4,low
17,Reviewer-gsUn,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This paper proposes to integrate the first-order gradient into the unstructured pruning of large language models and achieves superior performance compared to sparseGPT and Wanda. 1. A superior method compared to SparseGPT and Wanda on unstructured pruning of large language model
2. The authors have conducted extensive experiments to assess the method's effectiveness on LLaMa-1 and LLaMa-2. Additionally, the paper illustrates the impact of various gradient and activation combinations on the determination of parameter importance.
3. The paper is well-written, offering clarity and ease of understanding in its presentation. 1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.
2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.
3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important  to the readers to understand the efficiency of the pruned model. 1. Could you specify the error function utilized for calculating gradients in your approach?
2. Have you conducted any latency experiments on the pruned model, particularly under the 2:4 or 4:8 configurations?
3. Is the calibration set employed for your methods and Wanda, SparseGPT identical?",283,0,8,0.8014,0.127046131,0.9349081516,56,30.9022,0.4134,iclr,0.0,3,4,3,3,partially factual,4,3,65,polite,4,neutral,3,moderate,5,5,4,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
17,Reviewer-B7C2,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","* The paper proposes to integrate gradient information into pruning criteria currently used for LLMs.
* The corresponding GBLM method is evaluated on Llama models for perplexity and zero-shot tasks. * The paper is easy to follow and describes the proposed method in good detail.
* The method is evaluated on strong LLama models rather than older LLMs like OPT.
* Source code is provided, aiding reproducability. * Integrating gradient information into pruning criteria is a well studied area, see for example \[1, 2, 3, 4\]. This is currently not discussed under Related Work.
* Consequently, the novelty of GBLM is quite limited. For instance, the analysis in Section 2.3 is very similar to derivations presented in \[2\]. Ultimately, GBLM seems to be a minor variation of a diagonal Fisher scheme (using both gradients and activations while slightly tweaking norms in a heuristic manner).
* The most robust form of evaluation, perplexity, shows only very slight improvements relative to prior work of < 0.1 points, while dropping noticably from the baseline. I am not sure if this is a significant enough improvement in practice.
* It is unclear how the gradient calculation impacts the speed and compute/memory requirements of the pruning process. Being fast and memory efficient is one of the key strengths of SparseGPT and Wanda, hence I think a detailed comparison/discussion of this aspect would be important.

Unfortunately, at this time, I find neither the method itself nor the empirical results interesting enough to recommend acceptance.

\[1\] Pruning convolutional neural networks for resource efficient inference, Molchanov et al.

\[2\] WoodFisher: Efficient Second-Order Approximation for Neural Network Compression, Singh et al.

\[4\] The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, Kurtic et al.

\[3\] Movement Pruning: Adaptive Sparsity by Fine-Tuning, Sanh et al. * See weaknesses, in particular the compute/memory efficiency point.",308,6,0,0.8244,0.12046851,0.9186406136,56,35.8961,0.2025,iclr,0.0,3,4,4,3,factual,4,3,75,polite,4,neutral,4,none,4,5,4,4,factual,5,5,85,neutral,5,negative,5,none,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,3,4,4,3,factual,3,4,70,polite,5,negative,4,low,3,4,4,3,partially factual,4,4,75,neutral,5,negative,5,low
17,Reviewer-k6s5,Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models,"Large Language Models (LLMs) with a billion or more parameters are prime targets for network pruning, which aims to reduce a portion of the network weights without compromising performance. Prior approaches such as Weights Magnitude, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained large language models. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed **G**radient-**b**ased **L**anguage **M**odel **P**runer (**GBLM-Pruner**). Distinctively, GBLM-Pruner operates in a training-free manner by harnessing normalized gradients, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks. Intriguing, after incorporating gradients, the unstructured pruning method tends to reveal some structural patterns post-pruning, which mirrors the geometric interdependence inherent in the LLMs' parameter structure. Additionally, GBLM-Pruner functions without any subsequent retraining or weight updates to maintain its simplicity as other counterparts. Extensive evaluations on LLaMA-1 and LLaMA-2 across various language benchmarks and perplexity show that GBLM-Pruner surpasses magnitude pruning, Wanda (*weights+activations*), and SparseGPT (*weights+activations+weight update*) by significant margins. Our code and models will be publicly available.","This study introduces GBLM-Pruner, a new post-training pruning technique designed for large language models, which leverages gradient information. The authors provide both theoretical rationale and empirical assessments that demonstrate GBLM-Pruner outperforms other prominent baselines, such as Wanda and SparseGPT. - The paper is well-organized, effectively presenting the method with clear descriptions and comprehensive empirical evaluations.
- Both theoretical explanations and empirical results are presented to validate the theoretical explanations and empirical results.
- The paper includes plenty of ablation studies, encompassing diverse sparsity levels, different pruning metrics, assessments of dependency on calibration samples, and visualizations that highlight the specifics of sparse patterns. - The improvements achieved by GBLM-Pruner, as compared to other baselines like SparseGPT and Wanda, appear to be relatively modest. For instance, in Table 2, under 50% unstructured sparsity, GBLM-Pruner (l1) yields perplexity reductions of only 0.06, 0.09, and 0.05 compared to Wanda on LLaMA-2-7B/13B/70B, respectively. Additionally, in Figure 2, the curves for Wanda and GBLM-Pruner exhibit significant overlap.
  
- I'm unclear about the rationale behind experimenting with the pruning metrics listed in Line 7/8. It seems that some of these metrics may not provide meaningful insights.

- It's essential to understand the memory and time requirements during the pruning process of GBLM-Pruner. Obtaining gradient information can impose a significant memory cost, and it may not be feasible to conduct this process in a layer-wise manner. Storing intermediate features for the backward process could further impact memory usage. Thus, it would be valuable to compare these memory and time requirements with those of other baseline methods for a more comprehensive assessment of GBLM-Pruner's practicality. None",267,0,1,0.812,0.1120610871,0.9594182372,56,23.6709,0.1262,iclr,0.0,2,3,3,3,partially factual,3,3,55,polite,4,neutral,3,low,4,5,4,4,factual,4,4,88,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
51,Reviewer-nspR,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper studies the market design problem, specifically for data markets. In particular, different from existing analytic approaches, the proposed approach is based on (deep) learning to recover/discover market designs. They adopt and extend an existing RochetNet architecture to both single- and multi-buyer setting and empirically demonstrate the effectiveness of the approach in recovering/discovering the market design. - The paper studies the problem of market design and it is relevant for data market.
- The proposed learning-based approach is interesting in that it can recover some analytic solutions.
- There are relatively extensive empirical results. - The motivation and justification of a (deep) learning-based approach can be made stronger.
    
    In lines 40-42, ""The difficulty of using analytical tools for this problem of data market design is highlighted by this example, and it remains an open problem to obtain theoretical results for richer multi-buyer settings. This motivates the need for computational approaches."" While it is perceived that analytic solutions are difficult, and computational approaches seem a viable alternative. Is it really necessary to use deep learning? In other words, are there less complex computational approaches that can be tried first or reasons why they would not work as well? 

    In particular, (how) can the assumption of i.i.d. samples from $\mathcal{P}$ for training the deep learning model be satisfied? It requires the type of the buyer (i.e., both belief and the $v$) to remain fixed throughout observing the signals. Does this assumption have conflicts with ""Upon receiving a signal, the buyers update their prior beliefs and choose an optimal action accordingly"" (lines 143-144)?


- The inline equations in the paper can break the flow of the writing and make it more difficult for the reader to catch the most important points.

    For instance, equations (1)-(4) are used to discuss (different variants of) incentive compatbility. It is not so clear which equation the reader should pay most attention to. Furthermore, it seems that equation (4) (i.e., ex post incentivie compatible) is not interpreted after the equation.

- Some experimental results can be difficult to interpret (or understand their significance), due to the lack of (existing) analytic characterization of optimum solution. 

    For instance, in lines 294-296, ""We are aware of no theoretical characterization of optimal data market designs when both $v$ and $\theta$ vary. In such cases, we can use RochetNet to conjecture the structure of an optimal solution."" As a result, it is not clear to the reader how to understand whether the proposed method is effective. It further goes to the first point regarding the motivation/justification of a learning based approach: There lacks a solution or ground truth (i.e., analytic optimum or approixmate optimum) to evaluate the approach. Hence, it seems appealing to first establish such a solution before a computational approach, otherwise, how to effectively evaluate the proposed computational approach?
 - In lines 20-22, ""... hold vast quantities of data about individuals. In turn, this has led to data markets, where information about an individual can be purchased in real-time to guide decision-making (e.g., LiveRamp, Segment, Bloomreach)."" This seems to hint at that the aforementioned companies are selling data about individuals, is it what it means?

- In lines 60-62, ""Further, we give a training method that enables the efficient reuse of computed interim allocations and payments from other samples to swiftly calculate the interim utility of misreporting, dramatically speeding up training."" Is this empirically or theoretically demonstrated, specifically about ""dramatically speeding up training""? What is it comparing against, in terms of speed of training?

- In line 122, ""The state of the world, $\omega$, is unknown and is drawn from a finite state space ... "" Is there an assumption on the distribution of this?

- In line 127, ""where each $v_i$ is drawn independently from a distribution $\mathcal{V}_i$"". What is the interpretation of $v_i$ and what does the distribution $\mathcal{V}_i$ depend on?

- In lines 137-138, it seems that the negative externality is in the form of decreasing payment for one buyer $i$ as the gain for some other buyers. In other words, if another buyer $j$ gains (in ex post payoff), this buyer $i$ ""loses"" (i.e., has a lower utility), is this correct? How should this be interpreted in an example? 

- In line 139, ""There is a data seller who observes the world state ... "" How to justify or realize this assumption that the actual world state is exactly known by the data seller?

- In line 159 (5-th bulletin point), ""$u_i(a,\omega, V_i, \theta_i)$"", is it meant to be $V_i$ or $v_i$?

- In line 192, ""... an unsupervised learning problem."" Is it referring to optimizing the softmax version of Equation (9)? If so, it looks more like an optimization problem (i.e., parametric fitting) instead of a learning problem. Often, unsupervised learning is to learn about the inter or intra structure of the data instead of to fit a functional form. Please help interpret why the loss function in line 222 is an unsupervised learning problem.

 - Typically in an optimization approach, if the objective is non-convex (or more complex), it is difficult to establish theoretical guarantees in terms of the optimality or quality of the final solution obtained. This is also mentioned by the authors in lines 374 - 375. The implication is that, it is difficult to obtained a principled understanding of how good the solution (i.e., learnt market design) is, obtained from the gradient-based optimization.

- With regard to lines 378-380, ""we return to where we started, and underline that markets for trading data about individuals raise a number of ethical concerns."" In light of the potential ethical concerns of data trading, a (deep) learning-based approach potentially makes it even more difficult to manage and parse the working mechanism of the data trading. As a result, such an approach can make it even more difficult to reliably/verifably address those concerns.
",977,0,0,0.768,0.0907392027,0.9176636934,215,44.2756,0.0665,neurips,0.0,4,4,5,4,factual,4,4,100,polite,5,neutral,5,none,4,4,5,5,factual,5,5,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,5,3.0,4.0,70.0,4,4.0,2,3.0,1,4,4,5,4,factual,5,5,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
51,Reviewer-u4po,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper introduces a deep learning application to the data market designs that find optimal signaling schemes to maximize the revenue of data sellers. The proposed method is designed to handle truthfulness and obedience (i.e., buyers following recommendations). The overall approach follows the prior frameworks of RochetNet and RegretNet for auction design. The authors are able to demonstrate the method’s ability to recover existing analytical optimal solutions and extend to cases where analytical results are not available. Some experimental results are provided for both single-buyer and multiple-buyer settings. 1. The paper applies deep learning to the new domain of data market design, illustrating the feasibility of learning solutions to optimal data market design.
2. It considers the obedience of data buyers in the design. This makes the approach more practical.
3. The paper provides a sound analysis of Individual Rationality for the mechanism and payments. 1. The writing could be improved. Preliminaries could be better structured to explain essential terms like menu entry, signaling, state of the world, how the mechanism works, etc. Interpretations could be added after Lemmas and computation equations (e.g., (10)) to improve clarity.
2. The scales of the experiments are not large enough to be convincing. If larger experiments are not possible, challenges and limitations should be clearly stated. **Major**

1. Are there any references to support the assumptions made in the preliminaries section? For example, why is the matching utility payoff reasonable in data market design? How do you interpret that in the binary-state setting in the real world? How about a more complex non-binary setting?
2. For the single buyer setting Lemma 3.1, it is claimed that the mechanism is Incentive Compatible as it is agent optimizing. Why is it agent optimizing when the objective is to maximize the payment by the agents?
3. How to access the validity of the results from the networks when there is no analytical solution (more complex settings)? For example, for the price of 0.14 outputted for setting C, how do you know whether it is close to optimal? Also, could you provide a more intuitive interpretation of the price and results?
4. What are the challenges in conducting experiments on binary states, actions? Also, can you perform experiments on more than two buyers? Can the method be extended to much more complex scenarios with a large number of players, actions and states?

**Minor**

5. Grammar. Lines 80, 103, 242. Punctuations and formats: Lines 146, 153-160, 239.
6. Some notations can be confusing, especially the subscripts, superscripts and brackets.
7. What is $\Delta$ in Line 129, never explained before. The authors have sufficiently discussed the limitations of the approach in the limitation section. Additionally, I wonder how well this framework applies in real-world scenarios. Could the author clarify the limitations of adopting the method in real life for data pricing, or provide a practical example/application?",477,0,14,0.7548,0.1180152085,0.9419704676,215,41.0441,0.3841,neurips,0.0,4,5,5,4,factual,4,4,95,polite,4,neutral,5,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,5,4,5,5,factual,5,5,90,polite,5,neutral,5,low,3,3,4,4,partially factual,4,4,85,polite,5,neutral,3,low
51,Reviewer-wNRV,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors are concerned with a problem of ""data market design"". In such a setting, a mechanism designer with access to an unknown world state interacts with buyers who have private types, and need to take actions whose payoffs vary depending on the world state. These buyers purchase (in the single buyer case) or bid on (in the multi-buyer case) access to a signaling scheme which, given reports from the agents and the world state, sends a signal to the buyers (which without loss of generality can just be a recommended action). This mechanism, treated as a direct-revelation mechanism, needs to be both truthful (incentivizing honest reporting by the buyers) and obedient (once the buyers receive their signal, they should be incentivized not to deviate from the recommendation). Subject to those constraints (either Bayesian or ex post), the mechanism designer wants to maximize their revenue.

This problem shares some similarities to truthful revenue-maximizing auction design. In that domain, there has been recent progress using the tools of ""differentiable economics"" to approximately learn high-performing (and sometimes even provably optimal) auctions, in both single- and multi-bidder settings.

The authors apply very similar techniques to this data market problem. In single-buyer settings (as in auctions) they are able to ensure exact IC; for multi-buyer settings they use a Lagrangian during training to approximately enforce IC constraints. They experiment on a relatively wide variety of problem instances, reproducing known results, finding new optimal mechanisms, and conjecturing optimal mechanisms where they cannot find them. The paper comprehensively shows how to successfully apply differentiable economics to a new domain where it has not previously been applied. The authors are able to reproduce optimal mechanisms and find new ones, showing that their adaptation of these technique is in fact useful in producing novel results. This helps to further push these techniques towards being practically helpful tools for theorists and modelers. The network architectures here are essentially the same as those used in previous work for auctions, only adapted slightly for the data market setting. This is fine, but it does mean that from the perspective of differentiable economics, there is no novel methodological contribution.

The experiments appear to consider at most 2 buyers. While (as in the case of multi-parameter auctions) even selling to just two buyers may be a very challenging case, it would be more interesting to consider a slightly larger number of buyers. Can the method in fact scale to larger (even just 3-5 buyers) settings, or not? This should be discussed. See questions.",420,0,1,0.8172,0.1425933442,0.9128888845,215,37.1539,0.0499,neurips,0.0098039215686274,3,4,4,3,factual,4,3,80,polite,4,neutral,4,low,4,5,4,4,factual,5,5,88,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
51,Reviewer-QSAL,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","This paper introduces a deep learning framework for the automated design of data markets, a novel and timely application in the field of economics. The authors address the data market design problem, which involves designing a set of signaling schemes to maximize expected revenue. The paper extends previous work on deep learning for auction design by learning signaling schemes and handling obedience constraints that arise from the actions of agents. - Innovative Application: The paper introduces a novel application of deep learning to the data market design problem, expanding the scope of machine learning in the field of economics.

- The paper is well-written overall. -Incremental work: It seems that the core contribution, the proposed neural network architecture, is a simple extension of existing model called RochetNet, by slightly modifying the loss function.

-Lack of comparison with baselines: mechanism design for information acquisition is a long standing problem. I was surprised to see no baseline comparison in the experiments, and no discussion on how/why existing approaches may not work in the methodology.  What are some baseline methods to compare with? For example, how does the standard rochetnet perform on the proposed market settings? Yes.",194,0,1,0.799,0.0097222222,0.957269609,215,33.5689,0.1249,neurips,0.0,2,4,2,2,partially factual,3,3,60,polite,4,neutral,3,moderate,3,4,3,3,partially factual,4,4,65,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,positive,3.0,none,2,4,3,2,factual,3,3,60,neutral,4,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
51,Reviewer-zvsA,Data Market Design through Deep Learning,"The  _data market design_ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design, we must learn signaling schemes rather than allocation rules and handle  _obedience constraints_  &mdash; these arising from modeling the downstream actions of buyers &mdash; in addition to incentive constraints on bids.  Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs.","The authors present a novel approach to the problem of data market design, which seeks to find a set of signaling schemes, each revealing some of the information known to a seller and having a corresponding price, where the goal is to maximize expected revenue. Then, the authors introduce the application of a deep learning framework to the automated design of the data market. The paper discusses the importance of data market design and its potential applications in real-world settings, such as data marketplaces where sellers sell data to buyers for ML tasks. The authors demonstrate that their new learning framework can replicate known solutions from theory, expand to more complex settings, and establish the optimality of new designs. The paper also highlights some limitations of the approach, such as the need for interpretability of the mechanisms learned by the RegretNet approach for larger problems, the potential for local optima in non-convex problems, and the challenge of achieving exact incentive alignment in multi-buyer settings. + The paper presents a novel approach to the problem of data market design, which uses deep learning to automate the design of data markets.

+ The authors demonstrate that their new learning framework can almost precisely replicate all known solutions from theory, which shows that the approach is effective and reliable.

+ The paper shows that the new learning framework can be used to establish the optimality of new designs and conjecture the structure of optimal designs, which is a significant contribution to the field.

 + The paper acknowledges that for the approach to provide insights into the theoretically optimal design for larger problems, it will be important to provide interpretability to the mechanisms learned by the approach. However, the RegretNet approach used in the paper is not immediately interpretable, which limits its usefulness in this regard.

+ The paper notes that the approach uses gradient-based approaches, which may suffer from local optima in non-convex problems. This suggests that the approach may not always find the global optimum and may be limited in its ability to handle more complex problems.

+ The paper attains in the multi-buyer setting approximate and not exact incentive alignment, which leaves the question as to how much alignment is enough for agents to follow the intended advice of a market design. This suggests that the approach may not be able to achieve exact incentive alignment in all settings, which could limit its effectiveness.
 + Could you provide more details on how the RegretNet approach can be made more interpretable for larger problems? Are there any specific techniques or methods that could be used to achieve this?

+ Have you considered using other optimization techniques besides gradient-based approaches to address the potential for local optima in non-convex problems? If so, what are some alternative approaches that could be used?

+ What are some potential ways to provide more practical or theoretical guidance on how much alignment is enough for agents to follow the intended advice of a market design? Are there any existing frameworks or approaches that could be used to address this issue?
 The authors acknowledge the ethical concerns raised by markets for trading data about individuals and suggest that machine learning frameworks such as those introduced in this paper can be used to strike new kinds of trade-offs, such as allowing individuals to benefit directly from trades on data about themselves. This shows that the authors are aware of the broader implications of their work and are thinking critically about its potential impact.",584,0,0,0.7766000000000001,0.1128884508,0.9413257241,215,35.3831,0.1616,neurips,0.0,4,4,4,4,partially factual,4,4,90,polite,4,neutral,4,none,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,4.0,5.0,4.0,5.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,4,low
23,Bertil-Blok,Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis,"Behçet’s disease (BD) is an autoimmune vasculitis with an unclear etiology presenting with a classic triad of symptoms including oral and genital ulcers as well as iridocyclitis. A subset of BD patients exhibit neurological symptoms including psychiatric disturbances, balance problems, and voiding dysfunction, and the symptoms of BD can mimic other neurological diseases, including multiple sclerosis (MS).  Differentiating between potential diagnoses is challenging due to the lack of specific tests for these disorders and the overlap between clinical symptoms and radiological findings. We describe the case of a 52 year old woman initially diagnosed with and treated for MS.  From the urologic standpoint, she was treated for neurogenic detrusor overactivity with detrusor-sphincter-dyssynergia utilizing ileocecal augmentation cystoplasty with a continent stoma for intermittent catheterization. The patient was later diagnosed with BD in light of additional clinical findings.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The manuscript 'Case Report: A case of neurogenic bladder in the setting of Behçet's disease after an initial diagnosis of multiple sclerosis' is an interesting paper. However, I have some comments addressed below. The case report is a readable report on an interesting topic. However, the authors do not report on any vaginal child delivery nor do they mention the BMI of the patient. Both are risk factors for stress urinary incontinence. It is very possible that before the MMK a mixed urinary incontinence was present and in retrospect it is always easy to say that the previous physicians did not do a good job. The blaming distracts from the main important message that patients with a neurogenic bladder are different from patients without a neurogenic bladder. Both referring physicians and physicians who provided the irreversible surgical treatment were responsible for the patient. This means that also the general practitioner and neurologist should be informed and know to whom they send their patients to. On a regular basis we observe maltreatment because the referring physician did not care to refer his or her patient  specifically to an expert in the field. Some attention should be given to treatment with botulinum toxin and midurethral tapes, which were also around when the bladder augmentation was given.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? Yes  Is the case presented with sufficient detail to be useful for other practitioners? Yes",350,0,1,0.7703,0.1161290323,0.8287411332,788,32.33,0.157,f1000,0.0,4,4,3,3,factual,3,3,70,polite,4,neutral,4,low,4,4,4,4,partially factual,3,3,65,polite,5,neutral,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,4.0,none,3,4,3,3,factual,3,3,65,neutral,4,neutral,3,low,2,4,3,3,partially factual,3,3,70,polite,4,neutral,4,low
120,Peter-W.-Glynn,Multi-species consumer jams and the fall of guarded corals to crown-of-thorns seastar outbreaks,"Outbreaks of predatory crown-of-thorns seastars (COTS) can devastate coral reef ecosystems, yet some corals possess mutualistic guardian crabs that defend against COTS attacks. However, guarded corals do not always survive COTS outbreaks, with the ecological mechanisms sealing the fate of these corals during COTS infestations remaining unknown. In August 2008 in Moorea (17.539° S, 149.830° W), French Polynesia, an unusually dense multi-species aggregation of predators was observed feeding upon guarded corals following widespread coral decline due to COTS predation. Concurrent assaults from these amplified, mixed-species predator guilds likely overwhelm mutualistic crab defense, ultimately leading to the fall of guarded corals. Our observations indicate that guarded corals can sustain devastating COTS attacks for an extended duration, but eventually concede to intensifying assaults from diverse predators that aggregate in high numbers as alternative prey decays. The fall of guarded corals is therefore suggested to be ultimately driven by an indirect trophic cascade that leads to amplified attacks from diverse starving predators following prey decline, rather than COTS assaults alone.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The alpheid shrimp guard, Alpheus lottini, also should be noted as defending pocilloporid corals from COTS attacks.  This shrimp guard occurs world-wide on pocilloporid corals.  It would also be worth noting the defensive behaviour, if any, of the crustacean guards toward the fish corallivores.  ‘White feeding scars’ are referred to in Fig. 1 and Fig. 2 (supplementary image).  These are difficult to make out in the photographs.  I suggest adding arrows to make these easier to see.  Also, it would be useful to know the approximate diameters of the P. eydouxi colonies.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  If applicable, is the statistical analysis and its interpretation appropriate? No  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",221,0,2,0.7659,0.1385416667,0.8811582327,29,39.63,0.1695,f1000,0.011111111111111,4,4,1,4,partially factual,3,3,70,polite,3,neutral,4,moderate,5,5,4,5,factual,5,5,95,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,4,factual,4,4,75,polite,4,positive,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
106,Reviewer-pc5v,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper presents the Ranking-Constrained Actor-Critic algorithm, an offline reinforcement learning approach for optimizing Mixed Integer Linear Programs (MILPs). Traditional MILP solvers depend on hand-crafted heuristics for branching, limiting their efficiency and generalizability. Recent deep learning methods rely on high-quality training data, which can be scarce, particularly for large problems. The key contributions of the paper are the development of the new RL algorithm and its ability to efficiently learn branching strategies even from sub-optimal training data. The algorithm outperforms previous methods in terms of prediction accuracy and computational efficiency across various MILP problems, addressing the limitations of traditional solvers. This paper claims to be innovative by being the first to apply offline reinforcement learning algorithms in branch-and-bound methods. Furthermore, the essence of the proposed method lies in further refining the dataset, specifically selecting the top-k actions in the set Gω for Bellman operator operations. This can effectively enhance the performance of the branching strategy. I believe this perspective can also be inspiring for similar problems in other domains. This paper proposes training branch-and-bound strategies using offline reinforcement learning. However, in practice, interacting with solvers is relatively straightforward, and under these circumstances, using online reinforcement learning may yield better performance. The authors need to clarify the necessity of utilizing offline reinforcement learning. •	Considering that interacting with solvers online is convenient, is there a necessity to use offline reinforcement learning to train branch-and-bound strategies?
•	In Equation 7, when k is small, the distribution of Q-values over the dataset will be centered around -δ, which is unfavorable for training. How do the authors ensure training effectiveness in this scenario?
•	I believe that the essence of the method proposed by the authors lies in further refining the dataset, specifically selecting the top-k actions in Gω for Bellman operator operations. I am curious to know if, after obtaining the top-k actions in Gω, simple imitation learning on these state-action pairs would yield similar results as the current approach. In other words, my question is whether the key to the effectiveness of this algorithm lies in the dataset refinement rather than offline reinforcement learning. I suggest that the authors conduct further ablation experiments to validate this idea.",365,0,0,0.804,0.0780772006,0.9679618478,49,20.8673,0.1507,iclr,0.0,4,4,3,4,partially factual,3,4,70,neutral,3,neutral,4,moderate,5,5,5,5,factual,5,5,100,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
106,Reviewer-s5Ux,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This work proposes the usage of offline reinforcement learning for variable selection in the branch-and-bound algorithm. To do so, they introduce a novel offline algorithm that uses a classifier to determine whether a state-action pair is in the offline dataset. Their offline Q-values are now restricted towards picking only the top-k most likely actions for each state. The usage of offline reinforcement learning seems more fitting than current imitation learning algorithms due to its lack of reliance on high quality demonstrations. - The paper is a little unclear at some points. For instance, in the last paragraph of Section 2.2: Which variables are the selected ones? Just from the node chosen by the node selection policy, or all variables across the entire tree? In general, the distinction between node selection and variable selection doesn’t become clear: Does the method also do node selection (by picking variables from the entire tree), or just variable selection?
- Further, it is not exactly clear whether there is a single model trained and evaluated on all instances, or multiple independent models trained on and evaluated on individual datasets.
- One missing benchmark is the utilization of an off-the-shelf offline RL algorithm, such as conservative Q-learning as a baseline for the specific utility of RCAC over more established offline-RL algorithms (I.e. is the improvement in performance due to offline-RL or RCAC specifically?).
- The testing set is also rather small: 10k training instances, 2k validation instances and, 20 test instances is a strange ratio.
- The reward function is also a little bit strange: Why consider the dual bound, but ignore the primal one completely? Further, these bounds are not scale-invariant, meaning that the same problem, modulo a constant scalar, could have different dual bound improvements. Even if one takes care to normalize the objective vector c beforehand, most solvers like SCIP rescale this vector for increased numerical stability. Depending on which problems are chosen, the range of rewards across different instances might also be massive depending on the duality gap. However, we agree with the authors that this metric is still better than tree-size or number of nodes.

Some minor points:
- Abstract: hand-craft\[ed\]
- Intro: The sentence “All of these models are trained…” needs a re-write
- Intro: “To our knowledge, … to apply offline RL to MILP solving” (re-write)
- Sec. 2: typo pseudocsot
- Sec. 2.2. A\[n\] MDP
- Equation 4: one closing brace is too much (after $Q_\theta$)
- Sec. 3.1: when a\[n\] MILP instance
- Sec 3.1: discounted factor $\rightarrow$ discount factor
- Sec 3.3: citation of Gasse et al.: use cite instead of citep; same again happened in Sec. 4.1
- Sec. 4.1: please use cite and citep depending on how you add these citations into the text
- Sec. 5.2 does not add any benefit to the paper and can be omitted in its current state - Which set of variables if being selected from?
- What is the performance of other offline-RL algorithms?
- Can you evaluate on a larger testset?
- Why only look at the dual bound improvement (alternative: optimality gap between primal and dual)?
- In Sec 3.2. “In fact, a good action does no harm to policy optimization even if it is an OOD action” – can you please elaborate on this a bit more?",553,0,4,0.8156,0.0484206349,0.8696163893000001,49,48.758,0.2567,iclr,0.04,4,4,3,4,partially factual,3,3,75,neutral,3,negative,4,low,4,4,4,4,factual,4,4,82,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
106,Reviewer-3oNR,Learning to Branch with Offline Reinforcement Learning,"Mixed Integer Linear Program (MILP) solvers are mostly built upon a branch-and-bound (B\&B) algorithm, where the efficiency of traditional solvers heavily depends on hand-craft heuristics for branching.  Such a dependency significantly limits the success of those solvers because such heuristics are often difficult to obtain, and not easy to generalize across domains/problems.  
Recent deep learning approaches aim to automatically learn the branching strategies in a data-driven manner, which removes the dependency on hand-crafted heuristics but introduces a dependency on the availability of high-quality training data. Obtaining the training data that demonstrates near-optimal branching strategies can be a difficult task itself, especially for large problems where accurate solvers have a hard time scaling and producing near-optimal demonstrations.  This paper overcomes this obstacle by proposing a new offline reinforcement learning (RL) approach, namely the \textit{Ranking-Constrained Actor-Critic} algorithm, which can efficiently learn good branching strategies from sub-optimal or inadequate training signals. Our experiments show its advanced performance in both prediction accuracy and computational efficiency over previous methods for different types of MILP problems on multiple evaluation benchmarks.","This paper studies the problem of learning variable selection policies for mixed-integer linear programming (MILP). The authors propose an offline reinforcement learning (RL) approach to learn branching strategies from sub-optimal or inadequate training signals. Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The paper is easy to follow.
2.	Experiments demonstrate the proposed method outperforms baselines on various benchmarks. 1.	The novelty of the proposed method is incremental, as the proposed method is a simple application of offline reinforcement learning methods to branching strategies learning.
2.	The authors claim that the proposed method is the first attempt to apply the offline RL algorithms to MILP solving. However, I found one previous work \[1\] applies offline RL methods to branching strategies learning as well. 
3.	The authors may want to explain the novelty of their method over the work \[1\] in detail.  
4.	The experiments are insufficient. First, the authors may want to evaluate their method on the load balancing dataset from the ML4CO competition as well. Second, the baselines are insufficient. The authors may want to compare their method to the work \[1\]. Third, the authors may want to evaluate the generalization ability of the learned models.

\[1\] Huang, Zeren, et al. ""Branch Ranking for Efficient Mixed-Integer Programming via Offline Ranking-Based Policy Learning."" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Cham: Springer Nature Switzerland, 2022. Please refer to Weaknesses for my questions.",239,4,8,0.6839000000000001,0.0766666667,0.89818573,49,40.7962,0.1719,iclr,0.0,4,4,3,4,factual,3,4,75,neutral,4,neutral,3,low,5,4,3,5,partially factual,4,4,80,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,4,85,polite,5,neutral,4,low
172,Reviewer-LoZH,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This work studies the impact of imposing known conditional independence structure in fully-connected neural network architectures, notably in the setting of autoregressive normalizing flows. The independence is imposed by masking the weight matrices in the linear layers, similar to the MADE approach. The masks are determined by a factorization of the known adjacency matrix that (approximately) maximizes the number of connections (paths) between inputs and outputs. Through several experiments, the authors show generalization improvement over baselines.

**Edit** Most of my concerns were addressed during the rebuttal/discussion period. I am therefore upgrading my score from 4 to 6. * The manuscript is well written, with standard notations.
* The contributions are clearly defined and the assumptions (known adjacency) are explicit.
* The proposed approach to impose conditional independences is sound.
* The claims are supported by the experiments, notably concerning improved generalization. The idea of imposing prior independence knowledge into autoregressive flows was first introduced by Wehenkel and Louppe (2021), cited as \[25\] in this manuscript. As the authors mention (lines 246-247), StrAF only differs from GNF in the approach to impose the independences, but is conceptually identical. Actually, Wehenkel and Louppe (2021) already propose the approach of the present work:

    > An alternative approach would be to use masking scheme similar to what is done by Germain et al. (2015) in MADE as suggested by Lachapelle et al. (2019).
    
In addition, the official \[UMNN repository\](https://github.com/AWehenkel/UMNN), also cited in this work, links to the normalizing flow library \[Zuko\](https://github.com/francois-rozet/zuko), from the same lab. The latter library implements autoregressive flow conditioners as masked multi-layer perceptrons for which the masks are a factorization of the adjacency matrix between the inputs and outputs. The similarities with the proposed StrNN are too strong to be left unaddressed. * Algorithm 1: It is not clear to me how the factorization algorithm is applied when the StrNN has more than one hidden layer.
* Section 5.1: Are the same number of layers/neurons used for StrNN and MADE in this experiment?
* Line 317: ""As GNF permutes variables between flow steps"". I was not able to find a mention of this in \[25\].
* Figure 5/Table 1: I don't understand how ""ARF-10"" and ""GNF-10"" can be so much worse than ""GNF-1"", as they are strictly more expressive. Is it an overfitting issue? Or maybe an invertibility issue? UMNN is not always numerically invertible. What about ""ARF-1""?
* Table 1: The authors make a distinction between ""density estimation"" and ""sample quality"", which does not make sense to me. If a flow perfectly estimates the density, it necessarily generates probable samples, unless the invertibility is not guaranteed.
* Why not studying the use of StrNN in other settings than density evaluation, such as physics-informed machine learning, where it is common to infuse prior knowledge in the structure of the neural networks? * It is never mentioned that a StrNN is a (pruned) fully-connected network with element-wise activation functions. The approach does not apply to convolutional, attention-based or recurrent networks, and does not support skip/residual connections or normalization layers.
* This is not a limitation of this work, but one should be careful not to confuse Bayesian networks and causal graphs. A Bayesian network (or its adjacency matrix) over variables merely indicates independencies between the variables but in no way causalities.",549,8,1,0.7869,0.0911904762,0.8084603548,215,39.2951,0.0376,neurips,0.0,4,4,4,4,factual,4,3,80,polite,4,positive,5,low,5,5,5,5,factual,5,5,90,polite,5,positive,5,none,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
172,Reviewer-icwN,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","The authors propose a novel method for constructing structured neural networks (strNN) that are able to respect causal independencies between variables. Formal constraints for weight mask creation are discussed and evaluated empirically for an exact method and a greedy algorithm. The conditioned strNN are furthermore leveraged for conditioning autoregressive flow models. Practical application successfully is demonstrated over multiple synthetic datasets with comparisons between several baseline models, with and without the use of adjacency information. To the best of my knowledge related work is discussed sufficiently in the context of causal density estimation. 1. The authors propose a novel method for constructing structured models via weight masking that integrates seamlessly with existing neural architectures while respecting the strict independence assumptions of causal models. Preconditions and assumptions for application of the approach, specifically knowledge about the causal graph structure, are clearly stated.

2. To tackle the infeasibility of exact mask creation on larger graphs a greedy algorithm is proposed and its practical application is demonstrated.

3. The authors additionally include experiments on binary MNIST data, for which the underlying causal structure is unknown. By imposing a causal graph structure which promotes the usage of spatially local information, the authors improve performance for non-synthetic data over baselines. The example shown in Figure 1 decomposes the network into two separate networks. However, constructing the displayed a network would not require a complicated mask decomposition, but could be trivially solved by constructing two independent networks with constrained layer width. Only by inspecting the example provided in the appendix it is revealed that the presence of split-structures leads to shared weights between the outputs. 1. As causal mechanisms are often assumed to be independent in causal literature, I would like to ask the authors about the benefits or downsides of allowing for such shared weights within the network.

2. Furthermore, I would like to ask the authors to discuss possible simplifications for specific causal structures, e.g. in the case of independent causal mechanisms as seen in Figure 1.

Overall the idea is pretty good with a clever way of enforcing the causal independencies. However, all of this is assuming that the networks can leverage shared information between different mechanisms. If that is not the case then you could just train an independent density estimator for every single edge and (from a purely causal perspective) the problem becomes trivial to solve. No concerns here",397,0,6,0.8015,0.1236507937,0.8783051968000001,215,27.0767,0.1898,neurips,0.0,3,3,3,3,factual,4,4,70,polite,4,positive,4,low,4,4,4,4,partially factual,4,4,85,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,partially factual,4,3,85,polite,5,positive,4,low
172,Reviewer-Wq9i,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","In this paper, the authors present a neural network architecture that can fulfill the bayesian DAG conditional independencies needed for normalized density estimation.
In this work, the authors start from a binary lower adjacency matrix that encodes the independencies of a bayesian network DAG. Then they introduce a factorization of the global adjacency matrix into L adjacency matrices, which can be used as masks on each layer. This construction allows neural networks to be trained for a normalized density estimation task.
The authors introduce an heuristic to exactly factorize the adjacency matrix for the different layers using two objective functions.
With this building block the authors proceed to create a normalizing flow architecture that respects the independency restrictions end-to-end.  The authors then compare their approach empirically on different task against MADE, a neural density estimator that allows general dependencies for a given random variable ordering.  I found the paper insightful, and the results show that the approach is beneficial. The paper is technically sound and easy to read. The results showing an improvement in data efficiency for a given negative likelihood are also very interesting.

The introduction of the normalizing flow approach and the comparison in the causal setting are also nice additions that can have impact in the broader community.

The experiment on the sample quality shows the benefit of restricting the dependencies in the network as it cuts paths for noise in other random variables (and feature transformations) to propagate through the network. The major weakness of this paper is the limited empirical section in comparison to other papers in this domain. This can cause readers to wonder if the benefits of the new approach as density estimators are not significant for other datasets.
A broader comparison on other datasets would make the paper more robust. 

Also and I'm considering this as a minor weakness in my review, is that the method although insightful does not provide a way to obtain the global adjacency matrix.

 As I was reading the paper, my first thought would be that you would explore the possibility of discovering the dependencies for a given order. 
Here one can start with a dense adjacency matrix, train the network, clip nodes in the layers according to Lottery Ticket Hypothesis, and propagate the masks forward, i.e., multiplying all the masks to get the adjacency matrix. As you are clipping, the adjacency matrix is guaranteed to either be the same or introduce independencies. 
At that point, you can even use your factorization algorithm again to obtain a network with more/other nodes active while still respecting the new independencies. 
I'm wondering if you explored similar ideas during your research?
 The main limitations of the approaches presented are inherited from the restrictions on ordered models, e.g., marginalization and map queries are intractable for the general case. This is not mentioned in the paper. 

There are no potential negative societal impact to this work.",484,0,0,0.7857000000000001,0.0355132962,0.8869557381000001,215,36.2197,0.1669,neurips,0.0,4,4,4,4,factual,4,4,90,polite,4,positive,4,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,positive,5,low
172,Reviewer-Tj4q,Structured Neural Networks for Density Estimation and Causal Inference,"Injecting structure into neural networks enables learning functions that satisfy invariances with respect to subsets of inputs. For instance, when learning generative models using neural networks, it is advantageous to encode the conditional independence structure of observed variables, often in the form of Bayesian networks. We propose the Structured Neural Network (StrNN), which injects structure through masking pathways in a neural network. The masks are designed via a novel relationship we explore between neural network architectures and binary matrix factorization, to ensure that the desired independencies are respected. We devise and study practical algorithms for this otherwise NP-hard design problem based on novel objectives that control the model architecture. We demonstrate the utility of StrNN in three applications: (1) binary and Gaussian density estimation with StrNN, (2) real-valued density estimation with Structured Autoregressive Flows (StrAFs) and Structured Continuous Normalizing Flows (StrCNF), and (3) interventional and counterfactual analysis with StrAFs for causal inference. Our work opens up new avenues for learning neural networks that enable data-efficient generative modeling and the use of normalizing flows for causal effect estimation.","This paper introduces structured neural networks such that the resulting neural network represents the factorization of a given Bayesian network. For doing so, each layer of the neural network is masked and the product of the masks of all layers must be the same as the adjacency matrix of the DAG representing the Bayesian network. 
With this construction, the represented conditional dependencies with structured neural networks will be consistent with the given Bayesian network. The paper proposes a simple greedy algorithm to find the masks. It also proposes using the structured neural network to construct the coupling layers for normalizing flow and claims that the resulting generative model is better for casual inference (intervention and counterfactuals) than the prior approach.  The paper is well-written and the contribution towards causal inference is solid.   1- The structured neural network augments MADE with a better mask construction algorithm such that the factorization can be defined for any DAG structure. However, it is not a fundamentally different model.
2- The paper didn't propose any approach for learning the structure of DAG given the provided structured neural network parametrization.
3- MADE is not a strong density estimator and comparing only to MADE does not validate the strength of structured neural networks as density estimators. 
  1) How GNF would compare to StrAF if it does permute the latent variables after the first step? 
2) During the comparison with CAREFL did you provide CAREFL with external DAG orders that StrAF uses? If not, the learned causal order by CAREFL may not exactly specify the underlying DAG, which may result in lower performance in causal inference. 
 N/A",269,0,1,0.7182000000000001,0.0411458333,0.8982758522000001,215,39.2762,0.1163,neurips,0.0,4,4,3,4,factual,4,4,64,polite,4,positive,4,moderate,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,2,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
96,Alexander-Pico,"KEGGViewer, a BioJS component to visualize KEGG Pathways","Summary: Signaling pathways provide essential information on complex regulatory processes within the cell. They are moreover widely used to interpret and integrate data from large-scale studies, such as expression or functional screens. We present KEGGViewer a BioJS component to visualize KEGG pathways and to allow their visual integration with functional data. Availability: KEGGViewer is an open-source tool freely available at the BioJS Registry. Instructions on how to use the tool are available at http://goo.gl/dVeWpg and the source code can be found at http://github.com/biojs/biojs and DOI:10.5281/zenodo.7708.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The BioJS library of components has a lot of potential. It's encouraging to see a diversity of interactive viewers already registered with BioJS. The intersection of modern JavaSript (JS) components with network biology in particular is ripe for development to bring powerful perspectives on massive biomedical datasets to researchers. I decided to critique this article introducing the BioJS KEGGViewer from three points of view to acknowledge the broad set of use cases and challenges this work takes on. While there are a number of things to improve upon (as always) and a few points requiring clarification, the project is a nice addition to the BioJS library and may provide a useful data visualization option when deployed with a complementary set of web tools for selecting pathways, managing datasets and viewing details.Generic User:The ""play"" feature is great for comparing conditions. Nicely done!Panning is tricky, I seem to have to hold cmd, click, pause, and then drag. Without the 'pause' I invoke a selection tool.There is no additional information or link-outs when you click on a node; only the gene symbol is provided.There is no interface for accessing the data values underlying the visualization. There is a disconnect between the web developer who sets up the viewer with all the underlying expression data and the end-user who views the data with only limited access and controls.Biomedical Researcher:The default expression range appears to be set at min-min, which results in all data values visualized as up-regulation. I would recommend default values centered on 0 in addition to support for user-provided parameters.Unfortunately, the parameter names and value ranges for data overlays are unnecessarily restricted to ""expression"", ""upColor"" and ""downColor"". A generic solution for data overlay that could work with any type of data (KEGGViewer shouldn't care if it's expression or not) and color gradients or discrete mapping options would be much more useful.All of these sorts of options are in fact already available in closely related tools (also free and open source, and which I happen to work on) that the authors neglected to cite: PathVisio [1] and Cytoscape [2]. These projects have both Java and JavaScript flavors. The JS version of Cytoscape was obviously used and cited in this work, but the Java version with its built-in data import, style and overlay options -- as well as KEGG import -- was missed. Speaking of KEGG, I'm dubious about the blanket statement that it is ""free of charge for academics"". It's a complicated situation that I know many colleagues are unclear about, so I think it's important to describe it thoroughly. According to their own website [3], ""Academic users who utilize KEGG for providing academic services are requested to obtain a KEGG FTP subscription for organizational use, which includes a proper license agreement."" This leads to a licensing agent with various paid subscription options [4,5]. The KEGG API, which KEGGViewer uses, is indeed freely provided for academic use, but only for individual downloads. Bulk downloads, such as those required to do analysis of over representation or enrichment, are explicitly forbidden and require a KEGG FTP subscription [6].Software Developer:It is unfortunate that the EBI host site has resources in conflict with the KEGGViewer. This seems counter to the whole point of BioJS and should be addressed in future releases of the EBI web site, cytoscape.js and/or KEGGViewer (whichever CSS is the most intrusive or classes least specific).Beyond a bit of copy/paste JS (including a 5-level deep JS object), asking users to host a php proxy will likely turn some away. Is there any way around this? References 1. http://pathvisio.org2. http://cytoscape.org3. http://www.kegg.jp/kegg/legal.html4. http://www.bioinformatics.jp/en/keggftp.html5. http://www.pathway.jp/licensing/commercial.html6. http://www.kegg.jp/kegg/rest/",666,11,6,0.8075,0.1092482363,0.9074166417,15,32.12,0.1631,f1000,0.0105263157894737,4,5,5,5,factual,3,5,70,neutral,2,negative,5,moderate,5,5,5,5,factual,5,5,100,polite,5,positive,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
105,Reviewer-XVP7,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper proposes a new topic and method for training a mask-aware CLIP, which could serve as a core component for open-vocabulary segmentation. The designed structure could be used as a flexible plug-in but brings significant improvements for existing methods on various benchmarks. 1. This topic is promising.  Previous methods decouple open-vocabulary segmentation into class-agnostic segmentation and CLIP-guided recognition. However,  most of them fail to use CLIP effectively,  I think training a mask-aware CLIP is an ideal way to deal with this problem.  

2. The model design is reasonable, using a mask2former-style network and tasks the masks to perform masked attention sounds reasonable.

3. The experiment results are great with significant improvement. It would be an ideal solution for various open-vocabulary segmentation tasks incorporating strong class-agnostic segmentation models like SAM. 

4. The paper is clearly presented.   The experiment setting is unsatisfactory, which only tackles zero-shot semantic segmentation. 
As this topic and idea are good,  I expect the authors to extend the method into open-vocabulary panoptic settings, and use some large datasets for training. Currently, the datasets used are small. it is hard to distill universal knowledge from CLIP.
 See weakness yes",191,0,5,0.8227,0.2260687229,0.9103056788,215,37.0755,0.1262,neurips,0.0352941176470588,4,4,3,4,factual,4,4,80,neutral,3,neutral,4,low,4,4,4,4,partially factual,4,4,85,polite,5,positive,5,moderate,3.0,5.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,4,positive,4,low
105,Reviewer-uMA5,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","This paper mainly discusses how to use pre-trained CLIP to solve zero-shot segmentation task, and proposes a new method called Mask-aware Fine-tuning (MAFT) to address the issue of significant false positives in CLIP's classification of mask proposals. Specifically, the paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) to handle any number of images and mask proposals simultaneously, and designs mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder, ensuring that CLIP responds to different mask proposals without sacrificing its transferability. 1. This paper introduces an Image-Proposals CLIP Encoder (IP-CLIP Encoder) that is sensitive to different mask proposals.

2. This paper includes mask-aware loss and self-distillation loss to fine-tune the IP-CLIP Encoder without sacrificing its transferability.

3. The paper is well-written and easy to follow. 1. The ability to handle any number of mask proposals is not unique to this method and has already been a feature of previous methods such as ZegFormer.

2. The main effect of this method comes from the mask-aware loss, which utilizes mask proposals as prior knowledge to obtain more accurate prediction probabilities from the cls score map. Therefore, the effectiveness of this loss function is limited by the quality of the mask proposals, which limits the innovation of this paper.

3. In terms of experiments, it is necessary to conduct experiments on the updated methods such as ""Scaling Open-Vocabulary Image Segmentation with Image-Level Labels""(ECCV2022) where the performance of it has already surpassed this method on VOC and COCO.

4. Why is Table 2's benchmark experiment conducted under the setting of using only CLIP classifier? Same as weakness. The paper has a description of some limitations.",271,0,8,0.7274,0.0726217532,0.9545752406,215,30.0096,0.072,neurips,0.0,3,4,4,3,partially factual,3,4,75,neutral,3,neutral,4,low,4,4,4,4,partially factual,4,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
105,Reviewer-q7Qn,Learning Mask-aware CLIP Representations for Zero-Shot Segmentation,"Recently, pre-trained vision-language models have been increasingly used to tackle the challenging zero-shot segmentation task. Typical solutions follow the paradigm of first generating mask proposals and then adopting CLIP to classify them. To maintain the CLIP's zero-shot transferability, previous practices favour to freeze CLIP during training. However, in the paper, we reveal that CLIP is insensitive to different mask proposals and tends to produce similar predictions for various mask proposals of the same image. This insensitivity results in numerous false positives when classifying mask proposals. This issue mainly relates to the fact that CLIP is trained with image-level supervision. To alleviate this issue, we propose a simple yet effective method, named Mask-aware Fine-tuning (MAFT). Specifically,  Image-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary numbers of image and mask proposals simultaneously. Then, *mask-aware loss* and *self-distillation loss* are designed to fine-tune IP-CLIP Encoder, ensuring CLIP is responsive to different mask proposals while not sacrificing transferability. In this way, mask-aware representations can be easily learned to make the true positives stand out. Notably, our solution can seamlessly plug into most existing methods without introducing any new parameters during the fine-tuning process. We conduct extensive experiments on the popular zero-shot benchmarks. With MAFT, the performance of the state-of-the-art methods is promoted by a large margin: 50.4\% (+ 8.2\%) on COCO, 81.8\% (+ 3.2\%) on Pascal-VOC, and 8.7\% (+4.3\%) on ADE20K in terms of mIoU for unseen classes. Codes will be provided for reproducibility. Code is available at https://github.com/jiaosiyu1999/MAFT.git .","The paper proposes a mask-aware fine-tuning method to address challenges faced by frozen-CLIP-based zero-shot segmentation methods. It addresses the problem of CLIP being insensitive to different mask proposals and tending to produce similar predictions regardless of the variation in proposals. The proposed IP-CLIP successfully assigns appropriate scores to different proposals, unlike the frozen CLIP that exhibits similar scores. Instead of processing each mask individually, the proposed modified CLIP considers all mask proposals simultaneously, thereby reducing computational costs. The experimental results consistently demonstrate that the proposed method outperforms the baselines by a significant margin.  - The proposed method is designed as a plug-and-play approach, making it applicable to any frozen CLIP-based method.
- The proposed method consistently improves the performance of baseline methods, including SegFormer, ZSSeg, and FreeSeg, by substantial margins, particularly on unseen classes.
- The method significantly reduces the computational requirements of CLIP in FreeSeg, and the effectiveness of the proposed mask-aware loss and IP-CLIP is demonstrated through ablation studies.
 - The starting point of the mask attention layer L is determined by a user-defined hyperparameter. The proposed method specifically employs ViT-B/16 as the backbone in the paper. However, if a different backbone is utilized, the selection of this hyperparameter would necessitate a hyperparameter search.

- The notation presented in the paper would be better if it were simplified and clarified. - Including experiments with other backbones and proposal generators would enhance the comprehensiveness of the paper
 The limitations are briefly discussed in the paper, while the societal impact is not addressed.",253,0,0,0.7326,0.174537037,0.9198931456,215,18.35,0.0945,neurips,0.0,3,4,4,3,factual,3,4,85,neutral,4,neutral,5,none,4,4,4,5,5,5,5,88,polite,5,positive,4,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,3,4,4,4,factual,4,4,88,polite,5,positive,5,low
101,Reviewer-ST3b,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","This paper investigates the theoretical limitations of the current external censorship mechanisms in LLMs from the view of computing theory. Given these inherent limitations, the authors argue that LLM censorship should be addressed more as a security problem than a machine learning problem. - Trendy topic
- A novel perspective to study LLM censorship - Implications can be extended
- Readability can be improved In this paper, the authors first focus on the semantic censorship mechanisms, proving that the current mechanisms cannot reliably detect if LLM output is ""semantically impermissible."" They further show that such limitations are inherent and can extend beyond semantic censorship mechanisms by designing Mosaic prompts.

Overall, the authors study a trendy topic and offer a novel perspective to understand LLM censorship. However, I have the following concerns.

- The authors prove the impossibility of semantic censorship using string transformation by showing how the transformed string might break the ""invariance of semantic censorship."" Here, I have some doubts regarding the invariance property. In my opinion, the semantics of a string often change after the transformation. Thus, it is reasonable for the transformed string to bypass semantic censorship mechanisms. Moreover, LLMs do not necessarily output harmful texts with the transformed string. Why does the invariance property hold? Is this property an important goal considered by LLM censorship developers when designing their mechanisms?

- Implications can be extended. It appears to me that the current implication discussion stops at showing LLM censorship is more of a security problem than a machine learning problem. What are the direct implications for model developers when building censorship? Are there any defensive measures against the Mosaic prompts? The authors only briefly mention that there are standard approaches, such as access controls and user monitoring, to build censorship from the security view. However, there is no further analysis showing that these approaches can indeed overcome the theoretical limitations of current external censorship mechanisms and surpass them in censorship performances.

- Readability can be improved. Many sentences are too long and difficult to read. For example, ""Thus, we can understand censorship as a method of determining permissibility of a string and censorship mechanisms can be described as a function, f(x), restricting the string x to the set of permissible strings P by transforming it to another string x' ∈ P if necessary, e.g. x' ='I am unable to answer.'""",394,0,0,0.787,0.0838709677,0.8256777525000001,47,29.8058,0.1199,iclr,0.0196078431372549,2,4,2,2,factual,3,4,60,neutral,3,neutral,2,moderate,4,3,4,4,partially factual,4,4,75,polite,5,neutral,4,moderate,2.0,3.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,low,3,3,4,4,factual,4,4,80,polite,5,neutral,4,low,2,3,3,4,partially factual,3,3,70,polite,5,neutral,4,low
101,Reviewer-3fNc,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","The paper's topic studying censorship and its effectiveness is interesting, ie. what kinds of knowledge can be extracted from LLMs and whether protection mechanisms can be circumvented. But the paper contributes little of practical value. It also lacks a proper evaluation to claims and conceptual illustrations. The theoretical treatment would be interesting, but the paper claims are mostly direct implications of existing theorems or require minor enhancements. Overall, the contribution appears marginal.

Details:
* abstract:  LM -> LLM or define it.
*  The example, Figure 1 is not of any practical value and might be conceptually it is flawed - the three steps are the least challenge in making successful ransomware attack (deploying it is much more of an issue, avoiding being detected too). The Mosaic prompt is also not very convincing. Both should be shown to be actually working.
* The idea to use encryption (Appendix A) is interesting, but is this a practical concern? Does it add to the discussion of how protection mechanisms can be circumvented? It might, if it was shown to work. But as is, it seems incomplete.
* On a high level, the paper argues that censorship cannot work because a malicious person might not directly asked for censored actions, but for steps needed for these actions, which might not be censored. But this holds for almost anything in our world and is nothing new. Any technological knowledge can be abused.  A knife can be used to kill or to save a life (doctor during surgery).  A motor can power an ambulance saving life or a truck performing a terrorist act. This is general knowledge. The paper seems to sell this as a novel aspect. The fundamental question is: Should knowledge and technology be made available that can be abused?  This is also not really a security question as the paper argues. Obviously any abuse relates to security, but I don't see, why the paper's claim to say ""LLM censorship (ie. avoiding censorship through attacks) is a security concern"" should be a new insight. see above see above see above",346,0,1,0.7665000000000001,0.0904969069,0.7391343713,47,52.9766,0.0291,iclr,0.0306122448979592,2,3,3,2,unfactual,2,2,60,neutral,4,negative,2,high,3,3,3,2,partially factual,3,3,55,impolite,4,negative,4,moderate,1.0,4.0,3.0,2.0,partially factual,3.0,2.0,60.0,neutral,4.0,negative,4.0,low,2,3,3,2,partially factual,3,3,50,neutral,4,negative,3,moderate,2,4,3,2,partially factual,3,3,60,neutral,4,negative,4,low
101,Reviewer-xHLz,LLM Censorship: The Problem and its Limitations,"Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, and LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs from a collection of permissible ones. As a result, we propose that the problem of censorship needs to be reevaluated, and viewed as a security problem with adaptation of security-based defenses to mitigate potential risks.","This paper explores some of the theoretical limitations of LLM censorship, the problem of identifying permissible inputs and outputs to language models. In particular, the paper focuses on the limitations of semantic censorship, or filtering of strings based on their meaning. First, the paper shows that determining whether a “program” output by an LLM is permissible is an undecidable problem. Then, the paper discusses the impossibility of semantic censorship by showing that strings can undergo transformations which preserve their semantic meaning but are otherwise unintelligible except to a user who knows how to invert the transformation. Finally, the paper introduces Mosaic Prompts, a way of breaking up an impermissible prompt into permissible pieces. This paper’s primary strength is that it identifies an important issue to focus on that has been unexplored in the literature - what are the theoretical limits on the ability to filter LLM inputs or outputs based on their semantic meaning? The paper is a good exposition of this problem and the theoretical settings it considers highlight some important limitations for the task. The figures and tables also do a good job of clarifying some of the concepts in the text. Overall, the authors’ assertion that syntactic censorship is likely to be more successful than semantic censorship is well-taken from this work. This paper’s primary weakness is the number of assumptions and limitations that come into the different theoretical treatments that the paper covers. First, the paper itself admits that the treatment of Rice’s theorem for programs on Turing Machines is not generally applicable to the bounded inputs and outputs case of LLMs. Second, in the section 2.2 on the invertible transform, I believe there may be a flaw in the reasoning of the proof. Under assumption 1, the authors assume that the model is capable of following instructions such that it can produce the transformation $g$. This assumption is explicitly stated. It seems that the proof also requires that the LLM (or corresponding companion LLM that is doing censorship) is unable to compute the inverse transformation $g^{-1}$. If it were, then it could check the semantics of the un-transformed string for permissibility. This assumption weakens the power of the impossibility result in my opinion. Finally, while I think that the Mosaic Prompt approach is interesting, I do think the paper underestimates the LLM’s ability to attend to previous prompts. While in the mosaic approach the model is likely to answer early prompts, it is conceivable that once enough of the pieces of the impermissible prompt are present, one would be able to detect the impermissibility of the conversation overall. Does the impossibility result in Section 2.2 require an assumption that $g^-1$ is not computable by the permissibility model?

Is the problem space simplified at all by considering the compositionality of strings? For example, if there is an impermissible substring within a larger string, does that make the larger string automatically impermissible as well?

Does something like “fuzzy” permissibility fit into this framework at all? For example, many prompts and outputs would be considered “borderline” or have some level of “toxicity” if sent to a human rater, rather than a bright-line permissible vs. not rule. Does that make the problem any easier or harder?",537,0,0,0.7747,0.1567073171,0.8508368134000001,47,36.7413,0.0657,iclr,0.01,2,5,4,3,factual,4,4,80,polite,4,neutral,4,none,3,5,4,5,5,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
152,Ruth-E-Timme,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Boykin and co-authors present results from a pilot study exploring the use of MinION technology to detect a plant viral pathogen in real-time. Their data shows a huge advantage of using this hand-held sequence technology over the standard PCR methods. The authors propose implementing a MinION quality-control step on the plants before they are distributed, so that CMD-infected plants can be removed from the supply chain before they reach local farmers. This report is short, but its impact appears to be far-reaching. Most of my comments are minor editorial suggestions, but overall the writing and readability is excellent.  Minor revisions: Public availability of the DNA sequence data. While the authors technically made the sequence files public by posting to FigShare, the standard repository for DNA sequence data is the INSDC (NCBI/EBI/DDBJ).  I highly urge the authors to create a BioProject at NCBI or EBI that houses the raw and assembled sequences (fasta files) for this effort so that other researchers in this area can easily build off this important work.  Editorial comments: Results and Discussion “We utilized the MinION to test infected material and farmers were informed within 48 hours of the specific strain of the virus that was infecting their cassava, and a resistant cassava variety was deployed.” Consider converting to two sentences. One about using the MinION and the second to cover the response.  Were resistant cassava plants really deployed within 48 hrs?  wow.  “MinION sequencing is superior to traditional methods of PCR identification, given its generation of whole genome sequences which enable the identification of the plant virus strain even if it becomes mutated or divergent, as it is not biased using primers that rely on known virus sequences.” Consider a minor re-write: “In general MinION sequencing is superior to traditional PCR methods of identification because the virus can be detected even when the PCR primers don’t work, and 2) entire viral genome sequence is generated enabling the identification of the specific viral strain, along with other molecular information, which allows for a much higher resolution of surveillance.  “In addition, we could detect virus in a plant before it showed symptoms (Table 1).”  Change to present tense to match the rest of the paragraph?  “Utilizing traditional PCR methods, three samples collected from farmer 1’s field in Tanzania tested positive for EACMVs and none were positive for ACMV.” Define EACMV and ACMV before abbreviation.  Methods:  “In Tanzania, three cassava mosaic disease (CMD) symptomatic cassava leaf samples (Figure 1, Table 1) were collected from the smallholder cassava farmer 1’s field in Bagamoyo.”  CMD already defined in Intro.  “In Tanzania and Kenya, two primer pairs: EAB 555F/EAB 555F12 and JSP001/JSP00213, which amplify 556 bp and 774 bp, respectively, were used to detect East African CMVs (EACMVs) and African CMVs (ACMVs), respectively.”  Use the abbreviations here after adding the full names to the Results.",540,0,0,0.8038000000000001,0.1013716889,0.8762588501,8,30.7,0.1342,f1000,0.01010101010101,4,3,4,4,factual,4,3,70,neutral,4,positive,4,low,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,4.0,90.0,polite,5.0,positive,4.0,none,4,5,4,5,factual,5,4,85,polite,5,positive,5,low,5,5,4,5,factual,4,4,92,polite,5,positive,5,low
152,Alfonso-Benítez-Páez,Real time portable genome sequencing for global food security,"Crop losses due to viral diseases and pests are major constraints on food security and income for millions of households in sub-Saharan Africa (SSA). Such losses can be reduced if plant diseases and pests are correctly diagnosed and identified early. Currently, accurate diagnosis for definitive identification of plant viruses and their vectors in SSA mostly relies on standard PCR and next generation sequencing technologies (NGS). However, it can take up to 6 months before results generated using these approaches are available. The long time taken to detect or identify viruses impedes quick, within-season decision-making necessary for early action, crop protection advice and disease control measures by farmers. This ultimately compounds the magnitude of crop losses and food shortages suffered by farmers. The MinION portable pocket DNA sequencer was used, to our knowledge globally for the first time, to sequence whole plant virus genomes. We used this technology to identify the begomoviruses causing the devastating cassava mosaic virus, which is ravaging smallholder farmers’ crops in sub-Saharan Africa.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  Summary of the article Boykin et al present a pilot study aiming the application of real-time DNA sequencing for the detection of ACMV and EACMV in cassava plants in multiple crops of African East countries. This study represents the successful approaching of the most valuable feature of the MinION nanopore sequencing platform, its portability. At the same time, the authors made the maximum use of the singularities of the system by the translational application of their results, thus preventing the spread of plant viruses and to improve the crop efficiency by timely advising of farmers. This last exercise really highlights the value of such technology, particularly in the epidemiological surveillance and control of pathogens. Notwithstanding, I have some minor concerns that if addressed they would constitute an added value to the approach described. 1) I strongly recommend that authors store and make publicly available the genetic information retrieved from different sequencing runs to a specialized repository such as ENA or GenBank. 2) It would be very informative for future studies in this field that Table 1 contains additional information the average or median values of the sequence identity derived from the comparison between nanopore reads and reference sequences. In a similar way, they should declare the level of relationship between ACMV and EACMV, in terms of genome-wide nucleotide identity, in order to disclose any potential misidentification given the high error rate of nanopore-derived DNA reads. 3) The authors must be aware that there is a strong effect derived from sequencing kits used, different from Uganda/Tanzania and Kenya. They should make some correlations between the CMD severity scoring and DNA reads retrieved independently accordingly to the kits used. 4) In the same line of thoughts than above, it would be very elegant that authors will estimate the maximum time for expecting a viral DNA read just for setting a threshold and optimize the sequencing time. I have noticed that for CMD severity = 1, it took maximum 4h to retrieve viral DNA reads, using the sequencing kit SQK-RBK001. Another different history was the utilization of SQK-RBK004 in Kenya, where apparently there is not a correlation between CMD severity and viral DNA reads retrieved. In the last cases, the apparently not symptomatic plants were detected as positive in less than one hour. The setting of a time threshold for a proper detection (getting enough number of reads to estimate reliable identification) would be useful to speed up the farmers' advising and consequently the reduction of risks for the spread.",486,0,0,0.8153,0.1035542929,0.8796175122000001,37,23.8,0.103,f1000,0.0104166666666666,4,4,4,3,partially factual,3,3,68,neutral,4,positive,3,low,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,4.0,4.0,4.0,3.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,5,4,4,5,factual,4,4,85,polite,5,positive,5,low,5,4,4,5,factual,4,4,92,polite,5,positive,5,low
132,Maxime-Lefrançois,Ontologies for Observations and Actuations in Buildings: A Survey,"Spaces and elements in the buildings' environment have emerged as platforms where materializations of observations and actuations promise to be very profitable. The advent of the Internet of Things (IoT) paves the way to address this challenge but the heterogeneity of the represented knowledge about these artifact systems poses a real problem. Ontologies can be considered as part of the solution to overcome the IoT's inherent hurdles. A wise option promoted by recent approaches is to design networks of complementary ontologies. However, different points of view are possible and such diversity could lead to interoperability problems. This article advocates for a networked ontology infrastructure conceived on principled basis guided by documented judicious conceptualizations. In this regard, this survey points towards ontologies involved in conceptualizations of observations and actuations, where the utility of that conceptualization arises when some features of interest need to be observed or acted upon. For each of the reviewed ontologies, their fundamentals are described, their potential advantages and shortcomings are highlighted, and the use cases where these ontologies have been used are indicated. Additionally, use case examples are annotated with different ontologies in order to illustrate their capabilities and showcase the differences between reviewed ontologies. Finally, this article tries to answer two research questions: Is there a firm basis, broadly admitted by the community, for the development of such a networked ontology infrastructure for the observations and actuations in buildings? What ontologies may be considered helpful towards that goal?","In its current form, the article is a comprehensive comparative review of important ontologies that may be used to model observations and actuations in buildings. I believe that it is clearly useful as an introductory text for PhD students and researchers interested in this domain. The authors did addressed or answer each of the reviewers remaining comments.  In particular, I consider now that the abstract and introduction do clearly justify and contextualize the importance of the survey. The research questions, methodology, and scope, of the review are also clearly described.  The paper being 28 pages with 86 references, I do not agree that it can be qualified as ""merely an extension of the related work section of the initial submission"". The authors have put substantial effort to abstract the paper from the original goal, which was to introduce the EEPSA ontology. I see absolutely no research bias if the authors have already at hand an ontology that fills some of the representational gaps identified in the survey paper. Therefore, I consider that some of the main criticisms made on the first revision of this paper are stale.  Those criticisms that are not related to the goal of the initial submission have been clearly addressed in the paper, or answered to in the letter to the reviewers.  As a result, I do recommend to accept this paper.",226,0,0,0.7646000000000001,0.0615740741,0.93345052,52,42.21,0.1953,semanticweb,0.0275229357798164,0,4,1,0,unfactual,3,1,30,polite,2,positive,3,high,3,5,5,5,factual,5,5,95,polite,5,positive,5,none,3.0,4.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,2,4,4,3,factual,4,4,80,polite,5,positive,3,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
25,Ravi-Dadlani,Case Report: Sciatic nerve schwannoma - a rare cause of sciatica,Herein we report a rare case of a sciatic nerve schwannoma causing sciatica in a 69-year-old female. Sciatic nerve schwannoma is a rare entity. It should always be considered as a possible cause of sciatica in patients that present with symptoms of sciatica with no prolapsed disc in the lumbar spine and a negative crossed straight leg raise test. Timely diagnosis and complete excision of the lesion leads to complete resolution of the symptoms of such patients.,"Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I congratulate the authors on an interesting case report. It is a well written report but I would like to suggest a few additional points.  Since several such case reports have been published earlier, it would be interesting if the authors could add a 'review of literature'. A single tabulated format with some interesting characteristic, such as  the exact location of the tumor along the course of the sciatic nerve. It would also be interesting to see a small table with other 'sciatica mimicks'. Personally I have seen lumbosacral plexus tumors presenting with sciatica.  The article may be accepted for indexing with these minor additions.",172,0,0,0.7861,0.0818681319,0.7953575253,1,32.73,0.1213,f1000,0.0099009900990099,4,3,4,3,unfactual,3,3,70,neutral,4,positive,1,low,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,4.0,4.0,4.0,factual,4.0,5.0,80.0,polite,4.0,positive,3.0,none,4,5,3,4,factual,4,4,80,polite,5,positive,4,low,3,4,3,4,factual,4,4,85,polite,5,positive,4,low
116,Reviewer-5v8y,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper proposed a mutual learning approach to learn a pair of Bayesian Neural Network(BNN). The posterior of BNN is approximated by Variational Inference using a Gaussian distribution with a diagonal covariance matrix. To make the BNN learn different perspective of the data, the author proposed to increase the diversity in parameter space and intermediate feature space by adding the an estimate of distance between parameter distribution and fused feature distribution of two BNN models into the objective function. Empirically, the proposed method outperform existing mutual learning method and vanilla BNN model in terms of accuracy, negative log likelihood loss and expected calibration error. An ablation study is also provided to investigate the usefulness of each component.  The paper is well written and easy to follow. Increasing the diversity of parameter distribution and intermediate feature distribution of peer BNN models to boost performance is an interesting idea. Experiments and detailed ablation study demonstrate the effectiveness of proposed method. 1. It is mentioned in the abstract and introduction that the BNN model with variational inference may underperform deterministic model or BNN obtained by MCMC, the baseline only involves BNN model trained with(DML) or without(vanilla) mutual learning. Would the proposed method close the gap to some extent? Data augmentation, optimizer may all affect performance, so it is still helpful to include deterministic model results follow with same training setup. I would expect the BNN model to outperform deterministic model at least in NLL and ECE, and with the 50 ensemble, it can outperform the accuracy. 

2. Continue with last point, for MCMC method (e.g. in line 81 of the paper), I agree that traditional MCMC method(e.g. Metropolis Hasting) may not be feasible for large model, and memory storage can be an issue for MCMC method. But I don't think the stochastic gradient MCMC cited in line 81 would require prohibitive computational cost, it behaves like adding a noise to at each step of standard SGD training. 

3. The code is not provided so it may hurt the reproducibility of the paper. 1. To my knowledge, it is not very clear if variational distribution(e.g. Gaussian with diagonal covariance matrix) can approximate the true posterior very well, can the author comment a bit on this, e.g. how would different choice of variational family affect the model?

2. In line 264 and line 6 of algorithm 1, it is mentioned that one BNN model is initialized with a trained model and this lead to better results empirically. Can the author discuss more on why this happened? It is a bit wired for me as it seems in the implementation detail, the pre-trained model and the model from scratch are trained with same optimizer and learning rate schedule.

3. Seems like $\alpha$ $\beta$ are set to 1,2 for CIFAR and 1,1 for Imagenet, these two parameters controls the strength of proposed penalty to the model, can the author comments a bit more on how sensitive are the model to those parameters, It can help to illustrate how diversity helps model performance.

4. As mentioned in line 268, results are average of 3 trials, I think it would be better to include the standard deviation as well to boost the significance of the results.

5. In figure A.3 in supplementary material, looks like a sharp increase of KL divergence between the fused feature distributions at around 30 epochs, but the penalty for feature is only added for last 100 epochs, can the author explain more on this?  The authors addressed the limitations.",585,0,8,0.7803,0.1055805306,0.8540630937,216,40.5795,0.11,neurips,0.0,4,4,4,4,factual,4,4,80,polite,4,neutral,4,low,4,5,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,5.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
116,Reviewer-fyNb,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper proposes a method to combine deep mutual learning with BNN to diversify the weight distributions of each BNN networks in a pair or ensemble, to improve performance. 1. AFAIK this is the first work combining mutual learning with BNN, so the authors can claim this point.
2. The paper is in general written clearly and easy to follow.
3. Experiments are adequate with ablation studies on individual features impact on diversity. 1. Some design choices are found to be ""empirically"" working well without too much discussion or hypothesis.
2. Would be interesting to see how the model performs for o.o.d test data, especially uncertainty performance. Line 178-179: The authors said adding the D(...) term will rapidly increase of this term and impact training. Wouldn't putting a smaller scaling factor for this term fix this issue? None.",138,0,6,0.8457,0.1638888889,0.8492545485,216,54.2802,0.1041,neurips,0.010204081632653,3,4,2,3,factual,3,2,50,neutral,3,neutral,2,low,4,4,4,4,factual,4,4,75,polite,5,neutral,5,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,70,polite,4,positive,4,low,3,4,3,4,partially factual,4,3,78,polite,5,positive,4,low
116,Reviewer-phoh,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper titled addresses the challenge of improving the performance of Bayesian Neural Networks (BNNs) by leveraging the concept of mutual learning. BNNs provide a means for quantifying uncertainty in predictions through probability distributions of model parameters. However, BNNs often fall short in performance compared to their deterministic counterparts. The authors propose a novel approach that employs deep mutual learning to enhance the capabilities of BNNs. 1. Innovative Approach: The paper introduces a novel method that combines deep mutual learning with Bayesian Neural Networks. By promoting diversity in both network parameter distributions and feature distributions, the proposed approach enables peer networks to acquire distinct features, capturing different characteristics of the input data. This innovative technique enhances the effectiveness of mutual learning in BNNs.
2. Detailed algorithm description: The paper provides a thorough and detailed description of the proposed algorithm for improving the performance of Bayesian Neural Networks (BNNs) through deep mutual learning.
3. Comprehensive Experiments: The authors conduct extensive experiments to evaluate the proposed approach thoroughly. The experimental results are statistically sound and demonstrate significant improvements in classification accuracy, negative log-likelihood, and expected calibration error compared to traditional mutual learning methods for BNNs. 1. Limited variety in experimental validation: One weakness of the paper is that the proposed approach and its effectiveness are only verified through experiments conducted on Residual Neural Networks (ResNets). It would have been beneficial to include experiments on a diverse set of network architectures to demonstrate the approach's effectiveness across different model types and complexities. 
2. Lack of detailed explanation for temperature, α, and β: One weakness of the paper is the limited explanation provided for the temperature parameter (T), α, and β, which are crucial components of the proposed approach. These parameters play a significant role in controlling the diversity of network parameter distributions and feature distributions, but their specific effects and optimal values are not thoroughly discussed.
3. Weakness in the conclusion: The current conclusion merely restates the experimental results and does not highlight the broader implications of the proposed approach or its potential impact on the field. The author should supplement more experiments to prove its effectiveness. The author should supplement more experiments to prove its effectiveness and strengthen the conclusion.",368,0,6,0.788,0.1220982143,0.9327940345,216,14.7437,0.0999,neurips,0.0,2,4,3,3,factual,3,3,60,neutral,3,positive,3,low,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,positive,5.0,none,3,4,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
116,Reviewer-tB5V,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","The paper focuses on improving the accuracy of BNNs by promoting diversity in both parameter space and feature space while training two peer BNNs with mutual learning between them. More specifically, they train two variational BNNs with a mean-field Gaussian variational loss for each along with a KL divergence term between the (temperature-scaled) predictive distributions of the two models, a Wasserstein distance term between the corresponding approximate posterior distributions across the two models (added as a softplus(-distance) term), and a KL divergence term between corresponding feature distributions. On the latter term, instead of directly maximizing the distance between corresponding feature distributions, they instead do so on ""fused feature distributions"". To do so, they use learned cross-attention to fuse the features from multiple feature levels in a model (two at a time). Then, they use the KL divergence between the distributions of the fused feature distributions of the two peer networks. To derive the distributions, they use the conditional probability density defined as $p_{i|j} = \frac{K(F'_i, F'_j)}{sum_{k=1, k \noteq i}^n K(F'_k, F'_j)}$, where $K(F'_a, F'_b)$ is a kernel function between two fused feature representations. Given those conditional probs, they compute a KL divergence term. Similar to the parameter space diversity term, they add this term to the loss as softplus(-divergence). The paper claims to be the first to propose maximizing the distance between feature distributions to promote diversity. In terms of experiments, the paper includes results for ResNet models on CIFAR-10/100 and ImageNet, measuring accuracy, NLL, and ECE as metrics, and comparing different approaches. The paper does a great job of precisely articulating the modeling approach, and discussing the relevant background info. More specifically, the proposed approach of adding terms to promote diversity in parameter and feature space is clear and would be easy to reimplement. My main concern is with the experiment section. More specifically, a few key details are unclear in the text, and importantly a deterministic baseline is missing that I believe should be present given the framing of the paper and relevant literature. Please see the Questions below. Given updates, I believe the paper would be great and I would gladly update my rating. Main:
- In the experiments, a few details are currently unclear. The following points are on Table 1, but generalize to all three tables. Please clarify these details here and in the paper.
  - Consider the ResNet20 section of Table 1. Is my understanding correct that the ""ResNet20"" results are for a pair of BNNs trained from scratch, while the ""ResNet20*"" results are for a pair of BNNs trained with the approximate posterior means set to the values from a deterministic model?
  - Is it correct that all results (all three metrics across all three approaches) are computed after averaging the predicted probs from the pair of models?
- For the experiments, a deterministic baseline is missing. Given the intro that discusses how BNNs can lag behind deterministic models in acc (though not always), the experiments lack a comparison. It would be helpful to understand how the proposed approach compares to a deterministic baselines, specifically a single deterministic model and a size-2 deep ensemble. Could you add this as a baseline? I would consider this to be a blocker for the paper given the framing and relevant literature.

Other:
- The KL divergence term is scaled by the square of the temperature -- why?
- How did you choose the values for temp, alpha, and beta? They differ between CIFAR-10/100 and ImageNet. Did you ablate values?

Minor comments:
- updating lines 17 & 22 of Alg 1 could be helpful for readability No limitations are included.",603,0,0,0.716,0.1269510582,0.8272995353,216,44.5054,0.4643,neurips,0.0,4,4,4,4,factual,4,4,80,neutral,4,neutral,5,low,5,5,4,5,factual,5,5,90,polite,5,neutral,5,low,2.0,4.0,5.0,4.0,factual,3.0,4.0,80.0,polite,5.0,neutral,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,5,4,4,4,factual,4,4,88,polite,5,neutral,5,low
116,Reviewer-xPiJ,Model and Feature Diversity for Bayesian Neural Networks in Mutual Learning,"Bayesian Neural Networks (BNNs) offer probability distributions for model parameters, enabling uncertainty quantification in predictions. However, they often underperform compared to deterministic neural networks. Utilizing mutual learning can effectively enhance the performance of peer BNNs. In this paper, we propose a novel approach to improve BNNs performance through deep mutual learning. The proposed approaches aim to increase diversity in both network parameter distributions and feature distributions, promoting peer networks to acquire distinct features that capture different characteristics of the input, which enhances the effectiveness of mutual learning. Experimental results demonstrate significant improvements in the classification accuracy, negative log-likelihood, and expected calibration error when compared to traditional mutual learning for BNNs.","This paper presents a novel method for enhancing the performance of Bayesian Neural Networks (BNNs) by employing deep mutual learning. The proposed approach aims to enhance the diversity of both network parameter distributions and feature distributions, encouraging individual networks to capture unique characteristics of the input data. The effectiveness of the proposed method is demonstrated on datasets, including CIFAR10, CIFAR100, and ImageNet. The proposed method improves performance and uncertainty estimation while reducing the expected calibration error (ECE).  The technical approach is novel as the method introduces mutual learning in the context of BNNs and first to propose maximizing the distance between feature distributions and parameter distributions. The paper includes large scale data experiments (ImageNet) and ablation studies to demonstrate the effectiveness of each technical contribution introduced in this paper. The previous studies mentioned in the paper utilize alignments on feature maps \[4\] or predictions \[38\], rather than diversifying them. In contrast, the proposed method diversifies both feature distributions and parameter distributions which is an opposite approach to the previous works. Interestingly, both alignment-based and diversification methods improves performance over vanilla BNNs, as indicated in Table 1, 2, 3, and 5. However, the paper does not explicitly explain the reasons behind the performance improvements resulting from these contrasting approaches.

Given the observed contradicting results in the experiments, where the alignment-based method (DML \[38\]) also enhances the performance of BNNs, an important question arises: could combining alignment-based methods with parameter diversification further improve BNN performance? Alternatively, is it necessary to diversify both feature and parameter distributions to achieve significant improvements?

In the experiment section, the proposed method is only compared with \[38\] and not with \[4\]. 

Hyperparameters used for CIFAR experiments and ImageNet experiments are different. However, the paper does not describe details regarding the hyperparameter tuning or determination. 3 block resnet is used for CIFAR experiments while 4 block resnet is used for ImageNet experiments. Why different form of resents are used for different datasets? Limitations are shortly addressed in the supplementary.",331,5,0,0.7494000000000001,0.0582251082,0.8471010923000001,216,15.9032,0.072,neurips,0.0,3,3,3,3,factual,3,3,70,neutral,4,neutral,4,low,3,5,4,4,factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
29,Brian-M-Gurbaxani,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors take issue with the fixed, 18% efficacy figure for face masks in the economic evaluation of masks usage post-vaccination paper by Bartsch et al., and of course they are correct: the efficacy isn’t fixed, and it depends on a lot of factors. So the question is: if face mask impact on Rt is a function of 1) social behaviour (e.g. contact rates), 2) quality and quantity of face mask usage, and 3) intrinsic properties of the viral variant circulating (R0)1, and you’re trying to quantify the economic impact of maintaining facemask use during and after a vaccine campaign using a calibration of facemask impact on Rt from an earlier time when all 3 of those factors might be different, then couldn’t your economic impact assessment be off? Yes, it could. I’m not sure that the author’s suggestion of simply widening the uncertainty in the parameter value from 5 to 50% and doing a sensitivity analysis is going to do much good, however, because it won’t answer the policy questions people have, and will leave everyone more uncertain. I think it is possible, through modeling, to recalibrate the impact of facemasks on Rt for more recent times, when better quality masks are more widely available, but the variants are more easily transmissible as well, and society has less of a pandemic, lockdown mentality1,2. One could then present the results of different time periods corresponding to the spread of different variants, but with more certainty, and let the reader decide which scenario is more likely.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",376,0,1,0.7963,0.1586038961,0.9027335644,336,33.68,0.1443,f1000,0.0,3,3,3,4,factual,4,4,70,polite,3,neutral,3,moderate,3,5,4,3,partially factual,4,4,75,polite,5,neutral,4,moderate,3.0,5.0,5.0,4.0,factual,5.0,4.0,80.0,polite,5.0,positive,4.0,none,3,4,4,3,partially factual,4,3,70,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
29,José-L-Herrera-Diestra,Challenges in specifying parameter values for COVID-19 simulation models,"A recent modelling paper on the coronavirus disease 2019 (COVID-19) epidemic in the US (Bartsch et al.) suggested that maintaining face mask use until a high vaccine coverage (70–90%) is achieved is generally cost-effective or even cost-saving in many of the scenarios considered. Their conclusion was based on the assumed effectiveness of continued face mask use, cited from a study that reported an 18% reduction in the effective reproduction number associated with the introduction of state-level mask mandate policies in the US in the summer of 2020. However, using this value implicitly assumes that the effect of face mask use in 2021 through 2022 is the same as that of summer 2020, when stringent nonpharmaceutical interventions were in place. The effectiveness of universal mask wearing in 2021–2022 is probably more uncertain than considered in Bartsch et al. and rigorous sensitivity analysis on this parameter is warranted.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  I consider that the case made by the authors in this correspondence are valid and important. Changes in the conditions that lead to the 18% reduction of Rt are certainly a combination of all measures implemented in 2020, and may not be directly applicable in 2021-2022. I agree that a ""rigorous sensitivity analysis"" might be a good starting point. However, besides this sensitivity analysis, more elaborated methods need to be developed to assess more accurately the influence of the different interventions that were in play in the summer of 2020, and which of these interventions could be reasonably extrapolated to 2021-2022.  Is the rationale for commenting on the previous publication clearly described? Yes  Are any opinions stated well-argued, clear and cogent? Yes  Are arguments sufficiently supported by evidence from the published literature or by new data and results? Partly  Is the conclusion balanced and justified on the basis of the presented arguments? Yes",220,0,1,0.7723,0.1663711289,0.7168864608000001,351,29.79,0.1953,f1000,0.0096153846153845,3,4,2,3,factual,3,3,80,polite,3,positive,4,none,5,5,4,5,partially factual,5,5,85,polite,5,positive,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,partially factual,4,3,65,polite,4,positive,3,low,3,4,3,4,partially factual,4,4,78,polite,5,neutral,4,low
26,Rajinder-K.-Sharma,Case Report: The Sausage Technique using Anorganic Bovine Bone Mineral for Horizontal Bone Augmentation at the Crestal Part of a Posterior Mandibular Ridge: A Case Report.,"Following tooth extraction, the alveolar bone goes through a natural remodeling process resulting in a significant bone resorption which may complicate dental implant placement without prior bone augmentation treatment. The sausage technique is a modified guided bone regeneration (GBR) method that has been successfully used for horizontal bone augmentation. This technique was developed to increase the bone growth at the alveolar crest. Although the sausage technique uses a combination of autograft chips and xenograft particles with a native collagen membrane, several studies have questioned whether adding autograft chips is essential for bone formation with guided bone regeneration. Moreover, harvesting the bone graft may increase the donor site morbidity and patient discomfort. This case report aimed to investigate the bone gain radiologically when the sausage technique was applied to treat a healthy, thirty-year-old patient with a horizontal defect in the posterior mandibular region using anorganic bovine bone mineral (ABBM) particles with Jason membrane, assess the implant primary stability in the augmented ridge, and present the surgical procedure steps in details. After nine months of healing, the cone-beam computed tomography (CBCT) revealed approximately 4.32 mm of bone gain at the alveolar crest in the buccal-lingual direction. The graft particles were well integrated into the newly formed bone. Two implants were inserted with an insertion torque of 35 N/cm. The ISQ values were 76 for the most anterior implant and 78 for the posterior implant. Within the limitations of this case report, the sausage technique using ABBM particles without autograft chips was an effective approach in achieving the prerequisite bone width at the crest in cases with horizontal bone defects.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The case report is about horizontal bone augmentation through staged GBR using the sausage technique to facilitate implant placement. Please consider the following points to improve the quality of discussion section. 1. How the surgical procedure is different from the procedure proposed by Istvan Urban and colleagues, except the exclusion of autogenous graft. 2. What are the alternatives to bone augmentation to facilitate implant placement in this case. Please describe briefly the merits and limitations. 3. What are the probable outcomes of attempted bone augmentation in this case? And how the bone augmentation was ascertained? 4. What are the  long-term complications associated with fragmented bone graft materials?  5. Is the procedure described in this case relevant for improving the success of implant placement?  6.Ethical considerations for use of materials with animal origin.  Is the background of the case’s history and progression described in sufficient detail? Yes  Are enough details provided of any physical examination and diagnostic tests, treatment given and outcomes? Yes  Is sufficient discussion included of the importance of the findings and their relevance to future understanding of disease processes, diagnosis or treatment? No  Is the case presented with sufficient detail to be useful for other practitioners? Yes",268,0,6,0.7475,0.0722222222,0.8633317947,164,27.93,0.0515,f1000,0.0,2,3,3,2,partially factual,3,3,50,neutral,3,neutral,2,moderate,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
157,Reviewer-uRwm,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper provides an in-depth analysis of adversarial training with linear models and its relationship to regularized regression methods under the overparametrized regime.
Depending on the value of the perturbation radius, it is revealed that there exist three modes.
When the radius is small, solutions to adversarial training behave as minimum-norm interpolators (Theorem 1).
When the radius is medium, the solutions behave as solutions to the parameter shrinkage regression (Proposition 4).
When the radius is large, the zero solution is necessary and sufficient (Proposition 3).
In addition to these theoretical results, the authors observe the mode change experimentally and discuss how adversarial training is advantageous over parameter shrinkage regression. - A modern extension of theory on robust optimization and regularization: The relationship between robust optimization (somewhat encompassing adversarial training in this work) and regularization has been known in the literature, including Xu et al. (2009). This work contributes to studying what happens when it comes to overparametrization and nicely characterizes the relationship between the perturbation radius and the corresponding modes (as I summarized above).
- Demonstration of the benefit of overparametrization: In the numerical simulation of Figure 2, the authors demonstrate that the robustness radius increases as the model becomes more overparametrized, namely, $p/n$ increases. This clearly indicates the benefits of overparametrization (though the analysis hinges on norm matching, as mentioned in Remark 2).
- Clarity: Despite the thorough theory, the paper is written clearly and easy to follow.

Xu et al. (2009). ""Robustness and Regularization of Support Vector Machines."" (JMLR) One of the main weaknesses would be the restriction to linear models, which is crucial for the current analysis yet needed for further understanding adversarial training.

You may refer to Xu et al. (2009) when you show Theorem 4. Indeed, the equation right after l.322 can be regarded as a generalization of Theorem 3 in Xu et al. (2009) because $\\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta}) - \\delta\\|\\beta\_\*\\|) \\le \\ell(y(\\boldsymbol{x}^\\top\\boldsymbol{\\beta})) + \\delta\\|\\boldsymbol{\\beta}\\|\_\*$ when $\\ell$ is the hinge loss.

Below, I have other minor comments.

- In Figure 1, can you specify what $\\lambda$ and $\\delta$ are used for each line?
- In the proof of Theorem 1, you may need $-$ (negative) sign in front of either $\\epsilon\_i\boldsymbol{x}\_i$ in Eq. (6) or $n\\delta\boldsymbol{\\alpha}$ in l.132. Otherwise, ""the subderivative contains zero"" (l.132) does not seem to be correct.
- In l.150, the reason of $\\delta\_{\\text{test}} \\propto \mathbb{E}\[\\|\\boldsymbol{x}\\|\]$ is unclear to me. Can you elaborate on it?
- In Figure 4, can you specify what $n$ and $p$ are used?
- In Eq. (8), do you miss the exponent $2$ for the norm?
- In Eq. (9), it might be better to change the notation $\\epsilon$ for the noise because $\\epsilon$ has already been used in the proof of Theorem 1.
- In the equations after l.319 and l.322, should we need $+ \\Delta x$ on the left-hand sides?
- In the appendix, what is referred to as Theorem 3 seems to be Proposition 3. See the weaknesses. Obviously, the analysis is entirely limited to the linear model case. Nonetheless, the analysis provides a fair amount of insights to readers, so I don't think this is a big limitation.",523,4,7,0.7336,0.151984127,0.9391887188,221,42.6844,0.2594,neurips,0.0,4,4,3,4,factual,4,5,85,polite,4,positive,4,none,5,5,5,5,factual,5,5,92,polite,5,neutral,5,moderate,3.0,5.0,5.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,4.0,none,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
157,Reviewer-Zg7q,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper studies the connection between adversarial training and regularization methods in linear regression problem. Simulation studies are provided to justify the correctness of their theoretical observations. The authors conducts a comprehensive study on the relationship between adversarial training and regularization methods in linear regression setup. The writing is clear and easy to understand. My major concern towards this paper is the limit of its contribution. While the analysis is comprehensive, it is only restricted to linear models. Considering that the adversarial training is more commonly used in neural networks rather than linear models in reality, the contribution is limited. The authors are encouraged to add more discussions on neural networks.

In addition, the following paper considers the connection between regularization and adversarial robustness:

Jakubovitz, Daniel, and Raja Giryes. ""Improving dnn robustness to adversarial attacks using jacobian regularization."" Proceedings of the European Conference on Computer Vision (ECCV). 2018.

Please cite this paper and compare it to the submission from intuition aspect. Is it possible to extend the analysis to two-layer neural networks? NA",173,0,2,0.7644000000000001,0.1020337302,0.8797743320000001,221,19.2375,0.2191,neurips,0.0,1,3,2,1,partially factual,2,2,40,polite,2,positive,3,high,5,5,4,5,factual,5,5,95,polite,5,neutral,5,low,2.0,5.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,3,4,3,4,factual,4,4,85,polite,5,neutral,4,low
157,Reviewer-6WVQ,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","The paper studies adversarial training (AT) for linear regression for which the inner maximization problems has a closed form solution. They then attempt at relating the solutions to solutions of other optimization problems:

- They show that the minimum norm interpolator also minimizes the adversarial loss (iff the adversarial perturbation is sufficiently small)
- They show that adversarial training (under certain conditions) minimizes something closely related to the LASSO and ridge regression objective for $\ell_\infty$ and $\ell_2$ attacks respectively.
- They show that similarly to square-root LASSO, adversarial training does not need knowledge of the variance and they argue that this makes adversarial training a viable alternative.
 - It seems interesting to attempt connecting AT to sparse solutions
- The initial setup and the statements of the theorems are presented in a clean way
- Existing literature is well-covered
 - My main concern is that the theoretical claims are rather weak:
    - Concerning Thm. 1, l. 122 ""minimum-norm interpolators as the outcome of adversarial training"" seems a bit of a stretch, since it is not *consistently* the outcome of adversarial training (we might be able to find a minimizer of $R^{adv}$ that is *not* a min norm interpolator). AT would imply minimum-norm interpolator if the minimizer of $R^{adv}$ was unique, but this cannot be the case since LASSO is not unique in general.
    - Prop. 2 is concerning minimum norm interpolator (so not necessarily obtainable with AT!). What makes this statement interesting for adversarial training if we need to obtain the solution through other means?
    - Prop. 4 seems to not directly relate AT to LASSO/ridge regression. Whats is the conclusion of Prop. 4? 
    - Thm. 2 exists to show that AT can replace sqrt-root LASSO. You are comparison with Lasso though – doesn't the bound have a bias in comparison with sqrt-root LASSO (eq. 10 of \[29\]). The main feature of AT seems to be the claim that $\delta^*$ is invariant to rescaling of $\varepsilon$. Can you explicitly make $\delta^*$ in Thm. 2 independent of $\varepsilon$? (currently this is not the case in theorem statement)

Comments:

- Prop. 5 maybe pick a different variable than $p$ (already used for dimensionality)
- l. 144 should have been $\delta$ instead of $\delta_{train}$?
- Maybe write ""a solution"" in l. 144 instead of ""the"".
- l. 179: Please describe the dataset in the appendix or provide a more direct pointer to \[18\].
 - Thm. 1: $\bar \delta$ depends on the $\ell_\infty$-norm regardless of the choice of norm in the adversarial training? This seems potentially loose – could you comment on it?
- Prop. 2: Do you still rely on full row rank in Prop. 2? 
- l. 158-159: Isn't the claim in \[17\] about $\ell_2$ minimum norm while your Prop. 7 is a claim about choice of norm in the adversarial training? 
- Figure 3: Could you label the plot to explain the colors? I don't understan how to interpret the plot.
- Figure 4 / l. 179: what is ""regularization paths""? 
- What assumption breaks in Prop. 4 since it is no longer able to predict similarly after $\delta$ is made sufficiently small (as demonstrated in Fig. 4)?
 N/A",528,3,6,0.739,0.0506906288,0.9212126732,221,49.9975,0.7282000000000001,neurips,0.0,4,4,4,4,factual,4,4,80,polite,5,positive,5,low,5,5,4,5,factual,5,5,85,polite,5,negative,5,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,5.0,neutral,5.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
157,Reviewer-CYXN,Regularization properties of adversarially-trained linear regression,"State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the  minimum-norm  interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solution to adversarial training with a given radius. (B) Adversarial training can be equivalent to parameter shrinking methods (ridge regression and Lasso). This happens in the underparametrized region, for an appropriate choice of adversarial radius and zero-mean symmetrically distributed covariates. (C) For $\ell_\infty$-adversarial training---as in square-root Lasso---the choice of adversarial radius for optimal bounds does not depend on the additive noise variance. We confirm our theoretical findings with numerical examples.","This paper investigates adversarial training of linear regression. The authors compared the solution of adversarial training and other regularization frameworks (minimum-norm interpolating, ridge regression, Lasso and square-root Lasso), and established close relations between adversarial training and other methods under certain conditions depending on the disturbance radius and over/under-parameterization. The authors also consider extending the result to more general loss function for linear model. 1.	The paper provides valuable insights on the relation between adversarial training and other regularization frameworks for linear regression, which contributes to the area of robust learning. The analysis is sound.
2.	The paper provides good background knowledge and details in their work. 
3.	The paper is well-organized and easy to follow overall.
 1.	In the abstract, the authors claim that adversarial training can be equivalent to parameter shrinkage methods (like ridge regression and Lasso). However, from Proposition 4, it seems the two frameworks are not equivalent, since the regularization term in the equation of Proposition 4 equals $\delta^2\left\| \beta\right\|^2 + c\delta\left\| \beta\right\|$ for some constant $c$. I am curious about how the quadratic term can affect the solution, or how close the adversarial training solution is from the parameter shrinkage solution.

2.	In the numerical experiment, the authors have not mentioned how the adversarial training is carried out in these datasets. From the code in the supplementary materials, it seems the adversarial samples are generated by the PGD attack. Please consider including more details in the paper. Also, does PGD generate sufficiently strong attacks for linear regression?
 Here are some additional questions/comments:

1.	$\sigma_1$ and $\sigma_n$ in line 126 are undefined.

2.	The authors claim in line 151 and Remark 2 that the model becomes robust as feature dimension $p$ grows, which seems not precise to me. The authors suggest that the threshold $\bar{\delta}$ increases faster, but this only guarantees that the optimal solution of adversarial training and minimum-norm interpolator agree, which does not necessarily mean more robustness. Does the risk $\mathcal{R}^{\text{adv}}$ decrease as feature dimension $p$ grows? 

3.	The paper investigates the situation where the sample features $x_i$’s are disturbed in linear regression. In applications, it is also very common that the target $y_i$’s are disturbed.  
 The authors have adequately addressed the limitations.",368,0,7,0.7637,0.1577767857,0.9543603063,221,30.1542,0.1719,neurips,0.0120481927710843,4,5,4,4,factual,4,4,85,polite,4,positive,4,low,4,5,5,5,partially factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,3.0,4.0,80.0,polite,5.0,neutral,5.0,none,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,3,5,4,4,partially factual,4,4,85,polite,5,positive,5,low
10,Reviewer-7Q1Z,An interpretable error correction method for enhancing code-to-code translation,"Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. 
To address these issues, we introduce a novel approach, $k\text{NN-ECD}$, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k\text{NN-ECS}_{m}$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series,  utilizing $m$ diverse experts for multi-round error correction. Additionally, we put forward a unified name rule, encouraging the datastore to focus more on code logic and structure rather than diverse rare identifiers. Our experimental results show that our approach improves the translation accuracy from 68.9\% to 89.9\% of TransCoder-ST (for translation from Java to Python). This error correction method augments program translation, overcoming the inherent limitations of Transformer-based code translation models, such as resource-intensive retraining requirements and uninterpretable outcomes.","This paper focuses on improving Java $\rightarrow$ Python translation using error correction, rather than retraining the underlying translation model. They devise two error correction techniques (kNN-ECD and kNN-ECS) based on kNN-MT, which entails retrieving from datastores. To build this datastore, they first collect 82,665 Java functions and generate high-quality unit tests for them using EvoSuite. Then, they use TransCoder-ST to translate the Java functions paired with the unit tests to Python. From these they extract pairs of the form (failed Python function, successful Python function), which are then used to build (a) datastore(s). The datastore is organized based on two components: (1) (key, value) pairs and (2) (key, value) $\rightarrow$ token. The query to this datastore is formed by using the last decoder hidden states corresponding to the full source input (i.e., failed Python function) and partial target (i.e., possible correction generated so far). To reduce noise caused by diverse rare identifiers during retrieval, they apply the unified name rule. In kNN-ECD, only one round of correction is performed. In kNN-ECS_{m}, they perform $m$ rounds of correction, with m smaller datasets (after segmenting the large datastore into $m$ parts). Results show that kNN-ECS outperforms kNN-ECD as well as a vanilla TransCoder-ST with no error correction. - The proposed approach successfully corrects errors to a certain extent, without retraining the model or re-sampling the model many times, which is usually done in self-repair.
- The idea of multi-round error correction and the analysis done with this, varying the number of rounds, and analyzing the performance for each of these, is quite interesting and may inspire future work. - Evaluation is based on translated unit tests generated by the same model that the authors are trying to correct translation errors for. Therefore, the unit tests that are generated could be wrong, and so the evaluation is unreliable. Evaluation should be performed based on a separate, high-quality set of unit tests. Possibly datasets like HumanEval-X would be better alternatives here.
- The experiments and results are fairly limited. First, the authors focus on only Java $\rightarrow$ Python and fail to consider other languages or even the reverse direction of Python $\rightarrow$ Java. Next, Table 1 seems to be missing many baselines and other models to which their approach should be compared. Namely, the only baseline is the pure TransCoder-ST model, which is only the starting point of their approach. The authors discuss that the main advantage of their approach is that no retraining is required, so it would be important to see how their approach performs relative to a retraining-based one. For this, they could have simply fine-tuned TransCoder-ST on the error correction pairs they collected for building their datastore. Next, latency is not measured, even though the authors discuss latency in related work. It seems that retrieving from a large datastore or retrieving multiple times from smaller datastores could take a long time, so it would be important to understand how the overall latency compares to other approaches. Finally, the authors do not report results on state-of-the-art code models, so it is difficult to assess the true value of their approach.
- The authors present the unified name rule as a novelty; however, I do not find this to be that novel, given the work the authors discussed in the ""Related Work"" section.
- There are multiple aspects of the paper that are not clear.  Please see the ""Questions"" section. 1) Based on what is written in the paper, 10 Python functions with unit test cases are generated for each Java function. So, you have $(func_1, tests_1), (func_2, tests_2), (func_3, tests_3)... (func_{10}, tests_{10})$. Success is measured by executing the tests in some Python environment, where $func_i$ is considered a success if it passes all tests in $tests_i$. By this definition, suppose $func_1$ fails $tests_1$ and $func_2$ passes $tests_2$.  The paper states ""we combined the first failed Python function with the first successful Python function to form an error correction language pair."" Based on this, it seems that $(func_1, func_2)$ would be considered an error correction pair. However, there is no guarantee that $tests_1 = tests_2$, meaning that the two functions could be executed against different test suites. Therefore, $func_2$ may not actually correspond to a correction of $func_1$. Could this please be clarified? 
2) The ""interpretable decision-making"" idea is not clear to me. It seems that you are suggesting that the reasoning for predicting a specific token at a timestep $t$ can be attributed to the source and partial target function predicted so far. This is also the case for transformer-based decoders, so it is not clear to me how your approach can be considered more interpretable than a transformer as they claim.
3) In 3.2, you state that the hidden representations from the last layer of the decoder are used to build the (key,value) and query. My understanding is that the (key ,value) and query correspond to (failed Python function, partial Python function). It is not clear to me how there would be a decoder state corresponding to the failed Python function since that is passed into the encoder (Figure 1). Or is (failed Python function, partial Python function) meant to actually only represent the representation of the partial Python function generated so far, as labeled as ""Key"" in Figure 1? 
4) You claim that the improvement of ECS over ECD is ""primarily attributed to its distributed structure, which includes diverse datastore variants."" However, you do not seem to have multi-round experiments with ECD in which you repeatedly perform retrieval/correction on the same large datastore up to $m$ times. Therefore, isn't it possible that the advantage is actually from doing iterative code correction rather than the distributed nature of it?",949,0,0,0.7745000000000001,0.0229609929,0.8705494404,60,49.7816,0.2416,iclr,0.0,2,4,4,3,partially factual,4,4,82,neutral,4,negative,4,low,4,5,4,4,factual,4,4,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,partially factual,4.0,3.0,60.0,polite,5.0,neutral,5.0,low,4,5,5,4,factual,5,5,90,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
145,Reviewer-AWzJ,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","This work considers training an agent without online interaction or abundant offline data but only with the reward function of the target environment. Borrowing the idea of rehearsal from the cognitive mechanism, this work proposes policy rehearsal. In detail, this work hopes to train an array of models to imitate the target model. Theoretical analyses indicate that the target environment performance gap between the policy trained in these imitated models and the optimal policy can be bounded by three terms, which are further summarized as diversity and eligibility. Based on these two criteria, this work proposes two corresponding reward functions for training imitated models and then uses these models to train the policy. Also, the proposed ReDM can easily combined with offline datasets. Extensive results show the effectiveness of ReDM. - The ideas about the setting are novel and important, minimizing interaction with the environment as much as possible is an important problem in the RL community. Also, introducing rehearsal into RL is novel and enlightening.

- The writing of Sec 3.2 is clear and solid, I have roughly read all the proofs, which are written quite clearly.

- The proposed ReDM utilizes two novel terms for learning an imitated model, which is interesting and helpful.

Currently, my evaluation of this paper is really Boardline. If authors can address my concerns in Weaknesses and Questions, or point out what I have misunderstood, I'd like to update my scores accordingly. Also, I will keep active in the following discussion stage. - The connection between diversity and controlling $\epsilon_e, \epsilon_a$ is unclear. For example, if all environments are the same, i.e., there is no diversity, it is obvious that $\epsilon_a=0$ is minimal. There also needs more explanation about why $\epsilon_e$ can be controlled via diversity.

- Based on the previous points, one of my major concerns is why the proposed methods can help optimize the gap calculated in Thm 3.3. The authors have summarized the three errors in Thm 3.3 as diversity and eligibility, which indeed provides insights for analyzing this problem. But I think a more direct connection, like whether the objective in Sec 3.3 can be proven to directly control the three errors in Thm 3.3, will make the analyses more solid.

- In experiments, providing the results directly trained in the target environments as the reference will better show the results.

- Lack of some related works, like utilizing model-based methods for improving generalization \[1-3\], and finding diverse skills for unsupervised RL \[4-6\] as this work hopes to find diverse models.

\[1\] Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning

\[2\] Task Aware Dreamer for Task Generalization in Reinforcement Learning

\[3\] The Benefits of Model-Based Generalization in Reinforcement Learning

\[4\] Diversity is All You Need: Learning Skills without a Reward Function

\[5\] Effective diversity in population based reinforcement learning - In my opinion, the considered setting is that the agent can only get the reward function of the target task but has no knowledge about the dynamic of the target task. Is it right? Given the offline data, it is understandable that the agent can learn the dynamic to some degree. But without an offline dataset, it seems that there is no idea for the agent to learn the dynamic of the target task. 

- Based on the previous question, I'm confused about the setting of Experiment 4.1 "" ReDM With no Interaction Data"". As there are no data about the environment and the agent can not interact with the environment, how does the agent to learn about the environment?

- As Unsupervised RL considers training an agent in the environment without reward, in my opinion, the setting in this work is like training an agent and models in the environment with reward but without dynamic. As the dynamic of the target environment will vary a lot, whether finetuning the agent (as well as the model) in the target environment with few steps will be more reasonable?

- About $r_e$ for Eligibility. The proposed method is to randomly sample N trajectories and estimate the biggest return. Is this inefficient as the state space and action space are continuous in experiments? Also, what is the choice of N in experiments?

- I'm curious about the performance of ReDM in the D4RL setting (Sec. 4.3) but without any Interaction Data.",720,5,0,0.7541,0.0956459436,0.9084495306,57,42.3274,0.0512,iclr,0.009090909090909,4,4,4,4,factual,3,4,80,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,88,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
145,Reviewer-YzNF,Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning,"Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of \textit{rehearsal} into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with \emph{zero} interaction data. We further extend ReDM to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that ReDM produces high-performing policies compared to other offline RL baselines.","Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, the authors introduce the idea of *rehearsal* into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, they propose ReDM, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to natually generalize to previously unseen environments. Their experimental results demonstrate that ReDM is capable of learning a valid policy solely through rehearsal, even with zero interaction data. Besides, they further extend ReDM to scenarios where limited or mismatched interaction data is available. The provided empirical results reveal that ReDM produces high-performing policies compared with other offline RL baselines. 1. The problem of policy rehearsing in offline reinforcement learning is interesting and challenging as an academic topic.
2. The description to the problem modeling and the methods is clear and generally easy-understanding.
3. The proposed method is well motivated by comprehensive preliminary theoretical analysis.
4. The experiment analysis is in-depth and insightful, which helps the readers bettere understand the effectiveness and underlying mechanism of the propose methods. 1. The environments used in the experiments are still limited. I encourage to supplement more environments to demonstrate the applicability of your proposed method is possible. Otherwise, we may argue if the solution can only be effective on some specific kinds of tasks.
2. Considering the proposed method needs to train the new dynamics models and meta-policy simultaneously, the complexity of this method and the training stability/convegence are encouraged to be clarified and analyzed.
3. The assumed accessibility to the task reward function and initial state distribution is often unrealistic in the real applications. 1. I am curious if totally no interaction data, how can the generated dynamics model approximates the real dynamics in the target environment. It seems there lacks enough grounding points to support this potential. Does there exist the probability that the generated dynamics models are far from the dynamics in the target environment? I hope to see more analysis on this during the rebuttal.
2. The D4RL benchmark in your experiments is all Mujoco tasks with low input dimensions. Could you please consider incorporating some more high-dimensional task, in which the hypothesis space is too large to narrow down?
3. In the paper, you claim that the interaction data is only used to narrow down the hypothesis space. But could you please consider how to utilize these interaction data in a more direct way to better facilitate the policy learning as the complement to the purely dynamics model learning, like finetuning the learned meta policy? Besides, I cannot agree the statement that the biasedness in the interaction data will somehow hinder the policy optimization in traditional offline RL methods. If such pre-collected trajectories are expert ones or near-optimal ones, such *biasedness* can actually help avoid some low-value and dangerous states.
4. Considering your method encourages the diversity in the model learning part, some learned dynamics models may be unreasonable though the meta policy can still achieve high returns via planning in such models, like violating the physics laws or economics laws. And I can hardly expect the *eligibility* part in your method can help alleviate this 'short-path' issue. More explanations and discussions are encouaged during the rebuttal phase.",614,0,11,0.7928000000000001,0.0638390498,0.9844013453,59,22.4917,0.4435,iclr,0.0,3,4,3,4,partially factual,4,4,75,polite,4,neutral,4,low,4,5,4,4,factual,4,4,88,polite,5,neutral,5,none,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,positive,5,low,5,5,4,4,partially factual,4,4,85,polite,5,neutral,5,low
196,Reviewer-MuxB,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","This paper leverages publicly available data, termed as Proxy Data (PD), as a substitute for original data (OD). The paper addresses the limitations of existing ZSQ methods that rely solely on synthetic data (SD) by introducing a method to select optimal PD based on batch-normalization statistics. The ZeroP framework is applied to existing pure-SD methods, resulting in significant improvements in accuracy. Specifically, ZeroP outperforms state-of-the-art pure-SD methods by 3.9% in a 4-bit setting for ResNet-50 on ImageNet-1K. The paper also introduces a simple and effective method for guiding PD selection, thereby offering a promising solution for achieving high-performance low-bit networks without relying on original data. 1. The paper introduces a new approach to ZSQ by incorporating publicly available Proxy Data, filling a gap in the existing literature. A comprehensive methodology is provided, including a PD selection method based on batch-normalization statistics, which adds to its credibility.
2. ZeroP shows significant improvements in accuracy over existing methods in low-bit settings. 1. While the paper discusses improvements in accuracy, it does not provide sufficient information on the scalability of the proposed method, especially when dealing with larger datasets or more complex models.
2. Lack of performance in low-bit settings, such as 2-bit and 1-bit. I wonder whether the methods used PD can have a competitive performance over the previous quantization/binarization methods.
3. It is better to provide the preliminary knowledge of the proxy data, and how previous work uses the proxy data for the quantization. 1. How well does the proposed ZeroP framework generalize to other types of neural networks or tasks beyond image classification?
2. As for the computational overhead, could you elaborate on the computational cost involved in the PD selection process, and how the computational overhead of the selection of PD compared with the computations in the training process?",300,0,7,0.7922,0.1989015152,0.9175000787,54,22.975,0.3848,iclr,0.0,5,5,5,4,factual,5,5,80,polite,5,neutral,5,none,4,4,4,4,5,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,3,3,factual,4,4,75,polite,4,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,5,low
196,Reviewer-7Mby,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper introduces a new quantization-aware finetuning method for visual recognition that does not rely on the original training data (OD). The proposed method, ZeroP, instead leverages realistic proxy data (PD) in addition to the conventional synthetic data (SD) to further finetune the model for quantization. Here, incorporating PD based on the batchnorm statistic (BNS) is the key contribution of the paper. Experimental results show that ZeroP outperforms SD-only approaches and performs on par with OD-based works. (S1) \[Motivation\] Going beyond synthetic data for zero-shot quantization is interesting. The reviewer agrees with the author that it is not necessary to rely solely on synthetic data, especially when relevant  information of the target task is available.

(S2) \[Performance\] The proposed method demonstrates superior performance.

(S3) \[Ablation\] Ablations show that PD could be a plug-in solution that helps improve the performance of SD-only methods in general.

(S4) \[Writing\] The paper is easy to follow. (W1) The current method to select the optimal PD dataset is straightforward, i.e. ranking the PDs by the gap of the BNS. The technical contribution is weak.

(W2) Relying on BNS also limits the versatility of ZeroP (as also indicated in the Limitation section)

(W3) If I understood correctly, the key challenge here is to search for PDs that mimic the distribution of the OD. In this case, using only BNS may not be necessary. Depending on the target task, there may be more information we could make use of, e.g. the class names of the target task. (If the finetuning involves a classification loss, this information may already be available.) With such information, instead of searching for a specific PD dataset, we could search for relevant samples via a text-based search engine, e.g. CLIP. 

Overall, the reviewer likes the idea of incorporating PD for zero-shot quantization, and also appreciates the superior performance of ZeroP. The reviewer has concerns about the technical contributions and the potential impacts of the paper. Therefore, the reviewer rates the paper as marginally below the acceptance threshold. N.A.",335,0,2,0.7707,0.1599533279,0.8717119694000001,54,38.8684,0.157,iclr,0.0,2,5,3,1,factual,4,3,45,polite,4,neutral,5,low,3,5,5,4,5,5,5,75,5,5,negative,5,low,2.0,5.0,4.0,3.0,factual,4.0,4.0,60.0,polite,4.0,positive,3.0,none,3,5,4,3,factual,4,4,75,polite,5,neutral,4,low,3,4,4,4,factual,4,4,85,polite,5,neutral,4,low
196,Reviewer-AkjD,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","The paper presents ZeroP, a novel approach for the Zero-Shot Quantization (ZSQ) task. The approach aims to investigate the potential gain of Proxy Data (PD) across 16 commonly used CV datasets. In addition, the paper introduces the BNS distance as a simple yet effective metric for selecting suitable PD for a specific task. - The paper introduces the BNS distance metric which provides a simple yet effective means to select suitable Proxy Data for a given task.
- The paper conducts thorough experiments showing that ZeroP outperforms existing pure-SD methods by a significant margin across diverse datasets.
- The work is relevant given the need for efficient methods in the ZSQ space without relying on original data. - The approach, while novel in certain aspects, leans heavily on established methodologies such as pure-SD. The introduction and utilization of Proxy Data, although effective, do not drastically deviate from methods previously explored in the domain of data-free tasks.
- The paper mainly focuses on 4-bit and 5-bit quantization, leaving questions about the performance and relevance of other bit quantizations. - The focus on 4-bit and 5-bit quantizations was evident, but it raises the question: what about other bit depths? Were experiments conducted with other bit quantizations, and if so, what were the results? Elaborating on this could provide a broader understanding of the system's applicability.",223,0,0,0.7581,0.1654220779,0.9028347731,54,34.8749,0.0999,iclr,0.0206185567010309,5,4,4,4,factual,5,5,70,polite,5,neutral,5,none,4,5,4,4,factual,5,5,85,polite,5,neutral,5,low,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,positive,4,low,2,4,3,3,factual,3,4,78,polite,5,positive,4,low
196,Reviewer-qH2g,ZeroP: Zero-Shot Quantization via Proxy Data,"Zero-shot quantization (ZSQ) is a promising approach for achieving low-bit constraint networks without relying on the original data (OD). However, due to the high cost and privacy concerns associated with OD, it is often scarce, leading to the unsatisfactory performance of ZSQ. Most ZSQ methods rely solely on synthetic data (SD) to mitigate this issue. In this paper, we propose a novel ZSQ framework, named ZeroP, that leverages publicly available data - proxy data (PD) - as a substitute for the OD. We first explore the impact of PD on the performance of current ZSQ methods over 16 different computer vision datasets and introduce a simple and effective PD selection method based on batch-normalization statistics(BNS) to select the optimal PD. We then apply ZeroP to three state-of-the-art pure-SD (using only SD) methods, achieving 7% to 16% improvements in accuracy for MobileNetV1 on ImageNet-1K in a 4-bit setting. Furthermore, we demonstrate the effectiveness of ZeroP on extensive models and datasets. For example, ZeroP achieves a top-1 accuracy of 72.17% for ResNet-50 on ImageNet-1K in a 4-bit setting, outperforming the SOTA pure-SD method by 3.9%. Overall, our results indicate that ZeroP offers a promising solution for achieving high-performance low-bit networks without relying on original training data and opens up new avenues for using publicly available data for data-free tasks.","A simple but intuitive method that uses proxy data for ZSQ. To find the most suitable proxy data, a BNS-based distance is used where a small BNS distance indicates a higher relation between proxy data and original data. This method is simple but effective. It does provide a SOTA performance. Comprehensive and impressive experiment results.
This paper is valuable. The novelty of this paper appears constrained, particularly when considered with an earlier work that seemingly shares a similar idea.

\[1\] ""Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization,"" CVPR2021Workshop.

Note that \[1\] is an accepted paper, not a preprint paper. However, I can't find any reference to \[1\] within this manuscript. While it's not feasible to reference every related work, the conceptual overlap with \[1\] is pronounced. \[1\] primarily targets PTQ, but it does not involve real data and can be regarded as a ZSQ method.  And the only different point is the select metric.

I think this paper is valuable. However, more experiments for comparison are needed. See weaknesses",176,5,1,0.8331000000000001,0.2364035088,0.7975381613,54,44.2552,0.0529,iclr,0.021978021978022,0,5,2,0,partially factual,4,4,30,neutral,3,negative,3,none,3,4,4,3,partially factual,4,4,75,polite,5,positive,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,positive,3.0,none,2,4,3,3,factual,3,3,65,polite,4,positive,4,low,2,4,3,4,partially factual,3,3,75,polite,5,positive,4,low
48,Sergio-Luis-Náñez-Alonso,Cross-sectional data on stablecoin characteristics,"The article presents a dataset on the characteristics of stablecoins. Stablecoins represent a relatively young but increasingly important branch of the cryptocurrency market. Although they all share the same goal of maintaining a stable value in the digital market, they form a highly heterogeneous group. They differ in terms of collateral and stabilization mechanism, peg, availability of the technical documentation, presence on crypto exchanges or age. The dataset is cross-sectional and was created based on internet research. Individual information was collected from websites of the stablecoin projects and a crypto-data aggregator, and to a lesser extent from other auxiliary sources (websites related to finance and cryptocurrencies). The dataset is unique as there are no publicly available databases encompassing the features of stablecoins. It can be used in all stablecoin-related analyses to characterise the examined coins and to investigate the relationship between cryptocurrency market developments and stablecoin features.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The data note under review presents a brief introduction to the characterization of digital currencies called Stablecoins. This has allowed the authors to build up a novel database on stablecoins, mainly by searching the Internet. It is therefore a brief scientific review of the current state of the art on stablecoins, proposing a database that can be used by other researchers in their studies. It is in this last point that the value of the study lies. After reviewing the data note, it can be qualified as highly original, given that there are no other cross-sectional databases available for consultation by potential cryptocurrency researchers. This means that the contribution to scholarship is also high.  Regarding the structure, the data note under evaluation is of the short-paper type, so the introduction is sufficient.  There are a few issues that should be improved by the authors: In the methodology section, the authors should refer to previous database generation studies with their limitations. In the data description section, the authors should indicate a valid reason why only 30 Stablecoins were selected. In other words, originality in the attempt to construct this database is appreciated. The methodology details the criteria for selecting the sample of 30 stablecoins based on the information that appears in CoinMarketCap, the websites of the stablecoins themselves and other websites (at this point, they could mention some, perhaps including references). I understand that of the 98 listed on CoinMarketCap as of May 2022, many were excluded (down to 30) for the reasons stated. I don't know if Terra USD is no longer classified as a stablecoin after the crash that month (it dropped 40% in value). Do you guys consider keeping it in the sample? If so, I would like you to explain. I find table 1 very interesting as it raises 14 characteristics (a sufficient number) and a description of these. It is a research note that adds value to academic research on this topic. I recommend, however, to expand the references, either in the text or in Table 1, as there are many publications on stablecoins, in order to characterize stablecoins with previous studies and authors. Finally, I thank you for inviting me to review this data note. I found it relevant and interesting.  Is the rationale for creating the dataset(s) clearly described? Yes  Are the protocols appropriate and is the work technically sound? Yes  Are sufficient details of methods and materials provided to allow replication by others? Yes  Are the datasets clearly presented in a useable and accessible format? Yes",490,0,1,0.7868,0.118260582,0.9205379486,49,43.12,0.8817,f1000,0.010204081632653,5,5,5,5,factual,5,5,95,polite,5,positive,5,none,5,5,4,5,factual,5,5,95,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,4,4,85,polite,5,positive,4,low,3,4,4,4,factual,4,4,85,polite,5,positive,3,low
45,Elizabeth-A.-Stokes,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper reports a within-trial cost-utility analysis (CUA) and cost-effectiveness analysis (CEA). The paper is clearly written, and appropriate methods have been used to conduct analyses. The text around interventions being cost-effective where findings are reported in terms of cost per QALY lost is very well explained. I have the following comments: A CUA and a CEA were planned, did you pre-specify which was the primary analysis?  Introduction – first sentence – who is at-risk?  Resource use was captured at baseline, 6 and 12 months. Did the questionnaires at each of these time points ask participants to recall their resource use over the previous 6 months? Was resource use captured at baseline solely for the purpose of including baseline costs in the multiple imputation models?  Did you explore the missing at random assumption?  Costs – resource use was captured on day cases, but no unit cost for this is reported in Table 1. Were there no participants who reported a day case admission? Were hospital admissions not captured as there is no chance that this patient group would be admitted for hand OA? In the introduction, surgery is cited as one of the high costs in this patient group.  The mean difference between groups and 95% CI is presented in Table 3 for costs and Table 5 for EQ-5D utilities, but not in Table 2 for resource use? It would help the reader to include this.  Table 3 – did you consider separating medication costs into HCQ and other medications?  The time horizon for the CUA was 12 months but for the CEA was 6 months? While the primary clinical outcome of hand pain severity was measured at 6 months, this was also captured at 12 months. Why was your analysis for this outcome based on a shorter time horizon than the CUA analysis? Was a CEA over 12 months a pre-planned sensitivity analysis?  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",457,0,1,0.7362000000000001,0.1312373737,0.8965256214,35,45.96,0.1879,f1000,0.0,4,5,4,4,factual,4,5,90,polite,5,positive,4,none,4,5,4,4,factual,5,5,85,polite,5,neutral,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,neutral,4.0,none,4,5,4,4,factual,5,5,85,polite,5,positive,5,low,3,4,4,4,factual,4,5,85,polite,5,neutral,5,low
45,David-Mark-Epstein,Cost-effectiveness of hydroxychloroquine versus placebo for hand osteoarthritis: economic evaluation of the HERO trial,"Background: An economic evaluation alongside the Hydroxychloroquine Effectiveness in Reducing symptoms of hand Osteoarthritis (HERO) trial was undertaken to assess the cost-effectiveness of hydroxychloroquine compared with placebo for symptomatic treatment of hand osteoarthritis for patients with at least moderate hand pain and inadequate response to current therapies. Methods: A trial-based cost–utility analysis was undertaken from the perspective of the UK National Health Service and Personal Social Services over a 12-month time horizon, using evidence from 248 participants included in the HERO trial, conducted in England. Patient-level data were collected prospectively over a 12-month period, using participant-completed questionnaires and investigator forms, to collect healthcare utilisation, costs and quality-adjusted life years (QALYs) using the EQ-5D-5L. The base-case analysis was conducted on an intention-to-treat basis and used multiple imputation methods to deal with missing data. Results were presented in terms of incremental cost-effectiveness ratios (incremental cost per QALY) and net health benefit, with uncertainty surrounding the findings explored using cost-effectiveness acceptability curves. Results: The base-case analysis estimated slightly lower costs on average (−£11.80; 95% confidence interval (CI) −£15.60 to −£8.00) and marginally fewer QALYs (−0.0052; 95% CI −0.0057 to −0.0047) for participants in the hydroxychloroquine group versus placebo group at 12 months. The resulting incremental cost-effectiveness ratio of £2,267 per QALY lost indicated that although costs were saved, health-related quality of life was lost. Even assuming symmetrical preferences regarding losses and gains for health benefits, the findings do not fall within the cost-effective region. Similar findings arose for analyses conducted from the societal perspective and using complete cases only. Conclusions: This economic evaluation indicates that hydroxychloroquine is unlikely to provide a cost-effective pain relief option for improving health-related quality of life in adult patients with moderate-to-severe hand osteoarthritis.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The authors conduct an economic evaluation alongside the RCT. There were no differences found between the groups in terms of hand pain or quality-of-life and no significant differences in costs.  Although there were no differences, it is nevertheless worthwhile publishing these results, in order to avoid ""publication bias"" and guide future research in this area. The study, in general, is well conducted and I have no comments on technical matters.  Rather than calculate an ICER, which implies some measurable difference in outcomes and costs, personally, I would interpret the results in the abstract and conclusions that there were no meaningful or statistically significant differences in any outcomes or costs at 1 year.  The authors do not discuss other therapies or research in this area and this contextual comparison would be useful.  Is the work clearly and accurately presented and does it cite the current literature? Partly  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Partly  Are the conclusions drawn adequately supported by the results? Yes",274,0,1,0.7818,0.1495833333,0.8935310245,115,24.68,0.1213,f1000,0.01,2,4,2,2,partially factual,3,2,60,polite,3,positive,3,moderate,3,5,3,3,partially factual,4,4,65,polite,4,neutral,4,moderate,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,neutral,4.0,none,2,4,3,3,factual,4,4,70,polite,4,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
87,Setya-Haksama,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved With Reservations  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  1. All variables should be written clearly and systematically, first the independent variables should be described, then the dependent variables should be described. 2. Resources of data from World Bank was too old. 3. No data was obtained from 40 countries measured in relation to this research, there should be a ranking for each country that can indicate which countries have good scores and which countries have low scores.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Partly  If applicable, is the statistical analysis and its interpretation appropriate? Partly  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Partly",214,0,3,0.7434000000000001,0.191,0.7335164547,38,34.76,0.1041,f1000,0.0,3,3,2,2,partially factual,2,2,25,neutral,2,negative,2,moderate,4,4,3,4,partially factual,4,4,65,polite,4,negative,3,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,3,3,3,partially factual,3,3,60,neutral,4,neutral,3,moderate,3,4,3,4,partially factual,3,4,75,polite,4,neutral,4,low
87,Mohamed-Adil-AA,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  The article highlights important areas in the arena of globalization and the spread of infectious diseases. The article particularly looks into data from a number of countries globally, thus increasing the validity and reliability of the study across continents and also globally.  Could this study be replicated by using longitudinal data to establish causality and stronger inferences? Do the path regression results provide more robust results than OLS analysis? What was the main logic in choosing only a specific set of covariates and not all the possible covariates for tuberculosis?  This a good study and will help in addressing many lacunae in the area of global health research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",251,0,1,0.7655000000000001,0.1954301075,0.8643754721,41,26.51,0.072,f1000,0.0096153846153845,2,3,2,2,partially factual,3,3,35,polite,3,positive,3,low,3,5,4,4,partially factual,4,4,80,polite,5,positive,3,moderate,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,2,4,3,2,factual,4,3,60,polite,4,positive,3,moderate,2,4,3,3,factual,3,4,75,polite,5,positive,4,low
87,Arutselvi-Devarajan,Globalization and life lost due to tuberculosis: evidence from a multi-country study,"Background: Tuberculosis affects around 30% of the population of the world. Tuberculosis causes an increase in early mortality and thus has the potential to increase the number of years of life lost. Globalization directly or indirectly by affecting the factors that increase the susceptibility for tuberculosis infection has the potential to increase the spread and mortality due to tuberculosis. This study assessed the causal link between globalization and the years of life lost due to tuberculosis. Methods: Data from the Demographic and Health Survey (DHS) and World Bank for 2004 and 2005 were used for a number of covariates and possible mediators. Data from the Institute of Health Metrics and Evaluation (IHME) were used for the outcome variable and important globalization indicators. The primary health outcome that was studied is tuberculosis and the measure that was used to quantify tuberculosis mortality is the years of life lost (YLL). Path analysis was used. Results: The main independent variables of economic and social integration were not statistically significant. For every unit increase in the proportion of people that were using treated drinking water, there was a -0.0002 decrease in the YLL due to tuberculosis. For every unit increase in the proportion of people with earth floor, there was a 0.0002 units increase in YLL due to tuberculosis. For every unit increase in the proportion of people living using clean fuel, there was a 0.0004 decrease in the YLL due to tuberculosis. Conclusions: Social and economic globalization have no effect on the years of life lost due to tuberculosis, highlighting that globalization actually does not contribute to tuberculosis mortality. However, improving other important determinants such as sanitation, providing safe drinking water and clean households will reduce the mortality due to tuberculosis, highlighting the need to invest in them.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  This paper explored an important aspect of the public health issue of TB and its association with globalization. I have some suggestions for the authors: The nature of data was cited as the reason for not being able to completely explain the causal link, I suggest the authors mention only association (as the data may exhibit some correlation but not causation) instead of the ""causal link"" in the objective.  Although the current introduction is good, it would be better if there are more indirect indicators or covariates that affect tuberculosis incidence.  The methods section is good and elaborate. The aspects of globalization - economic and social, and other aspects of globalization could also be considered in this research or for future research.  The main outcome variable is Years of Life Lost due to tuberculosis. It would be much better if disability-adjusted life years could have been used in future papers to expand this research.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",297,0,1,0.765,0.2096153846,0.8910351992000001,42,33.14,0.2025,f1000,0.0097087378640776,4,4,4,4,factual,3,3,60,polite,4,positive,4,low,4,5,4,5,5,5,5,85,5,5,5,4,3,3.0,5.0,5.0,4.0,factual,5.0,5.0,80.0,polite,5.0,positive,4.0,none,4,5,4,4,factual,5,4,85,polite,5,positive,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
6,Reviewer-7LW7,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","The main contribution of this paper is presenting a scalable path-based method A*Net, for link prediction on large-scale knowledge graphs. A*Net is inspired by the A* algorithm for solving shortest path problems, where it learns a priority function to select important nodes and edges at each iteration. This allows for the time and memory reducing for both training and inference. From an efficiency perspective, this could be considered as a path-pruning method to progressively reduce the subgraph based on the learned priority function. The empirical results also demonstrate efficiency improvement. 1. The efficiency problem caused by the explosively increasing entities in deeper propagation layers is indeed serious in the recent GNN-based inductive methods. And the proposed method makes sense and technically sound. 

2. The experimental results are impressive. The paper demonstrates the practical applications of A*Net in various settings and datasets, with the efficiency improvement compared with several recent baselines. Furthermore, the paper sets a new state-of-the-art on the million-scale dataset ogbl-wikikg2 and converges faster than embedding methods. 

3. The paper's organization is well-executed and the content is easily comprehensible.
 1. The paper's comparison to the A* algorithm seems somewhat overstated. As a derivative work of NBFNet, this paper draws an analogy to another shortest path algorithm, A*. Contrary to the Bellman-Ford algorithm that resolves the shortest path problem from the source to all other points, the A* algorithm typically addresses the shortest path problem from the source to a specific target point. However, in the context of knowledge graph (KG) reasoning, the target point is unknown, rendering the core principle of A*, assessing the estimated remaining cost to the target point, unfeasible. In fact, the A* algorithm's priority rule, involving the distance to the target node, is not pertinent to the priority function in the proposed model. The A* algorithm appears to function primarily as a promotional point, rather than as a guiding principle.

2. Perhaps due to the overemphasis on the A* analogy, the paper's true technical contributions remain unclear. Comparing the core function of NBFNet in Eq. 3 and that of A*Net in Eq. 12, the only discernible difference lies in introducing the priority score, calculated based on the embeddings of the query and the current node. Stripping away the A* algorithm framework, it essentially seems to be a path-pruning technique reliant on an attention mechanism to select the top K nodes and top L edges in each layer for efficiency's sake.

3. The paper lacks insightful contributions regarding important paths beyond a weighted version of the NBENet method. The theoretical appendix focuses solely on integrating path selection into the NBFNet framework, premised on the assumption that a certain function can distinguish important nodes. However, how to ensure that important paths are chosen is not clear. In response to this, the authors propose weight sharing between the priority function and the predictor, asserting that the reasoning task can be seen as a weak supervision for the priority function. However, this appears counterintuitive, given that the priority score is dependent on a specific query. A high predictor score, indicating that the node x answers the query (u, r_1), should not contribute to the priority score of x for a different query (u, r_2).
 1. As addressed in Weaknesses 3, could you elaborate on how weight sharing aids in the selection of important paths?

2. I observe that two handcrafted priority functions, PPR and Degree, are employed in the ablation studies. Given that high connectivity doesn't necessarily denote the importance of paths, what about the effectiveness and efficiency of a random pruning strategy, particularly with respect to the obgl_wikikg2 dataset?

3. In the Visualization section, only the results of the proposed method are displayed without any comparison. Could you clarify what distinct paths the Neural function selects compared to the two handcrafted ones? Furthermore, does the Neural-based path selection align more closely with knowledge semantics?
 Yes. The authors stated the limitation, future work and social impact.",657,0,10,0.7662,0.1343045961,0.9303361773,218,36.4806,0.1798,neurips,0.0204081632653061,4,4,5,4,partially factual,4,4,90,polite,4,neutral,4,none,4,5,4,4,partially factual,4,4,88,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,5.0,positive,3.0,none,3,4,4,3,factual,4,4,80,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
6,Reviewer-rgXX,A*Net: A Scalable Path-based Reasoning Approach for Knowledge Graphs,"Reasoning on large-scale knowledge graphs has been long dominated by embedding methods. While path-based methods possess the inductive capacity that embeddings lack, their scalability is limited by the exponential number of paths. Here we present A\*Net, a scalable path-based method for knowledge graph reasoning. Inspired by the A\* algorithm for shortest path problems, our A\*Net learns a priority function to select important nodes and edges at each iteration, to reduce time and memory footprint for both training and inference. The ratio of selected nodes and edges can be specified to trade off between performance and efficiency. Experiments on both transductive and inductive knowledge graph reasoning benchmarks show that A\*Net achieves competitive performance with existing state-of-the-art path-based methods, while merely visiting 10% nodes and 10% edges at each iteration. On a million-scale dataset ogbl-wikikg2, A\*Net not only achieves a new state-of-the-art result, but also converges faster than embedding methods. A\*Net is the first path-based method for knowledge graph reasoning at such scale.","This paper proposes a scalable path-based knowledge graph reasoning approach. The idea is to extend only important paths from the exponentially growing set of all possible paths. A heuristic priority function is parametrized by a feed-forward network and is trained to predict the priority of nodes to expand. Experiments show that the proposed approach can significantly improve time and memory efficiency and also achieve good results. - Scalability is an important issue for path-based reasoning approaches. The idea of selecting only important paths is interesting and sounds reasonable
- The proposed approach is effective and supported by extensive experiments. Time and memory efficiency has been significantly improved. Benchmark results are also good. My concern is mainly about the design of the priority function Eq (10)

In Eq (10), the first part $h_q^{(t)}(u, x)$ is already conditioned on q, u, and x, so in principle the second part $g(\[h_q^{(t)}(u, x), q\])$ doesn't provide any additional information. Therefore, the priority function is purely based on the current path from the start and contains no information about the goal. In other words, the prediction of the priority function would be the same even if the goal changes. This is different from the design of the A* algorithm and may lose theoretical guarantees. 

It is not appropriate to present the approach in the manner of A* algorithm Please see Weaknesses properly addressed",228,0,0,0.7526,0.1886904762,0.8858633041,218,38.7064,0.2971,neurips,0.0,3,4,4,3,factual,4,3,65,polite,4,neutral,5,low,3,5,4,4,factual,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,neutral,4,neutral,4,low,2,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
193,Nitin-Liladhar-Rane,What we know and what should we know about the future of blockchain in finance,"Background In response to the transformative impact of blockchain technology on economic and financial landscapes, there is a critical need for a review study that analyses the knowledge landscape from diverse perspectives.  Methods This research VOSviewer, and Bibliometrix to undertake a bibliometric analysis of the expanding literature related to blockchain technology within the financial sector. Through a examination of 500 published articles, the study identifies insightful trends, patterns, and emerging domains on a global scale.  Results The findings highlight the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Conclusions While affirming the potential of blockchain, the analysis also sheds light on persistent impediments hindering its widespread adoption and utilization. This study not only contributes to the current understanding of blockchain in finance but also serves as a valuable resource for future researchers. It guides systematic reviews by pinpointing prominent journals and influential authors within the dynamic field of blockchain finance, thereby fostering a deeper understanding and facilitating further exploration in this evolving field.","Approved  info_outline Alongside their report, reviewers assign a status to the article:  Approved The paper is scientifically sound in its current form and only minor, if any, improvements are suggested  Approved with reservations A number of small changes, sometimes more significant revisions are required to address specific details and improve the papers academic merit.  Not approved Fundamental flaws in the paper seriously undermine the findings and conclusions  ""What we know and what should we know about the future of blockchain in finance"" Authors' have made a good attempt by highlighting the advancing trajectory of blockchain research in finance, with a notable concentration of studies originating from the United States and China, both in terms of total publications and citations. Key thematic clusters identified include “smart contracts,” “financial institutions,” “initial coin offerings,” and “big data analytics.” Intersections with financial risk management, digital transformation, and the integration of big data analytics with artificial intelligence and machine learning are particularly noteworthy, marking focal points of exploration.  Is the work clearly and accurately presented and does it cite the current literature? Yes  Is the study design appropriate and is the work technically sound? Yes  Are sufficient details of methods and analysis provided to allow replication by others? Yes  If applicable, is the statistical analysis and its interpretation appropriate? Yes  Are all the source data underlying the results available to ensure full reproducibility? Yes  Are the conclusions drawn adequately supported by the results? Yes",239,0,1,0.763,0.1354691877,0.9726446271,35,21.84,0.1303,f1000,0.010204081632653,1,1,2,3,unfactual,3,2,50,neutral,1,neutral,2,high,2,5,4,2,factual,4,3,70,polite,5,positive,5,none,3.0,5.0,5.0,4.0,factual,5.0,5.0,90.0,polite,5.0,positive,4.0,none,1,4,3,2,factual,4,4,60,polite,4,positive,3,low,3,4,4,3,factual,4,4,85,polite,5,positive,3,low
8,Reviewer-oy34,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","Briefly summarize the paper and its contributions. This is not the place to critique the paper; the authors should generally agree with a well-written summary.

The paper proposes AdaPlanner, an LLM-based adaptive planner for text-based sequential decision-making tasks. The planner is adaptive in the sense that it can refine the generated plan/policy based on feedback. 

The contributions made in this paper include the following
1. interacting with the environment with LLM in the loop
2. a code-style prompt is engineered for LLMs to output a policy 
3. refining the LLM policy for the current task based on feedback
4. prompt tuning for new tasks based on previous interaction (termed skill discovery)

The proposed AdaPlanner is evaluated on two text-based sequential decision-making environments ALFWorld and MiniWoB++. Their experiments indicate that with feedback, LLMs can adapt the plan.
 
* The paper is well written.
* The paper focuses on extremely relevant and signifcant problems. 
 * I find the paper lacks significant details. Please see the next section for the list of questions.
* The paper employs sloppy mathematical notations.
* The paper lacks the rigor of scientific evaluation. 
* Paper misses all references to LLM-based approaches for planning with PDDL. The one that I find most relevant for code generation is ""Generalized Planning in PDDL Domains with Pretrained Large Language Models, Tom Silver, Soham Dan, Kavitha Srinivas, Joshua B. Tenenbaum, Leslie Pack Kaelbling, Michael Katz”
 
**Major**

1. How is the programmatic response from LLM converted to action responses? Did the conversion require manual intervention? For instance, Figure 2 has an indentation error which would result in a wrong plan. Were such indentation errors evaluated manually? Can authors provide a list of errors made by LLMs? 
1. In line 167, what does an alignment between ‘anticipated plan’ and environment mean? How does the AdaPlanner observe the alignment? 
1. Can authors provide details about the size of the task used in the prompt (for samples) vs the size of the task that was successfully solved by AdaPlanner? To establish the claim of sample efficiency, it is important to understand if the planner is able to efficiently plan for tasks that are significantly different from the prompts.
1. The X-axis in Figure 3 indicates `# Samples per task`. Is this the number of samples provided for each trajectory? Or sum?  
1. What was the length of plans or length of trajectories generated by AdaPlanner vs other approaches? To claim the effectiveness of the AdaPlanner, it is important to compare the length of successful trajectories.
1. For skill discovery, how is the solution converted to the skill? How are skills represented? How large is the skill memory?  Were the discovered skills included in the count of samples used for training as they are training samples for the next set of trajectories?
1. It is not clear how skills are filtered and what criteria are used for the evaluation and ranking of skills.
1. What is the connection between skill discovery and prompt tuning?
1. The success rate of ""With SD"" in Figure 4d looks significantly reduced from  Figure 4a. Were different settings used for theses experiments?
1. At various places, the paper mentions ""environment feedback"". In my opinion, this is a misnomer. The feedback is not from the environment. The environment just provides the next observation, the feedback is generated by the agent itself. And the use of observation to refine a plan or next action is quite standard practice in RL. I would highly recommend dropping the term feedback from the title. 
1. The use of term plan and policy is a little confusing. A plan is a sequence of actions. A policy is a mapping from states to actions. By this definition, the `solution()` function is as a policy. In preliminaries, the planning policy ($\rho$) is conditioned on a previous plan $P_t$. However, the appendix describes the refinement prompt using the assertion error (instead of `solution()`). Isn't the assertion error providing information about the policy (the `solution()` function)? So I am confused by the terminologies. Is the $\rho$ refined conditioned on the policy or the plan? The usage of these terms is also confusing in the Preliminary section. Request authors to precisely define the mathematical notations and highlight what they represent in the examples.

**Minor**

12. In line 387, there are extra curly braces.
12. The notation $\rho$ is used in line 73 but introduced much later.
12. As the context $c_t$ is defined as a sequence of action and observations from time step $0$ to $t$, it is not clear what $c_{>t}$ means (in line 116).  
12. Open-Loop system in Figure 1 should have an arrow going from env to planner with $o_1$.
12. Statement in Line 144 ""To generate a plan .."" looks like a repetition of Line 141 ""To generate an initial plan...""
12. In line 116, if $h_t$ is obtained from $c_t$ then would it not be captured in $c_{>t}$? An example of $h_t$ would help better understand the proposed update.
12. In line 73, as $\rho$ is defined using $\Delta(A^{T})$. But the length $T$ is not fixed. 
12. In line 73 $\rho$ is defined where a plan is conditioned only on observation and goal. However, later it is conditioned on the context, plan, and goal. 



 
* The evaluations are restricted to text-based sequential decision-making problems and task where the inadmissible actions do not cause drastic changes in the environment. On the contrary, inadmissible actions are like no-ops. Further, the paper does not present analysis of plan length. Hence, the analysis is limited to zero risk environments. 
* The claim made in the abstract about skill discovery mechanism enabling agent to plan with fewer task demonstration is not substantiated in the evaluations. Evaluation in Fig. 4d only established improvement in success rate, not sample efficiency. ",965,0,17,0.7161000000000001,0.0723501082,0.8610098362,215,54.1142,0.1104,neurips,0.0,5,4,5,5,factual,3,4,90,neutral,4,neutral,4,none,5,5,5,5,5,5,5,95,polite,5,neutral,5,none,4.0,4.0,3.0,3.0,partially factual,3.0,2.0,60.0,polite,4.0,neutral,4.0,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3,4,4,4,partially factual,4,4,85,polite,5,neutral,5,low
8,Reviewer-4ZaS,AdaPlanner: Adaptive Planning from Feedback with Language Models,"Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively. The implementation of AdaPlanner is available at https://github.com/haotiansun14/AdaPlanner.","This paper looks at explicit closed-loop systems with LLMs for adaptive planning utilizing environmental feedback. They showcase better planning performance on ALFWorld and MiniWOB++ environments over existing state-of-the-art works like ReAct and Reflexion. The paper is well written and the experiments are thorough. They present an interesting improvement over the current works like ReAct and Reflexion.
 1. The kind of tasks in these domains don’t seem to have interaction resolution where there are multiple conflicting causal links from the initial to the goal state which have to be resolved (including negative interactions between subgoals). This could also lead to the human demonstrations helping significantly with the  It would be useful to analyze the performance of AdaPlanner specifically in such cases. 

2. I think non-ergodic environments could clearly pose danger to such agents. It would be interesting to see how AdaPlanner can perform against ReAct or Reflexion in such environments. 
 1. Given that the LLM seems to verify the plan to determine its feasibility, what was its efficiency in those assessments? Are there any results pertaining to that?

2. Is there any classification of the tasks with respect to their hardness?

3. For how many of these tasks did the human expert demonstration solve the task? 
 The authors have addressed some of the limitations. I have provided some limitations in the weaknesses section.",222,0,5,0.789,0.1708333333,0.7852613926,215,44.4049,0.1249,neurips,0.011111111111111,3,3,3,3,partially factual,2,4,50,neutral,3,neutral,3,moderate,4,4,4,4,partially factual,4,4,85,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,3,3,3,3,factual,4,3,70,polite,4,positive,4,moderate,4,4,3,4,partially factual,3,3,78,polite,5,positive,4,low
107,Reviewer-PbML,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The authors argue that an RL agent should use language to predict the next state of the world, which will empower them with the ability to understand the world and thus generate a better policy, instead of directly learn to map language into actions. They propose to build a world model that can predict future language, video and rewards, and demonstrate that training an agent with the world model achieves better performance over other baselines. 1. The motivation is interesting and convincing. The large language models learn rich knowledge about the world by only predicting the next word, so it is reasonable to hypothesize that utilizing language for future prediction is a better way to help agent understand the world.
2. Experimental results show that the proposed method outperforms the baselines. Although the motivation is promising, the method and experiments do not support the claim.
1. It is confusing that the authors use a multimodal model including both text and images to demonstrate the idea of using language to model the world. Images also convey general knowledge and describe the state of the world, then why can't we also model the world with images / videos? The authors should provide more evidence to demonstrate the unique importance of language to support their claim.
2. The method proposed in this paper is quite like the Dreamer V3 model \[1\] with additional text input. In Dreamer V3 paper, they have already demonstrated the effectiveness of their method, and the authors seem to simply apply it on environments that include text. Then, how to clarify that the improvements come from the the model architecture itself or the text part? There are no experiments to demonstrate this. Notice that the author even don't compare with other model-based methods that are more similar to their proposed method, although they claim they compared with them in the introduction.

\[1\] Hafner et al. Mastering Diverse Domains through World Models. arXiv 2023. The paper mentioned that at one time step only one text token will be included in the observations and the model output. I don't quite understand the setting here. If this is the case, then the setting is quite limited and it also conflicts with the example ""I put the bowl away"" you use in the introduction?",381,2,6,0.7718,0.1664021164,0.942080617,60,51.3976,0.2205,iclr,0.0092592592592593,3,4,3,3,factual,3,3,60,polite,4,neutral,3,low,4,4,4,4,factual,5,5,80,neutral,5,neutral,5,none,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,3,4,4,3,partially factual,3,3,70,polite,4,neutral,4,low,2,4,3,3,partially factual,3,3,65,polite,4,neutral,4,low
107,Reviewer-PggN,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This work proposes a conditional generative model that aligns both image frames and textual instruction tokens (one at a time) to produce multimodal future representations that can encompass visual frames, textual tokens, as well as motor actions, for controlling an agent in an environment.
The proposed method is claimed to align the visual-linguistic representations better, while encouraging the models to understand the world-dynamics in a generative modeling manner.
The method is tested on four simulated embodied environments where the agents follow certain language instructions, where performance gains are reported against two off-policy RL baselines. - The observed multimodal alignment mechanism is interesting and with experimental justification.
- The overall proposed method is neat, where the generative mechanism is a sound and interesting idea to model the visual-linguistic dynamics of the work.
- Consuming all modalities in one model as conditional generative models is neat.
- The paper is well written and easy to follow. - The title is a bit over-claimed, in the sense that the proposed model is still learning to model “one environment” at a time, particularly for the action dynamics as multimodal representation generation. At least an experiment or novel method is required to learn to model some worlds (environments) and generalize to a held-out test world – this would justify the “modeling the world” parts of the claims.
- While claimed to be flexible, in many applications, the instructions of a task will only take place at the beginning of the episode while the rest is the robots’ job to accomplish the instructed tasks, where the proposed multimodal alignment will only be performed from the beginning few frames of the episode. How does the proposed method work under such conditions? E.g., how would the method benefit from such an alignment in environments such as ALFRED \[1\] or TEACh \[2\]?
- In Section 4.4, the performance of the actual SOTA models need to be reported as well, even if the proposed method is inferior to them. There are reasons why modularization and use of certain foundation models is beneficial in these long horizon complex (at least closer to) real world tasks.
- The environments, if at all except for navigation, are all quite toy-ish, where the visual observations are of fairly low fidelity. Since the proposed method heavily relies on the future representation predictions, examining the method on more realistic embodied environments would strengthen the work more.

\[1\] Shridhar, Mohit, et al. ""Alfred: A benchmark for interpreting grounded instructions for everyday tasks."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.

\[2\] Padmakumar, Aishwarya, et al. ""Teach: Task-driven embodied agents that chat."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 2. 2022. - The proposed method shares some similarities with generative video-guided planning (at least at their high-levels), such as \[3\]. Could you elaborate more on why this is not an incremental concept on top of these works? (Also these works use supposedly much stronger generative models that can tackle more real-world visual observations.)
- What if the language instruction has a much shorter token span and the visual frames are much longer? How do they pad to each other or what would be the token used when language is exhausted out?
- Typos in “Future Prediction” of Section 3.1 – “whih” should be “which”.

\[3\] Dai, Yilun, et al. ""Learning universal policies via text-guided video generation."" NeurIPS 2023",568,6,11,0.7951,0.0924047619,0.8703980446,61,41.8289,0.2746,iclr,0.0,3,4,4,3,factual,3,3,65,neutral,4,negative,4,low,5,5,5,5,factual,5,5,95,polite,5,neutral,5,none,3.0,5.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,85,polite,5,neutral,5,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
107,Reviewer-X6yV,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","This paper addresses the challenge of enabling RL agents to comprehend and act based on complex language input. The proposed framework, Dynalang, enhances agent performance by incorporating language signals into the prediction of future states. Notably, Dynalang builds upon DreamerV3 by introducing text tokens into observations at each step. Experimental results demonstrate its effectiveness across various games, such as Homegrid, Messenger, Habbit, and LangRoom, outperforming previous language-conditioned RL baselines. 1. The paper addresses a compelling problem by enabling RL agents to understand intricate human language, expanding beyond straightforward task instructions, which is an understudied but important area in RL research.

2. The paper's writing, especially in the introduction, effectively highlights the core problem and how Dynalang provides a solution.

3. The study includes experiments across multiple game environments and consistently demonstrates improvements over existing language-conditioned RL methods. 1. The technical contribution is somewhat limited, primarily differing from DreamerV3 by adding text tokens to observations. A deeper exploration of Dynalang's components and their significance is needed. For example, an ablation study could help clarify the role of the language token in the world model.

2. The paper lacks a detailed ablation study that could validate the importance of each component in Dynalang. Explaining why the language token is necessary, particularly if it only serves as input for the policy network, would provide valuable insights.

3. While the paper explores various game environments, they appear simplistic. Evaluating the method on more challenging games, such as Crafter or Minecraft, would enhance the paper's credibility.

Overall, the paper presents an intriguing idea but requires further validation and clarification to strengthen its foundation. I look forward to discussing these points further in the rebuttal stage. 1. How does the paper ensure that the agent can effectively follow language corrections in the Homegrid environment? Are auxiliary reward signals used to guide agent learning?

2. Could you provide more details on the training process? Is the network trained from scratch, or is the world model pre-trained?

3. Have you considered using an LLM as the core of the world model, given its strong language modeling capabilities?",349,0,9,0.8536,0.1234353741,0.9215202332,49,30.7053,0.4104,iclr,0.0,4,5,4,4,factual,3,4,80,polite,5,positive,4,low,4,4,4,4,factual,4,4,82,polite,5,neutral,5,low,3.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,4,5,4,4,factual,4,4,85,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,3,low
107,Reviewer-bqw2,Learning to Model the World with Language,"To interact with humans and act in the world, agents need to understand the range of language that people use and relate it to the visual world. While current agents learn to execute simple language instructions, we aim to build agents that leverage diverse language—language like “this button turns on the TV” or “I put the bowls away”—that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that agents should interpret such diverse language as a signal that helps them predict the future: what they will observe, how the world will behave, and which situations will bring high reward. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We instantiate this in Dynalang, an agent that learns a multimodal world model to predict future text and image representations, and learns to act from imagined model rollouts. Unlike current agents that use language to predict actions only, Dynalang acquires a rich language understanding by learning to predict future language, video, and rewards. In addition to learning from online interaction in an environment, we show that Dynalang can be pretrained on text-only datasets, enabling learning from more general, offline datasets. From using language hints in grid worlds to navigating photorealistic home scans, Dynalang can leverage diverse types of language, e.g. environment descriptions, game rules, and instructions.","The paper proposes Dynalang, an agent that grounds language to visual experience via future prediction. The writing of this paper is clear, and the descriptions and justifications of the methods are comprehensible. This paper appears to have limited novelty, seeming more like a combination of existing techniques. What are the primary challenges addressed by the article? And what are its main contributions?",62,0,0,0.7567,0.1869047619,0.9115282297,49,40.0587,0.038,iclr,0.032258064516129,1,2,1,0,unfactual,1,1,20,neutral,2,negative,2,extreme,3,5,3,3,partially factual,3,3,65,polite,5,neutral,4,moderate,2.0,5.0,4.0,3.0,factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,none,1,3,2,1,partially factual,2,2,2,neutral,3,neutral,2,high,2,4,3,2,partially factual,4,3,60,polite,4,neutral,2,moderate
66,Reviewer-ocxo,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The motivation that inspired the work is important:  the pre-trained models are highly complex and difficult to fine-tune. However when you must train a model on a downstream task it could be that all the features that were learned on the source task are not necessary. So, if one had a way to select which features are relevant, then one could reduce the number of features needed to solve the downstream task and thus deploy smaller models. To address this task, they propose to  impose an L2 regularization which forces to find the most relevant features for the downstream task among all the features of the pre-trained models. The idea of using the features learned in different models on the same data points and merge them together to represent the input is nice and, to the best of my knowledge, novel. It also makes sense performing an automatic feature selection in that space, in order to select the feature combination which is more informative. The experimental result presented in support of the idea are partially convincing. I understand the attempt of providing a justification of the regularization loss using information theoretical bonds, but the way the authors arrive to the final form of the regularization they use, which is just a kernel version of the L2, eq. 9, is in my opinion unnecessarily involved and might create confusion. I suggest moving the text from eq 2 to eq 9 to the appendix.


MAJOR:  It is  not very clear why this form of R should help avoiding redundancy: the sigmoid function can set to zero irrelevant features, but if several features are simultaneously relevant (but correlated), I expect the solution  will not be sparse, but it will contain weights contributions from all. I suspect that this might lead to overfitting in data-scarce scenarios (see below).  

Given the topic of the article I would have expected a comparison with LORA (https://arxiv.org/abs/2106.), where the features for the downstream task are selected by multiplying the original features by a low rank matrix before downstream fine tuning. Since the focus of the paper is transfer learning, which typically happens towards data-scarce tasks, I would have liked to see if the procedure is robust with respect to aggressive decimation of the target task. What happens if one attempts to use ~100 examples for category, as typical in clinical image analysis applications?

The last paragraph of page 4 is not very clear. The variational parameters are the components of the vector s, which, via the sigmoidal function, set the weight of the corresponding psi component in the rhoPsi kernel?

Minor: when they present the setting on page 3, the labels seem to be linear regression labels, while the experiments are on classification datasets. Please clarify.",458,1,1,0.7757000000000001,0.0912545788,0.8472784758,47,43.831,0.174,iclr,0.0272727272727272,4,4,4,4,factual,4,4,85,polite,4,negative,4,low,4,4,3,4,partially factual,4,4,80,polite,5,neutral,4,moderate,2.0,4.0,4.0,3.0,partially factual,4.0,4.0,70.0,polite,4.0,neutral,3.0,low,4,4,4,4,factual,4,4,80,polite,5,neutral,4,low,3,4,4,4,partially factual,4,4,85,polite,5,neutral,4,low
66,Reviewer-U7ws,Efficient Transfer Learning from Arbitrary Pre-Trained Models,"Transfer learning typically involves loading pre-trained weights as an initialization, followed by fine-tuning on a downstream task. As pre-trained models become ever larger, this procedure is becoming prohibitively expensive, as we are forced to re-use the pre-trained architecture for fine-tuning. This procedure also precludes combining multiple pre-trained models that learn complementary information. Moreover, alternatives such as knowledge distillation do not reflect that we wish to transfer aspects of the pre-trained representation that are most relevant to the downstream task. To address these challenges, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the possibly smaller downstream model. AFT (1) enables transfer from multiple pre-trained models, even over multiple modalities, with minimal training overhead and no inference overhead; (2) selectively transfers the information in the pre-trained features most relevant for the downstream task, through a prior that favors low mutual information between the downstream inputs and features given the pre-trained features; (3) performs feature transfer in an efficient kernel formulation that prioritizes the most relevant degrees of freedom. Empirically, AFT delivers a substantial boost in performance across diverse vision, language, and multi-modal datasets, relative to both standard transfer learning and knowledge distillation with the downstream model.","The paper proposes Adaptive Feature Transfer (AFT), a downstream adaptation technique that operates directly on features, thereby decoupling the choice of the pre-trained model architecture from the downstream one. AFT enables combining different pre-trained architectures together during adaptation while distilling only the relevant information for the downstream task to the final model. The algorithm is validated across a diverse set of vision, language and vision-language tasks and compared against knowledge distillation and transfer learning algorithms. 1. The proposed method allows to distill features learned with different architectures on possibly different modalities to any given architecture 
2. The method is validated on both vision, language and vision-language tasks 1. The proposed method promises to distill features from **any** set of models to a given model once the downstream task is know. The paper is positioned as a generic method that could be applied to any set of models (possibly containing architectures different to the downstream one). However, while the presented theory to justify the method is sound and generic, the empirical results do not seem to support the claim. For example, in Figure 1 (right) and Figure 2 (b) adding convolutional features to a ViT based downstream model seem to reduce the performance of the model. Why is it the case? To me it seems to suggest that the proposed method is not strong enough to reject some features that will lead to a worse downstream model. 
    - If this is the case the current algorithm should be coupled with model selection techniques to pick the best features that are more likely to help (see \[1\] and reference therein). Can the authors comment on this more?
2. The previous limitation gets even worse when the set of conditioning models gets larger since the signal to noise ratio drops, making extracting the relevant information for the downstream task even harder. I suggest the authors to consider comparing with explicit sparsity inducing methods as the ones proposed in \[2\] and the references therein.
3. The final algorithm is optimizing theta and rho jointly. However, one would expect \rho being optimized more often than \theta. Typically, this is done with bi-level optimization techniques or simple rewriting \rho in closed form for each given \theta. Did the authors try those more natural alternatives? If \rho is not optimized fast enough the most likely trajectory induced by SGD will be around a stationary point of \rho which leads to a maximally insensitive/uninformative \rho which will be reasonably good on average for many possible \theta, however not optimal for any in particular. 


References:

\[1\] A. Deshpande, et al. “A linearized framework and a new benchmark for model selection for fine-tuning”

\[2\] M. Fumero, et al. “Leveraging sparse and shared feature activations for disentangled representation learning” 1. Why should invariance under orthogonal transformation be of help in the practical optimization optimization objective? Can the authors prove how the optimization landscape will change and get easier to optimize? As of now, this intuitive fact, is left to the ablation studies and only supported by empirical observations.
2. Why not using a different kernel than the linear one? This will make the optimization space much smoother (e.g. by choosing a Gaussian kernel).
3. Visual evaluation on CIFAR100 is quite limited, to increase the impact of the paper on the community I suggest the authors to extend the evaluation to other datasets as the ones used in \[1\]. 

Minor:
- Some typos and grammatical errors are present in the paper, please proofread the manuscript.
- Can you report in the paper the level of sparsity of the rho projection map? This could help the reader understanding what happens when irrelevant pre-trained models are added to the mix.  
- Make the scatter plots with learn probe accuracy vs test accuracy on the same scale. Is the proposed method worse than directly using a linear classifier on the concatenated features?",647,5,5,0.7894,0.0769884071,0.9436648488,65,41.9683,0.4636,iclr,0.0109890109890109,4,4,4,4,factual,4,4,80,polite,4,negative,4,low,5,4,4,5,factual,5,5,88,polite,5,neutral,5,none,2.0,5.0,4.0,3.0,factual,3.0,4.0,80.0,polite,4.0,neutral,5.0,low,4,4,4,4,factual,5,5,85,polite,5,neutral,5,low,4,4,4,5,partially factual,4,4,85,polite,5,neutral,5,low
114,Reviewer-2y3j,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","Inspired by the notion that most methods that work in a task-incremental scenario can achieve almost zero forgetting, the authors introduce AGILE (Attention-Guided Incremental Learning). The main idea is to break down a class incremental problem into two sub-problems: Task-ID prediction (TP) and within-task prediction (WP). Once the first one is solved, the problem can be treated as a Task-Incremental, as the predicted task-id is already available. The authors suggest using task-specific projections to condition the feature vector. This conditioned vector passes through a task-specific module: task prediction and feature importance. During inference, the output of each module is concatenated to obtain the prediction. The authors demonstrate good performance in both task and class incremental scenarios. - The authors work under the assumption that the incremental Class problem can be transformed into a task-incremental problem.
    - However, I can't entirely agree that this is a ""necessary and sufficient"" solution. In fact, there is a probability that working the problem in this way helps the model lose generalization in the representations it generates, and the only reason why this does not happen in the proposed solution is that they use a buffer to store previous tasks.
    - Even so it is a problem that is not widely attacked, but that can be a good option in many cases, especially if it's motivated by the idea of GWT.
- The approach comprises many different components that have a good synergy between them. It is beneficial that the authors add Table 2 to show the importance of each loss. - Using EMA is a critical point in the proposal, and the authors do not mention it too much. EMA can also be used to reduce weight modification, meaning that it can mitigate forgetting with a favorable beta. The authors present it to increase generalization.
    - Experiments showing evidence that it increases generalization could help mitigate the doubts.
    - Did you have an analysis of the beta value? 
- It is challenging to understand where there are linear layers and where there is soft attention in the proposed methods. The image does not help.
    - It could be helpful to decrease the amount of terms, names or losses used in the explanation.
    - For example, from the Figure, one can assume that there is one Task-Attention Module for each task. However, the Task-Attention Module is shared, no?
- Didn’t find Definition 1 and 2. - Is EMA used in every method for Table 1? Or just AGILE?
- How much overhead in terms of time is added when adding a Task-Attention Module?
    - Even if the Task-Attention module is shared, it must still be used independently for each task.
- Are you familiar with the work called Bias Correction (BiC) in Continual Learning? 
    - There are some similarities that you can find interesting.
    - I don’t remember if it works in class or task-incremental, but there have been extensions that work in class-incremental settings.
- Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl).
- Have you tried this approach with a fixed pre-trained model?",530,0,0,0.7574000000000001,0.2457885305,0.910656333,48,51.3527,0.0866,iclr,0.0123456790123457,4,4,4,4,factual,3,3,80,polite,4,neutral,4,none,4,4,4,4,partially factual,4,4,82,polite,5,neutral,5,moderate,3.0,4.0,4.0,4.0,partially factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,low,4,3,4,4,factual,4,4,80,polite,5,neutral,4,low,2,3,3,4,partially factual,3,3,70,polite,4,neutral,4,low
114,Reviewer-5d1d,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","The paper proposes a replay-based CL method utilizing a lightweight task attention module. The module receives features from the feature extractor and performs task-id prediction using the projection vectors for each task. This approach aligns with the findings of a prior theoretical study. The authors conduct comprehensive experiments to demonstrate the benefits of their approach compared to existing baselines and show the effectiveness of the proposed techniques. 1. The proposed approach is grounded in a theoretical study.
2. The proposed method outperforms the baselines. 1. I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.
2. It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.
3. I couldn't fully understand why this method is better than the existing task-id prediction methods. \[1\] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.

Overall, I think this approach is promising, but needs some improvements.

\[1\] Conditional channel gated networks for task-aware continual learning 1. How does the model make the final class prediction? Does it first predict the task-id using the attention module and make a within-task prediction?
2. What's the purpose of using the task projection vectors and why is it used to compute both z_s and z_tp?",262,2,6,0.7371000000000001,0.1460784314,0.8085629344,48,54.6912,0.1507,iclr,0.0,3,4,3,2,partially factual,4,3,55,polite,3,neutral,3,moderate,4,4,3,4,3,4,4,75,polite,5,neutral,4,moderate,2.0,4.0,3.0,3.0,partially factual,3.0,4.0,60.0,polite,4.0,neutral,4.0,low,3,3,3,3,factual,4,4,65,polite,4,neutral,4,moderate,4,4,3,4,partially factual,3,3,75,polite,4,neutral,4,low
114,Reviewer-pnnH,Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning,"Continual learning (CL) remains a significant challenge for deep neural networks, as it is prone to forgetting previously acquired knowledge. Several approaches have been proposed in the literature, such as experience rehearsal, regularization, and parameter isolation, to address this problem. Although almost zero forgetting can be achieved in task-incremental learning, class-incremental learning remains highly challenging due to the problem of inter-task class separation. Limited access to previous task data makes it difficult to discriminate between classes of current and previous tasks. To address this issue, we propose `Attention-Guided Incremental Learning' (AGILE), a novel rehearsal-based CL approach that incorporates compact task-attention to effectively reduce interference between tasks. AGILE utilizes lightweight, learnable task projection vectors to transform the latent representations of a shared task-attention module toward task distribution. Through extensive empirical evaluation we show that AGILE significantly improves generalization performance by mitigating task interference and outperforms rehearsal-based approaches in several CL scenarios. Furthermore AGILE can scale well to a large number of tasks with minimal overhead while remaining well-calibrated with reduced task-recency bias.","This paper introduces a novel rehearsal based continual learning approach which use a shared task-attention module to mitigate the task interference. The shared task-attention module compresses the task specific information to some trainable parameters. 1. The framework achieves fairly good results compared with baselines.
2. The paper is written clearly and easy to follow. 1. Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works \[*\] \[**\]. L2P has shown its effectiveness in continual learning areas in recent years. 

2. Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.

\[*\] Learning to prompt for continual learning, CVPR 2022.

\[**\] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022. Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?",159,0,5,0.776,0.2238095238,0.8488740325,48,32.7117,0.1695,iclr,0.0,3,4,3,3,factual,4,3,60,polite,4,neutral,3,moderate,5,5,4,5,5,5,5,85,polite,5,neutral,5,low,2.0,4.0,4.0,3.0,factual,4.0,4.0,70.0,polite,5.0,neutral,3.0,none,3,4,3,3,factual,4,4,70,polite,4,neutral,3,low,4,4,3,4,partially factual,3,3,75,polite,5,neutral,4,low
75,Reviewer-s437,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a new measure of natural language generation (NLG) quality based on similarity between the spectrum of cross-entropy in natural vs. generated text. Fourier Analysis of the Cross-Entropy of language (FACE) is inspired by NLP and psycholinguistic studies suggesting that surprisal is not uniformly distributed in natural text (e.g., content words tend to be more surprising than function words), occurring periodically. For a given generated text, FACE computes a discrete Fourier transform of the token-level cross-entropy sequence (under a separate FACE evaluation LM). Similarity between the vector of frequency magnitudes and that from a randomly selected, natural text corpus are then computed. The paper considers several definitions of FACE metrics, including spectral overlap, cosine similarity, and Pearson/Spearman’s rank correlation coefficients.

LMs from 125 million to over 7 billion parameters are evaluated on NLG of Wikipedia articles, news articles, and stories (with a short prompt of 35 subword tokens provided). Ultimately, FACE is found to be correlated with human judgments of how “human-like”, “sensible”, and “interesting” the generations are. The relationship is not as strong as an existing intrinsic measure, MAUVE. The relative ranking of decoding methods according to FACE agrees with prior works (e.g., greedy decoding < nucleus), as do model size (smaller models produce lower quality generations than larger models). The metric is well-motivated, evaluating whether generated text matches the surprisal statistics of natural text. The algorithm is simple and described sufficiently clearly. FACE is an automatic measure of NLG quality that is, on the face of it, complementary to existing measures. This paper would be of interest to many who work on (large) language models. While FACE is motivated by the desire to match surprisal statistics of natural text, it was not clear how different FACE is from existing metrics. Computing correlation between FACE and existing metrics would help alleviate this, as would providing anecdotes of cases with high/low FACE score vs. high/low MAUVE score, for instance. Have you also considered the spectrum of hidden LM embeddings rather than cross-entropy, and considered how such a metric might differ from FACE? Yes",345,0,1,0.8233,0.0747350351,0.9425024986,216,31.0628,0.1585,neurips,0.0120481927710843,2,3,3,3,partially factual,3,2,55,polite,3,neutral,3,low,4,5,4,4,factual,5,5,85,polite,5,neutral,5,none,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,5.0,neutral,3.0,none,3,5,4,3,factual,4,4,85,polite,5,positive,5,low,3,4,4,4,factual,4,4,85,polite,5,neutral,5,low
75,Reviewer-v6cq,FACE: Evaluating Natural Language Generation with Fourier Analysis of Cross-Entropy,"Measuring the distance between machine-produced and human language is a critical open problem. Inspired by empirical findings from psycholinguistics on the periodicity of entropy in language, we propose FACE, a set of metrics based on Fourier Analysis of the estimated Cross-Entropy of language, for measuring the similarity between model-generated and human-written languages. Based on an open-ended generation task and the experimental data from previous studies, we find that FACE can effectively identify the human-model gap, scales with model size, reflects the outcomes of different sampling methods for decoding, correlates well with other evaluation metrics and with human judgment scores.","This paper proposes a set of metrics to measure the distance between model-generated and human-written languages. Specifically, this paper uses FFT to analyze the cross-entropy sequences of the language data. 1. This new metric is efficient. Given the fact that our models are getting exponentially bigger, it is essential that we do not waste energy during evaluation.
2. This new metric correlates well with human judgment, and is statistically sound.

I personally really like the authors' attempt to interpret the metric. Understanding the why is sometimes much more important than understanding the how. 1. The related work on psycholinguistic motivation is limited. Entropy is also a popular metric in computational linguistics, which is probably worth citing.
2. The model size categorization seems to be very coarse.   1. Could the authors be more specific about their motivations for using spectral similarity as a metric? This paper is a good step towards addressing some of the problems brought by generative AI.",159,0,5,0.8219000000000001,0.2167388167,0.9213043451,216,39.0844,0.1108,neurips,0.0,2,3,2,3,partially factual,3,3,50,polite,3,positive,3,moderate,4,4,3,4,partially factual,4,3,75,polite,5,positive,4,moderate,2.0,4.0,4.0,3.0,factual,4.0,4.0,80.0,polite,4.0,neutral,3.0,none,3,4,3,3,factual,4,3,65,polite,4,positive,3,moderate,2,4,3,3,partially factual,4,3,75,polite,4,positive,4,low
